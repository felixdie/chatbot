Wang, Q., Huang, Y., Jasin, S., & Singh, P. V. (2023). Algorithmic transparency with strategic
users. Management Science, 69(4), 2297-2317.


Abstract 


Should 
rms 
that 
apply 
machine 
learning 
algorithms 
in 
their 
decision-making 
make 
their 
algorithm
s 
transparent 
to 
the 
users 
they 
aect? 
Despite 
the 
growing 
calls 
for 
algorithmic 
transparency
, 
most 
rms 
have 
kept 
their 
algorithms 
opaque, 
citing 
potential 
gaming 
by 
users 
that 
may 
negatively 
aect 
the 
algorithm's 
predictive 
power. 
In 
this 
paper, 
we 
develop 
an 
analytical 
model 
to 
compare 
rm 
and 
user 
surplus 
with 
and 
without 
algorithmic 
transparency 
in 
the 
presenc
e 
of 
strategic 
users, 
and 
present 
novel 
insights. 
We 
identify 
a 
broad 
set 
of 
conditions 
under 
which 
making 
the 
algorithm 
transparent 
actually 
benets 
the 
rm. 
We 
show 
that, 
in 
some 
cases, 
even 
the 
predictive 
power 
of 
the 
algorithm 
can 
increase 
if 
the 
rm 
makes 
the 
algorithm 
transparent. 
By 
contrast, 
users 
may 
not 
always 
be 
better 
off 
under 
algorithmic 
transparency. 
These 
results 
hold 
even 
when 
the 
predictive 
power 
of 
the 
opaque 
algorithm 
comes 
largely 
from 
correlational 
features 
and 
the 
cost 
for 
users 
to 
improve 
them 
is 
minimal. 
We 
show 
that 
these 
insights 
are 
robust 
under 
several 
extensions 
of 
the 
main 
model. 
Overall, 
our 
results 
show 
that 
rms 
should 
not 
always 
view 
manipulation 
by 
users 
as 
bad. 
Rather, 
they 
should 
use 
algorithmic 
transparency 
as 
a 
lever 
to 
motivate 
users 
to 
invest 
in 
more 
desirable 
features. 


Keywords: 
Algorithmic 
Transparency, 
Game 
Theory, 
Machine 
Learning, 
Strategic 
Classication
, 
Signaling 
Game 


‚àó 


Qiaochu 
Wang, 
Yan 
Huang 
and 
Param 
Vir 
Singh 
are 
at 
Carnegie 
Mellon 
University. 
Stefanus 
Jasin 
is 
at 
University 
of 
Michigan. 


 


1 
Introduction 


Machine 
learning 
algorithms 
are 
being 
increasingly 
applied 
to 
decision-making 
processes 
with 
far-reaching 
impacts 
extending 
to 
employment, 
access 
to 
credit, 
and 
education 
(Schellmann 
and 
Bellini, 
2018, 
Fu 
et 
al., 
2019). 
However, 
rms 
typically 
keep 
these 
algorithms 
as 
closely 
guarded 
secrets, 
on 
par 
with 
KFC 
or 
Coca-Cola's 
recipes. 
As 
a 
result, 
these 
algorithms 
stay 
opaque 
to 
the 
people 
whom 
they 
aect 
and 
lack 
clear 
explanations 
for 
the 
decisions 
they 
make. 


Our 
study 
is 
motivated 
by 
the 
growing 
calls 
from 
dierent 
parts 
of 
society 
to 
require 
rms 
to 
make 
their 
algorithms 
transparent. 
According 
to 
American 
privacy 
law 
expert 
Marc 
Rotenberg: 
\At 
the 
intersection 
of 
law 
and 
technology 
‚Äì 
knowledge 
of 
the 
algorithm 
is 
a 
fundamental 
right, 
a 
human 
right."1 
The 
European 
Union's 
General 
Data 
Protection 
Regulation 
(GDPR) 
dictates 
that, 
whenever 
personal 
data 
is 
subject 
to 
automated 
decision 
making, 
people 
have 
\the 
right 
to 
obtain 
human 
intervention 
on 
the 
part 
of 
the 
controller‚Äù 
or 
the 
right 
to 
explanation.2 


While 
making 
algorithms 
transparent 
is 
desirable, 
it 
can 
open 
a 
door 
to 
gaming, 
which 
potentiall
y 
adversely 
aects 
the 
algorithms‚Äô 
predictive 
power. 
If 
strategic 
agents 
were 
to 
know 
the 
information 
about 
a 
classier 
(i.e., 
how 
observed 
attributes 
aect 
the 
classication 
outcome), 
they 
may 
manipulate 
their 
attributes 
to 
receive 
a 
more 
desirable 
classication, 
hurting 
the 
predictive 
power 
of 
the 
algorithm. 
In 
nancial 
and 
economic 
policy 
making, 
this 
problem 
is 
widely 
known 
as 
Goodhart's 
Law, 
which 
proclaims 
that 
\when 
a 
measure 
becomes 
a 
target, 
it 
ceases 
to 
be 
a 
good 
measure‚Äù 
(Goodhart, 
1984). 
A 
similar 
notion 
is 
captured 
in 
the 
Lucas 
critique 
(Lucas 
et 
al., 
1976). 
In 
fact, 
Fair 
Isaac 
Corporation 
keeps 
its 
exact 
credit 
scoring 
formula 
secret 
to 
make 
it 
more 
dicult 
for 
consumers 
to 
game 
the 
algorithm 
(Citron 
and 
Pasquale, 
2014). 
Similarly, 
Google 
continues 
to 
modify 
its 
secret 
search 
ranking 
algorithm 
to 
keep 
businesses 
and 
people 
from 
gaming 
the 
system 
(Segal, 
2011). 


Motivated 
by 
the 
calls 
for 
algorithmic 
transparency 
and 
the 
threat 
of 
manipulation 
by 
agents 
to 
transparent 
algorithms, 
we 
investigate 
how 
algorithmic 
transparency 
may 
aect 
rms 
and 
agents. 
First, 
from 
the 
perspective 
of 
the 
rm 
(the 
decision-maker), 
we 
ask, 
is 
there 
any 
advantage 
in 
making 
its 
algorithm 
transparent 
even 
when 
there 
is 
the 
potential 
of 
manipulation 
by 
the 
agents? 
Second, 
we 
ask, 
would 
the 
agents 
be 
better 
off 
or 
worse 
off 
if 
rms 
make 
their 
algorithms 
transparent 
(i.e., 
if 
the 
agents 
receive 
more 
information 
about 
the 
factors 
aecting 
algorithmic 
decisions)? 
Third, 
we 
ask, 
how 
are 
the 
results 
aected 
by 
the 
predictive 
power 
of 
those 
features 
that 
are 
more 
susceptible 
to 
gaming 
by 
the 
agents? 
Finally, 
we 
ask, 
how 
does 
the 
market 
composition 
in 
terms 
of 
desirable 
and 
undesirable 
agents 
aect 
these 
results? 
In 
this 
paper, 
we 
develop 
and 
analyze 
a 


1See 
\Algorithmic 
Transparency: 
End 
Secret 
Proling,‚Äù 
https://epic.org/algorithmic-transparency/ 


2See 
\Algorithmic 
transparency 
and 
the 
right 
to 
explanation: 
Transparency 
is 
only 
the 
rst 
step,‚Äù 
https://www.apc.org/en/blog/algorithmic-transparency-and-right-explanation-transparency-only-rst-step 


1 


 


game-theoretic 
model 
to 
answer 
these 
questions. 


We 
explicitly 
model 
the 
agents 
as 
strategic 
and 
the 
algorithm 
designer 
(the 
rm) 
is 
aware 
of 
the 
potential 
for 
manipulation. 
Hence, 
the 
rm 
can 
react 
to 
gaming 
by 
the 
agents. 
For 
example, 
consider 
a 
setting 
where 
the 
rm 
collects 
data, 
trains 
an 
algorithm 
that 
maps 
a 
set 
of 
observed 
features 
to 
a 
classication 
outcome, 
and 
publishes 
a 
decision 
rule. 
If 
agents 
desire 
to 
be 
positively 
classied, 
they 
would 
manipulate 
their 
values 
of 
the 
features 
to 
achieve 
it. 
However, 
the 
rm 
would 
be 
aware 
that 
the 
behavior 
of 
the 
agents 
has 
changed. 
It 
will 
then 
update 
the 
model 
and 
the 
decision 
rule. 
The 
agents‚Äô 
behavior 
would 
change 
once 
again. 
Over 
time, 
the 
rm 
will 
iterate 
to 
a 
xed 
point 
‚Äì 
this 
decision 
rule 
would 
be 
the 
best 
response 
to 
the 
agents‚Äô 
strategic 
behavior. 


More 
specically, 
in 
this 
paper, 
we 
consider 
a 
job 
hiring 
scenario 
where 
a 
risk-neutral 
rm 
oers 
a 
xed 
wage 
and 
wants 
to 
recruit 
only 
highly 
productive 
agents. 
There 
are 
two 
types 
of 
agents, 
High 
talent 
(H 
) 
and 
Low 
talent 
(L). 
The 
H 
type 
agents 
are 
more 
productive 
compared 
to 
the 
L 
type 
agents. 
While 
the 
type 
of 
each 
agent 
is 
xed, 
it 
is 
not 
observed 
by 
the 
rm 
ex-ante. 
The 
rm, 
however, 
does 
have 
access 
to 
a 
number 
of 
observed 
features 
(i.e., 
the 
observables), 
which 
it 
uses 
to 
infer 
the 
agents‚Äô 
types 
(i.e., 
using 
historical 
data 
and 
an 
algorithm) 
and 
to 
determine 
a 
decision 
rule 
for 
hiring 
the 
agents. 


We 
model 
two 
types 
of 
observables, 
causal 
and 
correlational. 
Typically, 
in 
machine 
learning 
models, 
the 
designer 
focuses 
primarily 
on 
model 
accuracy, 
not 
causality. 
However, 
any 
features 
that 
are 
captured 
by 
the 
machine 
learning 
model 
can 
still 
be 
classied 
as 
either 
causal 
or 
correlational. 
For 
simplicity, 
we 
consider 
only 
two 
features; 
one 
is 
causal 
and 
the 
other 
is 
correlational. 
There 
are 
several 
characteristics 
of 
these 
features 
that 
are 
important 
for 
our 
model. 
By 
denition, 
the 
causal 
feature 
impacts 
the 
productivity 
of 
the 
agent, 
whereas 
the 
correlational 
feature 
does 
not. 
The 
dierence 
between 
`causal‚Äô 
and 
`correlational‚Äô 
features 
is 
similar 
to 
the 
dierence 
between 
`improvement‚Äô 
and 
`gaming', 
two 
terms 
commonly 
used 
in 
the 
strategic 
classication 
literature 
(Kleinberg 
and 
Raghavan, 
2019, 
Alon 
et 
al., 
2020, 
Haghtalab 
et 
al., 
2020). 
Putting 
an 
eort 
to 
change 
the 
value 
of 
a 
causal 
feature 
in 
the 
decision 
maker's 
favorable 
direction 
is 
called 
an 
`improvement‚Äô 
because 
this 
is 
benecial 
to 
the 
decision 
maker 
or 
the 
society. 
By 
contrast, 
putting 
eort 
on 
to 
change 
the 
value 
of 
a 
correlational 
feature 
does 
not 
benet 
the 
rm 
and 
is 
a 
waste 
from 
the 
perspective 
of 
social 
welfare. 


The 
agents 
can 
game 
(alter) 
both 
features 
by 
incurring 
a 
cost. 
As 
is 
typically 
assumed 
in 
most 
signaling 
game 
models 
(Spence, 
1973), 
the 
H 
type 
agents 
have 
a 
cost 
advantage 
on 
the 
causal 
feature 
over 
the 
L 
type 
agents. 
As 
for 
the 
cost 
of 
improving 
the 
correlational 
feature, 
in 
the 
main 
model, 
we 
assume 
that 
this 
cost 
is 
independent 
of 
the 
agents‚Äô 
type 
and 
its 
value 
is 
marginally 
above 
zero. 


3For 
completeness, 
we 
also 
discuss 
the 
setting 
where 
the 
cost 
is 
signicantly 
larger 
than 
zero 
in 
Appendix 
B.1. 


2 





The 
assumptions 
behind 
the 
cost 
structures 
of 
the 
causal 
and 
correlational 
features 
warrant 
some 
discussion. 
When 
an 
H 
type 
agent 
has 
a 
signicant 
cost 
advantage 
over 
an 
L 
type 
agent 
on 
the 
causal 
feature, 
it 
will 
trivially 
lead 
to 
a 
separating 
equilibrium 
where 
only 
the 
H 
type 
agents 
get 
high 
values 
on 
the 
causal 
feature. 
In 
the 
case 
where 
the 
cost 
advantage 
of 
the 
H 
type 
agents 
on 
the 
causal 
feature 
is 
not 
too 
large, 
the 
rm 
would 
want 
to 
include 
the 
correlational 
feature 
in 
the 
machine 
learning 
model 
and 
in 
the 
decision 
rule. 
In 
this 
case, 
the 
correlational 
feature 
would 
provide 
an 
additional 
value 
in 
separating 
the 
two 
types. 
It 
is 
easy 
to 
see 
that 
the 
correlational 
feature 
is 
the 
one 
that 
is 
more 
susceptible 
to 
gaming. 
If 
the 
agents 
game 
it, 
this 
feature 
can 
lose 
its 
predictive 
power. 
This 
is 
precisely 
the 
reason 
that 
is 
typically 
purported 
for 
opposing 
algorithmic 
transparency. 
If 
the 
cost 
to 
alter 
the 
correlational 
feature 
is 
very 
high, 
or 
if 
the 
H 
type 
agents 
have 
an 
advantage 
on 
it, 
either 
gaming 
will 
not 
happen 
or 
gaming 
will 
be 
more 
favorable 
to 
the 
H 
type 
agents. 
In 
such 
a 
case, 
making 
the 
algorithm 
transparent 
would 
be 
either 
better 
or 
at 
least 
as 
good 
as 
keeping 
it 
opaque 
for 
the 
rm. 
Our 
interest 
is 
in 
investigating 
whether 
algorithmic 
transparency 
can 
be 
better 
for 
the 
rm 
as 
opposed 
to 
keeping 
the 
algorithm 
opaque 
even 
when 
the 
H 
type 
agents 
have 
no 
cost 
advantage 
on 
the 
correlational 
feature 
and 
the 
cost 
to 
manipulate 
the 
correlational 
feature 
is 
minimal, 
which 
makes 
the 
algorithm 
particularly 
highly 
susceptible 
to 
gaming. 


We 
solve 
the 
game 
between 
the 
rm 
and 
the 
agents 
in 
two 
scenarios, 
an 
opaque 
algorithm 
scenari
o 
and 
a 
transparent 
algorithm 
scenario. 
In 
both 
scenarios, 
we 
use 
perfect 
Bayesian 
equilibrium 
(PBE) 
as 
the 
solution 
concept. 
In 
the 
opaque 
scenario, 
the 
agents 
move 
rst. 
In 
this 
scenario, 
we 
assume 
that 
the 
agents 
are 
aware 
of 
the 
causal 
feature 
but 
have 
little 
knowledge 
of 
the 
correlational 
feature. 
Consequently, 
the 
agents 
can 
only 
improve 
the 
causal 
feature. 
The 
agents 
know 
that 
the 
rm 
uses 
a 
correlational 
feature, 
but 
they 
do 
not 
know 
what 
that 
feature 
is. 
However, 
it 
is 
common 
knowledge 
that 
the 
feature 
has 
predictive 
power 
that 
can 
help 
the 
rm 
separate 
the 
H 
type 
agents 
from 
the 
L 
type 
agents. 
This 
assumption 
is 
reasonable: 
If 
a 
feature 
has 
no 
predictive 
power 
in 
separating 
the 
two 
types 
of 
agents, 
the 
machine 
learning 
algorithm 
would 
simply 
discard 
it. 


In 
the 
transparent 
scenario, 
the 
rm 
moves 
rst 
and 
publishes 
its 
algorithm. 
The 
agents 
observe 
this 
algorithm 
and 
know 
what 
features 
are 
considered 
by 
the 
algorithm 
including 
their 
respective 
weights. 
In 
this 
scenario, 
the 
agents 
can 
game 
both 
the 
causal 
and 
the 
correlational 
features. 
They 
also 
know 
how 
the 
correlational 
feature 
correlates 
with 
their 
types 
before 
any 
gaming 
occurring. 
Finally, 
the 
rm 
decides 
who 
to 
hire 
based 
on 
the 
agents‚Äô 
ending 
feature 
values. 


It 
is 
worth 
noting 
that 
although 
we 
assume 
the 
agents 
and 
the 
rm 
move 
sequentially 
in 
both 
the 
opaque 
and 
transparent 
scenarios, 
we 
do 
not 
model 
these 
processes 
as 
Stackelberg 
leadership 
models 
in 
our 
main 
analysis. 
The 
reasons 
are 
as 
follows. 
In 
a 
Stackelberg 
model, 
the 
rst 
mover 
typically 
anticipates 
that 
any 
deviation 
from 
the 
current 
strategy 
will 
trigger 
the 
second 
mover 
to 
react 
accordingly. 
However, 
when 
the 
rst 
mover 
consists 
of 
many 
uncoordinated 
agents, 
this 


3 





condition 
will 
be 
violated. 
Specically, 
in 
the 
opaque 
scenario, 
given 
that 
the 
agents 
are 
non-cooperative, 
the 
unilateral 
deviation 
of 
a 
single 
agent 
will 
not 
change 
the 
follower's 
(i.e., 
the 
rm's) 
strategy. 
As 
for 
the 
transparent 
scenario, 
there 
is 
a 
single 
rst 
mover 
(i.e., 
the 
rm), 
and 
it 
is 
possible 
for 
the 
rm 
to 
commit 
to 
the 
published 
algorithm. 
Consequently, 
a 
Stackelberg 
model 
is 
a 
valid 
model 
in 
this 
scenario. 
However, 
a 
Stackelberg 
model 
provides 
an 
advantage 
to 
the 
rst 
mover. 
As 
a 
result, 
the 
rm 
may 
prefer 
the 
transparent 
algorithm 
over 
the 
opaque 
algorithm 
simply 
because 
of 
the 
rst 
mover 
advantage 
that 
the 
transparent 
algorithm 
provides. 
To 
clearly 
explain 
the 
mechanisms 
of 
the 
rm's 
preference 
between 
algorithmic 
transparency 
and 
opacity 
and 
show 
that 
the 
rm 
could 
still 
prefer 
the 
transparent 
algorithm 
over 
the 
opaque 
algorithm 
even 
in 
absence 
of 
the 
rst 
mover 
advantage, 
we 
will 
rst 
focus 
our 
analysis 
in 
this 
paper 
on 
the 
case 
where 
the 
rm 
either 
does 
not 
have 
the 
commitment 
power 
or 
the 
rm 
only 
reveals 
which 
features 
it 
uses 
but 
not 
the 
full 
details 
of 
its 
hiring 
strategy 
(akin 
to 
`partial 
transparency'). 
Later, 
in 
Section 
5, 
we 
will 
discuss 
the 
case 
where 
the 
rm 
has 
commitment 
power 
and 
reveals 
the 
full 
details 
to 
the 
agents 
(akin 
to 
`full 
transparency'). 
Overall, 
we 
will 
show 
that 
the 
key 
insights 
from 
the 
non-Stackelberg 
model 
will 
only 
be 
strengthened 
in 
the 
Stackelberg 
model. 


In 
addition 
to 
analyzing 
the 
models 
discussed 
above, 
as 
brie
y 
described 
in 
Section 
6 
and 
further 
elaborated 
in 
Appendix 
B, 
we 
also 
analyze 
three 
extensions 
by 
relaxing 
some 
of 
the 
assumptions 
made 
in 
the 
main 
model. 
These 
extensions 
include: 
(1) 
the 
case 
where 
the 
cost 
of 
improving 
the 
correlational 
feature 
is 
signicantly 
larger 
than 
zero, 
(2) 
the 
case 
where 
the 
wage 
is 
endogenously 
chosen 
by 
the 
rm, 
and 
(3) 
the 
case 
where 
the 
agents 
have 
incorrect 
beliefs 
about 
the 
predictive 
power 
of 
the 
correlational 
feature. 
Our 
analysis 
for 
these 
three 
extensions 
shows 
that 
the 
results 
from 
the 
main 
analysis 
are 
robust 
to 
these 
alternative 
assumptions 
and 
modeling 
choices. 
Specifically
, 
the 
main 
results 
of 
our 
paper 
will 
not 
change 
qualitatively 
in 
extension 
(3), 
and 
they 
will 
only 
be 
strengthened 
in 
extensions 
(1) 
and 
(2). 


Key 
results 
and 
insights 


Our 
rst 
result 
in 
this 
paper 
challenges 
the 
conventional 
wisdom 
that 
making 
algorithms 
transparent 
will 
always 
hurt 
the 
rm 
economically. 
We 
identify 
a 
broad 
set 
of 
conditions 
under 
which 
making 
the 
algorithm 
transparent 
is 
actually 
benecial 
for 
the 
rm. 
The 
key 
intuition 
behind 
this 
result 
is 
driven 
by 
how 
the 
H 
type 
and 
L 
type 
agents 
respond 
to 
algorithmic 
transparency. 
Since 
investment 
on 
the 
causal 
feature 
is 
costly 
and 
since 
the 
H 
type 
agents 
have 
an 
advantage 
on 
the 
correlational 
feature 
(we 
assume 
that 
an 
H 
type 
agent 
has 
a 
higher 
probability 
of 
being 
`high‚Äô 
on 
the 
correlational 
feature, 
otherwise 
the 
rm 
will 
have 
no 
incentive 
to 
use 
this 
feature 
in 
the 
rst 
place), 
the 
H 
type 
agents 
would 
invest 
in 
improving 
the 
causal 
feature 
only 
to 
the 
extent 
that 
it, 
along 
with 
the 
correlational 
feature, 
helps 
them 
separate 
themselves 
from 
the 
L 
type 
agents. 
When 
the 
algorithm 
is 
made 
transparent, 
the 
L 
type 
agents 
game 
the 
correlational 
feature. 
As 
a 
result, 
the 
L 
type 
agents 


4 


 


become 
similar 
to 
the 
H 
type 
agents 
on 
that 
feature, 
and 
the 
predictive 
power 
of 
the 
correlational 
feature 
decreases. 
Hence, 
the 
H 
type 
agents 
have 
to 
invest 
more 
in 
the 
causal 
feature 
to 
separate 
themselves 
from 
the 
L 
type 
agents. 
When 
the 
H 
type 
agents 
have 
a 
signicant 
cost 
advantage 
over 
the 
L 
type 
agents 
on 
the 
causal 
feature, 
this 
leads 
to 
full 
separation, 
which 
benets 
the 
rm. 
When 
the 
H 
type 
agents 
only 
have 
a 
marginal 
cost 
advantage 
over 
the 
L 
type 
agents 
on 
the 
causal 
feature, 
the 
L 
type 
agents 
will 
also 
invest 
signicantly 
in 
the 
causal 
feature. 
In 
this 
case, 
both 
the 
H 
type 
and 
L 
type 
agents 
become 
more 
productive 
because 
of 
their 
higher 
investment 
in 
the 
causal 
feature. 
Although 
the 
rm 
cannot 
separate 
the 
two 
types 
of 
agents 
in 
this 
case, 
when 
the 
impact 
of 
the 
causal 
feature 
on 
productivity 
is 
above 
a 
certain 
threshold, 
the 
average 
productivity 
of 
the 
hired 
agents 
is 
signicantly 
higher 
than 
in 
the 
opaque 
scenario. 
In 
other 
words, 
making 
the 
algorithm 
transparent 
allows 
the 
rm 
to 
motivate 
the 
agents 
to 
invest 
in 
improving 
features 
that 
are 
valuable 
to 
the 
rm. 


Our 
second 
result 
in 
this 
paper 
is 
that 
the 
agents 
are 
not 
always 
better 
off 
under 
the 
transparent 
scenario. 
This 
might 
appear 
counter-intuitive 
at 
rst: 
Since 
the 
agents 
would 
have 
access 
to 
more 
information 
under 
the 
transparent 
scenario, 
one 
would 
think 
that 
they 
should 
be 
better 
off 
under 
the 
transparent 
scenario. 
However, 
we 
show 
that 
there 
are 
conditions 
under 
which 
the 
agents 
are 
worse 
off 
in 
the 
transparent 
scenario. 
Interestingly, 
in 
most 
cases 
where 
the 
rm 
prefers 
the 
transparent 
scenario, 
the 
agents 
would 
prefer 
the 
opaque 
scenario, 
and 
vice 
versa. 
But 
we 
also 
identify 
a 
set 
of 
conditions 
where 
both 
the 
rm 
and 
the 
agents 
prefer 
the 
transparent 
scenario. 
The 
intuition 
for 
our 
second 
result 
is 
similar 
to 
that 
for 
the 
rst 
result. 
The 
rm 
prefers 
the 
transparent 
scenario 
in 
situations 
where 
transparency 
motivates 
the 
agents 
to 
invest 
highly 
in 
the 
causal 
feature. 
When 
the 
cost 
of 
investment 
is 
high 
and 
the 
H 
type 
agents 
have 
a 
signicant 
advantage, 
only 
the 
H 
type 
agents 
will 
invest 
in 
the 
causal 
feature. 
In 
this 
situation, 
although 
only 
the 
H 
type 
agents 
are 
hired, 
they 
are 
worse 
off 
due 
to 
the 
high 
cost 
that 
they 
incur. 
When 
the 
investment 
cost 
is 
moderate 
and 
the 
H 
type 
agents 
have 
a 
marginal 
cost 
advantage 
over 
the 
L 
type 
agents, 
both 
types 
of 
agents 
invest 
in 
the 
causal 
feature, 
and 
both 
are 
hired. 
The 
agents 
are 
better 
off 
because 
they 
have 
to 
incur 
only 
a 
moderate 
cost 
for 
being 
hired. 
Simultaneously, 
the 
rm 
is 
also 
better 
off 
since 
the 
average 
productivity 
of 
the 
hired 
agents 
is 
higher 
when 
the 
impact 
of 
the 
causal 
feature 
on 
productivity 
is 
above 
a 
certain 
threshold. 


Our 
third 
result 
shows 
that 
it 
is 
possible 
for 
the 
rm 
to 
prefer 
algorithmic 
transparency 
when 
the 
correlational 
feature 
has 
high 
predictive 
power 
and 
prefers 
opaque 
algorithm 
when 
the 
correlational 
feature 
has 
low 
predictive 
power. 
This 
result 
also 
appears 
to 
be 
counter-intuitive 
since 
one 
would 
naturally 
expect 
that, 
as 
the 
predictive 
power 
of 
the 
correlational 
feature 
increases, 
the 
rm 
would 
be 
better 
off 
keeping 
the 
algorithm 
opaque. 
We 
nd 
that 
this 
is 
not 
always 
the 
case. 
The 
key 
intuition 
behind 
this 
result 
is 
as 
follows: 
When 
the 
correlational 
feature 
is 
good 
at 
separating 
the 


5 


 


H 
type 
agents 
from 
the 
L 
type 
agents, 
the 
H 
type 
agents 
have 
little 
incentive 
to 
invest 
in 
the 
causal 
feature 
in 
the 
opaque 
scenario. 
However, 
under 
transparency, 
the 
H 
type 
agents 
lose 
this 
advantage 
and 
have 
to 
invest 
in 
the 
causal 
feature 
to 
separate 
themselves 
from 
the 
L 
type 
agents. 
As 
a 
result, 
the 
rm 
can 
hire 
more 
productive 
agents 
under 
transparency. 


Our 
fourth 
result 
is 
that, 
when 
the 
fraction 
of 
the 
H 
type 
agents 
on 
the 
market 
is 
higher, 
the 
rm 
may 
have 
a 
stronger 
preference 
for 
the 
transparent 
algorithm 
under 
certain 
conditions. 
More 
specically, 
the 
fraction 
of 
H 
type 
agents 
aects 
the 
rm's 
surplus 
signicantly 
but 
does 
not 
have 
a 
large 
enough 
impact 
to 
alter 
the 
rm's 
decision 
for/against 
transparency 
when 
the 
cost 
for 
improving 
the 
causal 
feature 
is 
either 
too 
high 
or 
too 
low, 
or 
the 
H 
type 
agents 
have 
a 
large 
cost 
advantage 
over 
the 
L 
type 
agents. 
However, 
when 
the 
cost 
for 
improving 
the 
causal 
feature 
and 
the 
cost 
advantage 
of 
the 
H 
type 
agents 
are 
both 
moderate, 
the 
rm 
has 
a 
stronger 
preference 
for 
algorithmic 
transparency 
as 
the 
fraction 
of 
the 
H 
type 
agents 
on 
the 
market 
increases. 
In 
this 
cost 
range, 
both 
the 
H 
type 
and 
L 
type 
agents 
would 
improve 
the 
causal 
feature. 
While 
the 
rm 
is 
unable 
to 
separate 
the 
two 
types 
and 
hires 
both 
types, 
the 
number 
of 
the 
L 
type 
agents 
that 
are 
hired 
becomes 
smaller 
as 
the 
fraction 
of 
the 
H 
type 
agents 
on 
the 
market 
increases. 


This 
paper 
makes 
several 
contributions. 
It 
is 
one 
of 
the 
rst 
to 
provide 
an 
analytical 
model 
that 
systematically 
compares 
a 
rm's 
decision 
for 
algorithmic 
transparency 
versus 
opacity 
in 
the 
presence 
of 
strategic 
agents. 
We 
show 
that, 
counter 
to 
conventional 
wisdom, 
the 
rm 
can 
be 
better 
off 
under 
algorithmic 
transparency. 
Moreover, 
in 
most 
cases 
where 
the 
rm 
prefers 
algorithmic 
transparency, 
the 
agents 
will 
be 
worse 
o. 
Agents 
underinvest 
in 
the 
causal 
feature 
when 
the 
algorithm 
is 
opaque. 
Consequently, 
the 
rm 
depends 
heavily 
upon 
the 
correlational 
feature 
to 
separate 
dierent 
types 
of 
agents 
from 
one 
another. 
Our 
analysis 
and 
results 
show 
that 
the 
rm 
should 
not 
always 
worry 
about 
the 
potential 
loss 
of 
the 
predictive 
power 
of 
the 
correlational 
feature 
in 
its 
machine 
learning 
model 
under 
transparency. 
Rather, 
it 
can 
use 
algorithmic 
transparency 
as 
a 
lever 
to 
motivate 
the 
agents 
to 
invest 
more 
in 
the 
causal 
feature. 
The 
rm 
would 
typically 
be 
reluctant 
to 
adopt 
algorithmic 
transparency 
when 
their 
machine 
learning 
model 
derives 
large 
predictive 
power 
from 
the 
correlational 
feature. 
However, 
we 
show 
that 
the 
rm 
should 
recognize 
that 
investment 
in 
the 
causal 
feature 
by 
agents 
is 
endogenous. 
The 
H 
type 
agents 
are 
less 
likely 
to 
invest 
in 
the 
causal 
feature 
when 
the 
correlational 
feature 
is 
sucient 
to 
separate 
them 
from 
the 
L 
type. 
This 
is 
the 
situation 
where 
the 
rm 
should 
be 
willing 
to 
sacrice 
the 
predictive 
power 
of 
the 
correlational 
feature. 
We 
have 
demonstrated 
our 
results 
in 
the 
setting 
where 
(1) 
the 
rm 
is 
certain 
it 
will 
lose 
the 
predictive 
power 
of 
the 
correlational 
feature 
and 
(2) 
the 
rm 
does 
not 
have 
a 
rst 
mover 
advantage 
under 
algorithmic 
transparency. 
Intuitively, 
one 
would 
think 
that 
algorithmic 
transparency 
would 
be 
bad 
for 
the 
rm 
when 
these 
two 
conditions 
hold. 
However, 
we 
show 
that, 
once 
we 
consider 
the 
endogenous 
investment 
in 
the 
causal 
feature, 
the 
rm 
would 
be 


6 


 


better 
off 
making 
the 
algorithm 
transparent. 


Organization 
of 
the 
paper 


The 
rest 
of 
the 
paper 
is 
organized 
as 
follows. 
We 
discuss 
how 
we 
build 
upon 
and 
contribute 
to 
the 
literature 
in 
Section 
2. 
The 
details 
of 
our 
main 
model 
are 
presented 
in 
Section 
3. 
In 
Section 
4, 
we 
present 
the 
analysis 
of 
the 
main 
model. 
In 
Section 
5, 
we 
discuss 
the 
model 
of 
Stackelberg 
competition. 
In 
Section 
6, 
we 
describe 
three 
extensions 
of 
the 
main 
model. 
We 
conclude 
the 
paper 
in 
Section 
7. 
Unless 
otherwise 
noted, 
all 
the 
proofs 
can 
be 
found 
in 
Appendix 
D. 


Literature 


Algorithmic 
transparency 
is 
a 
relatively 
new 
topic, 
but 
it 
is 
closely 
related 
to 
the 
literature 
on 
information 
asymmetry. 
Following 
the 
canonical 
job 
market 
signaling 
model 
developed 
by 
Spence 
(1973), 
a 
rich 
stream 
of 
research 
has 
focused 
on 
the 
interaction 
between 
a 
decision 
maker 
and 
strategic 
agents 
under 
asymmetric 
information. 
Some 
of 
this 
research 
is 
focused 
on 
the 
agents‚Äô 
side 
and 
study 
how 
agents 
strategically 
reveal 
their 
type 
under 
various 
market 
conditions 
(e.g., 
the 
stream 
of 
signaling 
game 
literature). 
Other 
research 
is 
focused 
on 
the 
decision 
makers‚Äô 
side, 
studying 
how 
they 
can 
design 
an 
optimal 
algorithm 
to 
extract 
agents‚Äô 
private 
information 
(e.g., 
the 
stream 
of 
strategic 
classication 
literature). 
Our 
work 
on 
algorithmic 
transparency 
is 
built 
upon 
and 
contributes 
to 
both 
these 
streams 
of 
literature. 


In 
both 
the 
opaque 
and 
transparent 
scenarios, 
the 
interaction 
between 
the 
rm 
and 
the 
agents 
can 
be 
adapted 
into 
the 
signaling 
game 
framework, 
where 
individuals 
(senders) 
rst 
send 
signals, 
and 
the 
rm 
(receiver) 
then 
makes 
hiring 
decisions 
based 
on 
the 
observed 
signals 
(Spence, 
1973, 
Engers, 
1987, 
Weiss, 
1983, 
Daley 
and 
Green, 
2014). 
Signaling 
game 
models 
often 
focus 
on 
specifyin
g 
the 
equilibrium 
outcome 
under 
various 
market 
conditions. 
Receivers 
are 
assumed 
to 
be 
in 
a 
competitive 
environment 
and 
earns 
zero 
prot 
in 
the 
equilibrium. 
All 
the 
surplus 
is 
extracted 
by 
senders. 
The 
equilibrium 
concept 
typically 
used 
in 
signaling 
game 
models 
is 
the 
perfect 
Bayesian 
equilibrium 
(PBE), 
where 
three 
conditions 
are 
satised: 
senders 
use 
optimal 
strategies 
facing 
the 
wage 
oer, 
receivers 
give 
wage 
oers 
such 
that 
they 
will 
obtain 
zero 
prot, 
and 
the 
receivers‚Äô 
belief
s 
about 
the 
senders‚Äô 
type 
given 
the 
signal 
are 
consistent 
with 
the 
truth. 
Similar 
to 
the 
signaling 
games, 
we 
also 
specify 
the 
equilibrium 
outcome 
under 
various 
market 
conditions 
in 
both 
the 
transparen
t 
and 
opaque 
scenarios. 
However, 
in 
our 
model, 
the 
rm 
oers 
a 
xed 
wage 
and 
focuses 
on 
designing 
the 
algorithm 
to 
increase 
its 
chances 
of 
hiring 
the 
most 
productive 
agents, 
which 
contrast
s 
with 
the 
signaling 
games 
where 
all 
agents 
are 
hired 
and 
the 
rm's 
objective 
is 
to 
decide 
how 
much 
salary 
to 
oer 
to 
each 
agent. 


On 
the 
rm 
side, 
our 
model 
setup 
bears 
more 
similarity 
to 
the 
strategic 
classication 
problems 
‚Äì 
oering 
a 
xed 
wage, 
the 
rm 
decides 
whether 
to 
hire 
the 
agents 
based 
on 
their 
signal 
(Kleinberg 


7 


 


and 
Raghavan, 
2019, 
Frankel 
and 
Kartik, 
2019, 
Bonatti 
and 
Cisternas, 
2019). 
In 
other 
words, 
the 
rm 
is 
trying 
to 
classify 
agents 
based 
on 
whether 
their 
expected 
productivity 
exceeds 
the 
wage 
or 
not. 
This 
classication 
setting 
makes 
it 
possible 
for 
us 
to 
analyze 
the 
economic 
impact 
of 
algorithmic 
transparency 
on 
the 
decision 
maker 
(rm) 
by 
comparing 
its 
equilibrium 
payoff 
in 
the 
opaque 
and 
transparent 
scenarios. 
Moreover, 
we 
do 
not 
assume 
the 
signal 
(education 
level 
for 
example) 
to 
always 
be 
pure 
money-burning. 
Instead, 
we 
allow 
the 
causal 
feature 
to 
positively 
impact 
productivity 
and 
specify 
conditions 
regarding 
this 
positive 
eect 
under 
which 
algorithmic 
transparency 
benets 
the 
decision 
maker. 
In 
that 
aspect, 
our 
work 
is 
also 
closely 
related 
to 
the 
strategic 
classication 
literature 
that 
assumes 
the 
existence 
of 
both 
causal 
and 
non-causal 
features. 


Signaling 
Games. 
The 
signaling 
game 
literature 
studies 
how 
agents 
strategically 
reveal 
their 
type 
to 
a 
principle 
in 
a 
situation 
of 
information 
asymmetry. 
Traditional 
signaling 
models 
typically 
assume 
that 
costly 
actions 
are 
the 
only 
channels 
through 
which 
agents 
can 
signal 
their 
type 
(Spence, 
1973). 
In 
these 
models, 
standard 
assumptions 
such 
as 
the 
Spence-Mirrlees 
single-crossing 
condition 
ensure 
the 
existence 
of 
separating 
equilibria: 
equilibria 
that 
fully 
reveal 
agents‚Äô 
private 
information. 
While 
the 
machine 
learning 
models 
are 
trying 
to 
solve 
the 
same 
problem 
(i.e., 
trying 
to 
identify 
the 
type 
of 
agents 
under 
information 
asymmetry), 
they 
dier 
from 
decision 
makers 
in 
the 
classical 
signaling 
models 
in 
the 
following 
way. 
A 
machine 
learning 
model 
uses 
multiple 
features 
to 
learn 
an 
agent's 
type. 
Each 
feature 
is 
essentially 
an 
action 
taken 
by 
the 
agent 
that 
signals 
her 
type. 
Some 
of 
these 
features 
are 
costly 
to 
improve, 
while 
others 
are 
not. 


Our 
paper 
is 
related 
to 
recent 
signaling 
game 
papers 
that 
have 
also 
considered 
multiple 
actions 
as 
channels 
through 
which 
an 
agent 
can 
signal 
her 
type 
(Engers, 
1987, 
Frankel 
and 
Kartik, 
2019, 
Daley 
and 
Green, 
2014, 
Alos-Ferrer 
and 
Prat, 
2012). 
In 
these 
papers, 
the 
agents 
are 
always 
aware 
of 
the 
actions 
that 
are 
used 
as 
signals 
by 
the 
decision 
maker. 
In 
contrast, 
in 
our 
model's 
opaque 
scenario, 
the 
agents 
know 
that 
a 
correlational 
feature 
is 
being 
used 
by 
the 
rm, 
but 
they 
do 
not 
know 
exactly 
what 
feature 
that 
is. 


In 
our 
model, 
agents 
can 
use 
causal 
and/or 
correlational 
features 
to 
signal 
their 
type. 
The 
causal 
feature 
is 
similar 
to 
the 
costly 
signal 
typically 
captured 
in 
the 
traditional 
signaling 
game 
literature. 
A 
key 
dierence 
is 
that 
we 
allow 
it 
to 
not 
only 
act 
as 
a 
signal 
of 
an 
agent's 
type 
but 
also 
have 
an 
impact 
on 
the 
agent's 
productivity, 
similar 
to 
Weiss 
(1983). 
For 
example, 
if 
agents 
of 
the 
same 
type 
have 
dierent 
levels 
of 
education, 
in 
our 
model, 
the 
rm 
would 
receive 
dierent 
payos 
from 
hiring 
them. 
The 
correlational 
feature 
that 
we 
model 
bears 
some 
similarities 
to 
the 
information 
in 
cheap 
talk 
games 
(Crawford 
and 
Sobel, 
1982). 
This 
feature 
is 
almost 
costless 
to 
share, 
and 
it 
aects 
the 
eventual 
payoff 
of 
both 
the 
rm 
and 
the 
agents 
where 
their 
incentives 
are 
not 
perfectly 
aligned. 
In 
cheap 
talk 
games, 
the 
agent 
strategically 
manipulates 
this 
information, 
whereas, 
in 
our 
model, 
the 
agent 
does 
not 
know 
about 
this 
feature 
and 
cannot 
manipulate 
it 
in 
the 


8 


 


opaque 
scenario. 


Similar 
to 
our 
paper, 
a 
few 
recent 
papers 
have 
modeled 
the 
tradeos 
that 
an 
agent 
faces 
in 
the 
presence 
of 
multiple 
signals. 
For 
example, 
Daley 
and 
Green 
(2014) 
modeled 
a 
scenario 
where 
a 
student 
can 
send 
a 
costly 
signal 
(e.g., 
joint 
degree 
completion) 
to 
the 
recruiter 
or 
rely 
on 
a 
type-correlated 
noisy 
signal 
(e.g., 
grades). 
They 
characterized 
the 
results 
based 
on 
the 
informativeness 
of 
the 
noisy 
signal. 
The 
noisy 
signal 
is 
similar 
to 
the 
correlated 
feature 
in 
our 
model, 
and 
its 
informativeness 
is 
also 
modeled 
similarly 
to 
how 
we 
capture 
the 
predictive 
power 
of 
the 
correlated 
feature. 
A 
key 
nding 
of 
the 
paper 
is 
that, 
when 
grades 
are 
informative, 
H 
type 
individuals 
are 
less 
eager 
to 
send 
costly 
signals 
because 
they 
can 
now 
rely 
on 
grades 
to 
signal 
their 
type, 
while 
L 
type 
individuals 
are 
more 
willing 
to 
send 
a 
costly 
signal 
to 
de-emphasize 
the 
`grades‚Äô 
dimension. 
Consequently, 
a 
separating 
equilibrium 
on 
the 
costly 
signal 
dimension 
is 
harder 
to 
sustain. 
Our 
model 
also 
shares 
some 
similarities 
with 
this 
paper 
in 
that 
we 
also 
consider 
the 
possibility 
that 
the 
existence 
of 
an 
extra 
noisy 
signal 
will 
change 
individuals‚Äô 
decision 
on 
the 
more 
costly 
signal. 
However, 
there 
are 
key 
dierences 
in 
our 
model's 
assumptions 
and 
results. 
Unlike 
the 
`grades‚Äô 
dimension, 
whose 
value 
is 
impossible 
to 
manipulate, 
in 
our 
model, 
the 
rm 
can 
give 
individuals 
the 
opportunity 
to 
game 
the 
correlational 
feature 
by 
making 
the 
algorithm 
transparent. 
On 
the 
one 
hand, 
manipulation 
will 
make 
this 
dimension 
less 
informative. 
On 
the 
other 
hand, 
individuals‚Äô 
behavior 
on 
the 
more 
costly 
signaling 
dimension 
will 
also 
change 
and 
could 
lead 
to 
a 
separating 
equilibrium 
in 
many 
cases. 


Strategic 
Classication. 
Strategic 
classication 
literature 
considers 
the 
problem 
of 
designing 
optimal 
classication 
algorithms 
when 
facing 
strategic 
users 
who 
may 
manipulate 
the 
input 
to 
the 
system 
at 
a 
cost 
(Hardt 
et 
al., 
2016). 
Canonical 
strategic 
classication 
models 
deem 
that 
the 
user's 
manipulation 
always 
hurts 
the 
decision 
maker. 
Guided 
by 
this 
belief, 
a 
large 
stream 
of 
research 
on 
strategic 
classication 
is 
focused 
on 
developing 
algorithms 
that 
are 
robust 
to 
gaming 
(Meir 
et 
al., 
2012, 
Cummings 
et 
al., 
2015). 
Recently, 
several 
papers 
have 
argued 
that 
this 
gaming 
itself 
can 
be 
benecial 
to 
the 
decision 
maker; 
thus, 
instead 
of 
focusing 
on 
manipulation-proof 
algorithms, 
these 
papers 
focus 
on 
designing 
algorithms 
that 
incentivize 
individuals 
to 
invest 
in 
desirable 
features 
(Kleinberg 
and 
Raghavan, 
2019, 
Alon 
et 
al., 
2020, 
Haghtalab 
et 
al., 
2020). 
These 
papers 
are 
the 
ones 
we 
want 
to 
highlight 
since 
our 
paper 
also 
points 
out 
the 
dierence 
between 
`gaming‚Äô 
and 
`improvement': 
gaming 
is 
bad 
for 
the 
decision 
maker 
because 
it 
deteriorates 
the 
information 
contained 
in 
the 
relevant 
features, 
but 
`improvement‚Äô 
could 
be 
benecial 
to 
the 
decision 
maker 
since 
it 
will 
causally 
impact 
the 
target 
variable. 


Kleinberg 
and 
Raghavan 
(2019) 
studied 
the 
principle-agent 
problem 
where 
the 
agents‚Äô 
features 
(e.g., 
nal 
exam 
score) 
can 
be 
improved 
in 
two 
ways: 
by 
investing 
eort 
in 
a 
desirable 
way 
(e.g., 
spending 
time 
on 
course 
material) 
or 
by 
investing 
eort 
in 
an 
undesirable 
way 
(e.g., 
cheating). 
The 


9 


 


eectiveness 
of 
each 
kind 
of 
eort 
on 
the 
feature 
is 
called 
the 
eort 
prole. 
The 
decision 
maker 
can 
observe 
the 
agents‚Äô 
performance 
on 
the 
features 
but 
cannot 
observe 
in 
which 
way 
the 
agents 
achieve 
their 
scores. 
Alon 
et 
al. 
(2020) 
examined 
a 
similar 
setting 
but 
extended 
Kleinberg 
and 
Raghavan 
(2019)'s 
model 
into 
a 
multi-agent 
scenario. 
Instead 
of 
assuming 
all 
individuals 
share 
the 
same 
eort 
prole, 
they 
focused 
on 
designing 
optimal 
algorithms 
that 
can 
work 
for 
dierent 
groups 
of 
individuals 
who 
may 
have 
dierent 
eort 
proles. 
Our 
work 
is 
dierent 
from 
theirs 
with 
respect 
to 
one 
important 
aspect: 
while 
they 
assumed 
that 
a 
feature 
can 
be 
improved 
in 
either 
a 
causal 
or 
non-causal 
way, 
our 
model 
assumes 
that 
there 
are 
pure 
causal 
features 
and 
pure 
correlational 
features. 
Causal 
features 
(e.g., 
education 
level) 
can 
be 
improved 
only 
in 
a 
`causal‚Äô 
way 
(such 
that 
the 
value 
of 
the 
target 
variable 
will 
also 
increase). 
Correlational 
features 
(e.g., 
whether 
an 
applicant 
wears 
glasses 
or 
not) 
can 
be 
improved 
only 
by 
gaming. 
If 
there 
was 
a 
`causal‚Äô 
way 
to 
improve 
the 
correlational 
feature, 
then 
the 
rm's 
willingness 
to 
publish 
the 
correlational 
feature 
would 
be 
stronger 
and 
might 
come 
from 
the 
potential 
productivity-enhancing 
eect 
of 
individuals‚Äô 
improvement 
on 
the 
correlational 
feature. 
We 
show 
that 
even 
in 
the 
case 
where 
gaming 
on 
the 
pure 
correlational 
feature 
has 
no 
positive 
eect 
on 
productivity, 
the 
rm 
may 
still 
want 
to 
publish 
it. 
Furthermore, 
none 
of 
the 
papers 
above 
has 
studied 
how 
rms 
choose 
between 
opaque 
and 
transparent 
algorithms. 


Another 
two 
papers 
we 
want 
to 
highlight 
are 
Frankel 
and 
Kartik 
(2019) 
and 
Bonatti 
and 
Cisterna
s 
(2019). 
These 
two 
economics 
papers 
showed 
that 
the 
decision 
maker 
could 
be 
ex-ante 
better 
off 
by 
committing 
to 
some 
ex-post 
sub-optimal 
strategies 
such 
as 
down-weighting 
some 
relevant 
features. 
Our 
paper 
is 
related 
to 
these 
papers 
in 
the 
sense 
that 
`publishing 
the 
algorithms‚Äô 
could 
also 
be 
seen 
as 
a 
way 
to 
down-weight 
the 
correlational 
feature. 
However, 
there 
are 
critical 
dierence
s 
between 
these 
papers 
and 
ours 
in 
terms 
of 
mechanisms. 
In 
their 
papers, 
ex-post 
sub-optimal 
behavior 
such 
as 
`under-utilizing‚Äô 
some 
informative 
features 
might 
be 
preferred 
by 
the 
decision 
maker 
because 
individuals 
may 
have 
less 
incentive 
to 
manipulate 
these 
features 
if 
they 
anticipate 
that 
the 
features 
will 
be 
down-weighed. 
The 
decision 
maker 
loses 
some 
predictive 
accuracy 
due 
to 
under-utilizing 
those 
features, 
but 
the 
observed 
values 
for 
those 
features 
now 
better 
represent 
individuals‚Äô 
natural 
behavior 
instead 
of 
gaming 
behavior. 
Consequently, 
not 
fully 
exploiting 
the 
information 
contained 
in 
relevant 
features 
might 
be 
optimal 
ex-ante. 
Our 
paper, 
however, 
shows 
that 
even 
if 
such 
`feature 
down-weighting‚Äô 
cannot 
reduce 
individuals‚Äô 
gaming, 
it 
may 
still 
benet 
the 
decision 
maker. 
In 
our 
paper, 
the 
purpose 
of 
`down-weighting‚Äô 
the 
correlational 
feature 
is 
not 
to 
reduce 
or 
eliminate 
gaming 
behavior, 
but 
rather 
to 
increase 
the 
competition 
intensity 
regarding 
the 
causal 
features 
on 
which 
the 
H 
type 
agents 
hold 
a 
cost 
advantage. 


10 


 


Model 


In 
this 
section, 
we 
develop 
a 
parsimonious 
model 
that 
captures 
how 
the 
rm 
and 
agents 
act 
under 
opaque 
and 
transparent 
scenarios. 
We 
consider 
the 
hiring 
setting 
discussed 
above 
with 
two 
types 
of 
agents: 
high-talent 
(H 
type) 
agents 
and 
low-talent 
(L 
type) 
agents. 
For 
simplicity, 
we 
normalize 
the 
total 
number 
of 
agents 
to 
1 
and 
assume 
that 
a 
Œ∏ 
portion 
of 
them 
are 
of 
H 
type 
and 
the 
remaining 
1 
‚àí 
Œ∏ 
portion 
are 
of 
L 
type. 


Talent 
level 
is 
directly 
related 
to 
job 
performance 
and, 
ideally, 
the 
rm 
would 
like 
to 
hire 
only 
H 
type 
agents. 
However, 
the 
rm 
cannot 
directly 
observe 
an 
agent's 
type 
until 
she 
is 
hired 
and 
works 
at 
the 
rm 
for 
a 
while. 
Consequently, 
the 
rm 
can 
only 
use 
some 
observable 
agent 
features 
to 
dierentiate 
the 
two 
types 
of 
agents. 
We 
classify 
these 
features 
into 
two 
types: 
causal 
features 
and 
correlational 
features. 
For 
simplicity, 
we 
assume 
that 
the 
rm 
only 
uses 
one 
causal 
feature 
(which 
is 
common 
knowledge 
to 
the 
rm 
and 
the 
agents, 
e.g., 
education 
level) 
and 
one 
correlational 
feature 
(which 
is 
unknown 
to 
the 
agents 
unless 
the 
rm 
decides 
to 
reveal 
it). 
Both 
features 
take 
on 
a 
discrete 
value 
of 
0 
(low) 
or 
1 
(high). 
Each 
agent 
can 
be 
characterized 
by 
one 
of 
four 
possible 
combinations 
(or 
states) 
in 
the 
two-dimensional 
feature 
space: 


‚Ä¢ 
State 
A 
(low 
causal, 
high 
correlational); 
‚Ä¢ 
State 
B 
(high 
causal, 
high 
correlational); 
‚Ä¢ 
State 
C 
(low 
causal, 
low 
correlational); 
and 
‚Ä¢ 
State 
D 
(high 
causal, 
low 
correlational). 
The 
rm's 
hiring 
strategy 
can 
therefore 
be 
represented 
by 
four 
hiring 
probabilities 
for 
the 
four 
states. 
In 
the 
remainder 
of 
this 
paper, 
we 
will 
refer 
to 
these 
probabilities 
as 
PA, 
PB, 
PC 
, 
and 
PD, 
respectively. 


We 
assume 
that 
it 
is 
costly 
for 
the 
agents 
to 
improve 
the 
common-knowledge 
causal 
feature, 
and 
that 
H 
type 
agents 
have 
a 
cost 
advantage 
on 
this 
feature. 
Specically, 
we 
assume 
that 
CH 
(the 
cost 
of 
improving 
the 
causal 
feature 
for 
the 
H 
type 
agents) 
is 
smaller 
than 
CL 
(the 
cost 
of 
improving 
the 
causal 
feature 
for 
the 
L 
type 
agents). 
In 
contrast, 
the 
cost 
to 
improve 
the 
correlational 
feature 
is 
assumed 
to 
be 
the 
same 
for 
the 
H 
type 
and 
L 
type 
agents 
and 
is 
very 
small 
(i.e., 
marginally 
above 
zero). 
It 
is 
worth 
noting 
that, 
although 
the 
cost 
of 
improving 
any 
given 
correlational 
feature 
is 
small, 
there 
are 
many 
of 
them, 
and 
agents 
do 
not 
know 
which 
correlational 
feature 
will 
be 
used 
in 
the 
algorithm 
unless 
the 
rm 
decides 
to 
reveal 
it. 


To 
model 
the 
situation 
where 
the 
rm 
has 
an 
incentive 
to 
include 
the 
correlational 
feature 
into 
its 
decision 
making, 
we 
further 
assume 
that 
a 
Œª 
portion 
of 
the 
H 
type 
agents 
and 
a 
1 
‚àí 
Œª 
portion 
of 


11 


 


the 
L 
type 
agents 
have 
value 
1 
on 
the 
rm's 
chosen 
correlational 
feature.4 
Moreover, 
Œª 
‚àà 
[0:5, 
1], 
which 
indicates 
a 
positive 
correlation 
between 
an 
agent's 
value 
on 
the 
correlational 
feature 
and 
her 
type. 
The 
true 
value 
of 
Œª 
is 
known 
to 
the 
rm. 
We 
assume 
that 
the 
agents 
also 
have 
a 
correct 
belief 
about 
Œª 
in 
the 
main 
model. 
This 
assumption 
will 
be 
relaxed 
in 
Appendix 
B.3 
where 
we 
allow 
the 
agents 
to 
have 
an 
`incorrect 
belief‚Äô 
about 
. 


The 
game 
between 
the 
rm 
and 
the 
agents 
is 
played 
as 
follows. 
In 
the 
rst 
stage, 
the 
rm 
makes 
a 
decision 
on 
transparency 
(i.e., 
opaque 
or 
transparent), 
and 
this 
decision 
is 
known 
to 
all 
agents. 
If 
the 
rm 
chooses 
\opaque,‚Äù 
the 
remainder 
of 
the 
game 
proceeds 
as 
follows: 
The 
agents 
rst 
choose 
their 
strategies 
in 
terms 
of 
whether 
to 
improve 
their 
features, 
and 
then 
the 
rm 
makes 
its 
hiring 
decisions 
based 
on 
the 
observed 
agent 
features. 
If, 
on 
the 
other 
hand, 
the 
rm 
chooses 
\transparent,‚Äù 
the 
remainder 
of 
the 
game 
proceeds 
as 
follows: 
The 
rm 
rst 
discloses 
the 
algorithm 
to 
the 
agents; 
next, 
the 
agents 
choose 
their 
strategies; 
and, 
nally, 
the 
rm 
decides 
whom 
to 
hire 
based 
on 
the 
observed 
agent 
features. 
Recall 
from 
Section 
1 
that, 
in 
the 
main 
model, 
we 
focus 
our 
analysis 
on 
the 
case 
where 
the 
rm 
either 
does 
not 
have 
commitment 
power, 
or 
reveals 
only 
the 
features 
it 
uses 
but 
not 
its 
hiring 
probabilities.5 


In 
order 
to 
focus 
on 
the 
more 
interesting 
and 
realistic 
cases, 
we 
make 
the 
following 
assumptions 
regarding 
the 
strategies 
of 
the 
agents: 


1. 
In 
the 
opaque 
scenario, 
the 
agents 
will 
only 
focus 
on 
whether 
to 
improve 
their 
causal 
feature. 
This 
assumption 
is 
motivated 
by 
the 
fact 
that, 
in 
reality, 
while 
causal 
features 
are 
usually 
common 
knowledge 
between 
the 
rm 
and 
the 
agents 
(e.g., 
everyone 
knows 
that 
education 
level 
plays 
an 
important 
role 
in 
hiring 
decisions), 
correlational 
features 
are 
less 
so. 
In 
the 
opaque 
scenario, 
the 
rm 
does 
not 
reveal 
which 
correlational 
feature 
it 
will 
use 
in 
its 
algorithm 
to 
the 
agents. 
Consequently, 
the 
best 
that 
an 
agent 
can 
do 
is 
make 
a 
random 
guess. 
Since 
there 
are 
a 
large 
number 
of 
potential 
correlational 
features 
and 
the 
rm 
only 
uses 
one 
of 
them, 
from 
the 
individual 
agent's 
perspective, 
it 
is 
not 
protable 
to 
improve 
any 
of 
the 
potential 
correlational 
features 
because 
the 
probability 
of 
hitting 
the 
right 
one 
is 
essentially 
zero. 
From 
a 
theoretical 
perspective, 
we 
note 
that 
this 
assumption 
is 
without 
loss 
of 
generality 
and 
is 
useful 
to 
simplify 
our 
analysis. 
It 
is 
not 
dicult 
to 
show 
that 
the 
main 
insights 
of 
the 
paper 
will 
not 
change 
qualitatively 
even 
if 
we 
assume 
a 
small 
probability 
> 
0 
that 
the 
agents 
will 
hit 
the 
right 
correlational 
feature. 
4While 
agents 
may 
not 
know 
which 
correlational 
feature 
the 
rm 
is 
using, 
they 
may 
attempt 
to 
game 
on 
as 
many 
correlational 
features 
as 
possible. 
In 
this 
scenario, 
Œª 
(1 
‚àí 
) 
can 
be 
interpreted 
as 
the 
probability 
of 
H 
(L) 
type 
agents 
hitting 
the 
feature 
that 
is 
actually 
used 
by 
the 
rm 


5In 
Section 
5, 
we 
will 
discuss 
the 
case 
where 
the 
rm 
has 
commitment 
power 
and 
reveals 
both 
the 
features 
it 
uses 
and 
its 
hiring 
strategy. 


12 


 


2. 
In 
the 
transparent 
scenario, 
all 
agents 
will 
improve 
their 
correlational 
features.6 
Once 
the 
rm 
reveals 
the 
correlational 
feature 
that 
it 
will 
use 
in 
its 
algorithm, 
the 
probability 
that 
an 
individual 
agent 
hits 
the 
right 
feature 
becomes 
1. 
Since 
the 
cost 
of 
improving 
the 
correlational 
feature 
is 
minimal, 
as 
long 
as 
it 
increases 
an 
agent's 
probability 
of 
being 
hired, 
they 
will 
improv
e 
this 
feature. 
It 
is 
worth 
noting 
that 
assuming 
all 
agents 
will 
improve 
the 
correlational 
feature 
used 
in 
the 
algorithm 
does 
not 
cause 
a 
loss 
of 
generality. 
This 
is 
so 
because, 
under 
the 
scenario 
where 
all 
agents 
achieve 
a 
\high‚Äù 
state 
on 
the 
disclosed 
correlational 
feature, 
this 
feature 
completely 
loses 
its 
predictive 
power 
and, 
therefore, 
will 
drop 
out 
of 
the 
prediction 
algorithm. 
If 
it 
can 
be 
shown 
that 
the 
rm 
can 
still 
be 
better 
off 
by 
making 
its 
algorithm 
transparent 
in 
such 
an 
extreme 
case 
of 
\agent 
gaming,‚Äù 
it 
sends 
a 
strong 
message 
that 
algorithmi
c 
transparency 
can 
indeed 
be 
economically 
benecial. 
This 
assumption 
will 
be 
relaxed 
in 
Appendix 
B.1 
where 
we 
allow 
gaming 
cost 
of 
the 
correlational 
feature 
to 
be 
signicantly 
greater 
than 
zero. 
Our 
main 
result 
is 
strengthened 
after 
the 
relaxation 
of 
this 
assumption. 
3.1 
The 
Firm's 
Utility 
As 
previously 
mentioned, 
the 
causal 
feature 
(for 
consistency, 
hereafter, 
we 
will 
use 
education 
level 
as 
an 
example 
of 
a 
causal 
feature) 
has 
a 
direct 
in
uence 
on 
the 
agents‚Äô 
performance, 
while 
the 
correlational 
feature 
does 
not. 
Thus, 
an 
agent's 
performance 
is 
determined 
by 
both 
their 
type 
(T 
2fH, 
Lg) 
and 
their 
education 
level 
(Education 
2f0, 
1g). 
Here, 
we 
follow 
Weiss 
(1983) 
and 
allow 
education 
to 
not 
only 
act 
as 
a 
signal 
of 
an 
agent's 
type 
but 
also 
contribute 
to 
the 
productivity 
of 
the 
agent. 
We 
use 
Œ± 
and 
Œ≤ 
to 
denote 
the 
marginal 
eects 
of 
type 
and 
education, 
respectively. 
For 
convenience, 
we 
normalize 
the 
performance 
of 
an 
uneducated 
L 
type 
agent 
to 
0. 
The 
mathematical 
expression 
of 
an 
agent's 
performance 
is 
given 
by: 


W 
(T, 
Education)= 
Œ± 
√ó 
1(T 
= 
H)+ 
Œ≤ 
√ó 
1(Education 
= 
1). 
(1) 


In 
the 
opaque 
scenario, 
each 
agent 
is 
characterized 
by 
her 
type 
T 
2fH, 
L} 
and 
her 
state 
S 
2fA, 
B, 
C, 
D} 
at 
the 
end 
of 
the 
game. 
We 
can 
write 
an 
agent's 
performance 
as 
a 
function 
of 
her 
type 
and 
her 
nal 
state 
as 
follows: 


W 
T 
= 
Œ± 
√ó 
1(T 
= 
H)+ 
Œ≤ 
√ó 
(1(S 
= 
B)+ 
1(S 
= 
D)). 
(2)

S 


In 
the 
transparent 
scenario, 
since 
all 
agents 
are 
\high‚Äù 
on 
the 
correlational 
feature, 
they 
only 
dier 
on 
the 
causal 
feature 
(i.e., 
education). 
This 
means 
that 
we 
can 
reduce 
the 
number 
of 
possible 


6It's 
worth 
noting 
that 
although 
the 
correlational 
feature 
will 
be 
dropped 
if 
everyone 
improves 
it, 
the 
level 
of 
transparency 
actually 
increases. 
The 
reason 
is 
that 
agents 
know 
exactly 
the 
state 
they 
are 
in 
and 
thus 
are 
more 
certain 
about 
their 
chances 
of 
being 
hired. 


13 


 


states 
from 
four 
to 
two, 
i.e., 
state 
E 
(low 
education) 
and 
state 
F 
(high 
education). 
Using 
these 
state 
denitions, 
we 
can 
write 
an 
agent's 
performance 
as 
follows: 


W 
T 
= 
Œ± 
√ó 
1(T 
= 
H)+ 
Œ≤ 
√ó 
1(S 
= 
F 
). 
(3)

S 


Once 
the 
agents 
are 
hired, 
their 
performance 
will 
contribute 
to 
the 
rm's 
payo, 
and 
the 
rm 
will 
pay 
them 
a 
xed 
reward 
R 
(i.e., 
job 
compensation). 
In 
the 
main 
model, 
we 
assume 
that 
the 
reward 
R 
is 
the 
same 
for 
both 
transparent 
and 
opaque 
scenarios, 
and 
that 
its 
value 
is 
exogenously 
given. 
In 
Appendix 
B.2, 
we 
will 
endogenize 
R 
and 
allow 
the 
rm 
to 
potentially 
use 
dierent 
rewards 
for 
the 
transparent 
and 
opaque 
scenarios. 
We 
will 
show 
that 
our 
main 
insights 
still 
hold. 


T 
HL

Let 
n 
denote 
the 
number 
of 
T 
type 
agents 
whose 
nal 
states 
are 
S, 
and 
let 
nS 
= 
n 
+ 
n

S 
SS 
H

denote 
the 
total 
number 
of 
agents 
whose 
nal 
states 
are 
S. 
Furthermore, 
let 

e 
= 
n 
=nS. 
The

SS 


rm's 
expected 
total 
payoff 
(for 
narrative 
convenience, 
we 
use 
`payo‚Äô 
to 
refer 
to 
`expected 
payo‚Äô 
hereafter) 
under 
hiring 
strategies 
(or 
probabilities) 
P 
=(PA;PB;PC 
;PD), 
in 
the 
opaque 
scenario, 
or 
P 
=(PE;PF 
), 
in 
the 
transparent 
scenario, 
can 
be 
mathematically 
expressed 
as 


firm 
= 
X 
H 
LPS 
¬∑ 
nS 
(W 
H 
‚àí 
R) 
+ 
nS 
(W 
L 
‚àí 
R)S 
S 
= 
SX 

ePS 
¬∑ 
nS 
¬∑ 
S(W 
H 
‚àí 
R) 
+ 
(1 
‚àí 

e 
S)(W 
L 
‚àí 
R) 
:S 
S 
(4) 
S 


3.2 
The 
Agents‚Äô 
Utility 
In 
the 
opaque 
scenario, 
the 
agents 
do 
not 
know 
which 
correlational 
feature 
will 
be 
used 
by 
the 
rm's 
algorithm; 
therefore, 
they 
will 
only 
focus 
their 
decisions 
on 
whether 
to 
improve 
the 
causal 
feature 
(i.e., 
education). 
Agents 
of 
the 
same 
type 
use 
the 
same 
strategy. 
Let 
uT 
denote 
the 
expected 
utility 
(for 
narrative 
convenience, 
we 
use 
`utility‚Äô 
to 
refer 
to 
`expected 
utility‚Äô 
hereafter) 
of 
a 
T 
type 
agent. 
We 
have: 


‚éß 
>

<

PBR 
+ 
(1 
‚àí 
)PDR 
‚àí 
CH 
if 
the 
H 
type 
agent 
improves 
the 
causal 
feature, 
uH 
= 
(5)

>

:PAR 
+ 
(1 
‚àí 
)PC 
R 
otherwise; 


‚éß 


>

<

(1 
‚àí 
)PBR 
+ 
PDR 
‚àí 
CL 
if 
the 
L 
type 
agent 
improves 
the 
causal 
feature, 
uL 
= 
(6)

>

:(1 
‚àí 
)PAR 
+ 
PC 
R 
otherwise. 


In 
the 
transparent 
scenario, 
all 
agents 
will 
have 
\high‚Äù 
values 
on 
the 
correlational 
feature, 
and 
their 
decisions 
are 
whether 
to 
improve 
the 
causal 
feature 
or 
not. 
The 
utility 
of 
a 
T 
type 
agent 
in 
the 
transparent 
scenario 
is: 


‚éß 


>

<

PF 
R 
‚àí 
CH 
if 
the 
H 
type 
agent 
improves 
the 
causal 
feature, 
uH 
= 
(7)

>

:

PER 
otherwise; 


14 


 


‚éß 
><PF 
R 
‚àí 
CL 
uL 
= 
>:PER 
if 
the 
L 
type 
agent 
improves 
the 
causal 
feature, 
otherwise. 
(8) 


3.3 
Additional 
Parametric 
Assumptions 
In 
Section 
4, 
we 
will 
solve 
the 
game 
using 
backward 
induction. 
For 
each 
combination 
of 
(CH 
;CL), 
we 
will 
derive 
the 
payoff 
for 
the 
rm 
in 
both 
the 
opaque 
and 
transparent 
scenarios. 
We 
will 
then 
specify 
the 
range 
of 
values 
of 
the 
parameters 
under 
which 
the 
rm 
is 
better 
or 
worse 
off 
when 
choosing 
to 
be 
transparent 
instead 
of 
opaque. 
We 
will 
show 
our 
results 
in 
the 
CH 
-CL 
space. 


We 
make 
the 
following 
three 
additional 
assumptions 
regarding 
the 
relationships 
among 
the 
parameters 
to 
allow 
us 
to 
focus 
on 
non-trivial 
and 
more 
interesting 
cases. 


Assumption 
1 


0 
<Œ≤ 
<R<. 


Assumption 
1 
says 
that 
the 
performance 
of 
an 
individual 
H 
type 
agent 
always 
exceeds 
the 
salary 
R 
regardless 
of 
her 
education 
level, 
whereas 
the 
performance 
of 
an 
individual 
L 
type 
agent 
is 
always 
smaller 
than 
the 
salary 
R. 
This 
condition 
ensures 
that 
the 
rm 
only 
wants 
to 
hire 
the 
H 
type 
agents. 


Assumption 
2 


(Œª 
+ 
(1 
‚àí 
)(1 
‚àí 
))RR 


<< 
. 


Œª 
Œ∏ 
Recall 
that 
Œ± 
denotes 
the 
performance 
advantage 
of 
H 
type 
agents 
over 
L 
type 
agents. 
Assumptio
n 
2 
ensures 
that 
Œ± 
falls 
in 
a 
certain 
range 
that 
guarantees 
that 
the 
rm 
will 
have 
an 
incentive 
to 
include 
the 
correlational 
feature 
in 
its 
algorithm 
even 
when 
all 
agents‚Äô 
education 
levels 
are 
0. 
The 
derivations 
of 
the 
lower 
bound 
and 
the 
upper 
bound 
of 
Œ± 
can 
be 
found 
in 
Appendix 
C.1. 
Intuitively, 
if 
Œ± 
is 
too 
small, 
the 
rm 
will 
not 
hire 
anyone 
when 
all 
agents‚Äô 
education 
levels 
are 
0. 
If, 
on 
the 
other 
hand, 
Œ± 
is 
too 
large, 
the 
rm 
will 
hire 
everyone 
even 
when 
all 
agents‚Äô 
education 
levels 
are 
0. 
In 
either 
case, 
the 
correlational 
feature 
is 
useless 
for 
the 
rm. 
We 
refer 
to 
the 
lower 
(upper) 
bound 
of 
Œ± 
as 
Œ± 
() 
hereafter. 


Assumption 
3 


(1 
‚àí 
)Œ± 


R 
‚àí 
Œ± 
<Œ≤ 
<R 
‚àí 
. 


(1 
‚àí 
) 
+ 
(1 
‚àí 
)Œª 
Assumption 
3 
says 
that 
the 
marginal 
eect 
of 
education 
on 
performance 
() 
falls 
in 
a 
certain 
range 
that 
guarantees 
the 
rm 
will 
have 
an 
incentive 
to 
include 
the 
correlational 
feature 
in 
the 
hiring 
algorithm. 
The 
derivations 
and 
interpretations 
of 
the 
lower 
and 
upper 
bound 
of 
Œ≤ 
can 
be 


15 


 


found 
in 
Appendix 
C.1. 
Intuitively, 
if 
Œ≤ 
is 
too 
small, 
the 
rm 
will 
not 
hire 
any 
agent 
even 
when 
everyone 
has 
a 
high 
level 
of 
education 
in 
the 
transparent 
scenario, 
which 
leads 
to 
trivial 
results. 
If, 
on 
the 
other 
hand, 
Œ≤ 
is 
too 
large, 
the 
rm 
will 
hire 
everyone 
with 
a 
high 
level 
of 
education 
irrespective 
of 
their 
values 
on 
the 
correlational 
feature 
in 
the 
opaque 
scenario, 
which 
could 
not 
justify 
the 
rm's 
incentive 
to 
use 
the 
correlational 
feature 
in 
the 
rst 
place. 
We 
refer 
to 
the 
lower 
(upper) 
bound 
of 
Œ≤ 
as 
() 
hereafter.7 


A 
summary 
of 
notations 
can 
be 
found 
in 
Table 
1 
in 
Appendix 
A. 


Analysis 


Let 

b 
denote 
the 
proportion 
that 
H 
type 
agents 
represent 
among 
all 
the 
agents 
in 
state 
S 
at 
the

S 


beginning 
of 
the 
game.8 
Per 
our 
discussions 
in 
Section 
3, 
a 
Œ∏ 
portion 
of 
agents 
are 
of 
H 
type 
and 
the 
remaining 
1 
‚àí 
Œ∏ 
portion 
are 
of 
L 
type. 
Moreover, 
a 
Œª 
portion 
of 
the 
H 
type 
agents 
and 
a 
1 
‚àí 
Œª 
portion 
of 
the 
L 
type 
agents 
have 
value 
1 
on 
the 
rm's 
chosen 
correlational 
feature. 
Therefore, 



b 
= 
Œ∏ 
;

A 


Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
) 


(1 
‚àí 
)Œ∏ 



b 
=;

C 


(1 
‚àí 
)Œ∏ 
+ 
(1 
‚àí 
) 



B
b 
= 

D
b 
=0. 


Recall, 
per 
our 
denition 
in 
Section 
3.1, 

e 
denotes 
the 
proportion 
that 
H 
type 
agents 
represent 


S 


among 
all 
the 
agents 
in 
state 
S 
at 
the 
end 
of 
the 
game. 


4.1 
Opaque 
Scenario 
Per 
our 
discussions 
in 
Section 
3, 
in 
the 
opaque 
scenario, 
agents 
move 
rst 
and 
will 
only 
decide 
on 
whether 
to 
improve 
the 
causal 
feature 
(i.e., 
education). 
Given 
the 
agents‚Äô 
strategies, 
based 
on 
Equation 
4, 
the 
rm 
will 
be 
indierent 
between 
hiring 
and 
not 
hiring 
agents 
with 
a 
nal 
state 
S 
if 

e 
(W 
H 
‚àí 
R) 
+ 
(1 
‚àí 

e 
)(W 
L 
‚àí 
R) 
= 
0, 
or 
equivalently, 


SS 
SS 



e 
R 
‚àí 
WS
L 


= 
. 
(9)

S 
W 
H 
‚àí 
W 
L 


SS 


RRÙÄÄÄ

By 
Equation 
2, 
the 
above 
fraction 
equals 
when 
S 
2fA,C} 
and 
equals 
when 
S 
2fB,Dg.

Œ± 


RRÙÄÄÄ

Let 

th0 
= 
Œ± 
and 

th1 
= 
Œ± 
(by 
Assumption 
1, 
we 
have 
0 
<
th1 
<
th0 
< 
1). 
Quantities 

th0 


and 

th1 
are 
important 
in 
the 
analysis, 
especially 
in 
determining 
whether 
a 
certain 
outcome 
(i.e., 
a 
combination 
of 
the 
agents‚Äô 
strategies 
and 
the 
rm's 
strategy) 
can 
be 
sustained 
in 
the 
equilibrium. 


7We 
provide 
discussion 
on 
the 
relaxation 
of 
Assumption 
3 
in 
Appendix 
C.2. 


8If 
there 
are 
no 
agents 
in 
state 
S, 
we 
manually 
set 



b
S 


= 
0. 
The 
same 
applies 
to 



eS

. 


16 


 


There 
is 
a 
total 
of 
nine 
possible 
classes 
of 
outcomes 
for 
agents‚Äô 
strategies. 
Five 
of 
them 
have 
the 
potential 
to 
be 
sustained 
in 
an 
equilibrium 
but 
the 
other 
four 
do 
not. 
The 
rst 
ve 
cases 
are: 
case 
1 
(neither 
H 
type 
nor 
L 
type 
agents 
improve 
education); 
case 
2 
(only 
H 
type 
agents 
improve 
education); 
case 
3 
(both 
H 
type 
and 
L 
type 
agents 
improve 
education); 
case 
4 
(H 
type 
agents 
improve 
education 
with 
some 
probability, 
and 
L 
type 
agents 
do 
not 
improve 
education); 
and 
case 
5(H 
type 
agents 
improve 
education, 
and 
L 
type 
agents 
improve 
education 
with 
some 
probability). 


Aside 
from 
the 
above 
ve 
cases, 
there 
are 
four 
other 
cases: 
case 
6 
(only 
L 
type 
agents 
improve 
education; 
case 
7 
(L 
type 
agents 
improve 
education 
with 
some 
probability 
but 
H 
type 
agents 
do 
not 
improve 
education); 
case 
8 
(L 
type 
agents 
improve 
education, 
but 
H 
type 
agents 
improve 
education 
only 
with 
some 
probability); 
and 
case 
9 
(both 
H 
type 
and 
L 
type 
agents 
improve 
education 
with 
some 
probability). 
It 
is 
not 
dicult 
to 
see 
that 
cases 
6 
through 
9 
cannot 
be 
sustained 
in 
an 
equilibrium. 
For 
cases 
6 
through 
8, 
the 
L 
type 
agents 
have 
a 
higher 
value 
on 
education 
than 
the 
H 
type 
agents, 
and 
the 
rm 
will 
have 
an 
incentive 
to 
set 
higher 
hiring 
probabilities 
in 
states 
A 
and 
C 
than 
in 
states 
B 
and 
D. 
However, 
under 
this 
hiring 
strategy, 
the 
L 
type 
agents 
will 
have 
no 
incentive 
to 
improve 
education 
in 
the 
rst 
place. 
As 
for 
case 
9, 
the 
fact 
that 
both 
the 
H 
type 
and 
L 
type 
agents 
are 
using 
mixed 
strategies 
indicates 
that 
they 
are 
both 
indierent 
between 
improving 
and 
not 
improving 
education. 
Since 
the 
cost 
of 
improving 
education 
for 
the 
L 
type 
agents 
is 
greater 
than 
that 
for 
the 
H 
type 
agents, 
to 
compensate 
for 
this 
higher 
cost, 
the 
L 
type 
agents 
must 
have 
a 
higher 
chance 
of 
being 
hired 
by 
the 
rm 
than 
the 
H 
type 
agents 
in 
the 
equilibrium. 
However, 
the 
rm 
has 
no 
incentive 
to 
use 
such 
a 
hiring 
strategy. 
We 
conclude 
that 
although 
there 
are 
nine 
possible 
classes 
of 
outcomes 
for 
the 
agents‚Äô 
strategies, 
only 
ve 
of 
them 
(cases 
1 
through 
5) 
can 
potentially 
be 
equilibrium 
outcomes. 


Out 
of 
the 
ve 
feasible 
cases, 
the 
actual 
equilibrium 
strategies 
of 
the 
agents 
and 
the 
rm 
depend 
on 
the 
values 
of 
(CH 
;CL). 
The 
following 
lemma 
summarizes 
the 
agents‚Äô 
equilibrium 
strategies 
for 
dierent 
values 
of 
(CH 
;CL) 
and 
the 
corresponding 
payoff 
for 
the 
rm. 
The 
proof 
can 
be 
found 
in 
Appendix 
D.1. 
Since 
we 
assume 
that 
the 
H 
type 
agents 
have 
a 
cost 
advantage 
to 
improve 
the 
causal 
feature 
(i.e., 
CH 
<CL), 
the 
region 
above 
the 
diagonal 
line 
in 
Figure 
1 
is 
infeasible. 


Lemma 
1 
The 
equilibrium 
outcome 
depends 
on 
the 
values 
of 
(CH 
;CL), 
and 
this 
dependence 
is 


17 


 


shown 
in 
Figure 
1. 
The 
payos 
for 
the 
rm 
are 
given 
by 


firmO1 
= 
Œ± 
‚àí 
(Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
))R 
firmO2 
= 
(Œ± 
+ 
) 
‚àí 
R 
firmO3 
= 
(Œ± 
+ 
) 
+ 
(1 
‚àí 
)(1 
‚àí 
)Œ≤ 
‚àí 
(Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
))R 


 


R(1 
‚àí 
)(1 
‚àí 
)

firmO4 
= 
(Œ± 
+ 
Œ≤ 
‚àí 
R)1 
‚àí 


(Œ± 
‚àí 
R)Œª 
2Œª 
‚àí 
1 


firmO5 
= 
(Œ± 
+ 
Œ≤ 
‚àí 
R);

Œª 


where 
firmOi 
denotes 
the 
rm's 
total 
payoff 
in 
case 
i. 



Figure 
1: 
Equilibrium 
outcome 
in 
the 
opaque 
scenario 


4.2 
Transparent 
Scenario 
In 
the 
transparent 
scenario, 
the 
rm 
moves 
rst 
by 
announcing 
both 
the 
correlational 
feature 
that 
it 
uses 
and 
the 
probability 
of 
hiring 
for 
each 
state 
(i.e., 
PE 
and 
PF 
). 
Similar 
to 
the 
opaque 
scenario, 
there 
is 
a 
total 
of 
nine 
possible 
outcomes 
for 
the 
agents‚Äô 
strategies 
(we 
use 
the 
same 
numbering 
of 
the 
nine 
cases 
as 
in 
the 
opaque 
scenario). 
To 
determine 
whether 
a 
certain 
outcome 
can 
be 
sustained 
as 
an 
equilibrium, 
we 
use 
the 
fact 
that 
the 
rm 
is 
indierent 
between 
hiring 
and 
not 
hiring 
agents 


RÙÄÄÄW 
L 


with 
nal 
state 
S 
iff 

e 
= 
S 
. 
The 
fraction 
on 
the 
right-hand 
side 
of 
the 
equality 
equals 

th0

SW 
H 
ÙÄÄÄW 
L 


SS 


when 
S 
= 
E 
and 
equals 

th1 
when 
S 
= 
F 
. 


Consistent 
with 
the 
opaque 
scenario, 
cases 
6 
through 
9 
cannot 
be 
sustained 
in 
an 
equilibrium. 
Moreover, 
according 
to 
Assumption 
3, 
if 
everyone 
is 
at 
state 
F, 
the 
rm 
will 
hire 
all 
agents. 
In 
case 


18 


 


5, 
some 
L 
type 
agents 
are 
at 
state 
E, 
which 
gives 
the 
rm 
even 
more 
incentive 
to 
hire 
agents 
in 
state 


F. 
However, 
again, 
to 
be 
an 
equilibrium, 
the 
mixed 
strategy 
outcome 
in 
case 
5 
requires 
the 
rm 
to 
be 
indierent 
between 
hiring 
and 
not 
hiring 
agents 
from 
state 
F. 
Therefore, 
neither 
case 
4 
nor 
case 
5 
can 
be 
sustained 
in 
an 
equilibrium 
in 
the 
transparent 
scenario. 
Altogether, 
this 
leaves 
us 
with 
only 
the 
rst 
three 
cases 
as 
possible 
equilibrium 
outcomes. 
The 
following 
lemma 
summarizes 
the 
agents‚Äô 
equilibrium 
strategies 
for 
dierent 
values 
of 
(CH 
;CL), 
as 
well 
as 
the 
corresponding 
payoff 
for 
the 
rm. 
The 
proof 
can 
be 
found 
in 
Appendix 
D.2. 
Lemma 
2 
The 
equilibrium 
outcome 
depends 
on 
the 
values 
of 
(CH 
;CL), 
and 
this 
dependence 
is 
shown 
in 
Figure 
2. 
The 
payos 
for 
the 
rm 
are 
given 
by 


firmT 
1 
=0 


firmT 
2 
= 
(Œ± 
+ 
Œ≤ 
‚àí 
R) 
firmT 
3 
= 
(Œ± 
+ 
) 
+ 
(1 
‚àí 
)Œ≤ 
‚àí 
R, 


where 
firmTi 
denotes 
the 
rm's 
total 
payoff 
in 
case 
i. 



Figure 
2: 
Equilibrium 
outcome 
in 
the 
transparent 
scenario 


4.3 
The 
Firm's 
Decision 
on 
Algorithmic 
Transparency 
The 
rm 
can 
make 
a 
decision 
on 
algorithmic 
transparency 
by 
comparing 
the 
payos 
in 
the 
transparen
t 
and 
opaque 
scenarios. 
In 
this 
subsection, 
we 
will 
show 
how 
the 
rm 
is 
not 
always 
worse 
off 
making 
its 
algorithm 
transparent 
instead 
of 
opaque. 
We 
divide 
the 
blue 
region 
in 
Figures 
1 
and 
2 
into 
seven 
smaller 
regions: 
N1, 
N2, 
and 
N3 
and 
C1, 
C2, 
C3, 
and 
C4 
(see 
Figure 
3). 


19 





Figure 
3: 
Comparison 
of 
agents‚Äô 
equilibrium 
behavior 
in 
the 
transparent 
and 
opaque 
scenarios 


We 
rst 
consider 
what 
happens 
in 
regions 
N1 
through 
N3. 
Note 
that, 
in 
these 
regions, 
agents 
play 
the 
same 
equilibrium 
strategies 
on 
the 
causal 
feature 
in 
the 
opaque 
and 
transparent 
scenarios. 
For 
example, 
region 
N1 
corresponds 
to 
case 
1 
in 
both 
the 
opaque 
and 
transparent 
scenarios, 
where 
neither 
type 
of 
agents 
improve 
their 
education. 
We 
now 
discuss 
the 
payoff 
comparison 
in 
these 
regions: 


‚Ä¢ 
In 
region 
N1, 
agents 
play 
the 
strategies 
in 
case 
1 
in 
both 
the 
opaque 
and 
transparent 
scenarios.
9 
Mathematically, 
firmO1 
> 
firmT 
1 
= 
0. 
Therefore, 
the 
rm 
will 
always 
prefer 
to 
be 
opaque 
in 
this 
region. 
‚Ä¢ 
In 
region 
N2, 
agents 
play 
the 
strategies 
in 
case 
2 
in 
both 
the 
opaque 
and 
transparent 
scenarios. 
Since 
firmO2 
=firmT 
2 
, 
in 
this 
region, 
the 
rm 
is 
indierent 
between 
being 
opaque 
or 
being 
transparent. 
‚Ä¢ 
In 
region 
N3, 
agents 
play 
the 
strategies 
in 
case 
3 
in 
both 
the 
opaque 
and 
transparent 
scenarios. 
We 
can 
rewrite 
firmT 
3 
as 
follows: 
firmT 
3 
= 
(Œ± 
+ 
)(Œª 
+ 
(1 
‚àí 
)) 
+ 
(1 
‚àí 
)(Œª 
+ 
(1 
‚àí 
)) 
ÙÄÄÄR(Œª 
+ 
(1 
‚àí 
)(1 
‚àí 
) 
+ 
(1 
‚àí 
)Œª 
+ 
(1 
‚àí 
)). 


9It 
is 
worth 
mentioning 
that 
although 
agents‚Äô 
strategies 
on 
the 
causal 
feature 
are 
the 
same 
in 
the 
opaque 
and 
transparent 
scenarios, 
the 
rm's 
payoff 
is 
dierent 
because 
of 
the 
existence 
of 
the 
correlational 
feature. 


20 


 


Since 
firmO3 
= 
(Œ± 
+ 
) 
+ 
(1 
‚àí 
)(1 
‚àí 
)Œ≤ 
‚àí 
(Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
))R, 
we 
have: 


firmT 
3 
‚àí 
firmO3 
= 
(1 
‚àí 
)(Œ± 
+ 
Œ≤ 
‚àí 
R) 
‚àí 
(1 
‚àí 
)(R 
‚àí 
) 


= 
(1 
‚àí 
)Œ± 
‚àí 
((1 
‚àí 
) 
+ 
(1 
‚àí 
))(R 
‚àí 
) 
< 
0. 


where 
the 
inequality 
follows 
Assumption 
3. 
This 
means 
that 
the 
rm 
will 
always 
prefer 
to 
be 
opaque 
in 
this 
region. 


We 
conclude 
that, 
in 
regions 
N1 
through 
N3, 
being 
transparent 
is 
never 
strictly 
better 
than 
being 
opaque. 
This 
is 
quite 
intuitive 
since, 
in 
these 
regions, 
agents 
play 
the 
same 
strategies 
on 
the 
causal 
feature 
in 
both 
the 
opaque 
and 
transparent 
scenarios. 
Hence, 
the 
rm 
can 
only 
be 
worse 
off 
revealing 
its 
algorithm 
due 
to 
the 
loss 
in 
the 
predictive 
power 
of 
the 
correlational 
feature. 
Specically, 
in 
regions 
N1 
and 
N3, 
when 
the 
rm 
chooses 
to 
be 
opaque, 
the 
predictive 
power 
of 
the 
algorithm 
only 
comes 
from 
the 
correlational 
feature 
because 
the 
H 
type 
and 
L 
type 
agents 
play 
the 
same 
strategy 
on 
the 
causal 
feature. 
This 
suggests 
that 
the 
rm 
will 
incur 
a 
signicant 
loss 
due 
to 
the 
reduction 
in 
the 
algorithm's 
prediction 
accuracy 
when 
the 
algorithm 
is 
made 
transparent. 


We 
now 
discuss 
the 
payoff 
comparison 
in 
regions 
C1 
through 
C4. 
In 
what 
follows, 
we 
rst 
provide 
a 
summary 
of 
the 
payoff 
comparison 
result 
in 
each 
region, 
and 
then 
discuss 
the 
intuition. 


‚Ä¢ 
In 
region 
C1, 
the 
agents‚Äô 
strategies 
and 
the 
rm's 
payoff 
change 
from 
opaque 
scenario 
1 
to 
transparent 
scenario 
3. 
firmT 
3 
> 
firmO1 
iff 
>1 
= 
(Œ± 
‚àí 
R) 
‚àí 
(1 
‚àí 
)(1 
‚àí 
)R 
+ 
R 
‚àí 
. 
(10) 


Thus, 
in 
this 
region, 
the 
rm 
will 
prefer 
to 
be 
transparent 
when 
>1. 


‚Ä¢ 
In 
region 
C2, 
the 
agents‚Äô 
strategies 
and 
the 
rm's 
payoff 
change 
from 
opaque 
scenario 
4 
to 
transparent 
scenario 
3. 
firmT 
3 
> 
firmO4 
iff 
R(1 
‚àí 
)

>2 
= 
R 
‚àí 
. 
(11)

(Œ± 
‚àí 
R)Œª 
+ 
R(1 
‚àí 
) 


Thus, 
in 
this 
region, 
the 
rm 
will 
prefer 
to 
be 
transparent 
when 
>2. 


‚Ä¢ 
In 
region 
C3, 
the 
agents‚Äô 
strategies 
and 
the 
rm's 
payoff 
change 
from 
opaque 
scenario 
5 
to 
transparent 
scenario 
3. 
firmT 
3 
> 
firmO5 
iff 
Œ± 
‚àí 
Œ± 
‚àí 
2R 
+ 
R 
+ 
R 


>3 
= 
. 
(12)

Œª 
‚àí 
2Œª 
+ 
Œ∏ 


In 
this 
region, 
the 
rm 
will 
prefer 
to 
be 
transparent 
when 
>3. 
It 
is 
interesting 
to 
note, 


however, 
that 
3 
equals 
Œ≤ 
dened 
in 
Assumption 
3 
(see 
below 
for 
an 
explanation 
of 
why 
this 


is 
the 
case). 
Since 
the 
value 
of 
Œ≤ 
cannot 
exceed 
Œ≤ 
(according 
to 
Assumption 
3), 
this 
means 
that 
the 
rm 
will 
never 
prefer 
to 
be 
transparent 
in 
this 
region. 


21 


 


‚Ä¢ 
In 
region 
C4, 
the 
agents‚Äô 
strategies 
and 
the 
rm's 
payoff 
change 
from 
opaque 
scenario 
1 
to 
transparent 
scenario 
2. 
firmT 
2 
> 
firmO1 
regardless 
of 
. 
Thus, 
in 
this 
region, 
the 
rm 
will 
always 
prefer 
to 
be 
transparent. 
According 
to 
Assumption 
2, 
the 
value 
of 
Œ± 
falls 
in 
the 
following 
interval: 


(Œª 
+ 
(1 
‚àí 
)(1 
‚àí 
))RR 


<< 
. 


Œª 
Œ∏ 
Within 
the 
above 
interval, 
1 
and 
3 
are 
decreasing 
in 
, 
and 
2 
is 
increasing 
in 
. 
Moreover, 
1 
= 
2 
when 
Œ± 
= 
Œ± 
and 
2 
= 
3 
when 
Œ± 
= 
(Œ± 
and 
¬Ø 
are 
dened 
in 
Assumption 
2). 
Thus, 
we 
have 
1 
<2 
<3. 
To 
understand 
why 
we 
have 
increasing 
thresholds 
for 
Œ≤ 
as 
we 
move 
from 
regions 
C1 
to 
C3 
and 
why 
there 
is 
no 
threshold 
for 
Œ≤ 
in 
region 
C4, 
we 
must 
look 
at 
how 
the 
rm's 
decision 
to 
be 
transparent 
changes 
the 
agents‚Äô 
strategies 
in 
dierent 
regions. 
To 
facilitate 
our 
discussions, 
we 
rst 
dene 
the 
concept 
of 
\degree 
of 
separation.‚Äù 
Suppose 
that 
there 
are 
nH0 
H 
type 
agents 
and 
nL0 
L 
type 
agents 
who 
do 
not 
improve 
education 
and 
nH1 
H 
type 
agent 
and 
nL1 
L 
type 
agents 
who 
improve 
education. 
We 
dene 
the 
degree 
of 
separation 
(Dos) 
between 
the 
H 
type 
and 
L 
type 
agents 
as 
follows: 


min(nH0;nL0) 
+ 
min(nH1;nL1)

Dos 
=1 
‚àí 
. 


nH0 
+ 
nL0 
+ 
nH1 
+ 
nL1 
Note 
that, 
if 
either 
all 
agents 
improve 
education 
or 
no 
one 
improves 
education, 
then 
Dos 
reaches 
its 
minimum 
value: 
Dosmin 
= 
max(, 
1 
‚àí 
). 
If 
all 
H 
type 
agents 
improve 
education 
and 
no 
L 
type 
agents 
improve 
education, 
then 
Dos 
reaches 
its 
maximum 
value: 
Dosmax 
= 
1. 
If 
either 
H 
type 
or 
L 
type 
agents 
use 
a 
mixed 
strategy, 
the 
value 
of 
Dos 
is 
somewhere 
in 
between. 
The 
key 
observation 
here 
is 
that, 
the 
higher 
the 
value 
of 
Dos, 
the 
easier 
it 
is 
for 
the 
rm 
to 
dierentiate 
H 
type 
agents 
from 
L 
type 
agents 
using 
the 
causal 
feature. 


We 
now 
discuss 
how 
the 
rm's 
decision 
to 
be 
transparent 
changes 
agents‚Äô 
strategies 
in 
regions 
C1 
through 
C4. 
Note 
that 
transparency 
intensies 
agents‚Äô 
competition 
on 
the 
causal 
feature 
and 
that 
this 
intensied 
competition 
has 
the 
following 
two 
eects. 


1. 
The 
degree 
of 
separation 
between 
H 
type 
and 
L 
type 
agents 
on 
their 
causal 
feature 
changes. 
As 
an 
illustration, 
consider 
region 
C4. 
In 
this 
region, 
when 
the 
rm 
switches 
from 
being 
opaque 
to 
being 
transparent, 
agents‚Äô 
strategies 
also 
switch 
from 
opaque 
scenario 
1 
to 
transparent 
scenario 
2. 
Under 
opaque 
scenario 
1, 
neither 
the 
H 
type 
nor 
the 
L 
type 
agents 
improve 
education, 
whereas 
under 
transparent 
scenario 
2, 
only 
the 
H 
type 
agents 
improve 
education. 
This 
means 
that 
the 
H 
type 
agents 
are 
now 
more 
separated 
from 
the 
L 
type 
agents 
on 
the 
causal 
feature 
(i.e., 
there 
is 
a 
higher 
degree 
of 
separation). 
Similarly, 
it 
can 
also 
be 
veried 
that, 
in 
regions 
C2 
and 
C3, 
the 
two 
types 
of 
agents 
become 
less 
separated 
and, 
in 
region 
C1, 
there 
is 
no 
change 
in 
the 
degree 
of 
separation. 
22 


 


2. 
Agents‚Äô 
average 
value 
on 
the 
causal 
feature 
becomes 
higher 
and 
their 
work 
performance 
increase
s 
(according 
to 
Equation 
2). 
To 
see 
this, 
consider 
region 
C1. 
In 
this 
region, 
when 
the 
rm 
switches 
from 
being 
opaque 
to 
being 
transparent, 
agents‚Äô 
strategies 
also 
switch 
from 
opaque 
scenario 
1 
to 
transparent 
scenario 
3. 
Although 
the 
change 
in 
agents‚Äô 
strategies 
does 
not 
aect 
the 
degree 
of 
separation, 
since 
both 
types 
of 
agents 
improve 
education, 
the 
average 
level 
of 
education 
for 
both 
agent 
types 
increases. 
Similarly, 
in 
region 
C2, 
the 
average 
level 
of 
education 
and, 
thus, 
the 
performance 
level 
of 
both 
types 
of 
agents 
increase. 
In 
region 
C3, 
only 
the 
average 
performance 
of 
the 
L 
type 
agents 
increases, 
whereas 
in 
region 
C4, 
only 
the 
average 
performance 
of 
the 
H 
type 
agents 
increases. 
We 
can 
see 
that, 
in 
regions 
C1 
through 
C4, 
the 
agents‚Äô 
overall 
average 
performance 
always 
increases 
when 
the 
rm 
switches 
from 
being 
opaque 
to 
being 
transparent. 
Since 
the 
rm 
always 
loses 
useful 
information 
from 
the 
correlational 
feature 
that 
helps 
it 
differentiat
e 
between 
the 
two 
types 
of 
agents 
when 
switching 
from 
the 
opaque 
to 
the 
transparent 
algorithm, 
the 
rm's 
decision 
on 
algorithmic 
transparency 
will 
depend 
on 
whether 
the 
above 
two 
eects 
(i.e., 
the 
change 
in 
the 
degree 
of 
separation 
and 
the 
increase 
in 
the 
agents‚Äô 
average 
performance
) 
can 
oset 
the 
negative 
eect 
of 
information 
lost 
on 
the 
correlational 
feature. 
In 
region 
C1, 
even 
though 
the 
degree 
of 
separation 
does 
not 
change, 
both 
the 
H 
type 
and 
L 
type 
agents 
improve 
education, 
and 
the 
rm 
can 
benet 
from 
the 
increase 
in 
the 
average 
agents‚Äô 
performance. 
Whether 
this 
benet 
osets 
the 
negative 
eect 
of 
the 
information 
loss 
on 
the 
correlational 
feature 
depends 
on 
the 
value 
of 
. 
If 
Œ≤ 
is 
large 
enough 
(i.e., 
>1), 
then 
being 
transparent 
is 
preferred 
over 
being 
opaque. 
In 
region 
C2, 
the 
degree 
of 
separation 
decreases 
as 
the 
agents‚Äô 
distribution 
on 
the 
causal 
feature 
changes 
from 
partial 
separation 
to 
pooling. 
However, 
the 
average 
performance 
of 
the 
H 
type 
and 
L 
type 
agents 
increases, 
which 
suggests 
that, 
if 
Œ≤ 
is 
large 
enough, 
the 
rm 
can 
still 
be 
better 
off 
making 
the 
algorithm 
transparent. 
The 
condition 
on 
Œ≤ 
in 
this 
case 
is 
stricter 
than 
in 
region 
C1 
(i.e., 
>2 
>1). 
This 
is 
so 
because 
the 
marginal 
eect 
of 
education 
must 
now 
be 
large 
enough 
to 
oset 
not 
only 
the 
previously 
mentioned 
negative 
eect 
of 
the 
loss 
of 
information 
on 
the 
correlational 
feature, 
but 
also 
the 
worse 
degree 
of 
separation 
on 
the 
causal 
feature. 


In 
region 
C3, 
the 
rm's 
decision 
to 
be 
transparent 
aects 
fewer 
agents 
compared 
to 
in 
region 
C2. 
Switching 
from 
the 
opaque 
to 
the 
transparent 
algorithm 
incentivizes 
all 
L 
type 
agents 
and 
some 
H 
type 
agents 
to 
improve 
education 
in 
region 
C2, 
but 
it 
only 
incentivizes 
some 
L 
type 
agents 
to 
improve 
education 
in 
region 
C3. 
Since 
fewer 
agents 
are 
aected 
by 
the 
rm's 
switching 
from 
the 
opaque 
to 
the 
transparent 
algorithm 
in 
region 
C3 
compared 
with 
region 
C2, 
and 
since 
the 
increase 
in 
the 
agents‚Äô 
average 
performance 
is 
proportional 
to 
the 
number 
of 
agents 
being 
aected, 
a 
larger 
Œ≤ 
is 
needed 
in 
region 
3 
to 
achieve 
the 
same 
level 
of 
average 
performance 
found 
in 
region 
C2. 
This 
is 
why 
3 
>2. 


23 


 




To 
see 
why 
3 
= 
Œ≤ 
(as 
dened 
in 
Assumption 
3), 
note 
that 
in 
region 
C3 
all 
H 
type 
agents 
have 
already 
improved 
education 
in 
the 
opaque 
scenario, 
and, 
only 
some 
L 
type 
agents 
will 
switch 
from 
not 
improving 
to 
improving 
education 
when 
the 
algorithm 
is 
made 
transparent. 
According 
to 
Assumption 
1, 
the 
individual 
productivity 
of 
the 
L 
type 
agents 
cannot 
exceed 
R, 
which 
means 
that 
the 
rm 
will 
not 
benet 
from 
hiring 
any 
L 
type 
agents 
even 
if 
their 
education 
levels 
are 
high. 
Consequently, 
making 
the 
algorithm 
transparent 
has 
a 
negative 
eect 
on 
the 
rm's 
payoff 
since 
the 
degree 
of 
separation 
on 
the 
causal 
feature 
decreases. 
This 
negative 
eect 
is 
partially 
oset 
by 
some 
L 
type 
agents‚Äô 
increased 
investment 
on 
the 
causal 
feature. 
The 
net 
eect 
is 
negative 
and 
its 




magnitude 
is 
decreasing 
in 
. 
When 
Œ≤ 
reaches 
, 
the 
net 
eect 
becomes 
0. 
To 
see 
why, 
consider 
¬Ø 


a 
range 
of 
Œ≤ 
values 
that 
are 
very 
close 
to 
, 
according 
to 
Equation 
D.4 
in 
Appendix 
D.1, 
nearly 
all 
L 
type 
agents 
have 
improved 
education 
in 
the 
opaque 
scenario; 
thus, 
the 
degree 
of 
separation 
is 
already 
near 
zero. 
Therefore, 
making 
the 
algorithm 
transparent 
has 
little 
negative 
eect 
on 
the 
degree 
of 
separation. 
Additionally, 
the 
positive 
eect 
of 
the 
increased 
investment 
on 
the 
causal 
feature 
is 
also 
close 
to 
zero. 
When 
Œ≤ 
= 
, 
both 
eects 
are 
zero. 


In 
region 
C4, 
the 
degree 
of 
separation 
increases 
when 
the 
rm 
switches 
from 
being 
opaque 
to 
being 
transparent. 
In 
fact, 
the 
agents‚Äô 
distribution 
on 
the 
causal 
feature 
changes 
from 
pooling 
to 
perfect 
separation. 
In 
other 
words, 
the 
rm 
now 
can 
perfectly 
separate 
the 
H 
type 
agents 
from 
the 
L 
type 
agents 
based 
on 
the 
causal 
feature 
alone 
without 
needing 
the 
correlational 
feature. 
This 
eect 
in 
itself 
is 
sucient 
to 
oset 
the 
negative 
eect 
of 
information 
lost 
on 
the 
correlational 
feature. 
This 
is 
the 
reason 
why, 
in 
this 
region, 
the 
rm 
prefers 
to 
be 
transparent 
regardless 
of 
the 
value 
of 
. 


The 
following 
theorem 
summarizes 
our 
ndings 
about 
the 
rm's 
decision 
on 
transparency: 


Theorem 
1 
In 
regions 
N1 
through 
N3, 
being 
transparent 
is 
never 
strictly 
better 
than 
being 
opaque. 
In 
regions 
C1 
through 
C4, 
depending 
on 
the 
value 
of 
, 
the 
rm 
may 
prefer 
being 
transparent 
to 
being 
opaque. 
Specically, 
in 
region 
C1, 
the 
rm 
will 
prefer 
to 
be 
transparent 
if 
>1; 
in 
region 
C2, 
the 
rm 
will 
prefer 
to 
be 
transparent 
if 
>2; 
in 
region 
C3, 
the 
rm 
will 
never 
prefer 
to 
be 
transparent; 
and, 
in 
region 
C4, 
the 
rm 
will 
prefer 
to 
be 
transparent 
regardless 
of 
the 
value 
of 
. 


4.4 
The 
Eect 
of 
the 
Predictive 
Power 
of 
the 
Correlational 
Feature 
() 
and 
the 
Fraction 
of 
High-Talent 
Agents 
() 
We 
have 
shown 
that 
the 
rm 
will 
be 
strictly 
better 
off 
making 
the 
algorithm 
transparent 
if 
the 
(CH 
;CL) 
pair 
lands 
either 
in 
region 
C4, 
or 
in 
regions 
C1 
or 
C2 
with 
some 
additional 
conditions 
on 
. 
In 
region 
C4, 
the 
transparent 
algorithm 
is 
preferred 
regardless 
of 
, 
, 
and 
Œ∏ 
since 
removing 
the 
correlational 
feature 
changes 
the 
agents‚Äô 
behavior 
on 
the 
causal 
feature 
from 
pooling 
to 
perfect 
separation, 
and 
perfect 
separation 
is 
the 
case 
where 
the 
rm 
receives 
the 
maximum 
prot. 
In 
regions 


24 


 


C1 
and 
C2, 
the 
main 
force 
that 
makes 
the 
transparent 
algorithm 
preferable 
is 
the 
agents‚Äô 
increased 
average 
level 
on 
the 
causal 
feature. 
As 
long 
as 
Œ≤ 
exceeds 
the 
threshold 
1(2), 
the 
increased 
level 
on 
the 
causal 
feature 
will 
have 
a 
large 
enough 
positive 
eect 
on 
work 
performance 
to 
make 
the 
rm 
better 
o. 
We 
now 
examine 
how 
the 
thresholds 
on 
Œ≤ 
changes 
when 
Œª 
or 
Œ∏ 
changes. 


Taking 
the 
derivate 
of 
the 
expressions 
of 
1 
and 
2 
in 
Equations 
10 
and 
11 
with 
respect 
to 
Œª 
yields 
the 
following: 


@1 


= 
ÙÄÄÄ2R 
+ 
Œ± 
+ 
R. 
(13)

@Œª 


@2 
R(Œ± 
‚àí 
R) 


= 
. 
(14)

@Œª 
(2R 
‚àí 
R 
‚àí 
)2 


Both 
of 
these 
derivatives 
are 
greater 
than 
0 
in 
the 
parameter 
ranges 
that 
we 
consider 
(i.e., 
those 
given 
by 
Assumptions 
1, 
2, 
and 
3). 
This 
means 
that, 
within 
each 
region, 
as 
Œª 
becomes 
larger, 
a 
higher 
value 
of 
Œ≤ 
is 
needed 
to 
make 
the 
transparent 
algorithm 
preferable. 
This 
is 
because 
a 
larger 
Œª 
implies 
that 
more 
information 
is 
contained 
in 
the 
correlational 
feature, 
so 
a 
higher 
causal 
eect 
is 
needed 
to 
oset 
the 
loss 
of 
information 
from 
the 
correlational 
feature 
under 
the 
transparent 
algorithm. 
However, 
the 
eect 
of 
Œª 
on 
algorithmic 
transparency 
is 
not 
this 
straightforward 
since, 
apart 
from 
the 
equilibrium 
payo, 
Œª 
can 
also 
determine 
the 
kind 
of 
strategy 
combination 
that 
can 
be 
sustained 
as 
an 
equilibrium 
given 
a 
(CH 
,CL) 
pair 
(i.e., 
the 
regions‚Äô 
shapes 
in 
Figure 
3 
will 
change 
as 
Œª 
changes). 
Consider 
the 
region 
just 
beneath 
the 
dividing 
line 
of 
regions 
N2 
and 
C4: 
as 
Œª 
increases, 
it 
changes 
from 
belonging 
to 
region 
N2 
to 
belonging 
to 
C4. 
This 
means 
that 
the 
rm 
will 
prefer 
the 
opaque 
algorithm 
when 
faced 
with 
a 
small 
Œª 
but 
prefer 
a 
transparent 
algorithm 
when 
faced 
with 
a 
large 
. 
Although 
it 
appears 
counter-intuitive, 
it 
can 
be 
explained 
as 
follows. 
When 
Œª 
is 
small, 
making 
the 
algorithm 
transparent 
is 
not 
eective 
enough 
to 
change 
the 
agents‚Äô 
behavior 
on 
the 
causal 
feature. 
However, 
when 
Œª 
is 
large, 
the 
agents‚Äô 
investment 
on 
the 
causal 
feature 
will 
increase 
drastically, 
and 
the 
rm 
is 
able 
to 
hire 
agents 
who 
are 
on 
average 
more 
productive 
under 
the 
transparent 
scenario 
than 
under 
the 
opaque 
scenario. 


The 
following 
proposition 
summarizes 
our 
ndings 
about 
how 
Œª 
aects 
the 
rm's 
decision 
on 
transparency: 


Proposition 
1 
An 
increase 
in 
Œª 
has 
the 
following 
eects 
on 
the 
rm's 
decision 
on 
transparency: 


1. 
The 
area 
of 
regions 
C1, 
C2, 
and 
C4 
increases, 
which 
means 
that 
the 
transparent 
algorithm 
is 
preferred 
under 
more 
(CH 
,CL) 
value 
pairs. 
2. 
Within 
regions 
C1 
and 
C2, 
the 
conditions 
on 
Œ≤ 
to 
make 
the 
transparent 
algorithm 
preferred 
to 
the 
opaque 
algorithm 
become 
stricter 
(i.e. 
a 
larger 
Œ≤ 
is 
needed). 
25 


 


We 
now 
discuss 
the 
impact 
of 
Œ∏ 
on 
algorithmic 
transparency. 
Taking 
the 
derivative 
of 
the 
expression
s 
of 
1 
and 
2 
in 
Equations 
10 
and 
11 
with 
respect 
to 
Œ∏ 
yields: 


@1 


= 
ÙÄÄÄ2R 
+ 
Œ± 
+ 
R 
‚àí 
. 
(15)

@Œ∏ 
@2 


=0. 
(16)

@Œ∏ 


It 
can 
be 
shown 
that 
@Œ≤ 
@Œ∏ 
1 
is 
smaller 
than 
0 
in 
the 
parameter 
ranges 
that 
we 
consider. 
This 
means 
that, 
in 
region 
C1, 
as 
the 
proportion 
of 
the 
H 
type 
agents 
increases, 
the 
conditions 
on 
Œ≤ 
to 
make 
algorithmic 
transparency 
more 
desirable 
become 
milder. 
This 
is 
because 
the 
degree 
of 
separation 
on 
the 
causal 
feature 
in 
region 
C1 
does 
not 
change 
(from 
pooling 
at 
0 
to 
pooling 
at 
1). 
The 
only 
negative 
eect 
of 
algorithmic 
transparency 
is 
the 
loss 
of 
the 
correlational 
feature. 
When 
Œ∏ 
is 
high, 
most 
agents 
are 
of 
H 
type, 
and 
losing 
the 
correlational 
feature 
is 
less 
harmful 
to 
the 
rm 
(because 
there 
are 
fewer 
L 
type 
agents 
who 
can 
be 
mistakenly 
hired 
when 
the 
correlational 
feature 
is 
lost). 
Thus, 
a 
smaller 
Œ≤ 
is 
needed 
to 
oset 
this 
negative 
eect. 
In 
region 
C2, 
however, 
Œ∏ 
has 
no 
in
uence 
on 
the 
rm's 
decision 
on 
algorithmic 
transparency. 
This 
is 
because, 
in 
region 
C2, 
there 
are 
two 
negative 
eects 
of 
algorithmic 
transparency: 
the 
loss 
of 
the 
correlational 
feature 
and 
a 
smaller 
degree 
of 
separation 
on 
the 
causal 
feature. 
A 
higher 
Œ∏ 
will 
mitigate 
the 
rst 
eect 
(as 
explained 
above) 
and 
amplify 
the 
second 
(more 
H 
type 
agents 
will 
be 
left 
out). 
Overall, 
Œ∏ 
does 
not 
aect 
the 
value 
of 
Œ≤ 
needed 
for 
making 
the 
transparent 
algorithm 
preferable. 


The 
following 
proposition 
summarizes 
our 
ndings 
about 
how 
Œ∏ 
aects 
the 
rm's 
decision 
on 
transparency: 


Proposition 
2 
In 
region 
C1, 
a 
higher 
Œ∏ 
will 
increase 
the 
rm's 
incentive 
to 
make 
the 
algorithm 
transparent. 
In 
other 
regions, 
Œ∏ 
has 
no 
impact 
on 
the 
rm's 
decision 
on 
algorithmic 
transparency. 


4.5 
Agents‚Äô 
Welfare 
In 
Section 
4.3, 
we 
have 
specied 
conditions 
under 
which 
the 
transparent 
algorithm 
will 
yield 
a 
strictly 
higher 
payoff 
to 
the 
rm 
than 
the 
opaque 
algorithm. 
We 
will 
next 
investigate 
the 
impact 
of 
algorithmic 
transparency 
on 
the 
agents‚Äô 
welfare. 


The 
total 
payoff 
across 
all 
agents 
in 
the 
equilibrium 
is 
summarized 
in 
the 
following 
lemma. 


Lemma 
3 
For 
each 
equilibrium 
outcome 
shown 
in 
Figure 
1 
(for 
the 
opaque 
scenario) 
and 
Figure 


26 


 


2 
(for 
the 
transparent 
scenario), 
the 
corresponding 
total 
payoff 
for 
all 
agents 
is 
given 
by 


agentsO1 
=(Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
))R 


agentsO2 
= 
(R 
‚àí 
CH 
) 


agentsO3 
=(Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
))R 
‚àí 
CH 
Œ∏ 
‚àí 
CL(1 
‚àí 
) 


(Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
))(R 
‚àí 
CH 
)agentsO4 
= 


Œª 


(2RŒª 
‚àí 
R 
‚àí 
CH 
Œª 
‚àí 
CLŒª 
+ 
CL)Œ∏ 
agentsO5 
= 


Œª 


agentsT 
1 
=0 


agentsT 
2 
= 
(R 
‚àí 
CH 
) 


agentsT 
3 
= 
R 
‚àí 
CH 
Œ∏ 
‚àí 
CL(1 
‚àí 
), 


where 
agentsOi 
denotes 
the 
agents‚Äô 
total 
payoff 
in 
case 
i 
of 
the 
opaque 
scenario 
and 
agentsTi 
denotes 
the 
agents‚Äô 
total 
payoff 
in 
case 
i 
of 
the 
transparent 
scenario. 


As 
previously 
discussed, 
Figure 
3 
shows 
how 
the 
agents‚Äô 
behavior 
changes 
on 
the 
causal 
feature 
when 
the 
algorithm 
is 
made 
transparent. 
First, 
we 
consider 
the 
three 
regions 
where 
the 
agents‚Äô 
behavior 
on 
the 
causal 
feature 
does 
not 
change 
(regions 
N1, 
N2, 
and 
N3). 
In 
directly 
comparing 
the 
agents‚Äô 
payoff 
in 
the 
equilibrium, 
we 
have 
the 
following 
observations: 


‚Ä¢ 
In 
region 
N1, 
the 
agents 
play 
the 
strategies 
in 
case 
1 
in 
both 
the 
opaque 
and 
transparent 
scenarios. 
agentsO1 
> 
agentsT 
1 
. 
Therefore, 
the 
agents 
will 
receive 
a 
higher 
total 
payoff 
under 
the 
opaque 
algorithm 
in 
this 
region. 
‚Ä¢ 
In 
region 
N2, 
the 
agents 
play 
the 
strategies 
in 
case 
2 
in 
both 
the 
opaque 
and 
transparent 
scenarios. 
Since 
agentsO2 
=agentsT 
2 
, 
in 
this 
region, 
the 
agents 
are 
indierent 
to 
whether 
the 
algorithm 
is 
opaque 
or 
transparent. 
‚Ä¢ 
In 
region 
N3, 
the 
agents 
play 
the 
strategies 
in 
case 
3 
in 
both 
the 
opaque 
and 
transparent 
scenarios. 
agentsO3 
< 
agentsT 
3 
. 
Therefore, 
the 
agents 
will 
receive 
a 
higher 
payoff 
under 
the 
transparent 
algorithm 
in 
this 
region. 
In 
regions 
N1, 
N2, 
and 
N3, 
the 
agents‚Äô 
behavior 
on 
the 
causal 
feature 
does 
not 
change. 
Since 
improving 
the 
correlational 
feature 
is 
assumed 
to 
be 
costless, 
the 
agents‚Äô 
total 
cost 
stays 
the 
same 
before 
and 
after 
the 
algorithm 
is 
made 
transparent. 
The 
only 
thing 
that 
varies 
is 
the 
benet 
they 
can 
obtain 
under 
the 
rm's 
hiring 
strategy. 
In 
region 
N1, 
more 
agents 
will 
be 
hired 
under 
the 
opaque 
algorithm 
(a 
Œª 
portion 
of 
the 
H 
type 
agents 
and 
a 
1 
‚àí 
Œª 
portion 
of 
the 
L 
type 
agents 
are 


27 


 


hired 
under 
the 
opaque 
algorithm 
but 
no 
one 
will 
be 
hired 
under 
the 
transparent 
algorithm). 
In 
region 
N3, 
more 
agents 
will 
be 
hired 
under 
the 
transparent 
algorithm 
(a 
Œª 
portion 
of 
the 
H 
type 
agents 
and 
a 
1 
‚àí 
Œª 
portion 
of 
the 
L 
type 
agents 
are 
hired 
under 
the 
opaque 
algorithm 
and 
everyone 
will 
be 
hired 
under 
the 
transparent 
algorithm). 
In 
region 
N2, 
the 
same 
number 
of 
agents 
will 
be 
hired 
regardless 
of 
whether 
the 
algorithm 
is 
opaque 
or 
transparent 
(only 
the 
H 
type 
agents 
will 
be 
hired). 


Next, 
we 
consider 
the 
four 
regions 
where 
agents‚Äô 
behavior 
on 
the 
causal 
feature 
changes 
after 
the 
algorithm 
is 
made 
transparent 
(regions 
C1, 
C2, 
C3, 
and 
C4). 
By 
directly 
comparing 
the 
agents‚Äô 
payos 
in 
the 
equilibrium, 
we 
obtain 
the 
following 
observations: 


‚Ä¢ 
In 
region 
C1, 
the 
agents‚Äô 
strategies 
and 
their 
total 
payoff 
change 
from 
opaque 
scenario 
1 
to 
transparent 
scenario 
3. 
agentsT 
3 
‚â§ 
agentsO1 
iff 
CH 
Œ∏ 
+ 
CL(1 
‚àí 
) 
‚â• 
(1 
‚àí 
Œ∏ 
‚àí 
(1 
‚àí 
)(1 
‚àí 
))R. 
(17) 


The 
smallest 
possible 
value 
for 
the 
left-hand 
side 
(LHS) 
of 
the 
inequality 
is 
reached 
when 
a(CH 
,CL) 
pair 
lands 
at 
the 
lower 
left 
corner 
in 
region 
C1, 
or 
in 
other 
words, 
when 
CH 
= 
(1ÙÄÄÄ)R 
and 
CL 
= 
R. 
It 
can 
further 
be 
shown 
that 
this 
smallest 
value 
equals 
the 
right-hand 
side 
(RHS). 
Thus, 
Equation 
17 
is 
satised 
for 
any 
(CH 
,CL) 
pair 
in 
region 
C1. 
In 
this 
region, 
the 
transparent 
algorithm 
will 
give 
the 
agents 
a 
lower 
total 
payoff 
compared 
with 
the 
opaque 
algorithm. 


‚Ä¢ 
In 
region 
C2, 
the 
agents‚Äô 
strategies 
and 
their 
total 
payoff 
change 
from 
opaque 
scenario 
4 
to 
transparent 
scenario 
3. 
agentsT 
3 
‚â• 
agentsO4 
iff 
1 
‚àí 
Œª 
1 


CH 
‚àí 
CL 
‚â• 
( 
‚àí 
2)R. 
(18)

Œª 
The 
smallest 
possible 
value 
for 
the 
LHS 
of 
the 
inequality 
is 
reached 
when 
a 
(CH 
,CL) 
pair 
lands 
at 
the 
lower 
right 
corner 
in 
region 
C2, 
or 
in 
other 
words, 
when 
CH 
= 
(1 
‚àí 
)R 
and 
CL 
= 
R. 
It 
can 
further 
be 
shown 
that 
this 
smallest 
value 
equals 
the 
RHS. 
Thus, 
Equation 
18 
is 
satised 
for 
any 
(CH 
,CL) 
pair 
in 
region 
C2. 
In 
this 
region, 
the 
transparent 
algorithm 
will 
give 
agents 
a 
higher 
payoff 
compared 
with 
the 
opaque 
algorithm. 


‚Ä¢ 
In 
region 
C3, 
the 
agents‚Äô 
strategies 
and 
their 
total 
payoff 
change 
from 
opaque 
scenario 
5 
to 
transparent 
scenario 
3. 
agentsT 
3 
‚â• 
agentsO5 
iff 
Œ∏ 


(2Œ∏ 
ÙÄÄÄ‚àí 
1)CL 
‚â• 
(2Œ∏ 
ÙÄÄÄ‚àí 
1)R. 
(19)

Œª 
Since 
2Œ∏ 
‚àí 
Œ∏ 
‚àí 
1 
‚â§ 
0, 
and 
CL 
‚â§ 
R 
in 
region 
C3, 
Equation 
19 
is 
satised 
for 
any 
(CH 
,CL)

Œª 


pair 
in 
region 
C3. 
In 
this 
region, 
the 
transparent 
algorithm 
will 
give 
agents 
a 
higher 
payoff 
compared 
with 
the 
opaque 
algorithm. 


28 


 


‚Ä¢ 
In 
region 
C4, 
the 
agents‚Äô 
strategies 
and 
their 
total 
payoff 
change 
from 
opaque 
scenario 
1 
to 
transparent 
scenario 
2. 
agentsT 
2 
‚â§ 
agentsO1 
iff 
(R 
‚àí 
CH 
) 
‚â§ 
(Œ∏ 
+ 
(1 
‚àí 
)(1 
‚àí 
))R. 
(20) 


The 
largest 
possible 
value 
for 
the 
LHS 
is 
reached 
when 
a 
(CH 
,CL) 
pair 
lands 
at 
the 
lower 
bound 
of 
region 
C4, 
or 
in 
other 
words, 
when 
CH 
= 
(1ÙÄÄÄ)R. 
It 
can 
be 
shown 
that 
this 
largest 
value 
is 
smaller 
than 
the 
RHS. 
Thus, 
Equation 
20 
is 
satised 
for 
any 
(CH 
,CL) 
pair 
in 
region 
C4. 
In 
this 
region, 
the 
transparent 
algorithm 
will 
give 
agents 
a 
lower 
payoff 
compared 
with 
the 
opaque 
algorithm. 


The 
following 
theorem 
summarizes 
our 
ndings 
regarding 
the 
agents‚Äô 
welfare 
under 
the 
opaque 
and 
transparent 
algorithms. 


Theorem 
2 
Whether 
the 
agents 
are 
better 
off 
under 
either 
the 
opaque 
or 
the 
transparent 
algorithm 
depends 
on 
the 
values 
of 
(CH 
,CL), 
and 
this 
dependence 
is 
shown 
in 
Figure 
4. 
In 
regions 
N3, 
C2 
and 
C3, 
the 
agents‚Äô 
welfare 
is 
higher 
under 
the 
transparent 
algorithm. 
In 
regions 
N1, 
C1 
and 
C4, 
the 
agents‚Äô 
welfare 
is 
higher 
under 
the 
opaque 
algorithm. 
In 
region 
N2, 
the 
agents‚Äô 
welfare 
is 
not 
aected 
by 
algorithmic 
transparency. 



Figure 
4: 
Comparison 
of 
agents‚Äô 
welfare 
in 
the 
transparent 
and 
opaque 
scenarios 


The 
intuition 
behind 
Theorem 
2 
can 
be 
explained 
as 
follows. 
The 
pattern 
here 
is 
that 
the 
agents 
as 
a 
whole 
will 
prefer 
the 
opaque 
algorithm 
when 
CH 
and 
CL 
are 
large. 
They 
will 
prefer 
the 
transparent 
algorithm 
when 
CH 
and 
CL 
are 
small, 
and 
will 
be 
indierent 
when 
CL 
is 
large 


29 


 


but 
CH 
is 
small. 
Making 
the 
algorithm 
transparent 
may 
force 
the 
agents 
to 
invest 
in 
the 
causal 
feature. 
Of 
course, 
they 
can 
also 
benet 
from 
the 
increased 
investment 
in 
the 
causal 
feature 
(i.e., 
a 
higher 
chance 
of 
being 
hired). 
The 
cost 
of 
this 
investment 
increases 
with 
CH 
and 
CL, 
but 
the 
benet 
does 
not 
vary 
with 
CH 
and 
CL. 
Consequently, 
the 
agents 
will 
be 
worse 
(better) 
off 
under 
the 
transparent 
algorithm 
if 
CH 
and 
CL 
are 
large 
(small). 


Comparing 
Theorem 
1 
with 
Theorem 
2, 
we 
nd 
that 
the 
rm's 
and 
agents‚Äô 
interests 
con
ict 
in 
some 
regions. 
For 
example, 
in 
regions 
N3 
and 
C3, 
the 
transparent 
algorithm 
will 
give 
the 
agents 
a 
higher 
payo, 
but 
the 
rm 
prefers 
the 
opaque 
algorithm. 
In 
region 
C4, 
the 
opaque 
algorithm 
will 
give 
the 
agents 
a 
higher 
payoff 
but 
the 
rm 
prefers 
the 
transparent 
algorithm. 
In 
regions 
C1 
and 
C2, 
whether 
there 
is 
a 
con
ict 
of 
interest 
between 
the 
rm 
and 
the 
agents 
depends 
on 
the 
value 
of 
. 


In 
region 
C2, 
if 
the 
condition 
of 
>2 
is 
satised, 
both 
the 
rm 
and 
the 
agents 
would 
prefer 
the 
transparent 
algorithm 
over 
the 
opaque 
algorithm. 
In 
other 
words, 
making 
the 
algorithm 
transparent 
will 
Pareto 
dominate 
keeping 
the 
algorithm 
opaque. 


The 
Stackelberg 
Model 


Our 
analysis 
in 
Section 
4 
has 
focused 
on 
the 
setting 
where 
the 
rm 
does 
not 
have 
commitment 
power. 
As 
noted 
in 
Section 
1, 
this 
is 
useful 
to 
highlight 
the 
insight 
that 
the 
rm 
could 
still 
prefer 
the 
transparent 
algorithm 
to 
the 
opaque 
algorithm 
even 
in 
the 
absence 
of 
the 
`rst 
mover 
advantage'. 
In 
this 
section, 
we 
consider 
the 
case 
of 
`full 
transparency‚Äô 
where 
the 
rm 
publishes 
all 
details 
of 
the 
hiring 
algorithm, 
including 
the 
features 
being 
used 
and 
the 
hiring 
strategy, 
and 
the 
rm 
has 
commitment 
power 
on 
the 
published 
algorithm. 
In 
this 
case, 
the 
Stackelberg 
model 
would 
be 
a 
more 
appropriate 
model 
when 
analyzing 
the 
transparent 
scenario. 
Our 
main 
objective 
in 
this 
section 
is 
to 
show 
that 
the 
key 
insights 
in 
the 
previous 
section 
will 
only 
be 
further 
strengthened 
when 
the 
rm 
switches 
from 
using 
partial 
transparency 
to 
using 
full 
transparency. 


We 
start 
with 
discussing 
the 
rm's 
equilibrium 
payos. 
The 
exact 
payos 
for 
all 
cases 
are 
summarized 
in 
Lemma 
4. 


Lemma 
4 
The 
equilibrium 
outcome 
depends 
on 
the 
values 
of 
(CH 
;CL), 
and 
this 
dependence 
is 
shown 
in 
Figure 
5. 
The 
corresponding 
total 
payos 
for 
the 
rm 
are 
given 
by 


firmFT 
1 
=0 


firmFT 
2 
= 
(Œ± 
+ 
Œ≤ 
‚àí 
R) 


firmFT 
3 
= 
(Œ± 
+ 
) 
+ 
(1 
‚àí 
)Œ≤ 
‚àí 
R 
CL(Œ± 
+ 
Œ≤ 
‚àí 
R) 


= 
;

firmFTS 


R 


30 


 


where 
firmFTi 
denotes 
the 
rm's 
total 
payoff 
in 
case 
i, 
i 
2f1, 
2, 
3;Sg. 



Figure 
5: 
Equilibrium 
outcome 
in 
the 
transparent 
scenario 
in 
the 
Stackelberg 
model 


Compared 
with 
the 
payos 
discussed 
in 
Lemma 
2, 
we 
can 
see 
that 
the 
rm 
always 
gets 
a 
weakly 
higher 
payoff 
in 
the 
full 
transparency 
scenario 
than 
in 
the 
partial 
transparency 
scenario. 
This 
is 
so 
because, 
in 
the 
full 
transparency 
scenario, 
the 
rm 
can 
always 
commit 
to 
the 
equilibrium 
hiring 
strategy 
in 
the 
partial 
transparency 
scenario 
and 
achieve 
the 
same 
payoff 
as 
in 
the 
partial 
transparency 
scenario, 
which 
sets 
a 
lower 
bound 
on 
the 
rm's 
payo. 
Given 
that 
the 
rm's 
payoff 
is 
weakly 
higher 
in 
the 
full 
transparency 
scenario 
than 
in 
the 
partial 
transparency 
scenario, 
naturally, 
the 
conditions 
under 
which 
full 
transparency 
is 
preferred 
over 
opacity 
are 
less 
strict 
than 
the 
conditions 
under 
which 
partial 
transparency 
is 
preferred 
over 
opacity. 
In 
particular, 
it 
can 
be 
shown 
that 
there 
are 
conditions 
under 
which 
the 
rm 
prefers 
full 
transparency, 
but 
not 
partial 
transparency, 
over 
opacity. 
These 
conditions 
are 
summarized 
in 
Proposition 
3. 


Proposition 
3 
The 
rm 
prefers 
full 
transparency 
but 
not 
partial 
transparency 
over 
opacity 
if 
and 
only 
if 
any 
of 
the 
following 
conditions 
is 
satised: 


CH

‚Ä¢ 
Value 
pair 
(C
R 
L 
, 
) 
falls 
in 
region 
C1 
and 
Œ≤ 
‚â§ 
1;
R 


CH

‚Ä¢ 
Value 
pair 
(CL 
) 
falls 
in 
region 
C2 
and 
Œ≤ 
‚â§ 
2 
and 
CL 
>CC2;
R 
, 
RL 
CH

‚Ä¢ 
Value 
pair 
(C
R 
L 
, 
) 
falls 
in 
region 
C3 
and 
CL 
>CC3;
RL 


CH

‚Ä¢ 
Value 
pair 
(CL 
) 
falls 
in 
region 
N3 
and 
CL 
>CN3;
R 
, 
RL 
R2(1ÙÄÄÄ)(1ÙÄÄÄ) 
(2ÙÄÄÄ1)RŒ∏ 
R(1ÙÄÄÄ)(1ÙÄÄÄ)(ÙÄÄÄR)

where 
CLC2 
= 
R 
‚àí 
;CLC3 
= 
;CLN3 
= 
RŒ∏ 
+ 
.

(ÙÄÄÄR)Œª 
Œª 
(+ÙÄÄÄR) 


31 


 


In 
Section 
4.4, 
we 
discussed 
how 
the 
values 
of 
Œ∏ 
and 
Œª 
aect 
the 
rm's 
preference 
on 
algorithmi
c 
transparency 
under 
partial 
transparency. 
Most 
of 
the 
results 
in 
Section 
4.4 
will 
not 
change 
qualitatively 
when 
full 
transparency 
is 
considered. 
The 
detailed 
discussions 
and 
analysis 
can 
be 
found 
in 
Appendix 
C.3. 
In 
what 
follows, 
we 
brie
y 
discuss 
the 
key 
ndings: 


1. 
In 
general, 
when 
Œª 
is 
large, 
the 
conditions 
under 
which 
the 
transparent 
algorithm 
is 
preferre
d 
over 
the 
opaque 
algorithm 
become 
stricter. 
However, 
there 
are 
also 
(CH 
;CL) 
pairs 
under 
which 
the 
rm 
switches 
from 
never 
preferring 
transparency 
to 
possibly 
preferring 
transparenc
y 
(depending 
on 
the 
values 
of 
the 
other 
parameters) 
when 
Œª 
gets 
larger. 
This 
nding 
is 
consistent 
with 
that 
under 
the 
partial 
transparency 
model. 
2. 
In 
general, 
when 
Œ∏ 
increases, 
there 
will 
be 
more 
(CH 
;CL) 
pairs 
under 
which 
the 
rm 
will 
prefer 
the 
transparent 
algorithm 
over 
the 
opaque 
algorithm. 
This 
nding 
is 
unique 
under 
full 
transparency 
since, 
under 
partial 
transparency, 
Œ∏ 
does 
not 
aect 
the 
rm's 
decision 
on 
algorithmic 
transparency 
in 
most 
regions. 
This 
dierence 
is 
due 
to 
the 
fact 
that 
in 
some 
regions, 
the 
rm 
can 
get 
a 
higher 
payoff 
under 
full 
transparency 
than 
under 
partial 
transparency
, 
and 
the 
payoff 
dierence 
is 
increasing 
in 
. 
In 
other 
words, 
when 
Œ∏ 
is 
large, 
the 
rm's 
payoff 
in 
the 
full 
transparency 
scenario 
will 
increase 
by 
a 
larger 
amount 
compared 
to 
the 
partial 
transparency 
scenario, 
which 
gives 
the 
rm 
more 
incentive 
to 
make 
the 
algorithm 
transparent. 
Next, 
we 
discuss 
how 
agents‚Äô 
welfare 
is 
aected 
by 
full 
transparency. 
Lemma 
5 
summarizes 
the 
agents‚Äô 
equilibrium 
payoff 
in 
the 
full 
transparency 
scenario. 


Lemma 
5 
In 
the 
full 
transparency 
scenario, 
for 
each 
equilibrium 
outcome 
shown 
in 
Figure 
5 
the 
corresponding 
total 
payoff 
for 
agents 
is 
given 
by 


agentsFT 
1 
=0 


agentsFT 
2 
= 
(R 
‚àí 
CH 
) 


agentsFT 
3 
= 
R 
‚àí 
CH 
Œ∏ 
‚àí 
CL(1 
‚àí 
) 


agentsFTS 
= 
(CL 
‚àí 
CH 
), 


where 
agentsFTi 
denotes 
the 
agents‚Äô 
total 
payoff 
in 
case 
i 
of 
the 
full 
transparency 
scenario. 


Comparing 
the 
agents‚Äô 
payoff 
in 
the 
full 
transparency 
scenario 
with 
that 
in 
the 
partial 
transparenc
y 
scenario, 
we 
can 
see 
that 
agents‚Äô 
welfare 
becomes 
strictly 
lower 
in 
region 
S 
and 
stays 
the 
same 
in 
all 
other 
regions. 
The 
nding 
is 
interesting 
as 
it 
shows 
that, 
as 
the 
level 
of 
transparency 
in 
the 
algorithm 
increases 
(i.e., 
from 
partial 
to 
full 
transparency), 
the 
rm 
may 
become 
better 


32 


 


off 
while 
the 
agents 
may 
become 
worse 
o. 
It 
strengthens 
our 
conclusion 
in 
Section 
4.5 
that, 
in 
most 
cases 
when 
the 
rm 
prefers 
transparency, 
the 
agents 
will 
be 
worse 
off 
under 
the 
transparent 
algorithm. 
The 
results 
presented 
in 
this 
section 
are 
consistent 
with 
recent 
research 
in 
both 
economic
s 
and 
computer 
science, 
which 
shows 
that 
compared 
to 
the 
case 
where 
the 
decision 
maker 
does 
not 
have 
commitment 
power, 
under 
the 
Stackelberg 
competition 
where 
the 
decision 
maker 
has 
commitment 
power, 
the 
decision 
maker 
can 
receive 
a 
higher 
payoff 
but 
at 
a 
cost 
of 
decreasing 
agents‚Äô 
welfare 
(Frankel 
and 
Kartik, 
2019, 
Milli 
et 
al., 
2019). 


Extensions 


In 
this 
section, 
we 
analyze 
several 
model 
extensions 
that 
relax 
some 
of 
the 
assumptions 
in 
our 
main 
model 
described 
in 
Section 
3. 
Due 
to 
space 
limitation, 
we 
focus 
our 
discussion 
on 
the 
impact 
of 
algorithmic 
transparency 
on 
the 
rm, 
and 
brie
y 
report 
the 
key 
ndings 
here. 
The 
detailed 
discussions 
and 
proofs 
can 
be 
found 
in 
Appendix 
B. 
Overall, 
the 
results 
show 
that 
relaxing 
those 
assumptions 
does 
not 
alter 
the 
main 
results 
and 
insights 
of 
the 
study. 


First, 
we 
relax 
the 
assumption 
that 
the 
cost 
of 
improving 
the 
correlational 
feature 
is 
close 
to 
zero. 
We 
consider 
the 
case 
where 
the 
cost 
of 
improving 
the 
correlation 
feature 
is 
substantial: 
ch 
for 
the 
H 
type 
agents 
and 
cl 
for 
the 
L 
type 
agents 
where 
cl 
‚â• 
ch 
> 
0. 
We 
nd 
that 
under 
this 
condition, 
the 
rm's 
equilibrium 
payoff 
in 
the 
transparent 
scenario 
weakly 
increases 
while 
the 
rm's 
equilibrium 
payoff 
in 
the 
opaque 
scenario 
is 
not 
aected. 
Thus 
the 
main 
result 
of 
the 
paper, 
under 
certain 
conditions, 
algorithmic 
transparency 
benets 
the 
rm, 
is 
strengthened. 
Second, 
we 
address 
the 
xed 
wage 
assumption 
by 
allowing 
the 
rm 
to 
strategically 
choose 
the 
wage. 
After 
solving 
this 
extended 
model, 
we 
nd 
that 
the 
rm 
in 
the 
transparent 
scenario 
is 
able 
to 
set 
a 
lower 
wage 
than 
in 
the 
opaque 
scenario 
without 
needing 
to 
worry 
about 
worsening 
the 
degree 
of 
separation 
between 
the 
two 
types 
of 
agents 
on 
the 
causal 
feature. 
Since 
the 
agents‚Äô 
productivity 
(aected 
by 
Œ± 
and 
) 
is 
assumed 
to 
be 
unaected 
by 
wage, 
the 
rm 
will 
benet 
more 
from 
endogenizing 
the 
wage 
in 
the 
transparent 
scenario 
than 
in 
the 
opaque 
scenario; 
thus, 
the 
rm's 
preference 
for 
the 
transparent 
algorithm 
will 
be 
strengthened. 
Lastly, 
we 
show 
that 
even 
when 
the 
agents 
severely 
underestimate 
or 
overestimate 
the 
prediction 
power 
of 
the 
correlational 
feature, 
, 
the 
regions 
in 
which 
the 
rm 
prefers 
the 
transparent 
algorithm 
will 
not 
vanish. 


33 


 


7 
Conclusion 


7.1 
Summary 
of 
Results 
In 
this 
paper, 
we 
studied 
how 
rm 
and 
agent 
welfare 
is 
aected 
by 
algorithmic 
transparency. 
We 
allowed 
the 
agents 
to 
be 
strategic 
such 
that 
they 
can 
invest 
in 
their 
causal 
and 
correlational 
features 
to 
increase 
their 
chances 
of 
being 
hired 
in 
response 
to 
the 
rm's 
algorithm. 
We 
also 
investigated 
how 
the 
predictive 
power 
of 
the 
correlational 
feature, 
the 
market 
composition 
in 
terms 
of 
the 
fraction 
of 
H 
type 
agents, 
and 
the 
impact 
of 
the 
causal 
feature 
on 
agent 
productivity 
aect 
the 
rm's 
decision 
to 
make 
their 
algorithm 
transparent 
or 
opaque. 


As 
a 
rst 
result, 
we 
identied 
a 
broad 
set 
of 
conditions 
under 
which 
the 
rm 
would 
be 
better 
off 
with 
algorithmic 
transparency 
than 
opacity. 
Our 
second 
result 
is 
that 
the 
agents 
may 
not 
always 
be 
better 
off 
under 
algorithmic 
transparency. 
Our 
third 
result 
is 
that, 
even 
when 
the 
correlational 
feature 
has 
high 
predictive 
power 
in 
the 
opaque 
scenario, 
the 
rm 
could 
still 
be 
better 
off 
making 
the 
algorithm 
transparent. 
Our 
nal 
result 
is 
that, 
when 
the 
fraction 
of 
H 
type 
agents 
on 
the 
market 
is 
high, 
the 
rm 
would 
be 
better 
off 
by 
making 
its 
algorithm 
transparent. 
We 
also 
provided 
several 
extensions 
to 
our 
main 
model. 
After 
relaxing 
several 
assumptions 
and 
considering 
several 
model 
alternatives, 
we 
found 
that 
the 
main 
insights 
of 
the 
paper 
do 
not 
change. 


7.2 
Implications 
for 
Managers 
Our 
paper 
shows 
that 
managers 
using 
machine 
learning 
models 
for 
decision 
making 
could 
be 
better 
off 
by 
making 
their 
algorithms 
transparent. 
Algorithmic 
transparency 
does 
not 
always 
mean 
a 
loss 
of 
predictive 
power. 
In 
some 
cases, 
it 
can 
in 
fact 
lead 
to 
greater 
predictive 
power. 
In 
other 
cases, 
while 
it 
may 
reduce 
predictive 
power, 
it 
can 
still 
make 
managers 
better 
off 
by 
improving 
the 
desirability 
of 
the 
whole 
market. 
Our 
results 
are 
particularly 
promising 
for 
managers, 
as 
they 
are 
now 
facing 
growing 
calls 
to 
make 
their 
algorithms 
transparent. 


We 
identied 
a 
set 
of 
conditions 
where 
managers 
should 
prefer 
algorithmic 
transparency. 
There 
are 
three 
factors 
that 
managers 
should 
consider: 
(a) 
access 
to 
a 
good 
set 
of 
causal 
features; 
(b) 
the 
predictive 
power 
of 
the 
correlational 
features; 
(c) 
the 
market 
composition 
in 
terms 
of 
the 
fraction 
of 
H 
type 
(desirable) 
agents. 


We 
provide 
some 
guidance 
on 
what 
makes 
a 
good 
causal 
feature 
here. 
The 
causal 
feature 
serves 
two 
purposes: 
(a) 
a 
signaling 
purpose 
‚Äì 
H 
type 
agents 
have 
a 
cost 
advantage 
on 
this 
feature, 
and 
thus, 
it 
can 
help 
separate 
H 
type 
agents 
from 
L 
type 
agents; 
and 
(b) 
a 
human 
capital 
purpose 
-the 
feature 
itself 
contributes 
to 
the 
productivity 
of 
the 
agents. 
To 
serve 
the 
signaling 
purpose 
well, 
the 
identied 
causal 
feature 
should 
be 
neither 
too 
costly 
nor 
too 
cheap 
to 
improve. 
If 
it 
is 
too 
costly, 
no 
one 
will 
improve 
it. 
By 
contrast, 
if 
it 
is 
too 
cheap, 
everyone 
will 
improve 
it. 
Furthermore, 
in 


34 


 


terms 
of 
the 
human 
capital 
purpose, 
the 
higher 
the 
feature's 
impact 
on 
productivity, 
the 
better 
it 
is. 
Even 
when 
this 
causal 
feature 
is 
unable 
to 
completely 
separate 
the 
H 
type 
agents 
from 
the 
L 
type 
agents, 
if 
it 
is 
moderately 
costly 
and 
contributes 
to 
productivity, 
the 
rm 
could 
still 
be 
better 
o. 
Typically 
algorithm 
designers 
are 
not 
focused 
on 
causality 
or 
identifying 
causal 
features. 
Our 
results 
indicate 
that 
they 
should. 
The 
recent 
stream 
of 
research 
in 
computer 
science 
that 
examines 
causal 
inference 
in 
machine 
learning 
models 
bodes 
well 
for 
them 
in 
this 
regard. 


The 
second 
factor 
that 
managers 
should 
consider 
is 
the 
predictive 
power 
of 
the 
correlated 
features. 
Intuitively, 
managers 
may 
think 
of 
keeping 
their 
algorithms 
opaque 
when 
the 
correlational 
features 
provide 
signicant 
predictive 
power. 
Our 
results 
show 
that 
this 
thinking 
is 
incorrect. 
We 
show 
that 
rms 
are 
more 
likely 
to 
be 
better 
off 
by 
making 
their 
algorithms 
transparent 
when 
correlational 
features 
provide 
signicant 
predictive 
power 
in 
the 
opaque 
counterpart. 
Though 
incorrectly, 
managers 
would 
be 
particularly 
concerned 
about 
making 
the 
algorithms 
transparent 
when 
the 
marginal 
eect 
of 
the 
causal 
feature 
in 
separating 
the 
H 
type 
agents 
from 
the 
L 
type 
agents 
is 
small 
in 
the 
presence 
of 
correlational 
features 
in 
an 
opaque 
algorithm. 
However, 
they 
should 
realize 
that 
the 
eect 
of 
the 
causal 
feature 
is 
suppressed 
largely 
due 
to 
the 
strategic 
behavior 
of 
the 
agents 
when 
the 
algorithm 
is 
opaque. 
All 
agents, 
but 
more 
importantly, 
the 
H 
type 
agents, 
underinvest 
in 
the 
causal 
feature 
when 
they 
know 
the 
correlational 
feature 
can 
separate 
them 
from 
the 
L 
type 
agents: 
the 
higher 
the 
predictive 
power 
of 
the 
correlational 
feature, 
the 
lower 
the 
incentive 
of 
the 
agents 
to 
invest 
in 
the 
causal 
feature. 
When 
the 
algorithm 
is 
made 
transparent, 
in 
the 
new 
equilibrium, 
the 
agents 
have 
a 
higher 
incentive 
to 
invest 
in 
the 
causal 
feature, 
making 
the 
rm 
better 
o. 


The 
third 
factor 
managers 
should 
consider 
is 
the 
market 
composition 
in 
terms 
of 
the 
fraction 
of 
the 
H 
type 
agents. 
If 
the 
causal 
feature 
is 
unable 
to 
separate 
the 
H 
type 
agents 
from 
the 
L 
type 
agents, 
algorithmic 
transparency 
could 
still 
benet 
the 
rm 
if 
the 
cost 
of 
improving 
the 
causal 
feature 
is 
moderate 
and 
the 
market 
is 
composed 
of 
more 
H 
type 
agents. 


Overall, 
our 
results 
suggest 
that 
managers 
should 
not 
view 
manipulation 
by 
agents 
as 
bad. 
Rather, 
they 
should 
embrace 
it 
and 
use 
algorithmic 
transparency 
as 
a 
lever 
for 
motivating 
agents 
to 
invest 
in 
more 
desirable 
actions. 


7.3 
Implications 
for 
Public 
Policy 
There 
are 
two 
key 
arguments 
typically 
put 
forth 
in 
support 
of 
algorithmic 
transparency. 
First, 
making 
algorithms 
transparent 
could 
highlight 
any 
hidden 
biases 
in 
algorithms 
and 
make 
them 
accountable 
(Citron 
and 
Pasquale, 
2014). 
Second, 
the 
users 
who 
are 
aected 
by 
an 
algorithm's 
decision 
making 
have 
the 
right 
to 
see 
which 
factors 
aect 
decisions 
made 
about 
them. 
Our 
paper 
has 
implications 
related 
to 
this 
second 
argument. 


35 


 


Recent 
legislation 
like 
the 
General 
Data 
Protection 
Regulation 
has 
aorded 
individuals 
a 
right 
to 
explanation 
under 
which 
rms 
have 
to 
provide 
an 
explanation 
regarding 
how 
a 
decision 
has 
been 
made 
by 
their 
algorithms 
(Goodman 
and 
Flaxman, 
2017). 
Our 
results 
show 
that 
such 
a 
regulation 
may 
not 
improve 
consumer 
welfare. 
When 
agents 
know 
which 
features 
in 
an 
algorithm 
aect 
important 
decisions 
about 
them, 
they 
can 
improve 
those 
features. 
Consequently, 
algorithmic 
transparency 
is 
generally 
viewed 
as 
helping 
the 
agents 
at 
the 
cost 
of 
the 
rm. 
However, 
our 
results 
show 
that 
the 
agents 
may 
not 
benet 
from 
algorithmic 
transparency 
as 
expected. 
When 
algorithms 
are 
opaque, 
they 
derive 
their 
accuracy 
from 
both 
causal 
and 
correlational 
features. 
Therefore, 
the 
H 
type 
agents 
do 
not 
need 
to 
invest 
in 
the 
costly 
causal 
feature 
to 
separate 
themselves 
from 
the 
L 
type 
agents. 
In 
the 
transparent 
scenarios, 
in 
many 
cases, 
the 
agents 
have 
to 
invest 
in 
the 
costly 
causal 
feature 
to 
achieve 
similar 
or 
even 
less 
separation 
with 
no 
change 
in 
their 
wage. 


7.4 
Generalizability 
of 
the 
Results 
While 
our 
model 
focuses 
on 
the 
job 
hiring 
scenario 
which 
is 
an 
example 
of 
a 
screening 
problem, 
our 
model 
and 
results 
are 
generalizable 
to 
other 
screening 
problems. 
Specically, 
problems 
comprising 
of 
two 
types 
of 
economic 
agents 
who 
have 
asymmetric 
information 
and 
who 
are 
attempting 
to 
engage 
in 
a 
transaction. 
The 
\screener‚Äù 
(i.e. 
agent 
with 
less 
information, 
e.g. 
the 
rm 
in 
our 
case) 
attempts 
to 
gain 
further 
insight 
or 
knowledge 
into 
the 
private 
information 
of 
the 
other 
agents 


(i.e. 
hidden 
type 
of 
the 
agent 
in 
our 
case). 
ML/AI 
algorithms 
are 
proving 
useful 
in 
helping 
the 
rms 
\screen‚Äù 
the 
applicants 
better. 
Job 
hiring 
is 
only 
one 
example 
of 
a 
screening 
problem. 
Other 
example 
include, 
insurance 
markets 
(e.g. 
car 
insurance, 
health 
insurance) 
or 
credit 
lending 
markets. 
7.5 
Future 
Research 
Directions 
Firms 
have 
typically 
kept 
their 
algorithms 
opaque 
to 
protect 
them 
from 
gaming 
by 
agents. 
In 
this 
study, 
we 
show 
that 
this 
strategy 
may 
not 
be 
the 
best, 
and 
the 
rm 
could 
be 
better 
off 
by 
making 
its 
algorithm 
transparent 
in 
the 
presence 
of 
strategic 
users. 
However, 
there 
are 
three 
other 
reasons 
as 
to 
why 
rms 
may 
still 
not 
want 
to 
make 
their 
algorithm 
transparent 
‚Äì 
interpretability, 
privacy 
and 
competition. 
With 
access 
to 
big 
data 
and 
large 
computational 
power, 
machine 
learning 
models 
have 
become 
complicated 
to 
the 
extent 
that 
they 
are 
rendered 
uninterpretable. 
While 
recent 
research 
has 
made 
advances 
in 
developing 
interpretable 
machine 
learning 
models, 
Bertsimas 
et 
al. 
(2019) 
show 
that 
model 
interpretability 
comes 
at 
a 
cost 
of 
accuracy. 
As 
a 
result, 
when 
considering 
the 
issue 
of 
algorithmic 
transparency, 
it 
may 
be 
interesting 
to 
consider 
the 
tradeoff 
between 
interpretability 
and 
accuracy. 
Similarly, 
when 
a 
rm 
makes 
its 
algorithm 
transparent, 
this 
can 
lead 
to 
privacy 
concerns. 
Others 
may 
be 
able 
to 
infer 
information 
about 
agents 
when 
they 
are 
selected 
by 
a 
transparent 
algorithm. 
In 
these 
cases, 
algorithmic 
transparency 
may 
impose 
a 
privacy 


36 


 


cost 
on 
agents. 
Future 
research 
can 
investigate 
algorithmic 
transparency 
in 
the 
presence 
of 
privacy 
concerns. 
Finally, 
it 
would 
be 
interesting 
to 
investigate 
whether 
and 
how 
algorithmic 
transparency 
may 
aect 
the 
intensity 
of 
competition 
among 
rms. 
We 
believe 
these 
are 
interesting 
avenues 
for 
future 
research. 

