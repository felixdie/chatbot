Wang, L., Huang, N., Hong, Y., Liu, L., Guo, X., & Chen, G. (2023). Voice‐based AI in call
center customer service: A natural field experiment. Production and Operations
Management, 32(4), 1002-1018.

Voice-based AI in Call Center Customer Service:A Natural Field Experiment

Abstract
Voice-based artificial intelligence (AI) systems have been recently deployed to replace traditional
interactive voice response (IVR) systems in call center customer service. However, there is little
evidence that sheds light on how the implementation of AI systems impacts customer behavior, as well
as AI systems’ effects on call center customer service performance. By leveraging the proprietary data
obtained from a natural field experiment in a large telecommunication company, we examine how the
introduction of a voice-based AI system affects call length, customers’ demand for human service, and
customer complaints in call center customer service. We find that the implementation of the AI system
temporarily increases the duration of machine service and customers’ demand for human service;
however, it persistently reduces customer complaints. Furthermore, our results reveal interesting
heterogeneity in the effectiveness of the voice-based AI system. For relatively simple service requests,
the AI system reduces customer complaints for both experienced and inexperienced customers.
However, for complex requests, customers appear to learn from the prior experience of interacting with
the AI system, which leads to fewer complaints. Moreover, the AI-based system has a significantly
larger effect on reducing customer complaints for older and female customers as well as for customers
who have had extensive experience using the IVR system. Finally, we find that the speech-recognition
failures in customer-AI interactions lead to increases in customers’ demand for human service and
customer complaints. The results from this study provide implications for the implementation of an AI
system in call center operations.

Keywords: Artificial intelligence; customer service; service flexibility; natural field experiment;
difference-in-differences

Advances in machine learning (ML) technology have accelerated the application of voice-based
artificial intelligence (AI) systems in various business functions, performing tasks such as speech
recognition and natural language processing.1 With the intention of improving customer experience as
well as reducing service costs, an increasing number of companies are deploying voice-based AI to
complement or replace current systems and services provided by human agents (Xiao and Kumar
2021). According to Markets and Markets (2021), the global conversational AI market size is predicted
to grow from $6.8 billion in 2021 to $18.4 billion by 2026, and AI-supported customer service is a
major factor driving the growth. Moreover, the value of global call center AI market reached 959.80
million in 2020 and is predicted to reach $ 9,949.61 Million by 2030 (Valuates Reports 2022).
Our study examines the implementation of a voice-based AI system that replaces the traditional
interactive voice response (IVR) system in a customer service call center. In the absence of the AI
system, customer calls are first connected to the IVR system and customers communicate with the IVR
system through phone keypads to obtain specific services. The service requests that the IVR system
cannot handle are then transferred to human agents. Upon the rollout of a voice-based AI system,
customers communicate with the AI in natural dialogues, and the AI system performs tasks, such as
processing natural language in a manner that resembles human intelligence. Note that the voice-based
AI system is different from the traditional IVR system in several ways, as summarized in Table 1. To
begin with, the AI system continuously evolves with the accumulation of a large amount of service data,
enhancement in computing power, and improvement in learning algorithms (LeCun et al. 2015). In
contrast, the IVR system was designed by industry experts based on their service experiences, does not
change with service data, and requires customers to strictly follow pre-set rules when interacting with
the system (Resnick and Virzi 1995). Through speech recognition in the AI system, customers can tell
the system which kinds of services they require; on this basis, they are directly routed to certain services
(Tang et al. 2003). In contrast, the IVR system typically relies on a hierarchical structure that directs
customers in a step-by-step manner to locate specific services (Suhm et al. 2002). To switch services,
customers are required to return to the main service menu and repeat the above-mentioned actions to
select another service.
Table 1. Voice-based AI System vs. IVR System
Voice-Based AI System IVR System
Inputs for Building the System Large amounts of service data Expert knowledge
Technological Characteristics
Natural language processing
Speech recognition
Improves with service data
Pre-designed services transferring rules
Remains the same as originally designed
Customer-System Interaction Interacting in natural dialogues Inputting specific information
Service Organization Direct routing Hierarchical structure
Considering the technological advantages of the voice-based AI system, the implementation of
such a system as a replacement for the traditional IVR system might significantly influence customer
service experiences. First, the AI system improves the flexibility of service flows and enables
personalized customer service. Instead of strictly following pre-defined service flows like in the IVR
system, customers can actively control the pace of service when they interact with an AI-based system.
For example, because of the AI system’s flexible navigation structure, customers can skip over the
layers of IVR structures and directly access the desired services. Second, the voice-based AI system can
adapt to customers’ interaction preferences to improve their service experiences. While the IVR system
provides highly structured and limited choices for customers, the AI system can interact with customers
in natural dialogues, allowing customers to express their needs adequately (Fountain et al. 2019).
Moreover, the voice-based AI system has the ability to learn from prior interactions with the input data
from customers and iteratively improve its performance. In scenarios wherein the AI system gets stuck,
it can tag the problems with the help of human agents and learn from the scenarios to resolve similar
problems in the future (Wilson and Daugherty 2018).
According to Forbes Insights, call centers are predicted to be the new sandbox for AI-powered
customer experience, as they are expected to deploy AI-based tools to boost retention, loyalty, and
profit.2 AI technology has gained widespread adoption in call center services over the past few years.3
Nonetheless, despite the growing interests in AI, its implementation has a
proof-of-concept-to-production gap (Perry 2021). In other words, AI can work well theoretically or on
test data, but it may fail to reach expectations in practical settings. In our study context, an IVR system
operates with pre-designed fixed logic, while an AI-based system relies on a complex algorithmic
structure. In turn, given the high variability of customer interactions in call center service, the service
efficacy of an AI system is likely to be subject to variation (Brynjolfsson and Mcafee 2017). For
example, customers may speak with accents or dialects while communicating with AI, thereby resulting
in speech-recognition failures that influence the effectiveness of the systems. However, without the
rigid logic of an IVR system, an AI-based system might learn to adapt to the customer interactions and
even be more effective than an IVR system. The high variability of tasks within customer service
requires more flexible, human-like responses, and the AI system may work better than the strictly
programmed IVR systems that are inflexible. Therefore, we believe it is important for researchers to
empirically test the effectiveness of AI in real-life settings.
Most prior studies on AI in operation management (OM) have mainly focused on the effects of
AI-supported automation and smartness, and examine how related technologies are deployed to
facilitate operation decisions or redesign operation process in product pricing (Karlinsky-Shichor and
Netzer 2019), order decision-making (Li and Li 2022), and quality management (Senoner et al. 2021).
Limited work explores the role of AI in interactions between customers and service systems,
particularly in service contact design scenarios (Roth and Mentor 2003). With few exceptions, for
example, Cui et al. (2021) examined the role of AI in buyer price requests and its influence on seller
price quotations in business-to-business (B2B) wholesaling. Contributing to this knowledge gap in
prior literature, our study focuses on the effects of implementing a voice-based AI system in the
business-to-consumer (B2C) customer service setting. Specifically, analyzing data from a natural field
experiment in a large telecommunication company’s customer service call center, we seek to answer the
following research questions:
How does the introduction of voice-based AI systems impact call length, customer’s
demand for human service, and customer complaints in call center customer services? How do the
effects of AI implementation in customer service vary for different customers?
To this end, we examine a natural field experiment4 with a voice-based AI system implemented
in a telecommunication company’s call center customer service operation. In the experiment, the
company’s customer service operation rolls out the AI system to replace the IVR system in different
phases, serving a portion of its customers based on the last digit of the customer’s phone number,
4 A natural field experiment (NFE) is the type of experiment ―where the environment is one where the subjects naturally
undertake these tasks and where the subjects do not know that they are participants in an experiment. Such an exercise
represents an approach that combines the most attractive elements of the lab and naturally-occurring data: randomization and
realism. (List 2007)
thereby allowing them to engage in customer service calls through natural dialogues with AI. We then
use difference-in-differences (DID) estimations to identify the effects of the AI-based system on key
outcomes. Our results reveal that the duration of machine service and customers’ demand for human
service increases temporarily after the introduction of the voice-based AI system, suggesting a possible
novelty effect. Meanwhile, the AI system significantly and persistently reduces customer complaints.
Moreover, we find interesting heterogeneity in the main effects of the AI system. To begin
with, the effects of the AI system on customer complaints appear to depend on the complexity of the
service requests. Compared with the customers who continue to use the traditional IVR system, the
customers assigned to use the AI-based system tend to make fewer complaints when they have
relatively simple service requests. In contrast, with relatively complex service requests (i.e., service
calls transferred to human agents), customers learn from their prior interactions with AI; this learning
effect leads to fewer complaints. Lastly, we find that the AI-based system exerts a significantly greater
effect on reducing customer complaints for older customers, female customers, and for customers with
longer user tenure.
Our study makes several important contributions to the related literature on AI applications and
call center operations. First, our study adds to research on AI applications by extending the scope to the
call center customer service setting and offers useful insights into how AI-powered service flexibility
impacts different outcomes in human-AI interactions (Luo et al. 2019, Sun et al. 2019, Cui et al. 2021).
Second, we contribute to the literature on call center customer service operations by empirically
examining how the implementation of the AI system affects customer behavior and the performance of
customer service, responding to calls for research on using disruptive technologies like AI to address
OM problems in general (Kumar et al. 2018, Karlinsky-Shichor and Netzer 2019) and to explore the
direct effects of technology-mediated customer-involved service contact designs in particular (Roth and
Menor 2003). In addition, building on related OM literature on call center operations, which views
customers as mostly homogenous and uses a single metric to represent the performance of operation
systems for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), our study further explores
customer heterogeneity in responding to the operations of the voice-based AI system.
Furthermore, our findings also offer useful implications for practice. We demonstrate that using
the voice-based AI system to replace the IVR system does not result in customer aversion to the AI
system, thereby validating the effectiveness of using voice-based AI systems in call center customer
service. We also demonstrate the novelty effect of implementing an AI system and find significant
heterogeneity in the effectiveness of voice-based AI systems on reducing customer complaints based on
the complexity of customer requests as well as customers’ age, gender, and tenure with the company
service, respectively. These results provide actionable insights into the implementation and further
development of voice-based AI systems. For example, companies must consider the possible short-term
increases in the duration of machine service and customers’ demand for human service while
scheduling for a service system that applies voice-based AI to replace the IVR system. Instead of
relying on customers’ self-learning, companies could educate their customers on using an AI system
with relatively complex requests. Moreover, the details obtained from customer-AI conversations
reveal that speech-recognition failures may lead to negative consequences. Therefore, it is necessary for
companies to continuously improve the capability of their AI systems to cater to a diverse customer
base.
2. Related Literature
2.1. Application of Artificial Intelligence (AI) Systems
Following prior literature, we define AI systems as algorithms that perform perceptual, cognitive, and
conversational functions typical of the human mind (Longoni et al. 2019). In recent years, the
significant development of AI systems has led to wide adoptions and applications in various domains.
Specifically, in the OM literature (see the summary of related literature in E-Companion A), from the
technical perspective, some prior work attempted to design AI-based algorithms to solve operational
problems such as demand or sales forecasting (Cui et al. 2018), product pricing (Yang et al. 2022), and
quality inferring (Senoner et al. 2021). Meanwhile, scholars have also explored how AI-enabled
automation and smartness features facilitate or support operational decisions in contexts such as price
request (Cui et al. 2021), order decision-making (Li and Li 2022), and automated pricing
(Karlinsky-Shichor and Netzer 2019).
Recently, a few studies on the application of AI systems have begun to understand the use of
such systems in commerce operations, where the AI system directly interact with individuals. For
example, Cui et al. (2021) examined how AI chatbots affect suppliers’ price quoting strategies. They
found that automation of chatbots alone leads to discrimination against chatbot buyers, but signaling the
use of a smart recommendation enabled by an AI system effectively reduces suppliers’ price quote for
chatbot buyers. In addition, Sun et al. (2019) suggested that the use of voice-based AI in online
shopping significantly affects consumers’ search behavior and purchase decisions. These prior studies
primarily focused on the role of AI systems in facilitating sales, but not much is known regarding the
application of voice-based AI systems in post-sales—that is, the scenario of customer service. In this
regard, our study aims at addressing this research void in the stream of work on AI system applications
by empirically examining the impact of implementing a voice-based AI system as a replacement for an
IVR system for customer service on the key outcomes related to customer experience and call center
operations.
2.2. Information Technology and Service Operation
Information technology plays an important role in improving service operations (Roth and Menor
2003). Companies increasingly rely on technology-based services to reduce service costs (Krishnan et
al. 1999) and increase service efficiency (Beckman and Sinha 2005). In recent years, with the
accumulation of large amounts of data on customers and transactions, companies have gradually
applied data-driven algorithms to automatically process service-related tasks—such as customer
segmentation, pattern identification, service instruction, and real-time personalization—which, in turn,
help companies to improve service quality (Sodhi et al. 2022, Sun et al. 2022).
Customers play an essential role in the delivery of services (Roth and Menor 2003). The design
of customer contact—the interaction between a customer and a service provider—is important for
shaping customers’ service experiences (Kellogg and Chase 1995). Prior research has empirically
examined the contact between customers and employees (Kellogg and Chase 1995, Soteriou and Chase
1998) and demonstrated that the physical service environment significantly influences customers’
perceptions and behavior (Bitner 1992). However, the advancement of information technology is
changing the ways in which customers interface with service providers. For example, companies
commonly establish self-service systems to cater to customers’ real-time service needs (Tezcan and
Behzad 2012). More recently, AI is being implemented to replace or complement conventional service
providers (Xiao and Kumar 2021). Froehle and Roth (2004) extended the customer contact perspective
to technology-mediated services and called for research on exploring the effects of virtual service
contact designs. Therefore, this paper focuses on the effects of different virtual service contact designs
(i.e., IVR and AI systems) in the context of call center customer service and specifically investigates
how replacing IVR systems with voice-based AI directly influences customers’ interaction outcomes.
2.3. Information Technology and Call Center Customer Service
Call center customer service has been an essential channel through which customers interact with firms
(Aksin et al. 2007, Tezcan and Behzad 2012). New developments in information technology provide an
opportunity to redesign and improve service-delivery operations in call centers. For example,
information technology supports a call center to expand to a larger scale (Adria and Chowdhury 2004).
In such contexts, researchers have examined the effects of call center centralization (Adria and
Chowdhury 2004) and discussed the risks caused by large-scale service systems (Pang and Whitt 2009).
Meanwhile, capacity management translates into a complex process in modern call centers. Researchers
have thus investigated the impacts of flexible labor resources (Kesavan et al. 2014) and attempted to
develop real-time schedule adjustment frameworks (Mehrotra et al. 2010). Another prevalent
technology-enabled change in call center operation is outsourcing; a wealth of research has explored
issues related to outsourcing (Kocaga et al. 2015) and call-routing (Gans and Zhou 2007) strategies in
such contexts. In the above studies, researchers have mainly focused on optimizing system designs in
contexts where technologies have been deployed to facilitate service operations (e.g., Aksin et al.
2007). However, little research has explored the effects of different technology-mediated contact
designs with customers directly involved in service delivery (Roth and Mentor 2003).
Specifically, in call center customer service, one typical technology-mediated service contact
design is the IVR system, which enables self-service at the front end of phone calls (Tezcan and Behzad
2012). Well-implemented IVR systems have the potential to automate a significant portion of services
and lead to improved customer service experiences (Tezcan and Behzad 2012). Thus, ample prior work
has examined the design of IVR-equipped service systems (Khudyakov et al. 2010, Suhm and Peterson
2002). Meanwhile, related studies from the user’s perspective reveal that customers often feel
when they interact with an IVR system because they perceive the services provided by IVR systems
be less customized and report that such systems occasionally do not understand their needs (Dean
2008). Consequently, customers often attempt to avoid IVR systems due to the lack of personalized
services or social interactions; instead, they seek direct interaction with human agents (Tezcan and
Behzad 2012).
Recent developments in AI technologies have enabled its applications in various contexts
(Brynjolfsson et al. 2019, Cui et al. 2021, Sodhi et al. 2022). For example, in 2017, Google’s machine
learning algorithms achieved a 95% accuracy rate for speech recognition in the English language, a
level that is close to actual human dialogue.5 In the customer service setting, voice-based AI systems
can understand customer needs through their voice inputs and can interact with customers in a
human-like manner (Van Doorn et al. 2017, Wilson and Daugherty 2018, Xiao and Kumar 2021).
However, considering the complexity of AI technology, it is challenging to determine the
effectiveness of AI systems in a real-world setting that goes beyond training data (Brynjolfsson and
Mcafee 2017). Once an AI system is deployed, it is expected to handle a large variety of situations
that may be unforeseen in training data. For example, customers may speak with accents or dialects
while communicating with AI, thereby resulting in speech-recognition failures that influence the
effectiveness of systems. Therefore, how AI implementation affects customer behaviors and the
performance of customer services remains an important empirical question that warrants further
investigation.
3. The Effects of the Implementation of AI on Call Center Customer Service
Based on the above discussions, in this section, we seek to discuss a few predictions on how the
implementation of a voice-based AI system in call center customer service will affect three key
metrics that are of interest to OM researchers: call length, demand for human service, and customer
complaints. While we do not provide any directional hypotheses in this section, the discussion serves
as a theoretical basis that guides our empirical analyses, which we report in subsequent sections.
5 Google’s ability to understand natural language is almost equivalent to that of humans.
Call length. Call length represents the duration of a customer’s service call (Gans et al. 2003),
which is important in the management of call center customer service operations because it directly
impacts scheduling and routing designs (Gans et al. 2003). In a traditional IVR system, the services
are organized in a tree-like hierarchical structure, whereby the leaves represent different services, the
nodes indicate customer states in the system, and the connections among different nodes indicate the
paths to specific services. All paths are pre-designed and customers can move only from one node to
another by inputting information in accordance with the guidance of the system. Typically, customers
must pass through several nodes before reaching certain services. Meanwhile, they must pay attention
to obtaining information on how to move from one node to another. An IVR system design typically
entails a time-consuming service experience. In contrast, with an AI-based service system, customers
can skip all the layers of IVR structures and directly access intended services by briefly summarizing
their needs to the system, which is likely to result in shorter call lengths, compared to a traditional
IVR system.
Conversely, it is also possible that an AI system leads to an increase in call lengths, as
compared to IVR systems, due to the characteristics of the speech-based interaction mode. To begin
with, when using the AI system, customers need to take time to summarize their needs in the form of
dialogues for the AI system to predict the intended services. Second, according to prior research on
communication modes, individuals interacting with text-based service systems (e.g., by inputting
numbers in the IVR systems) follow the cognitive economy principle, such that they are more likely
to focus on service requests and use keyword commands to improve communication efficiency (Le
Bigot et al. 2007). In contrast, the speech-based interaction mode enhances users’ involvement; users
tend to use quest-irrelevant expressions, such as politeness expressions in their interactions (Chafe
1982, Le Bigot et al. 2007), which could make information exchanges less effective (Le Bigot et al.
2007). In addition, when in conversational mode with an AI, users are expected to adapt their
behavior to the interaction system (Le Bigot et al. 2007, Cowan et al. 2015). Consequently, users
might devote more time and cognitive effort to formulate their speech and repeat information heard
during the interactions in order to share a common lexicon and syntactic structure with the interaction
system (Le Bigot et al. 2007). Therefore, speech-based AI service interactions may have longer
service durations than services handled by an IVR system. Based on the above discussions, it is
challenging to clearly predict the direction as well as the magnitude of changes in call length after the
introduction of the voice-based AI system; thus, the effect of the voice-based AI system (vs. IVR
system) remains an open question that warrants further empirical investigation.
Demand for human service. Customers’ demand for human service has direct implications
for staffing problems and operational costs within call center customer service (Tezcan and Behzad
2012). The introduction of AI may have mixed effects on customers’ demand for human service. On
the one hand, prior studies on AI applications demonstrate that, in certain contexts, individuals have a
subjective perception against AI and, thus, might be reluctant to interact with it even though AI now
offers high-level performance (Dietvorst et al. 2015; Longoni et al. 2019; Luo et al. 2019). When the
AI system offers the flexibility of transferring to human agents, customers may skip interacting with
AI and turn directly to human agents. Therefore, the AI-based system may increase customers’
demand for human service.
On the other hand, previous research also suggests that providing individuals with even a
slight amount of control over the AI’s behavior has the potential to mitigate their aversion to it
(Dietvorst et al. 2018), and this could be the case in our study. For example, the AI system enables
customers to control the pace of service and customers have the freedom to decide when to transfer to
human agents. Such a user-friendly design may mitigate customers’ aversion to interacting with AI
systems as well as mitigate any potential increase in customers’ demand for human service.
Considering both sides of the arguments, it is unclear whether and to what extent the implementation
of a voiced-based AI system would influence customers’ demand for human services; thus, we seek to
test this relationship empirically.
Customer complaints. Customer complaints are manifestations of customers’ negative
service experience (Singh 1988). Firms expend significant effort to improve customer service
experience and reduce customer complaints. According to the service operations literature, firms
create standardized service routines to control service delivery and ensure a uniform service level
(Leidner 1993). Standardized service routines reflect the preference of service providers with regard
to the manner in which customer needs must be met, with the process steps being organized in a
particular order. The service processes are largely determined by average customer demands and
preferences (Victorino et al. 2013). Since they follow service routines designed for an average
customer, service systems lack flexibility, cannot spontaneously react to unforeseen situations (Groth
et al. 2009), and are likely to overlook customer heterogeneities (Ashforth and Fried 1988). Dealing
with customer heterogeneities (e.g., request and preference variations) was a major challenge for
service operations (Frei 2006), and flexibility is one of the important capabilities in operational design
(De Groote 1994, Aksin et al. 2007).
A diverse environment and heterogenous needs are best fitted with flexible technology (de
Groote 1994). Specifically, improving the degree of flexibility in how service systems react to
customer requests enables the delivery of customized services (Tansik and Smith 1991), thereby
enabling an enhancement of customers’ service experiences (Roth et al. 2006). For example,
Victorino et al. (2013) reported that customers’ perceived lower service quality from dinner
recommendation services provided by an employee who rigidly follows the service script. In contrast,
customers gave high ratings to service interactions in which the employee offers the flexibility of
reacting to customer varieties (Victorino et al. 2013). In addition, Heim and Sinha (2002) showed that
the flexibility of the service process in electronic retailing is positively associated with customer
satisfaction. However, increased flexibility in a rule-based service system could potentially be
accompanied by an increased complexity of the system, which makes it more likely to result in
subjective service failures.
In the context of our study, the voice-based AI system accommodates customers’
communication preference heterogeneities by enabling customers to express their service needs in
ways that are most suitable for them. Meanwhile, compared with the IVR system, the AI system
enhances the flexibility of service flows so that customers can directly locate their intended services,
switch among different services, and transfer to human agents whenever they want. Therefore, we
expect the implementation of the voice-based AI system to enhance customers’ experience and reduce
customer complaints; moreover, we also seek to empirically evaluate this effect.
4. Background and Data
4.1. Natural Field Experiment
Our study considers a natural field experiment conducted by a large telecommunication company’s
call center customer service, which serves as an important channel for customer-firm interaction
(Aksin et al. 2007). The company was established in 1995 and now has 14 branches and over 8,000
employees, providing services to over three million customers in a major city (covering an area of
approximately 53,100 km2) in northeast China, with a market share of 33%. The company rolled out
its voice-based AI system in its call center customer service system based on the last digit of customer
phone numbers. Figure 1 summarizes the timeline of the natural field experiment. Before Dec. 19,
2018, all service calls were connected to the IVR system. From Dec. 19, 2018, the AI system was
implemented to replace the IVR system for a certain portion of the company’s customers, which was
chosen on the basis of a set of randomly selected last digits of the customer’s phone number.
Specifically, service calls from phone numbers with the last digit 1 or 7 were connected to AI, while
calls from other phone numbers remained connected to the IVR system. Between Dec. 19 and Dec.
31, 2018, the updated service system was in the beta-testing phase and was not connected to the
internal databases. Therefore, no service records were stored. Beginning on Jan. 1, 2019, the AI
system was connected to internal databases with phone records stored. Thereafter, beginning Jan. 10,
2019, in addition to service calls from phone numbers ending in 1 or 7, service calls from phone
numbers ending in 3, 5, or 9 were also connected to the AI system. After Jan. 15, 2019, the AI system
completely replaced the IVR system.
Figure 1. Timeline of the Natural Field Experiment
While interacting with the AI-based system, customers verbally state their requests briefly
and the AI system provides instant responses based on the analysis of information input by the
customers. If customers do not describe their requests clearly, the AI system asks specific questions to
guide customers to providing more information so that the AI can route them to the specific services
in order to meet their needs (e.g., payments, check balance, temporarily stop service). If the IVR
system or AI system is unable to provide specific services that customers are looking for, the
customers have the option to be transferred to human agents. When interacting with the IVR system,
customers need to strictly follow pre-designed service flows. After navigating the entire voice
guidance on possible services, the system tells customers to press a specific number to be transferred
to human agents. In contrast, the AI-based system sets no restrictions on when and how customers can
transfer to human agents. At the beginning of the service, the AI system tells customers ―If you need
help from human agents, please say ‘Transfer to human agents’.‖
4.2. Data & Measures
The implementation of the voice-based AI system in call center customer service provides exogenous
variations on the type of service system (voice-based AI vs. IVR system) that a customer experiences.
The observation duration in our study was 30 days, including a 21-day pre-treatment period (Nov. 28
to Dec. 18, 2018) and a 9-day treatment period (Jan. 1–9, 2019).6 We exclude the beta-testing phase
between Dec. 19 and Dec. 31, 2018, because we were unable to observe the outcome variables during
this phase.
Our data set contains timestamps of customers’ phone call records, such as the start and end
times of service calls, customers’ profile information, such as age and gender, when a customer began
using the telecommunication service, as well as the transcripts of customer-AI conversations from all
the customers served by the company. We constructed the variable Call Length to measure a service
call’s duration using the difference between a call’s start and end times. Considering the skewed
distribution of Call Length (Gans et al. 2003), we log-transformed this variable in our estimations. In
addition, we constructed the variable Human Service to capture whether a customer transferred to
human agents in the service call. Further, extending the related research on customer service
6 In this natural field experiment, we have a nine-day treatment period (between Jan. 1 and Jan. 9, 2019, when the customers
with the last digit of phone number 7 connected to AI system vs. those with the last digit of phone number 9 connected to
IVR system). Also, a three-week pre-treatment period allows us to check the parallel trend of our key outcome variables
before the experiment (from Nov. 28 to Dec. 18, 2018, when all service calls were connected to the IVR system). In
addition, the cooperating telecommunication company designs its services on a monthly basis (e.g., customers choose a
monthly service package, charge bills for the next month). Taken together, we chose a 30-day observation window.
experiences in OM (Aksin et al. 2007), we considered customer complaints as one typical
consequence of negative service experiences. Here, we constructed the variable Customer Complaint
to capture whether the customer complained about the service within 30 minutes after a service call.
Following prior research on technology acceptance (Venkatesh et al. 2012), we also included
a few additional variables on the observable individual characteristics to understand whether
individual differences (e.g., age, gender, experience) moderate users’ acceptance and use of the
voice-based AI system. Specifically, we observed user age and gender. In addition, we constructed the
variable Service Tenure to measure how many years a customer has been using his or her phone
number and consider it a proxy for the customer’s experience using the IVR system. Table 2 presents
a summary of the operationalization of the main variables.
Table 2. Variables and Definitions
Variables Definitions
Call Length The duration of a service call. It is measured by the difference between a call’s
end time and start time
Human Service Whether a customer chose to transfer to human agents, with Yes = 1 and No =
0
Customer
Complaint
Whether a customer complained about the service within 30 minutes, with Yes
= 1 and No = 0
Age The actual age of a customer calculated based on his/her birthday information
Gender Female = 1 and Male = 0
Service Tenure The number of years a customer has been using his/her phone number. It is
regarded as a proxy for the customer’s experience using the IVR system
5. Methodology & Results
5.1. Econometric Identification
First, we processed the data to ensure comparability between our treatment and control groups in the
company’s natural field experiment. In the experiment, the group assignment hinges on the last digit
of the customers’ phone number, instead of randomly assigning customers to either the treatment or
control group. To ensure comparability of observations, given the lack of individual random
assignment, we tried to balance our samples before data analyses. If certain individuals prefer an even
number as the last digit, it may result in significant differences between the two groups (i.e., even
numbers vs. odd numbers); this is also observed in our data (see E-Companion B). To address this
issue, we first excluded data from customers with phone numbers that end in even numbers (i.e., 0, 2,
4, 6, and 8). Further, we conducted a series of pairwise comparisons of the observable covariates (i.e.,
Age, Gender, Service Tenure) and the pre-treatment values of outcome variables by the last digit of
the customers’ phone numbers (see Table B1 in E-Companion B). We found no significant
differences in the observable covariates across the groups of customers whose phone numbers end
with 7 and 9, as indicated in Table 3. Table 4 reports the descriptive statistics of the main variables.
Table 3. Comparisons of Observable Covariates and Pre-Treatment Values of Outcome
Variables
Last Digit Age Gender
Service
Tenure
Log (Call
Length)
Human
Service
Customer
Complaint
7 43.579
(11.516)
0.639
(0.480)
8.507
(3.740)
4.453
(0.953)
0.320
(0.466)
0.014
(0.116)
9
43.954
(11.590)
0.648
(0.478)
8.650
(3.728)
4.470
(0.922)
0.316
(0.465)
0.012
(0.109)
p-value 0.466 0.993 0.207 0.345 0.658 0.455
Notes: Standard errors are given in parentheses. We observed insignificant differences on the observable
covariates and the pre-treatment values of the outcome variables, thereby suggesting comparability of the groups
with customers whose phone numbers end with 7 vs. 9.
Table 4. Descriptive Statistics
Variables Observations Mean SD Min Max Median
Log (Call Length) 18,580 4.446 0.869 2.996 7.365 4.407
Human Service 18,580 0.288 0.453 0 1 0
Customer Complaint 18,580 0.013 0.114 0 1 0
Age 18,580 43.765 11.554 15 70 43
Gender 18,580 0.643 0.479 0 1 1
Service Tenure 18,580 8.578 3.734 2 23 8
To provide additional model-free evidence for the treatment effect and comparability of the
two groups, we also show the pre-treatment parallel trends of the outcome variables in Figures C1–C3
in E-Companion C. Overall, the evidence indicates that customer groups with the last digit as 7 vs. 9
are comparable. Thus, we opt to use the customer groups whose phone numbers ended with 7
(treated) and 9 (control) to estimate the effects of the AI-based system on the outcome variables in our
main analyses.
Thereafter, we performed difference-in-differences (DID) analyses to estimate the effects of
the voice-based AI system on service call length, customers’ need for human service, and customer
complaints. We specify the DID estimations with Equations (1), (2), and (3). In addition, we also
incorporate customer-level random effects in these estimations.7
( )
. (1)
( )
( )
. (2)
7 In this paper, we selected random-effects models for two reasons. First, in our field experiment, randomization occurred
based on the last digit of the phone number. Thus, for the same customer, all his/her service records were either in the
treatment group or the control group. When conducting regressions with customer fixed effects, a few variables would have
been subsumed, like the variable AI_agent that indexes whether a record was in the treatment or control group, and dummies
that capture the features of specific customers (e.g., Age, Gender, Service Tenure) in the regression models. Second, for
dummy variables Human Service and Customer Complaint, the records of these variables with a value of 1 are relatively
sparse. If we chose fixed-effects models, many observation groups would have been omitted because of all negative
outcomes (i.e., Human Service = 0 or Customer Complaint = 0). In order to check the robustness of our results, we also
present the estimation results of fixed-effects models.
( )
( )
.
(3)
In the equations above, i denotes customers, and t denotes observation time; AI_agent is a
dummy variable, with 1 representing service calls from customers in the treatment group (i.e.,
customers whose phone numbers end with 7) and 0 representing service calls from customers in the
control group (i.e., customers whose phone numbers end with 9). After_AI is a dummy variable that
equals 1 for observations that took place after the introduction of the AI system and 0 for observations
on or prior to Dec. 18, 2018; Day Dummyt is a vector of time dummies representing each day during
our observational period; ui represents customer-specific random effects; and is the error term. In
Equation (1), the dependent variable is Log (Call Length). In Equations (2) and (3), we observed the
binary indicators of whether a call is transferred to human service and whether a call service
eventually received a customer complaint; we estimated these outcomes with logistic regressions. We
are interested in the coefficients of the interaction term, AI_agent * After_AI, as they capture the
effects of the AI system (compared with the IVR system) on the outcomes. For example, if the
coefficient in Equation (1) (i.e., ) is positive and statistically significant, it suggests that compared
with control customers—who used the IVR system and did not access the AI system—the treated
customers, who did use the AI system experienced longer average call length after the implementation
of the AI system.8
5.2. Main Findings
The regression estimations are presented in Tables 5 and 6, demonstrating the effects of the AI system
on Log (Call Length), Human Service, and Customer Complaint, respectively. To explore how the
durations of machine and human calls change after the implementation of the AI system, we separated
8 We also conducted a series of placebo tests to check whether the identified effects existed before the introduction of the AI
system or between customer groups who did not get access to the AI system. The results of the placebo tests are reported in
E-Companion D.
the duration of machine service (Machine_Call Length) from human service (Human_Call Length)9
and then conducted regressions. The results presented in Columns 1–4 in Table 5 suggest that the
implementation of voice-based AI significantly increases the duration of machine service by 5.65%
(i.e., 100 × ( −1) %) but exerts no effect on the duration of human service. These results indicate
that customers tend to spend more time interacting with the AI system as compared to the IVR
system. The results echo previous literature stating that, compared to mechanical self-service systems,
customers tend to engage more with natural language-based service robots (Huang and Rust 2021).
Specifically, diving into the records of customer-AI conversations, we found evidence for the wide
use of quest-irrelevant characteristics (e.g., the use of first- or second-person pronouns, politeness
expressions, and hesitation expressions), suggesting enhanced involvement during speech-based
interactions, which can lead to an increase in the service duration (Hauptmann and Rudnicky 1988, Le
Bigot et al. 2007).10 Meanwhile, per the results in Columns 5 and 6 in Table 5, we found inconclusive
evidence for the effects of the voice-based AI system on the total call length (i.e., Log (Call Length)).
Table 5. Effects of AI Implementation on Call Length
Variables (1) Log
(Machine_
Call
Length)
(2) Log
(Machine_
Call
Length)
(3) Log
(Human_C
all Length)
(4) Log
(Human_C
all Length
(5) Log
(Call
Length)
(6) Log
(Call
Length)
AI_agent 0.003
(0.018)
0.021
(0.071)
0.005
(0.022)
AI_agent * After_AI 0.055**
(0.021)
0.041*
(0.022)
0.082
(0.082)
0.012
(0.083)
0.050**
(0.023)
0.032
(0.023)
Age -0.003***
(0.001)
-0.027**
(0.003)
-0.006***
(0.001)
9 Machine_Call Length measures the duration of a machine (i.e., AI or IVR) service, while Human_Call Length captures the
duration of a service delivered by human agents.
10 We examined the records of customer-AI conversations to provide possible explanations for the increase in call length
(particularly in machine call length) and found that, in 28.8% of the conversations, customers use first- or second-person
pronouns (e.g., ―I‖, ―you‖), suggesting enhanced involvement during conversations. In addition, approximately 21.3% of the
conversations include at least one utterance to express politeness (e.g., ―Thank you‖) and 16.0% of the conversations contain
hesitation expressions (e.g., ―Uh‖) when customers form utterances during conversations. Consistent with research on
interaction mode, all these quest-irrelevant characteristics in speech-based interactions—although making the interaction
more natural—can also lead to an increase in the service duration (Hauptmann and Rudnicky 1988, Le Bigot et al. 2007).
Gender -0.024
(0.017)
-0.022
(0.065)
-0.027
(0.021)
Service Tenure -0.004*
(0.002)
-0.022**
(0.009)
-0.006**
(0.003)
Observations 18,580 18,580 18,580 18,580 18,580 18,580
Between R-square 0.080 0.061 0.045 0.006 0.043 0.013
Number of Customers 3,625 3,625 1,818 5,359 3,625 3,625
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 3 and 4, the
values of Human_Call Length are 0 for services successfully handled by the AI system or IVR system and we
calculated Log (Human_Call Length) = log (Human_Call Length + 1).
Table 6 reports the results from estimating Equations (2) and (3). Specifically, the estimates
presented in Columns 1 and 2 in Table 6 suggest that the introduction of a voice-based AI system
does not appear to exert a significant effect on customers’ demand for human service, even though it
is easier for customers to transfer to human agents when interacting with the voice-based AI system.
The above results provide null evidence on the possible negative consequence of implementing
voice-based AI in supporting customer service. When interacting with the AI system, customers can
choose to transfer to human agents at the beginning of the services, which may increase the workload
of human agents. However, we did not observe such an effect in our results. One possible explanation
for the results is that the AI-based service system in our research context enables customers to control
the service pace by transferring to human agents anytime they want. Giving customers the freedom to
control AI can reduce their aversion against AI (Dietvorst et al. 2018).
Table 6. Effects of AI Implementation on Human Service and Customer Complaint
Variables (1) Human
Service
(2) Human
Service
(3) Customer
Complaint
(4) Customer
Complaint
AI_agent 0.007 (0.083) 0.129 (0.369)
AI_agent * After_AI 0.103 (0.089) 0.036 (0.092) -1.037** (0.406) -1.101** (0.438)
Age -0.035*** (0.004) -0.069*** (0.017)
Gender -0.030 (0.079) 0.249 (0.358)
Service Tenure -0.025** (0.011) 0.117** (0.048)
Observations 18,580 10,621 18,169 959
Number of Customers 3,625 1,658 3,625 107
Day Dummies Y Y Y Y
Customer Random Effects Y - Y -
Customer Fixed Effects - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 2 and 4, some
observations were excluded when we conducted logistic regressions considering customer fixed effects.
We also considered the effects of AI implementation on Customer Complaint. In Columns 3
and 4 in Table 6, we observed that the AI system significantly reduces customers’ likelihood of filing
complaint reports. We also quantified the magnitude of this effect in accordance with Hosmer et al.
(2013). Specifically, compared with the average Customer Complaint before the implementation of AI
in the sample (M = 0.013), we estimated that the implementation of AI reduces the probability of
customer complaints to 0.005 (i.e., 0.013 * / (1 + 0.013 * )), with a decrease of 61.54%
in customer complaints. As an extension to the literature that examines the impacts of AI-enabled
features in operation management, such as automation (Cui et al. 2021, Li and Li 2022) and smartness
(Cui et al. 2021), our results reveal that the service flexibility (a reflection of AI smartness) enhanced
by the AI systems does, indeed, improve the overall service performance. In E-Companion E, we
replicated our main analysis and found similar results from the data from customers with phone
numbers ending in odd numbers.
5.3. Additional Analyses
In the additional analyses, we first explored the heterogeneity in our main results (Section 5.3.1). Prior
work has demonstrated that user characteristics play critical roles in affecting the performance of
technology designs (Venkatesh and Morris 2000, Venkatesh et al. 2012), and we thus tested the
moderating effects of customer characteristics (e.g., age, gender, and service tenure). Next, we dived
into the customer-AI conversations and considered the consequences of possible AI service failure
(Section 5.3.2). Since AI cannot work perfectly to handle all service tasks, we tried to understand how
customer-AI interaction and AI’s speech-recognition failures influence customer service outcomes.
In addition, prior studies have found that individuals are more likely to accept and use new
technologies when they get used to them (Taylor and Todd 1995). In our research context, a customer
can use the call service system several times during our observation period. Therefore, in
E-Companion F, we further analyzed whether the effects of AI implementation change as customers
accumulate experience in interacting with the AI system. That is, the learning effects in customer-AI
interactions. Our results suggest that, for relatively simple requests, the implementation of
voice-based AI directly increases machine service duration and reduces customer complaints. When
dealing with complex requests (service calls handled by human agents), the AI system only reduces
customer complaints for customers who are experienced in using the AI system.
Finally, one potential explanation for the observed effects of the voice-based AI system on the
outcomes of interests is the novelty effect. For example, customers may be unfamiliar with the AI
system when the system is first introduced in the call center. In such a scenario, they are more likely
to spend a longer amount of time interacting with the AI system or they may be more tolerant of the
services provided by the AI system. In order to understand the possible novelty effect, in
E-Companion G, we re-estimated our regression equations in the main analysis by considering or
eliminating records on the voice-based AI system’s first- and second-time services for each customer.
The results suggest that the implementation of the AI system persistently reduces customer
complaints. However, possible novel effects of the AI system during the period of its early
introduction indicate that the duration of machine service and customer demand for human service
increases only temporarily after the introduction of the AI system and these effects are not significant
in the long term.
5.3.1. Heterogeneity by Customer Characteristics
After estimating the main effects of the AI system, we further examined how customer-level
covariates—including age, gender, and experience of using the IVR system—moderate the effects of
the voice-based AI system. The variable Service Tenure measures the number of years that a customer
has been using his/her phone number and we used it as a proxy for customers’ experience using the
IVR system. In terms of continuous variables, including Age and Service Tenure, we first
mean-centered these variables before constructing the interaction terms. Table 7 presents the results of
the moderation analyses.
As indicated in Panel A of Table 7, we found that Age moderates the effects of the AI system.
Specifically, older (vs. younger) customers may benefit more from the dialogue-based services
supported by AI, such that after the implementation of voice-based AI system, they spend less time on
call services, have less demand for human service, and register fewer complaints. In line with Meuter
et al. (2005), who found that older customers are not proficient at using traditional IVR systems and
thus are more reluctant to interact with these systems. Consequently, the convenience and flexibility
enabled by the voice-based AI system are more helpful for improving the service experience of older
customers. With regard to the moderating role of Gender in Panel B, the AI-based system is more
effective in reducing complaints from female customers (i.e., Gender = 0). Studies on the use of
self-service technology indicated that females are strongly influenced by their perceptions of ease of
use of technologies (Venkatesh and Morris 2000). Therefore, they experience a significant
improvement in AI-supported flexible services. Furthermore, we find that customers’ experience of
using the IVR system, Service Tenure, moderates the effect of the AI system on Customer Complaint
(Panel C). For customers who have more (vs. less) experience using the IVR system, the
implementation of the AI system has a greater effect in reducing their complaints. One possible
explanation for this is that experienced users are more familiar with the drawbacks of the IVR system
and more likely to appreciate the benefits of the AI system, thereby tending to have fewer complaints.
As an extension of prior literature that assumes service systems have the same service performance
for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), our results indicate that the
effects of AI-based systems vary in terms of customer gender, age, and service tenure.
Table 7. Heterogeneity by Customer Characteristics
Panel A.
Moderation by Age
(1)
Log (Call
Length)
(2) Log
(Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.004
(0.022)
0.009
(0.084)
0.332
(0.398)
AI_agent * After_AI 0.053**
(0.023)
0.035
(0.023)
0.084
(0.090)
0.003
(0.093)
-1.441***
(0.443)
-1.613***
(0.486)
AI_agent * After_AI * Age -0.004*
(0.002)
-0.004**
(0.002)
-0.014*
(0.008)
-0.022**
(0.009)
-0.109***
(0.041)
-0.146***
(0.047)
Age -0.009***
(0.001)
-0.036***
(0.005)
-0.104***
(0.027)
AI_agent * Age 0.001
(0.002)
0.000
(0.007)
0.058
(0.036)
After_AI * Age 0.008***
(0.001)
0.008***
(0.001)
0.008
(0.006)
0.010
(0.006)
0.055**
(0.028)
0.060**
(0.029)
Gender -0.027
(0.021)
-0.031
(0.079)
0.250
(0.364)
Service Tenure -0.006**
(0.003)
-0.025
(0.011)
0.119**
(0.049)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Panel B.
Moderation by Gender
(1)
Log (Call
Length)
(2) Log
(Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.005
(0.037)
-0.028
(0.140)
0.657
(0.673)
AI_agent * After_AI 0.038
(0.039)
0.015
(0.039)
0.053
(0.152)
-0.057
(0.156)
-2.669***
(0.904)
-2.873***
(0.990)
AI_agent * After_AI *
Gender
0.017
(0.478)
0.025
(0.049)
0.075
(0.188)
0.143
(0.193)
2.118**
(1.015)
2.306**
(1.116)
Age -0.006***
(0.001)
-0.035***
(0.004)
-0.069***
(0.017)
Gender -0.043
(0.033)
-0.054
(0.124)
0.599
(0.587)
AI_agent * Gender 0.001
(0.046)
0.056
(0.174)
-0.738
(0.809)
After_AI * Gender 0.031
(0.034)
0.025
(0.034)
-0.048
(0.133)
-0.070
(0.138)
-0.634
(0.584)
-0.700
(0.629)
Service Tenure -0.006**
(0.003)
-0.025**
(0.011)
0.118**
(0.049)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Panel C. Moderation by
Service Tenure
(1)
Log (Call
Length)
(2)
Log (Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.005
(0.022)
0.002
(0.083)
0.117
(0.402)
AI_agent * After_AI 0.050**
(0.023)
0.032
(0.023)
0.111
(0.090)
-0.049
(0.092)
-0.991**
(0.423)
0.998**
(0.445)
AI_agent * After_AI *
Tenure
-0.007
(0.006)
-0.006
(0.006)
0.033
(0.025)
0.036
(0.025)
-0.257**
(0.110)
-0.217*
(0.116)
Age -0.006***
(0.001)
-0.035***
(0.004)
-0.073***
(0.019)
Gender -0.027
(0.021)
-0.030
(0.079)
0.246
(0.390)
Service Tenure -0.015***
(0.004)
-0.037**
(0.017)
0.023
(0.082)
AI_agent * Service Tenure 0.002 -0.010
(0.023)
0.124
(0.108)
(0.006)
After_AI * Service Tenure 0.021***
(0.004)
0.020***
(0.004)
0.026
(0.018)
0.025
(0.018)
0.198***
(0.075)
0.170**
(0.078)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 4 and 6, some
observations were excluded when we conducted Logistic regressions considering customer fixed effects.
5.3.2. Speech-Recognition Failures in Customer-AI Interactions
We analyzed the transcripts of customer-AI conversations to examine how AI’s speech recognition
failures in customer-AI interactions may affect customers’ demand for human service and customer
complaints. To this end, we calculated the variable Failure_Count to measure the number of times
that the AI system failed to recognize a customer’s intention during a service call by counting the
number of times the AI system repeated the same question. On average, during our observational
window, approximately 28.5% of the customer-AI system service sessions involved
speech-recognition failures. We also measured Conversation_Count to quantify the rounds of
interaction between the AI system and a customer during a service call. Considering the skewed
distributions of the variables Failure_Count and Conversation_Count, we used the log-transformed
values of these variables in our regression estimations.11 Table 8 presents the results pertaining to
speech recognition failures of AI. The results suggest that Log(Conversation_Count) is negatively
related to Human Service and Customer Complaint. One possible explanation is that, for service
requests that can be handled by the AI system, customers tend to have more interaction rounds with
the AI system and are less likely to turn to human agents and complain about the service. Meanwhile,
Log(Failure_Count) is positively related to Human Service and Customer Complaint, thereby
indicating that speech-recognition failures in customer-AI conversations can lead to significant and
11 In cases when variables include 0, we added 1 before the logarithm transformation. For example, we calculated Log
(Failure_Count) = log (Failure_Count+1).
negative effects on call center service performance by increasing customers’ demand for human
service, thus leading to more customer complaints.
Table 8. Effects of Details in Customer-AI Interactions
Variables (1) Human
Service
(2) Human
Service
(3) Customer
Complaint
(4) Customer
Complaint
Log (Conversation_Count) -3.858*** (0.096) -1.971*** (0.104) -3.347*** (0.608) -1.122** (0.461)
Log (Failure_Count) 0.966*** (0.090) 0.242* (0.128) 1.610*** (0.594) 0.213 (0.585)
Age -0.033*** (0.003) -0.027 (0.017)
Gender -0.122* (0.067) 0.136 (0.388)
Service Tenure -0.042*** (0.009) -0.057 (0.052)
Observations 17,274 6,950 17,274 366
Number of Customers 9,042 1,880 9,042 78
Day Dummies Y Y Y Y
Customer Random Effects Y - Y -
Customer Fixed Effects - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 2 and 4,
most observations were excluded when we conducted logistic regressions considering customer fixed effects. In
Column 4, the coefficient of Log (Failure_Count) is not significant and one possible reason for the result may
be that some observations were excluded when we conducted Logistic regressions considering customer fixed
effects.
6. Discussion
6.1. Key Findings
This study investigates how the implementation of a voice-based AI system in call center customer
services affects customer behavior and call center performance. The results reveal several interesting
findings. First, we find that the voice-based AI system temporarily increases the duration of machine
service and customers’ demand for human service when the system is first introduced to customers,
but these effects were not significant after the customers gained experience with the AI system.
Second, the effects of the AI system on customer complaints vary in accordance with the complexity
of the customers’ service requests and the customers’ experience of using the AI system. Specifically,
the AI-system effectively reduces customer complaints for both experienced and inexperienced
customers when customers have relatively simple requests. In contrast, with regard to complex
requests, the AI system improves customers’ service experience only after they accrue sufficient
experience interacting with the AI system. Third, we explore how customer characteristics moderate
the effects of the AI system. The results reveal that the AI system is more helpful in reducing
complaints for older customers, female customers, and customers who are experienced in using the
IVR system.
6.2. Theoretical and Practical Implications
Our study contributes to the related literature on the application of AI systems and call center
customer service operations. To begin with, this work extends the literature on AI applications by
improving the understanding of the effects of AI systems in the customer service setting. Previous
studies have either focused on deploying AI-based algorithms to support or optimize operational
processes from the technical perspective (Senoner et al. 2021, Sun et al. 2022, Yang et al. 2022) or
examined the effects of different AI-enabled features in contexts such as price request (Cui et al.
2021), order decision-making (Li and Li 2022), and automated pricing (Karlinsky-Shichor and Netzer
2019). These studies mainly investigated certain advantages and drawbacks of AI-enabled automation
(Karlinsky-Shichor and Netzer 2019, Li and Li 2022, Cui et al. 2021) or smartness (Cui et al. 2021).
As an extension, our study explores the effectiveness of AI-enabled service flexibility—a specific
reflection of AI smartness—in call center customer service. Technology-based self-service systems
(e.g., ATMs) have already been demonstrated to work well in dealing with highly structured service
tasks (Barua et al. 1991), while our study suggests that AI-enabled service flexibility is more likely to
improve the service effectiveness when dealing with tasks with high variability (e.g., call center
services). Thus, IT investment decisions in service operations should match the features of both
service tasks and technologies.
In addition, prior studies have indicated that the realization of AI’s value in the context of
human-AI interaction crucially depends on institutional settings and the role that customers play in
using AI applications (Dietvorst et al. 2018, Luo et al. 2019). Often, customers are reluctant to interact
with AI when they passively receive AI-supported marketing information (Luo et al. 2019),
forecasting results (Dietvorst et al. 2015), or medical care (Longoni et al. 2019). However, our study
suggests that when customers have the freedom to control the flow and direction of the service, the
AI-based service system does not result in a significant increase in demand for human service, even
though the AI system allows customers to transfer to human agents at any time during the interaction.
Furthermore, our results offer preliminary insight into the negative effects of speech-recognition
failures on customer-AI system interactions, thereby enriching research on imperfect AI (Dietvorst et
al. 2015, 2018).
Our study also contributes to the literature on call center customer service operations by
investigating how AI technologies impact customer behavior and the performance of call center
customer service. Previous research has examined changes in call center customer service operations
elicited by technological advances, such as call center centralization (Adria and Chowdhury 2004),
flexible resource management (Kesavan et al. 2014), and outsourcing (Kocaga et al. 2015), but
limited attention has been paid to exploring the effects of the contact designs of different
technology-mediated services with direct customer involvement (Roth and Mentor 2003, Froehle and
Roth 2004). Moreover, the OM literature mainly focuses on measuring the performance of call center
customer services from the firm’s perspective, using easily trackable metrics, such as operational
costs (Tezcan and Behzad 2012) and wait time (Khudyakov et al. 2010, Singhal et al. 2019). There is
limited research on customers’ service experience (Aksin et al. 2007). Through the customers’
perspective, our study examines the effects of an AI-based service system on customer complaints,
which is a key consequence of customers’ negative service experiences. We find that customers tend
to make fewer complaints when served by the voice-based AI system. In addition, enriching prior
studies that treat customers as homogeneous and use a single metric to represent the performance of
service systems for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), this study
further examines how the effects of the AI-based system vary in accordance with customers
characteristics, such as age, gender, and experience in using the traditional IVR system.
Furthermore, our findings also have important practical implications. First, we find that the
implementation of the voice-based AI system in call center customer services helps improve customer
service experiences (i.e., reduces customer complaints) and that the flexibility of transferring to
human agents, enabled by the AI system, does not lead to a significant increase in customers’ demand
for human service in the long term. These findings showcase the value of voice-based AI systems in
the provision of customer service. Thus, companies can continue implementing AI systems to support
customer services. Second, our findings also shed light on bridging the
proof-of-concept-to-production gap (Perry 2021). We find that the effectiveness of the AI system is
closely dependent on the service tasks (e.g., the complexity of customers’ service requests) and
customers’ experience of using AI systems. As predicted, the implementation of the AI system
directly improves customers’ service experience in relatively simple service tasks. In terms of
handling complex requests, the voice-based AI system only operates effectively to reduce complaints
from customers who have gained enough knowledge about the interacted AI system. Thus, users may
suffer the proof-of-concept-to-production gap in the early stages of adoption, particularly when
dealing with complex tasks (Sodhi et al. 2022). Correspondingly, customer service operations that are
equipped with AI systems can initially distinguish simple customer requests from complex ones based
on historical service records and then encourage customers with simple requests to use AI-assisted
services. Platforms can consider guiding customers to establish appropriate expectations of the AI
system and transfer customers with complex requests to human agents as quickly as possible. Third,
our results indicate the possible novelty effect of a newly implemented AI system. We find that the
duration of machine service and customers’ need for human service temporally increases after the
introduction of the AI system. Companies can take these effects into account when scheduling
resources for their newly implemented AI-based service systems. Moreover, in our study, suggestive
evidence from customer-AI conversations reveals that customers are more likely to turn to human
agents and complain about services after experiencing speech-recognition failures. This is likely not a
huge concern in the longer run, as AI-based services are likely to improve in terms of speech
recognition, given their learning capabilities.
6.3. Limitations and Future Research
Our study has several limitations, which also indicate ample opportunities for future research. First,
our experimental randomization is based on the last digit of customers’ phone numbers, rather than
being performed at the individual level, and thus, we balanced our samples before data analysis.
Second, due to data limitations, we could not observe detailed records of specific service requests
handled by the IVR system; therefore, we were unable to categorize service requests based on
objective service types. It will be interesting for future researchers to extend our findings based on the
objective complexity of customer service requests. Third, the current study focuses on the effects of a
voice-based AI system on call length, customers’ demand for human service, and customer
complaints. Future research could explore other outcome variables, such as service satisfaction,
customer retention, and future customer engagement, which reflect the value of AI implementation for
businesses. Moreover, leveraging the transcripts of customer-AI conversations, we conducted a few
preliminary analyses on customer-AI interactions by examining the negative effects of
speech-recognition failures. It would be interesting for future research to explore other factors in
human-AI interactions, such as emotions expressed by AI and service tones used by AI in
conversations, which inform the design of voice-based AI systems. Lastly, we analyze the
effectiveness of deploying a voice-based AI system to replace the traditional IVR system in the
telecommunication customer service setting in China. The generalizability of our findings might be
subject to the technical designs of the AI and IVR systems, cultural variation, as well as differences in
levels of technology development among countries, all of which may affect users’ attitude to and
adoption of AI. Thus, we encourage future research to further explore the implications of voice-based
AI systems among different user populations or in other service settings.






----------------------------------------------------------------------------------







Lu, T., & Zhang, Y. (2024). 1+ 1> 2? information, humans, and machines. Information
Systems Research.

Abstract
With the explosive growth of data and the rapid rise of artificial intelligence (AI) and automated working processes, humans
inevitably fall into increasingly close collaboration with machines, either asemployees or consumers. Problems in human-machine
interaction arise as a consequence, not to mention the dilemmas posed by the need to manage information on ever-expanding
scales. Considering the general superiority of machines in this latter respect, compared to human performance, it is essential to
explore whether human–machine collaboration is valuable, and if so, why. Recent studies have proposed diverse explanation
methods to uncover machine learning algorithms’ “black boxes,” aiming to reduce human resistance and enhance efficiency.However,
the findings of this literature stream have been inconclusive. Little is known about the influential factors involved or the
rationale behind their impacts on human decision processes.We aimed to tackle the above issues in the present study by specifically
examining the joint impact of information complexity and machine explanations. Specifically, we cooperated with a large Asian
microloan company to conduct a two-stage field experiment. Drawing upon studies in dual-process theories of reasoning that have
proposed different conditions necessary to arouse humans’ active information processing and systematic thinking,we tailored the
treatments to vary the level of information complexity, the presence of collaboration, and the availability of machine explanations.
We observed that with large volumes of information and with machine explanations alone, human evaluators could not add extra
value to the final collaborative outcomes. However, when extensive information was coupled with machine explanations, human
involvement significantly reduced the default rate compared with machine-only decisions.We disentangled the underlying mechanisms
with three-step empirical analyses.We revealed that the co-existence of large-scale information and machine explanations
can invoke humans’ active rethinking, which in turn, shrinks gender gaps and increases prediction accuracy. In particular, we
demonstrated that humans could spontaneously associate newly emerging features with others that had been overlooked but had
the potential to correct the machine’s mistakes. This capacity not only underscores the necessity of human-machine collaboration
but also offers insights into system designs. Our experiments and empirical findings provide non-trivial implications that are
both theoretical and practical.

Key words: Decision-making, Gender Biases, Human-Machine Collaboration, Information Processing,Machine Explanations,
Micro-finance, Rethinking

1. Introduction
Given the fast rate of artificial intelligence (AI) commercialization and its penetration into daily life, humans have
started to closely collaborate with machines, both as employees and consumers (Alibaba 2018,Wang et al. 2023a).
For example, many companies have introduced AI-based coaching systems to assist humans and improve their
decision-making effectiveness and efficiency (Loutfi 2019). In reality, humans and machines can complement each
other. Previous research has found that the decision-making accuracy of machine-learning algorithms is generally
higher than that of humans under normal circumstances (Grove et al. 2000). However, humans are more likely to
use experience to identify and process low-frequency cases that are difficult to include in machine-learning algorithms;
humans also have more advantages than machines in terms of flexibility (Sawyer 1966).More importantly,
humans’ deep thinking is awell-established andwell-understood tool for augmenting performance on independent
or team tasks (Amit and Sagiv 2013).
Unfortunately, there are various constraints, such as information opacity, machine-learning algorithms’ complexity,
and personnel’s lack of experience with or understanding of advanced technologies. Accordingly, the realized
performance of human-machine collaborations falls short of the expectation due to distrust of machines
(Jacovi et al. 2021) or over-reliance on them (F¨ ugener et al. 2021). Even worse, without properly designed collaboration
systems, humans’ involvement could reduce the collaborative performance for various reasons, such as their
being over-cautious (Lu et al. 2023b) or hyper-focused on details (Wang et al. 2023c).
To address the urgent, essential question regarding how to efficiently change humans’ responses to machines
from either aversion or over-reliance to active contribution, researchers have recently begun to turn to machinelearning
model explanations (Schmidt et al. 2020, Bauer et al. 2023). However, previous investigations in this vein
have predominantly concentrated on technical solutions and lacked a comprehensive examination of the conditions
and underlying mechanisms that influence the solutions’ impact on human decision processes. This omission
introduces certain limitations, as not all model explanations prove effective in every scenario (Chen et al. 2023).
In this study,emphasis is placed on task complexity, particularly information complexity, a contingent factor that
plays a pivotal role in shaping the effectiveness of machine explanation implementations.We posit that task complexity
and machine explanations shouldwork concurrently to foster deep thinking in humans, thereby contributing
to the efficacy of human-machine collaborations. Specifically, task complexity and information richness engage
humans in deliberate information processing by capturing their attention and interest in complex decision tasks
(Levin et al. 2000). The presentation of machine explanations that serve as valuable cues and decision-making references
prompt humans to carefully reassess decisions, address conflicts, and actively process information through
cognitive reasoning (Mantel and Kardes 1999). Through the alignment of these conditions, humans are more likely
to employ enhanced decision-making strategies, ultimately improving the performance of human-machine collaborations.
Notably, prior studies exploring the value of machine explanations have typically conducted lab experiments
or simulations alone. This approach proves challenging, as participants tend to present differently and participate
more actively in a controlled lab environment (Keil et al. 2000). Consequently, there is a compelling need to adopt
a more pragmatic approach––a realization that led us to design and implement field experiments. These experiments
serve as a crucial means of observing and analyzing human behavior in more authentic, real-world scenarios,
particularly with regard to their ability to navigate and respond to varying levels of information complexity and
cues.
Therefore, in this paper,we apply field experiments to determine whether and howhumans’ potential to achieve
“1 + 1 > 2” can be realized, particularly in the context of increasing technological development and humanmachine
collaboration. Our three research questions are: (1) What is the realized performance when humans and
machines collaborate under different levels of information complexity and different system designs? (2) What are
the underlying mechanisms? (3) How do human characteristics affect collaborative performance?
We focused on the microloan industry and partnered with a large Asian microloan company to conduct a twostage
field experiment. We dove into the dual-process theories of reasoning (Evans 2003), suggesting two prerequisites
for invoking humans’ deep thinking: information complexity initially draws humans’ attention and engages
them in the tasks, and useful cues drive humans to actively consider the task. Accordingly,we experimentally manipulated
how much information about borrowers was provided to evaluators, whether evaluators got to see the
machine’s recommendation, and whether the machine’s recommendation was explained to the evaluators.
Our empirical analyses yielded several interesting findings. First, with small information volumes, human evaluators
could not add extra value to the final outcome (i.e., the default rate prediction accuracy). Second, the human
evaluators outperformed the machines when the human evaluators were allowed to observe the machine’s suggestions
before making their final decisions and when the machine explanations were offered and the information
volume was large. In these cases, human evaluation resulted in a 2.02% reduction in the default rate (from 5.15 to
3.13%). However, this improvement disappeared if either machine explanations or information complexity were
not given. Third, we observed that when humans and machines made decisions independently, a certain amount
of disagreement was inevitable. In the human-machine collaboration modes, a disagreement of 62.82% resulted
from a small information volume without machine explanations, compared with 85.67% disagreement resulting
from large amounts of information and disclosure of machine explanations.
To disentangle the potential mechanisms and explain the above findings, we employed a three-step analytical
framework. Our findings suggested several important insights. First, human evaluators tended to stick with traditionally
important features such as income or education level, while machines explored more possibilities using
other sources of information, including shopping and offline trajectory behavior. This explains why machines, in
general, performed better than humans, especially when large amounts of information were offered. Second, with
the availability of machine explanations and large information volumes, evaluators performed active rethinking
when inconsistent decisionswere made. This improved their final decision accuracy by, for example, correcting the
risk evaluation of female borrowers. However, such a rethinking process did not occur if either condition was not
satisfied. Third, we disentangled the “rethinking” procedure in which humans associate the machine explanations
with other features if they considered the displayed features to be “non-informative”.
Furthermore, when considering individual heterogeneity among human evaluators,we found that though more
experienced evaluatorswere less likely to followthe machines’ suggestions, theywere stimulated in their rethinking
by the machines’ suggestions and explanations, and this, in turn, improved company performance. In addition,
we compared repayment behavior to examine the existence of potential gender-based decision biases. Our findings
suggest that with more data and machine explanations, human-machine collaboration could potentially shrink the
inter-gender default rate gap, which was initially and unintentionally produced by machine-learning algorithms.
This further highlights the value and necessity of collaboration between humans and machines.
The contributions of our study are multi-fold. First, it adds to the emerging literature on human–machine collaboration.
Whereas a few of the most recent studies have investigated whether humans and machines complement
each other in decision-making in different contexts (e.g., Cao et al. 2021, Luo et al. 2019, Zhang et al. 2023), the
majority have suggested outcomes only implicitly or ostensibly. Through in-depth mechanism detection analyses,
our study unravels how and why properly designed collaboration can invoke humans to contribute. Thus, we
advance this stream of literature by revealing the existence and value of humans’ rethinking processes, both theoretically
and empirically. Second, we contribute to the recent literature on the value of offering machine explanations
within the context of human-machine collaboration. The existing literature has not reached a consensus on how
humans respond to machines’ advice in the case of machine explanations (Krishna et al. 2022). Our study proposes
and verifies one reason of inconclusive findings in prior literature: the outcome of providing machine explanations
is related to other conditions such as humans’ perception of the environmental or task complexity. Whereas previous
studies have largely suggested that displaying (feature-based) machine explanations would invoke humans’
System 1 thinking (i.e., heuristics or rules-of-thumb for making quick judgments) rather than System 2 (active
reasoning and rethinking) (Chen et al. 2023), we demonstrate that with a proper collaboration design, machine
explanations can prompt humans’ rethinking and improve human–machine collaboration. Third, we add to the
recent stream of literature regarding machine biases. Recent studies have proposed the utilization of multi-source
data to alleviate algorithmic discrimination and sample biases. In fact, there is evidence that alternative data sources
would eliminate biases related to race and socioeconomic factors (Lu et al. 2023a). However, machine failure has
already been proven (Fuster et al. 2022,Huet al. 2022), so this paper not only identifies the sources of gender biases
but also uncovers the value and necessity of human involvement to make up for machine failure.
2. Related Studies
This section first summarizes three related streams of literature, then offers an introduction to the theoretical framework
underpinning experimental treatment design.
2.1. Human CollaborationWith and Aversion to Machines
AI applications require human intervention and assistance. Previous studies have explored the pros and cons of
human–machine collaboration in decision-making. For example, studies have shown that most statistical models
exceed or approach the judgment accuracy of the average clinician (Camerer et al. 2019).Machine algorithms have
been extensively shown to manage substantial amounts of data more proficiently than humans (Peukert et al. 2023,
Wang et al. 2023c). However, despite the fact that machines can make highly accurate predictions, it is difficult for
them to handle random or uncertain cases and boundary cases whose features show contradictory patterns on the
prediction objectives (labels) (Guo andWang 2015). By contrast, humans are found to be better at identifying rare
cases (Sawyer 1966) and to perform more effectively in innovative areas such as new product development (Lou and
Wu 2021). Recent studies have shown the superiority of human–machine collaborations over both full machine
automation and human-only operations (F¨ ugener et al. 2022), and have shed light on the merits of “the humanin-
the-loop” (F¨ ugener et al. 2021). On the one hand, machines can augment the capabilities of humans, such as
managers (Davenport et al. 2020); and on the other hand, humans can complement machines by contributing their
general intelligence (Te’eni et al. 2023) and diverse ideas (Wang et al. 2023d, Zhang et al. 2023) and incorporating private
information (i.e., data that only humans can use such as in-house data) (Choudhury et al. 2020, Ibrahim et al.
2021, Sun et al. 2022). Cao et al. (2021) showed that when analysts are given access to a small amount of alternative
data and in-house machine resources, combining machines’ computational power and humans’ understanding of
soft information produces the best performance in generating accurate forecasts.
However, recent research has also revealed that humans might resist the adoption or usage of machines, resulting
in low efficiency of human–machine collaboration (Allen and Choudhury 2022, de V´ericourt and Gurkan
2023,Wang et al. 2023b). This resistance exists not only among those who accept machines’ advice (e.g., Commerford
et al. 2022, Liu et al. 2023), but also among machine-based service targets, namely ordinary consumers. For
example, the adoption of chatbots has had negative effects on user acceptance and efficiency due to consumers’
insufficient knowledge and relative lack of empathy from chatbots (Luo et al. 2019). However, this negative impact
may be mitigated by users’ experience levels (Luo et al. 2021, Tong et al. 2021), flexibility, and willingness to make
adjustments based on machines’ predictions (Dietvorst et al. 2018). Human aversion to machines could also be
due to the potential of machines to threaten human jobs. AI robots have replaced and will replace human labor
in different ways in various fields (Brynjolfsson and Mitchell 2017, Lu et al. 2018). Machines have outperformed
humans in many jobs, especially low-skilled, repetitive, and dangerous ones (Autor and Dorn 2013). Conversely,
F¨ ugener et al. (2021) warned that we must also attend to humans’ over-reliance on machines, which would render
human–machine collaboration useless.
2.2. Machine Explanations
The lack of model explanations could result in human aversion to machines, stemming from a sense of distrust
(Siau andWang 2018). To avoid such negative outcomes, the existing literature has examined multiple approaches.
Acommonlyadopted approach improves trust in human–machine collaboration settings by offering more detailed
information of machine-learning decisions (Lu et al. 2019, Rai 2020). Through various post-hoc explanation methods,
human participants can be assisted in constructing suitable mental models under diverse conditions, thereby
enhancing their trust and the model efficiency (Mohseni et al. 2020). However, this approach should be employed
with caution. Schmidt et al. (2020) indicated that offering unintuitive explanations (i.e., those dealing with features
humans are unfamiliar with) may fail to boost humans’ trust in machines. Rudin (2019) also cautioned that
post-hoc explanations tend to offer incomplete and biased information regarding the mechanisms underlying algorithms.
This may lead participants to overestimate their ability to explain decisions declaratively, resulting in misinformation.
Our research aligns with this common practice. However, while some previous studies have explored the impact
of machine explanations on human–machine collaboration, few have delved into the specific mechanisms of how
and why such an approachworks in influencing human decision processes. The most similar study to ours is Bauer
et al. (2023), which revealed that humans can dynamically adjust the importance they attribute to available information
and adapt their mental models based on machine explanations. Additionally, their findings highlighted
that the provision of machine explanations might reinforce confirmation bias, potentially resulting in suboptimal
or biased decisions.However, our study differs from Bauer et al. (2023) in at least two key aspects. First, while Bauer
et al. (2023) only attended to a limited number of borrower features,we additionally consider information complexity.
As outlined in Section 2.4, we contend that the effectiveness of machine explanations in shaping individuals’
information processing depends on the complexity of the information presented to them.Machine explanations
stimulate active cognitive information processing only under specific conditions of information complexity. Furthermore,
under certain conditions, the overall performance of human–machine collaboration may see improvement
rather than deterioration. Second, the findings of the study by Bauer et al. (2023) could have been influenced
by their use of online lab experiments. By their nature, lab experiments present challenges related to sample representativeness
(Compeau et al. 2012). Of greater significance is the potential for participants to react differently
within the confines of a lab setting, which is characterized by specific monitoring and anchoring conditions. Participants
might naturally respond more actively and attentively to the experimental manipulations, potentially
leading to an overestimation of their behavioral outcomes (Keil et al. 2000). In contrast, our study adopts a field
experiment approach within a real-world micro-finance context to examine individuals’ decision-making in a more
natural setting.
2.3. Investors’ Decision-Making in Micro-finance
Many scholars have focused on individual investors’ decision-making in micro-finance businesses, including P2P
lending, crowdfunding, and microloans.Asubset of the literature has revealed the important factors that investors
consider in their decision-making (e.g., Gonzalez and Loureiro 2014, Tao et al. 2017, Wang et al. 2019). Studies
have also identified biases in micro-finance investors’ decisions, including preferences regarding gender (Chen et al.
2017) or location (Lin and Viswanathan 2016). Recent research has paid attention to the value of machine-assisted
tools in financial decision-making. For example, Ge et al. (2021) found that P2P lending investors experiencing
more defaulted loans are more likely to perceive the market to be risky and thus tend to rely more on their own
judgment rather than a robot advisor. Additionally, some investors attempt to intervene in machine usage. They
may be more concerned about returns and less likely to lose confidence in machines immediately after observing
a machine failure (Germann andMerkle 2019), or they may tend to adjust their machine usage based on the latest
performance (Ge et al. 2021). In our study, we also delve into both decision-making accuracy and potential biases
within the micro-finance context. However, unlike existing studies, our emphasis lies in examining how machine
decisions function as recommendations to influence users’ decision-making.
2.4. Theoretical Underpinning: The Dual-Process Theories of Reasoning
Humans’ and machines’ respective advantages in decision-making and their collaborative value lie in their complementarity
(Feuerriegel et al. 2022). However, humans fall easily into aversion toward or over-reliance on machines;
neither situation yields better decision outcomes than either human-only or machine-only decision-making.
Therefore, one key to promoting the value of collaboration between humans and machines is to invoke humans’
deep thinking in their co-working with machines. The literature on dual-process theories of reasoning (Evans
2003), our theoretical underpinning, raises the question of howhumans’ deep thinking can be aroused in machineassisted
tasks. The dual-process theories of reasoning propose the existence of two cognitive systems, “System 1”
and “System 2”, that underlie thinking and reasoning. System 1 processes information and reasoning fast, automatically,
and with minimal effort, leading to quick and instinctive decision-making as a rapid response to familiar
situations and stimuli. In contrast, System 2 operates at a slower pace, involves deliberate thought, and requires
conscious effort. It incorporates logical reasoning and analysis and involves the application of cognitive resources
(Kahneman 2011).
Several factors can determine whether individuals opt for System 1 or System 2 information processing and reasoning.
To encourage individuals to embrace System 2 processing, certain conditions must be met. Specifically,
since System 2 is typically involved in complex tasks, problem-solving, critical thinking, and decision-making in
novel or challenging situations, task complexity is a primary condition.Task complexity, often represented by information
complexity (Amit and Sagiv 2013), stimulates deep thinking in individuals by capturing their attention
and interest in decision tasks (Levin et al. 2000). As proposed by Endsley (1995), being well-informed about the
situation at hand is a prerequisite for subsequent deep reasoning and action selection. Information complexity,
manifested as multiple alternatives and/or numerous attributes, influences users’ situational processing of observed
information (Bauer et al. 2023, Sun and Taylor 2020). Specifically, new attributes provide novel pieces of information
that enhance one’s recognition of the decision tasks and domain (He et al. 2020). Faced with greater volumes
of more diverse, unfamiliar information, individuals are inclined to invest more effort in reasoning through more
ambiguous task situations (Van der Schalk et al. 2010). In other words, although more complex information may
not necessarily result in increased decision-making accuracy, it does enhance individual’s willingness to actively participate
in decisions (Oskamp 1965). With complex information, people are more willing to perceive the increase
in information as useful and desirable, even if it comes with a certain level of burden (Amit and Sagiv 2013). In
contrast, with simple information, people tend to make rapid decisions via System 1 processing (Speier 2006).
The second condition for motivating people to engage in high-quality System 2 processing (i.e., active consideration
and systematic deep thinking) is the presence of useful cues for reference. A well-designed reference cue has
the potential to prompt individuals to meticulously reassess their decisions and compare them with the provided
references (Weiss 1982). Consequently, individuals can rectify their initial decisions, address conflicts, and even generate
novel ideas through cognitive reasoning, association, and imagination (Hollnagel 1987). Several approaches
can be effective in fostering such deep thinking. First, high information quality leads to elevated epistemic motivation
(Cacioppo et al. 1996). For instance, structured and concrete information can encourage individuals to
engage more deeply in a task and, therefore, process information more actively and positively (Mantel and Kardes
1999). Additionally, when individuals are provided with explicit reference points (Chernev 2003), they maintain
high motivation to engage in cognitive reasoning and adopt superior information-processing strategies to navigate
complex decision-making. Moreover, the decision to employ System 2 processing can be influenced by individuals’
experience and expertise. When faced with novel and unfamiliar situations, individuals are more inclined to
activate System 2 processing to tackle challenges and gain new knowledge (Smerek 2014).
Applying the lens of this theoretical literature stream to human–machine collaboration,we propose two designs,
each of which corresponds to one of the two conditions mentioned above: (1) offering humans and machines rich
information for decision-making, and (2) exposing humans to structured machine explanations for final decisions.
Specifically, decision-making with rich information requires strong cognitive abilities for information processing
(Icard 2018); this arouses humans’ perception of the task complexity (Sun and Taylor 2020). We thus posit that,
compared with limited information, offering rich information could enhance and maintain humans’ awareness
of decision-making tasks and their willingness to participate in the tasks, regardless of their capability for handling
large information volumes. Furthermore, presenting machines’ decisions as recommendations along with
proper machine explanations showing how the prediction outcomes were obtained by machines in a faithful and
human-interpretable manner (Krishna et al. 2022) can trigger individuals’ active cognitive reasoning. For example,
if machine explanations are provided, humans can learn from machines’ decision-making rationale, trace back their
own decision rules, and double-check whether the new knowledge from machines fits and actually improves decision
accuracy (Mohseni et al. 2020).We call this the rethinking process. In this paper, rethinking or reconsideration
refers to the process of carefully reviewing a decision or conclusion that has previously been made to determine
whether the initial decision should be changed. It is usually an inquiry into, or reflection on, the most basic given
information, or the asking of fundamental questions such aswhy and howbreakthrough improvementswere made
after observing new signals or outcomes (Jain and Pagrut 2001). Such a self- and system-monitoring process aligns
with the concept of active consideration (i.e., Pattern 5) developed by Jussupow et al. (2021), which was concluded
to be the best practice for achievement of satisfactory outcomes from human–machine collaboration.
Broadly speaking, notwithstanding the many and broad investigations into human–machine collaboration,
there is a dearth of literature unraveling the decision-making process during human interactions with machine
assistants under diverse conditions. This paper aims to bridge that void. Particularly, we focus on the role of information
complexity and machine explanations in prompting humans to actively rethink and improve the consequent
decision outcomes. Given the complex environments covering interactions among information volumes,
machine explanations, human experience, and behavioral biases, this question might not have a fixed and intuitive
answer.We also reveal the scenarios that can leverage humans’ and machines’ respective advantages to realize 1 + 1
> 2.
3. Experimentation
3.1. Experimental Background
We partnered with a large Asian microloan company to conduct a field experiment. The microloan company was
founded in 2011 and served over 250,000 borrowers by 2018, with unsecured microloans of approximately US$465.
The company uses only the owner’s money for lending, and their loans are mostly used for temporary financial
needs such as supplementary cash flow for small businesses and irregular shopping needs. The loans have a term of
1–7 months and are repaid in monthly installments starting one month after their issuance. The company sets its
annual interest rate from 12 to 16%.1
To apply for a loan, a borrower is required to provide their basic personal information such as name, phone
number, gender, age, educational level, and income level. Subsequently, borrowers are required to choose the loan
amount (US$46.5—US$1,240, US$465 by default) and loan term as well as check the annual interest rate. In this
study, we focused only on loans with a term of 1, 2, or 3 months.2 In addition, borrowers are required to clearly
state the purpose of the loan. They can then submit their application. Every new application is randomly assigned
1 The annual interest rate is set on a daily basis, rather than assigning different rates to borrowers with different assessed credit risks. This
daily interest rate is generally determined at a mediate level in the online lending market. The company does not announce the actual rates to
the market in advance, and borrowers are therefore less likely to decide strategically when to apply to get a lower rate. Such a design allowed
us to tease out the potential endogeneity issues brought by interest rates.
2 This was to facilitate our experimental observation, because it takes a long time to observe and confirm the repayment or default behavior
when the loan term is long. We compared the repayment performance among loans of different terms with the historical data and found
that the loan term was not highly related to repayment performance.
to a human evaluator who assesses the borrower’s credit risk (i.e., default probability) based on the collected information,
and makes the final loan-approval decision accordingly. The focal company’s loan-approval rate is approximately
47%, similar to the competitors in the market. The main goal of loan screening is to minimize the number
of defaulted cases while maintaining the approval rate specified above.
3.2. Experimental Setup
3.2.1 Implementation of Treatment I: Information Complexity
Inspired by the dual-process theories of reasoning, we introduced two factors that could influence human evaluators’
decision-making in collaboration with machines in Section 2.4. As the first step,we utilized the focal empirical
setup to incorporate variations in information complexity. Before our experiment, the focal platform granted loans
based entirely on human evaluators’ decisions. Evaluators only accessed borrowers’ basic information, loan history,
and current loan attributes (12 variables [features] in total) to make their credit risk evaluation. Thus, this information
comprises the first level of information complexity: small information volumes. To construct an alternative
information scenario (i.e., with large information volumes),we asked the focal company to collect additional information
from the borrowers starting June 1, 2017. The additional information included recent (past six months)
online shopping activities on the largest e-commerce platform in the focal country and cellphone usage information
collected from the pertinent communication carriers.3 Previous studies have suggested that shopping and cellphone
usage may be correlated with borrowers’ socioeconomic status and credit behaviors (e.g., Blumenstock et al.
2015). Therefore, based on the relevant literature and canonical behavioral theories (Lu et al. 2023a), we extracted
32 features for each source in order to comprehensively describe borrowers’ online shopping and cellphone usage
and mobility trace characteristics. Table A1 in Appendix A1 describes these features.
3.2.2 Machine Preparations and Implementation of Treatment II: Machine Explanations
Since the focal company had not sought any machine assistance before our collaboration, it was necessary for us
to design and train prediction models for each of the two information scenarios. Our training samples comprised
borrowers who submitted loan applications June 1–30, 2017. For these sampled borrowers, the human evaluators
3 The groups with large information volumes had access to multi-sourced information, emphasizing information diversity (i.e., new
attributes). Labeling one treatment as “large information volumes” is intentionally contrasting it with the small-sized demographic feature
set used in the other groups. Therefore, our manipulation is intricately aligned with the concept of information complexity, as elucidated
in Section 2.4.
assessed their credit risks and made loan-approval decisions using small-scale information, as usual. At this stage,
the human evaluators did not have access to the additional information collected. We then gathered repayment
information for the approved borrowers from more than 9,000 training sample loans made between July 1 and
November 30, 2017. Since the loan term was no longer than 3 months, a 5-month observation period was sufficient
for us to confirm borrowers’ repayment and default behaviors. Default is defined as the failure to fully repay the loan
at least 60 days after the loan due date. At the end of November, we obtained the borrowers’ basic and additional
information, as well as their repayment behaviors.
Based on the above information, we then trained machine-learning algorithms. For both information scenarios,
we implemented standard operationalizations (e.g., 10-fold cross-validation, out-of-sample prediction, and hyperparameter
tuning) and replicated the training procedures multiple times until they achieved stable loan default
prediction performance.We tried diverse, widely accepted machine-learning models, including logistic regression,
support vector machine, k-nearest neighbor, multi-level perceptron, random forest, and extreme gradient boosting
(XGBoost).XGBoost achieved the best performance, sowe employed it in our experiment.To maintain a relatively
comparable performance across experimental groups,we did not updateXGBoost during the experimental period.
Meanwhile,we leveraged the same training samples to train the human evaluators. Specifically,we randomly separated
the human evaluators into two groups: one group maintained the previous loan evaluation process with the
small information volume, and the other group evaluated credit risks and made loan-approval decisions with the
large information volume. After a 7-day training period, all human evaluators reached a stable evaluation performance.
Please refer to Appendix A2 for detailed information on the human evaluators and the training procedure.
With the pre-trained prediction models, we were able to design the second treatment. Specifically, to prepare
the machine explanation information based on the above machine-learning algorithms, we implemented a SHAP
analysis method, which yields Shapley values representing the average expected marginal contribution to predicting
the default probability of one feature after all possible combinations have been considered (Roth 1988). In Figure 1,
we present the most important features under the two information volume scenarios.
3.3. Experimental Design
To identify the loan approval decision performance under human-only, machine-only, and human–machine collaboration
decision-making scenarios, we designed and implemented a two-stage experiment, as illustrated in Figure
2.
(a) Decisions with Small Information Volume (b) Decisions with Large Information Volume
These features rank in the top 5 or 7 in respective analyses. The other features play only limited roles in machine-learning-based predictions
(i.e., they have very small absolute scores). Positive (negative) values mean that the features are positively (negatively) related to default
behavior.
Figure 1 Important Features in Machines’ Decision-making Processes
Figure 2 Experimental Process
Experimental Stage 1. The first stage began on December 8, 2017, and lasted for oneweek. The relatively short term
of the treatments helped tease out the potential confounders stemming from the substantial evolution (learning
or change) of the human evaluators, machine-learning algorithms, and borrower-characteristic distributions with
long-term experience. At this stage, the company collected basic and additional information from every new borrower,
and we randomly assigned the borrowers to one of the four groups. In Groups 1 (H & S) and 2 (H & L), a
credit risk assessment was completed by human evaluators. They had access to the small (Group 1) or large (Group
2) information volumes to inform their approval or rejection of each loan application. The two human evaluator
groups were consistent with those in the training process described earlier. In Groups 3 (M & S) and 4 (M &
L), we employed the corresponding pre-trained XGBoost to predict each application’s default probability based
on a small (Group 3) or large (Group 4) number of features and to make loan-approval decisions by ranking the
predicted default probability from lowest to highest. Following the company’s usual practice, we maintained the
loan-approval rate at 47% in all four experimental groups. For all granted loans,we continued tracing and collecting
their repayment behavior from January 8 toMay 14, 2018.
Experimental Stage 2. We spent another two weeks (from December 15 to 28, 2017) conducting the second stage
of our experiment. The two-week period ensured that the evaluation workload was similar to that in the first stage.
Again, we randomly assigned each new loan application to one of the four groups. In all groups, the human evaluators
were instructed to collaborate with the machine. Specifically, human evaluators in Group 1 were randomly
assigned to Groups 5 and 6 and those in Group 2 were assigned to Groups 7 and 8, with an equal number of evaluators
in each group to manage the same amount of information. As illustrated in Figure A2 in Appendix A3, the
loan-approval decision process had two steps. In the first step, human evaluators made credit risk evaluation and
loan-approval decisions independently with small (Groups 5 and 6) or large (Groups 7 and 8) information volumes;
this is identical to the situation in Stage 1. In the second, decision-making step, the machine-learning algorithm’s
loan-approval decision for the same loan was presented to the human evaluators. In Groups 5 and 6, the machinelearning
algorithm used the trained model with a small number of features (corresponding to Group 3), and in
Groups 7 and 8, it used a large number of features (corresponding to Group 4). The human evaluators did not
have much knowledge of the applied machine-learning algorithm; theywere simply notified that machine-learning
algorithms usually have strong decision-making abilities.
Next, we incorporated the second treatment, the existence of machine explanations. Specifically, in Groups 5
((H + M)&S&w/o Expl) and 7 ((H + M)&L&w/o Expl),we gave only the machine’s loan-approval decisions
to the human evaluators, without explanations regarding how the decision had been reached (see Figure A2a). In
Groups 6 ((H + M)&S&w/ Expl) and 8 ((H + M)&L&w/ Expl), the human evaluators could see not only the
machine’s loan-approval decisions but also the post-hoc explanations (i.e., the most important features presented
in Figure 1). For these features, the human evaluators could find and compare the values of the fixed features of
the focal borrower and the average values of non-defaulters (see Figure A2b). The human evaluators in Groups
6 and 8 were provided this information at the beginning of experimental stage 2. We conjecture, based on our
theoretical framework, that this information (strengthened by the value comparison) served as an ideal reference
due to machines’ superior capability (Chernev 2003). Then, human evaluators were required to make their final
loan-approval decisions. When their initial decisions were incongruent with the machine’s, they could either insist
on their own decisions or adjust them to followthe machine’s recommendations. As mentioned before, the human
evaluators were told to maintain a consistent approval rate before and during the experiment, and so the approval
rates in all of our experimental groups were maintained at approximately 47%. Similarly, we continued to collect
the Stage 2 borrowers’ repayment performance data over the subsequent 5 months.
3.4. Experimental Data
We obtained our experimental data after completing repayment information collection. The dataset contained
the borrowers’ basic and additional information, the human evaluators’ and machines’ initial approval decisions
(Groups 1 to 8), the human evaluators’ final approval decisions (Groups 5 to 8), and the repayment performance
(default or not) of the approved loans. Additionally, we collected background information on the human evaluators,
including their gender, education level, number of months’ experience (discretized by six month period), and
historical decision accuracy (i.e., the ratio of defaulted loans to all approved loans in the three months before our
experiment).
Table 1 Randomization Check
Loan characteristics Borrower characteristics
Group #Obs. Loan amount (US$) Interest rate (%) Loan purpose Gender Age Living city DPI (US$) Monthly income level Education level
1.H& S 2,924 472.8 13.888 0.446 0.235 25.17 6,528.9 4.886 4.252
2.H& L 2,930 473.7 13.911 0.437 0.241 25.18 6,505.0 4.886 4.256
3.M& S 3,001 472.9 13.930 0.431 0.249 25.12 6,524.7 4.902 4.201
4.M& L 3,020 472.8 13.913 0.430 0.237 25.19 6,565.2 4.942 4.9206
5. (H + M) & S & w/o Expl 2,885 474.7 13.920 0.437 0.245 25.07 6,545.5 4.960 4.223
6. (H + M) & S & w/ Expl 2,918 470.7 13.902 0.437 0.241 25.15 6,588.1 4.884 4.218
7. (H + M) & L & w/o Expl 2,978 475.2 13.924 0.428 0.233 25.09 6,563.7 4.874 4.216
8. (H + M) & L & w/ Expl 2,946 475.4 13.904 0.434 0.240 25.11 6,571.6 4.943 4.257
Group #Unique evaluators Evaluator gender Evaluator education level Evaluator months working Evaluator historical (decision) accuracy
1.H& S 31 0.774 4.452 2.516 2.000
2.H& L 31 0.774 4.452 2.516 2.065
a H= human decision,M= machine decision,H+M= human + machine decision, w/o Expl = without AI explanations, w/ Expl = with AI explanations.
b Loan purpose: 1 = consumption, 0 = others (e.g., for emergency). Gender: 1 = female, 0 = male.
c Monthly income level: 1 = US$150 or below, 2 = US$150–US$300, 3 = US$300–US$450, ..., 8 = US$1,050–US$1,200, 9 = US$1,200 or above.
d Education level: 1 = middle school or below, 2 = vocational school, 3 = high school, 4 = technical school, 5 = undergraduate, 6 = graduate or above.
e Evaluator months working: 1 = not longer than 6 months, 2 = 6–12 months, 3 = 13–18 months, 4 = longer than 18 months.
f Evaluator historical (decision) accuracy: 1 = low (default rate>15%), 2 = medium (10%<default rate<15% ), 3 = high (default rate<10%). Refer to Table A2 for descriptive statistics on evaluator
historical accuracy.
g Groups 3 and 4 did not involve human evaluators. In experimental stage 2, the human evaluators in Group 1 (or 2) were randomly and equally assigned to Groups 5 and 6 (or Groups 7 and 8).
h For every feature, the values show no significant differences across the groups based on the F-test.
Figure 3 Default Rates of Experimental Groups

There were a total of 23,805 loans in the 8 groups involved in our experiment.We removed 203 repeat borrowers
from the company to avoid interference from the previous experience. The final experimental sample size was
23,602. Table 1 reports the sample size and the major characteristics of borrowers, loans, and evaluators across the
experimental groups.Most of the borrowerswere men (>75%); 28.43% of the borrowers had received an undergraduate
education, and the average (self-reported) monthly income ranged between US$450 and US$600. Approximately
44% of the loans were for personal consumption purposes. Regarding the human evaluators, most were
female (77%) with a technical school or undergraduate-level education background. On average, the human evaluators
had been working for the company for approximately 1 year, and those with high, medium, and low levels of
historical decision performances were evenly distributed between the groups (i.e., around 1/3 each).We detected
no statistically significant differences between the groups, which suggested that the randomization had been successful.
4. Empirical Findings
Our key variable of interest was borrowers’ default rates. This is a common metric in the microloan industry (Fu
et al. 2021) and within the focal company. We defined it as the ratio of defaulted loans to the total number of
approved loans. Figure 3 plots the default rates across all groups, and Table 2 calculates the inter-group differences
with between-group t tests. The default rate in Group 1 was 12.83%, echoing the average performance of the focal
company before our experiment. The comparison yielded several interesting patterns. First, as expected, when making
decisions separately, the human evaluators performedworse than the machines, and the large-scale information
volumes increased the performance gap (i.e., Comparisons B and D). Second, the human evaluators did not add
additional value when jointly deciding based on a small information volume, regardless of whether the machine
explanationswere offered (i.e., Comparisons E, G, andHwith insignificant differences in the mean value of default
rates). Third, we observed different outcomes in the scenarios with large information volumes. In particular, when
the human evaluatorswere presented with the machines’ suggestions and the machine explanations before making
their final decisions, they performed better than the machines’ independent decisions, showing a 2.02% reduction
in default rates, from 5.15 to 3.13% (i.e., Comparison J). This suggests that the human evaluators contributed additional
value to the evaluation process that only they, as humans, could provide. However, this improvement disappeared
if no machine explanation was provided (i.e., Comparison I). In sum, the collaborative values were only

Table 2 Comparison of Default Rates among Different Experimental Groups
Comparison Experimental groups Difference in means p-values
A 1.H& S vs. 2.H& L 0.0228 0.0650*
B 1.H& S vs. 3.M& S 0.0268 0.0275**
C 3.M& S vs. 4.M& L 0.0500 0.0000***
D 2.H& L vs. 4.M& L 0.0539 0.0000***
E 5. (H+M) & S & w/o Expl vs. 6. (H+M) & S & w/ Expl 0.0032 0.7833
F 7. (H+M) & L & w/o Expl vs. 8. (H+M) & L & w/ Expl 0.0287 0.0003***
G 3.M& S vs. 5. (H+M) & S & w/o Expl -0.0048 0.6779
H 3.M& S vs. 6. (H+M) & S & w/ Expl -0.0016 0.8892
I 4.M& L vs. 7. (H+M) & L & w/o Expl -0.0085 0.3215
J 4.M& L vs. 8. (H+M) & L & w/ Expl 0.0202 0.0071***
a As our experiment comprised multiple treatments, we followed multiple hypothesis testing in experimental economics (List et al.
2019) to address the potential bias. Thus, p-values are multiplicity-adjusted values based on between-group t tests. *p < 0.10, **p <
0.05, ***p <0.01.
realized if the two conditions, information complexity and useful cues, were satisfied. We also considered profit
gains and evaluated the dollar values of the different factors. The results in Figure B1 in Appendix B1 confirmed the
consistency.
Noticing the above diverse patterns, we then further decomposed the decision-making behavior of human evaluators
after they had observed machines’ suggestions. Specifically, in Figure 4, we compared the decision consistency
between humans (initial decision) and machines (in Figure 4a), and calculated the adjustment ratios when
inconsistency arose (in Figure 4b). Our results indicate that when the human evaluators were making decisions
independently (i.e., before observing the machines’ suggestions), there were a certain number of cases in which
the humans disagreed with the machine’s decisions. As shown in Figure 4a, the agreement proportion was smaller
with the large information volume (83.78% consistency in Group 5 vs. 78.58% consistency in Group 7). This pattern
was similar regardless of whether machine explanations were available. In the human–machine collaboration
scenario, the human evaluators adjusted their decisions by following the machines’ recommendations. The proportion
of adjustment, however, varied across the experimental groups. In particular,we observed that only 62.82%
disagreement was eliminated with a small information volume and no machine explanation. The adjustment rates
significantly increased when the information volume was large or machine explanations were offered. For example,
compared with limited information, the availability of large amounts of information could mitigate the human
evaluators’ unwillingness to follow machines, decreasing it by 18.21% (Group 5 vs. Group 7, p-value<0.001).Meanwhile,
machine explanations also encouraged human evaluators to accept the machines’ decisions by improving
the ratio of following from 81.03 to 85.67% (Group 7 vs. Group 8, p-value = 0.026).

(a) Ratio of Decision Consistency between Humans andMachines (b) Ratio of FollowingMachines’ Decisions
Notes: p-values are multiplicity-adjusted values (List et al. 2019) based on between-group t tests.
Figure 4 Consistency and Following between Humans and Machines
5. Mechanism Examinations
This section aims to disentangle the potential mechanisms driving the differences in performance between humans
and machines and the contributions made by humans when collaborating with machines. This part consists of
three steps.We first examined empirically why humans and machines decided differently when making decisions
separately and how decision inconsistency explained the performance differences. Second, we isolated the underlying
behavioral mechanisms explaining why humans disagreed with the machines’ recommendations when collaboration
was allowed. Third, we discussed how disagreement affects decision quality, and decomposed the human
evaluators’ “rethinking” procedure in the collaborative mode.
5.1. Why Do Humans and Machines Behave Differently?
To answer this question, we explored decision-making processes by identifying the important features involved.
First, to determine the information that had played a part in either the human evaluators’ or the machines’ decisionmaking
processes, we considered a (loan-)application-level Probit model with all available information as independent
variables and defined the dependent variable (DV) using a dummy variable, IfApprove, which equaled one
if the loan was approved. We derived two sets of Probit models using all loan applications with either small or
large information volumes. The estimated coefficient of each information variable suggested the predictive power,
which served as a proxy for feature importance in the humans’ or the machines’ decision-making process. Features
with significant coefficients in the regressions were important features.

Table 3 Regressions on Humans’ and Machines’ Approval Decision (Groups 1, 3, 5, and 6; Probit Model)
Groups 3, 5, 6 (machines’ decision) Groups 1, 3, 5, 6 (humans vs. machines)
DV: IfApprove Model 1 Model 2
Loan purpose -0.161*** (0.034) 0.042 (0.048)
Gender 0.030 (0.029) 0.062 (0.058)
Age 0.087*** (0.005) 0.068 (0.062)
Living city DPI 0.212*** (0.007) 0.139*** (0.010)
Monthly income level 0.126*** (0.008) 0.082*** (0.008)
Education level 0.163*** (0.021) 0.055*** (0.028)
MInd -0.086 (0.203)
Loan purpose × MInd -0.208*** (0.040)
Gender × MInd -0.030 (0.045)
Age × MInd 0.024*** (0.006)
Living city DPI × MInd 0.066 (0.060)
Monthly income level × MInd 0.043 (0.036)
Education level × MInd 0.103 (0.087)
Other borrower-related variables Included Included
Other loan-related variables Included Included
Evaluator-related variables Included Included
Log likelihood -4,951.40 -10,363.18
#obs. 8,804 17,531
a Model 2 considers human evaluators’ initial decisions before displaying machines’ recommendations to them when using Groups 5 and
6.We duplicated the sample for Groups 5 and 6 to consider the humans’ initial decisions and machines’ decisions, respectively.
b The variables concretely reported in the table are those that might be useful in this paper’s analyses (although they may be insignificant
here).Most of the other variables were insignificant, and we do not report their details. Living city DPI was divided by 1,000.
c Standard errors are in parentheses. Significant results are in bold. *p <0.10, **p <0.05, ***p <0.01.
Furthermore, to compare the decision-making processes between humans and machines in a more explicit way,
we ran two additional Probit models, in which we included all related loan-level features as well as their interaction
terms and a new binary indicator, MInd, denoting whether the approval decision was made by a machine learning
model (=1 if yes,=0 otherwise).We reported the estimation results in Tables 3 and 4 for the small and large information
volumes scenarios. Model 1 in both tables reports the estimates of machine-only decisions. We estimated
the coefficients using samples from Groups 3, 5, and 6 for the small information volume scenario and from Groups
4, 7, and 8 for the large information volume scenario.Model 2 in both tables reports the models with interaction
terms. We included all human-only decisions (i.e., humans’ initial decisions without machine interventions) and
machine-only decisions in Groups 1, 3, 5, and 6 (in Table 3) and Groups 2, 4, 7, and 8 (in Table 4). The coefficients
of the interaction terms in Model 3 elaborate on whether and to what extent the corresponding features explain
the divergence between humans’ and machines’ decision-making processes.4
4 In Appendix C1, we conducted multiple robustness checks. First, we reran our regressions within each experimental group using different
samples. The results indicated that thehumanevaluators’ initial decisions did not involve any learning from the machines’ recommendations.
This also confirmed that the comparisons between the two stages in our experiment were reasonable. As another robustness check, we
employed decision tree approaches to infer the decision rules implemented by human evaluators. The results in Figure C1 confirm the
consistency. Additionally, we incorporated an alternative DV to offer more insights into how humans and machines reached the same or
different initial decisions.

Table 4 Regressions on Humans’ and Machines’ Approval Decision (Groups 2, 4, 7, and 8; Probit Model)
Groups 4, 7, 8 (machines’ decision) Groups 2, 4, 7, 8 (humans vs. machines)
DV: IfApprove Model 1 Model 2
Loan purpose -0.028*** (0.004) 0.021 (0.049)
Gender 0.045 (0.032) 0.070 (0.053)
Age 0.088*** (0.005) 0.060 (0.074)
Living city DPI 0.154*** (0.007) 0.091*** (0.011)
Monthly income level 0.121*** (0.009) 0.065*** (0.014)
Education level 0.075*** (0.023) 0.072*** (0.036)
Avg amount of game card -0.018*** (0.001) -0.009 (0.016)
ATV shopping durable 0.001 (0.003) 0.005 (0.005)
ATV shopping virtual -0.001 (0.001) -0.002 (0.002)
#Outgoing contacts -0.052*** (0.010) -0.050*** (0.018)
#Office by week 0.077*** (0.003) 0.004 (0.005)
#Recreational place by week -0.026 (0.028) -0.026 (0.042)
#Commercial place by week -0.097*** (0.008) -0.034 (0.069)
#Public service place by week 0.014 (0.014) 0.042 (0.043)
MInd -0.083 (0.225)
Loan purpose × MInd -0.064** (0.030)
Gender × MInd -0.022 (0.048)
Age × MInd 0.028*** (0.006)
Living city DPI × MInd 0.056 (0.049)
Monthly income level × MInd 0.006 (0.011)
Education level × MInd 0.006 (0.008)
Avg amount of game card × MInd -0.008*** (0.002)
ATV shopping durable × MInd 0.001 (0.001)
ATV shopping virtual × MInd -0.001 (0.001)
#Outgoing contacts × MInd -0.002* (0.001)
#Office by week × MInd 0.063*** (0.004)
#Recreational place by week × MInd 0.001 (0.002)
#Commercial place by week × MInd -0.063*** (0.011)
#Public service place by week × MInd -0.027 (0.029)
Other borrower-related variables Included Included
Other loan-related variables Included Included
Evaluator-related variables Included Included
Log likelihood -4,155.33 -9,642.50
#obs. 8,944 17,798
a Model 2 considers the human evaluators’ initial decisions before the machines’ recommendations were displayed to them when using
Groups 7 and 8.We duplicated the sample when using Groups 7 and 8 to consider the humans’ initial decisions and the machines’ decisions,
respectively. Other table notes are the same as [b] and [c] in Table 3.
Several interesting patterns explain the differences in performance between the human evaluators’ and machines’
individual decisions. When decisionswere made with the small information volume, the human and machine evaluators
considered similar features (i.e., living city DPI, monthly income level, and education level). The machines
additionally captured the applicants’ age and the loan purpose, which is known to have a relatively high correlation
with default behavior (refer toTable 5). This explains why the machines performed slightly better than the humans
with the small information volume. When a large information volume was available, the human and machine evaluators
deviated. Interestingly, we found that the human evaluators generally tended to stick with traditionally
important features (e.g., living city DPI, monthly income level, education level); the only new feature that human
evaluators adopted was the frequency of outgoing contacts. In contrast, the machines explored additional sources

Table 5 Correlations of Major Variables
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12)
(1) IfDefault 1
(2) Gender 0.025 1
(3) Living city DPI -0.195 -0.010 1
(4)Monthly income level -0.164 -0.041 0.022 1
(5) Age -0.120 -0.054 0.047 0.104 1
(6) Education level -0.103 -0.036 0.002 0.028 0.091 1
(7) Loan purpose 0.093 0.178 -0.036 -0.033 -0.065 -0.026 1
(8) Avg amount of game card 0.169 -0.247 0.001 -0.004 0.014 -0.017 -0.002 1
(9) #Outgoing contacts 0.104 -0.054 -0.023 0.036 0.012 0.033 0.001 -0.027 1
(10) #Office by week -0.115 -0.013 -0.034 -0.010 -0.021 0.017 0.009 0.046 0.049 1
(11) #Commercial place by week 0.090 0.096 -0.030 -0.016 -0.026 0.032 0.014 -0.089 0.019 -0.055 1
(12) ATV shopping virtual 0.094 0.098 -0.005 -0.070 -0.078 0.035 0.045 -0.038 0.010 -0.028 0.013 1
a Correlations are based on all loan samples. Relatively large values are in bold.
of information, with a particular focus on factors potentially linked to default behavior (refer to Table 5). These
factors included shopping behavior (e.g., average amounts spent on game cards), cellphone call behavior (e.g., the
frequency of outgoing contacts), and offline trajectory behavior (e.g., frequency of visiting the office or commercial
places per week). This is reasonable because humans might resist or be incapable of handling new and complicated
information (Chapman and Chapman 1967).Moreover, with their increased processing efficiency, machines have
been confirmed to have predictive advantages using novel features from alternative data sources (Lu et al. 2023a,
Zhou et al. 2021). This also explains the significant improvement achieved by machines with large information
volumes.
5.2. Why Do Humans Disagree with Machines’ Recommendations?
We next disentangled the underlying behavioral mechanisms when collaboration was employed.We noticed that
after observing the machines’ recommendations, the human evaluators sometimes adjusted their final decisions
to follow the machines’ recommendations, but not always. Table 2 shows that only with machine explanations
and large information volumes did the human evaluators contribute additional value. This value would disappear
if either of the two conditions were removed. In order to understand the human evaluators’ behavior, we conducted
regression tests using observations in which the human evaluators’ initial decisions differed from those of
the machines.
We employed Probit models, in which the DV was IfApprove and the independent variables included all
available loan features.To understand howmachines’ recommendations influenced humans’ decision-making processes,
we compared the discrepancies betweenhumanevaluators’ initial and final decisions. Empirically,we defined

a new binary indicator, IfFinal, which equaled one if the approved decision was made after a machine recommendation
was present. Again, we included this binary indicator and the interaction terms of the features to
investigate which features played a significant role in changing the human evaluators’ decisions.
We reported the results in Tables 6 and 7. With small information volumes (Groups 5 and 6 in Table 6) and
with large information volumes but no machine explanations (Group 7 in Table 7), the factors that explained the
human evaluators’ final approval decisions remained similar to those in the first stage (shown in Tables 3 and 4).
For example, with the small information volume, the interaction terms for three features (i.e., applicant age, living
city DPI, and monthly income level) presented significant coefficients in Model 2 in Table 3. This suggests that
humans rely on these three features to decide whether to change their initial decisions (i.e., whether to follow the
machines’ recommendations).Take the feature of monthly incomelevel as an example. The corresponding estimate
is positive, implying that humanswould switch from rejection to approval even if an applicant’s income is not high
enough. That is, the weight of the monthly income level in human evaluators’ credit risk assessment became larger
than before, and human evaluators were more tolerant of cases with relatively lower levels of income. To further
illustrate humans’ willingness to follow machines’ suggestions and to capture human behavior in the second stage,
we defined another dependent variable, IfFollow. The detailed empirical strategy and corresponding results are
presented in Appendix C2. All the empirical results imply that, under these experimental conditions, the reasons
(i.e., key features) explainingwhy the human evaluators disagreed with the machines initiallywere the same as those
explaining why they continued to disagree with machines after receiving the machine recommendations.
In Groups 5 to 7, the features with significant estimates of interaction terms with IfFinal (i.e., indicating
the reasons explaining the differences between initial and final decision-making processes) included both humanfamiliar
ones (i.e., those they had used in the first stage, such as living city DPI and number of outgoing contacts)
and machine-only ones (e.g., number of commercial place visits). There are two possible ways that humans
and algorithms might reach diverse decisions. One is that humans might have some uncertainty surrounding
“borderline” cases (i.e., those with important features showing values near the evaluators’ or machines’ thresholds).
Humans and machines may make inconsistent decisions on such borderline loans when their feature values
are located in such threshold gaps. When handling these relatively complicated applications, humans may lack

Table 6 Regression on Human Evaluators’ Initial and Final Approval Decisions (Groups 5 and 6; Probit Model)
Group 5 Group 5 Group 6 Group 6
(humans’ final decision) (initial vs. final) (humans’ final decision) (initial vs. final)
DV: IfApprove Model 1 Model 2 Model 3 Model 4
Loan purpose -0.025 (0.022) -0.013 (0.024) -0.113* (0.065) -0.102 (0.124)
Gender 0.160 (0.140) 0.131 (0.139) 0.037 (0.143) 0.035 (0.144)
Age 0.073** (0.034) 0.032 (0.040) 0.070** (0.032 0.024 (0.032)
Living city DPI 0.155** (0.024) 0.123*** (0.026) 0.103*** (0.027) 0.098*** (0.030)
Monthly income level 0.117*** (0.034) 0.096*** (0.033) 0.059* (0.033) 0.052* (0.028)
Education level 0.024*** (0.008) 0.020** (0.008) 0.067*** (0.015) 0.031** (0.014)
IfFinal -0.421*** (0.085) -0.598*** (0.085)
Loan purpose × IfFinal -0.012 (0.010) -0.011* (0.006)
Gender × IfFinal 0.028 (0.198) 0.002 (0.183)
Age × IfFinal 0.041* (0.024) 0.045** (0.023)
Living city DPI × IfFinal 0.023* (0.013) 0.005** (0.002)
Monthly income level × IfFinal 0.022** (0.010) 0.005* (0.003)
Education level × IfFinal 0.004 (0.003) 0.035** (0.017)
Other borrower-related variables Included Included Included Included
Other loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -306.15 -599.31 -293.04 -584.36
#obs. 468 936 461 922
a Models 1 to 4 are based on the samples in which human evaluators’ initial decisions were inconsistent with machines’ decisions (i.e.,
IfConsistent = 0).We duplicated the sample because we considered the humans’ initial and final decisions separately. Other table notes
are the same as [b] and [c] in Table 3.
confidence (Kunimoto et al. 2001) and be more likely to follow machines’ suggestions, regardless of their initial
approval or rejection decisions. Considering the following ratios and the performance improvement from Group
1 to Groups 5 and 6 and from Group 2 to Group 7, our findings indicate that the machines were relatively better
at evaluating cases with feature values near the borderline.
Conversely, it is likely that humans and machines could reach distinct conclusions about an applicant’s default
probability because of differences in evaluating important features. As a result, humans would tend to stick with
their initial opinions. Considering that the machines incorporated extra features to assess the loans, these additional
features might dominate the human-familiar ones, and human evaluators could find that the values of their familiar
features were beyond their expectations. This echoes the literature about humans’ aversion toward AI when
humans cannot successfully interpret the reasoning behind a machine’s decision (Wang and Benbasat 2016). Figure
5 provides empirical evidence with feature distributions to support these arguments. In specific, we visualized
the distributions using four sub-samples, which were separated by two standards: whether human evaluators ultimately
accepted or rejected the applications (A vs.R), and whether humans followed or continued to disagree with
the machines’ recommendations (F vs. D). Interestingly, we observed that the means of (F&A) are close to those
of (F&R), implying that when dealing with borderline cases, humans place more trust in the machines. On the

Table 7 Regression on Human Evaluators’ Initial and Final Approval Decisions (Groups 7 and 8; Probit Model)
Group 7 Group 7 Group 8 Group 8
(humans’ final decision) (initial vs. final) (humans’ final decision) (initial vs. final)
DV: IfApprove Model 1 Model 2 Model 3 Model 4
Loan purpose -0.059 (0.115) 0.111 (0.117) -0.017 (0.128) -0.011 (0.124)
Gender 0.207 (0.136) 0.045 (0.032) -0.154** (0.069) 0.032 (0.034)
Age 0.130** (0.056) 0.073 (0.059) 0.100*** (0.018) 0.051 (0.057)
Living city DPI 0.135*** (0.025) 0.098*** (0.024) 0.188*** (0.030) 0.140*** (0.033)
Monthly income level 0.076** (0.032) 0.032*** (0.010) 0.140*** (0.034) 0.110*** (0.033)
Education level 0.191** (0.081) 0.130*** (0.040) 0.032* (0.019) 0.030* (0.016)
Avg amount of game card -0.002 (0.005) -0.030 (0.057) -0.038 (0.063) -0.014 (0.014)
ATV shopping durable 0.003 (0.002) -0.001 (0.002) 0.007 (0.009) 0.008 (0.012)
ATV shopping virtual -0.005 (0.004) -0.001 (0.001) -0.020*** (0.005) -0.010 (0.008)
#Outgoing contacts -0.036*** (0.010) -0.015* (0.008) -0.025** (0.012) -0.022* (0.012)
#Office by week 0.028** (0.011) 0.067 (0.042) 0.027** (0.012) 0.019 (0.012)
#Recreational place by week -0.126 (0.100) -0.029 (0.095) -0.034 (0.117) -0.027 (0.129)
#Commercial place by week -0.044* (0.025) -0.022 (0.024) -0.111*** (0.039) -0.058 (0.036)
#Public service place by week 0.064 (0.048) 0.015 (0.048) 0.010 (0.040) -0.028 (0.040)
IfFinal -0.337*** (0.091) -0.349*** (0.091)
Loan purpose × IfFinal -0.170 (0.164) -0.006 (0.178)
Gender × IfFinal 0.149 (0.134) -0.185* (0.105)
Age × IfFinal 0.056** (0.028) 0.048** (0.025)
Living city DPI × IfFinal 0.053*** (0.014) 0.048*** (0.014)
Monthly income level × IfFinal 0.044*** (0.014) 0.030** (0.015)
Education level × IfFinal 0.061*** (0.015) 0.003 (0.017)
Avg amount of game card × IfFinal 0.027 (0.025) -0.023 (0.043)
ATV shopping durable × IfFinal -0.002 (0.004) -0.001 (0.002)
ATV shopping virtual × IfFinal -0.004 (0.005) -0.010** (0.004)
#Outgoing contacts × IfFinal -0.021** (0.010) -0.003 (0.013)
#Office by week × IfFinal 0.017* (0.010) 0.008 (0.006)
#Recreational place by week × IfFinal -0.095 (0.108) -0.007 (0.114)
#Commercial place by week × IfFinal -0.022* (0.014) -0.052*** (0.011)
#Public service place by week × IfFinal 0.049 (0.050) 0.018 (0.051)
Other borrower-related variables Included Included Included Included
Other loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -329.26 -646.33 -265.37 -546.14
#obs. 638 1,276 649 1,298
Table notes are the same as those for Table 6.
contrary, once they found the featureswere far belowor above their thresholds, they held on to their own views.We
offer additional evidence to support this assertion by considering all relevant loan features in Figure C2 (Appendix
C2).
The above result, however, was not found in Group 8. Interestingly, in Models 3 and 4 of Table 4, we noticed
that some alternative features, such as gender and the average transaction amount for purchases of virtual goods
(i.e., “ATV shopping virtual”), had significant coefficients. That is, those additional features explained why the
human evaluators shifted from their initial decisions.5 More importantly, given that those features did not reach
significance when we compared the human evaluators’ initial decisions with those of the machines, it suggests
5 It is possible that the evaluators might have strategically chosen to follow the machines’ decisions if the machine recommended either
approval or rejection.We alleviated this concern in Appendix C3.

(a) Distributions of Living City DPI (b) Distributions of #Outgoing Contacts
a All distributions are based on the samples in which human evaluators’ initial decisions were inconsistent with machines’ decisions.
b F & A: Cases wherein humans followed the machines’ recommendation and ultimately approved the loan applications; F & R: Cases wherein
humans followed the machines’ recommendation and ultimately rejected the loan applications; D & A: Cases wherein humans disagreed with
the machines’ recommendation and ultimately approved the loan applications; D & R: Cases wherein humans disagreed with the machines’
recommendation and ultimately rejected the loan applications.
Figure 5 Feature Distributions of Diverse Cases (Group 7)
that the human evaluators reconsidered their initial decisions. In other words, the presence of large information
volumes and machine explanations provoked evaluators to engage in active rethinking, which improved their final
decision accuracy.
5.3. Disagreement and Decision Quality: Decomposition of the Rethinking Process
As discussed earlier, we observed that with the presence of large information volumes and machine explanations,
humans reconsidered an interesting feature, “ATV shopping virtual”. This feature had not been used by either
humans or machines in the independent decision-making process. The prediction models might have ignored or
downplayed the values of this feature due to its correlations with other features.We conjectured that the attention
to the “ATV shopping virtual” feature stemmed from human evaluators associating it with the “average amount
spent on game cards” feature. When the human evaluators saw the machines making different decisions, they also
noticed that the loans had some irregular patterns on features that the evaluatorswere unfamiliar with (e.g., “average
amount spent on game cards”). However, such features could hardly be applied by human evaluators, as the most
common value by far across all loan applications was 0 (refer to Figure A1a in Appendix A1; the median is 0). Such
a distribution would lead to human evaluators perceiving those features as non-informative. The literature has
suggested that humans are good at building connections between given information and other relevant, familiar, or
understandable information in cognitive processing (Br˚aten and Samuelstuen 2007, Hollnagel 1987). Since game

cards are typical virtual goods and “ATV shopping virtual” had many more salient non-zero values (Figure A1b;
the median is 8.70), human evaluators are likely to attend more to this feature when making decisions.
In Appendix C4, we compared the default rates between Groups 7 and 8 after separating loans “saved” by
the machines (i.e., those that were originally rejected by human evaluators but ultimately approved due to the
machines’ approval recommendations) and those “saved” by human evaluators.We showed that using the updated
decision rules with new and correct features (i.e., significantly correlated with default behavior), human evaluators
were more likely to correctly select “good” loans from those rejected by the machines, whereas humans’ decisions to
overrule the machines resulted in no change or a decrease in efficiency (i.e., replacing some “bad” applications with
other “bad” ones) in Group 7 where humans relied on their priors.Meanwhile, the use of gender features might be
due to their relatively high correlations with “ATV shopping virtual” (refer to Table 5). Such findings also explain
the alleviation of gender bias (which we will demonstrate later, in Section 6.2).Moreover, we conducted a straightforward
post-hoc analysis in Appendix C4 to clarify the allocations of different loan types by humans, machines,
and collaborative efforts. This provided additional insights into how machines and humans could assume distinct
roles to improve overall collaborative performance.
Taking all of the findings together, our results suggest that with a proper design that invokes humans’ active
rethinking (e.g., the presence of effective machine explanations when processing complicated information), the
collaboration between humans and machines could potentially achieve “1+1>2” in practice.Machines would take
responsibility for handling borderline cases, and humans would have the potential to invoke active rethinking to
correct machines’ mistakes in the “random” cases (e.g., those without explicitly congruent feature patterns) when
they perceive that machines have made contradictory decisions, inspired by suggestive information cues.
6. Empirical Extensions
6.1. Heterogeneity by Human Evaluator Characteristics
Recent studies have shown that human agents’ degree of decision-making experience might affect their acceptance
of machine recommendations as well as their performance in collaboration with machines (Luo et al. 2019,Wang
et al. 2023b). Therefore, we decomposed the heterogeneity regarding individual evaluators’ characteristics. Below,
we focus on the evaluators’ experience, based on the length of time (in months) that they had worked in the focal
company before we started the experiment. FollowingMarcotte (1998), the experience was measured at four levels

Table 8 Heterogeneity Analysis of Human Evaluators’ Months Working (Probit Model)
Groups 1 & 2 (only human) Groups 5–8 (human + machine)
Model 1 - DV: IfDefault Model 2 - DV: IfDefault Model 3 - DV: IfConsistent Model 4 - DV: IfFollow
Large info. (L) -0.384*** (0.143) -0.093*** (0.018) -0.221*** (0.083) 0.469** (0.187)
Month of working=1 (Work=1) (baseline) (baseline) (baseline) (baseline)
Work=2 -0.237* (0.127) -0.052 (0.157) 0.141 (0.089) 0.141 (0.163)
Work=3 -0.400*** (0.140) -0.109 (0.162) 0.153* (0.084) -0.212 (0.174)
Work=4 -0.549* (0.146) -0.147* (0.087) 0.194** (0.078) -0.284 (0.186)
L ×Work=1 (baseline) (baseline) (baseline) (baseline)
L ×Work=2 0.305 (0.187) -0.103 (0.259) 0.042 (0.112) 0.093 (0.253)
L ×Work=3 0.355* (0.212) 0.383 (0.254) 0.084 (0.120) 0.229 (0.254)
L ×Work=4 0.356* (0.203) 0.057 (0.263) 0.048 (0.113) 0.359 (0.248)
Explanation (Expl) -0.150 (0.179) 0.114 (0.088) 0.146 (0.192)
Expl ×Work=1 (baseline) (baseline) (baseline)
Expl ×Work=2 0.405 (0.296) 0.036 (0.113) 0.193 (0.251)
Expl ×Work=3 0.057 (0.236) 0.013 (0.120) 0.720*** (0.273)
Expl ×Work=4 -0.021 (0.264) 0.022 (0.122) 0.442* (0.267)
L × Expl -0.278 (0.305) -0.051 (0.121) 0.159 (0.281)
L × Expl ×Work=1 (baseline) (baseline) (baseline)
L × Expl ×Work=2 -0.196 (0.395) 0.070 (0.158) -0.480 (0.366)
L × Expl ×Work=3 -0.383** (0.180) 0.059 (0.165) -0.641* (0.377)
L × Expl ×Work=4 -0.637* (0.375) 0.078 (0.172) -0.998** (0.390)
Borrower-related variables Included Included Included Included
Loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -822.87 -5,565.09 -957.51 -1,107.73
#obs. 2,716 11,727 5,603 2,216
a Models 1 and 2 are based on the approved samples. Model 3 is based on all loan samples. Model 4 is based on the samples in which human evaluators’ initial
decisions were inconsistent with the machines’ decisions (i.e., IfConsistent= 0).We introduce the definition of IfFollow in Appendix C2.
b Large info. = 1 for the treatment using large information volumes for decision making, 0 for small. Evaluator months working: 1 = not longer than 6 months, 2
= 6–12 months, 3 = 13–18 months, 4 = longer than 18 months. Interpret. = 1 for treatment of disclosing machine explanations, 0 for not.
c Standard errors are in parentheses. Significant results are in bold. *p <0.10, **p <0.05, ***p <0.01.
(1 = not longer than 6 months, 2 = 6–12 months, 3 = 13–18 months, 4 = longer than 18 months). To quantify the
impact of experience levels,we considered another Probit model, this one with three-way interaction terms including
the existence of large information volumes, the availability of machine explanations, and experience levels.We
also included all lower-level interaction terms in the regression.We presented the estimated coefficients in Table 8,
whereinModel 1 considers the default rate asDV and includes humans’ independent decisions only, whileModels
2–4 are in the human–machine collaboration modes. Specifically, we replicated our mechanism tests with heterogeneous
experience levels: whether a loan defaultedwasModel 2’sDV, whether initial decisionswere consistentwas
Model 3’s DV, and whether to follow machines’ decisions was Model 4’s DV. Note that the estimation of Model
4 incorporated only samples where human evaluators’ initial decisions were inconsistent with machine decisions.
Additionally,we offer more comprehensive heterogeneity analyses with alternative characteristics in Appendix D1.
Table 8 yields several interesting findings. First, the positive estimate of L ×Work = 3 (or 4) inModel 1 indicates
that without machine assistance, experienced human evaluators performedworse with a large information volume
than with a small one. Given the definition of work experience, evaluators with a higher experience level might
have accumulated significantly more knowledge in handling small data over a long time, and thus, they might have

found it hard to switch their mindset (i.e., experience inertia) (Becker 1995). Another plausible explanation is that
these more senior evaluators might have less trust in AI, as suggested by Wang et al. (2023b). On the other hand,
evaluators who were new to the company might have still been in the learning stage when the experiment started,
and in such cases, persistent learning could have brought more benefits. Second, we observed that experienced evaluators
tended to make more decisions that were consistent with those of the machines (as shown in the results
of Model 3), especially with small information volumes. This is reasonable because experienced evaluators were
more likely to have learned the feature values comprehensively and reached a similar level of performance as the
machines. Third, the estimates inModel 4 suggest that, when we focused on loans with different initial decisions,
experienced evaluators were more likely to follow machine explanations in a small information scenario but more
likely to overrule machines’ decisions and stick to their own opinions given the availability of large information volumes.
Combining all of these results with those inModel 2 makes it clear that the satisfaction of both conditions
encouraged experienced evaluators to initiate an active rethinking process and thereby achieve reduced borrower
credit risk. Furthermore, to deepen our understanding of how individual heterogeneity influences behavior in the
presence of machine assistance, we replicated our mechanism examinations with different experience levels. The
findings, detailed in Appendix D2, offer more nuanced and straightforward evidence indicating that experienced
evaluators were more inclined to initiate an active rethinking process when provided additional external information
in Group 8.
6.2. Decision Biases
As implied inTable 5, most of themajor variables considered in both the human evaluators’ and machines’ decisionmaking
processes were relatively highly correlated with the performance metric. This confirms the fact that both
humans and machines made decisions based on their estimated credit risk. In the meantime, it is worth noting
that some of the major variables (e.g., loan purpose, average amount spent on game cards, and number of visits to
commercial places)were also highly correlated with gender.Anatural question arises: will this correlation cause any
fairness issues? For example, will it affect the loan-approval decisions of borrowers of different genders, especially
when considering different information volumes and human–machine collaboration modes?
To address this question, we first focused on the final performance as measured by the non-default rates. We
recorded the statistics of each group inTable D5 in Appendix D3.We observed that with large information volumes

(i.e., Group 4), machines tended to favor female applicants, because the non-default rate of the approved male
applicants (98.03%) was much larger than that of female applicants (93.99%). That is, machines seemed to have
exerted a higher loan-approval criterion for males than females. The involvement of human evaluators without
machine explanations (i.e., Group 7) could not alleviate such gender bias. However, when human evaluators were
presented with machine explanations (i.e., Group 8), the final repayment performance of the approved female and
male applicants became better and similar (96.67% vs. 97.55% ), suggesting the mitigation of gender bias.
Following Teodorescu et al. (2021), we additionally applied the criterion of “equalized opportunity” (EOR),
which requires positive outcomes to be independent of the protected attribute, in order to alternatively measure
decision fairness (biases) between genders in our different experimental groups. Let G be the gender
indicator (G = 0 or 1), and Y=1 and ^Y=1 be the correct and actual positive outcomes (i.e., a loan application
being approved in our context), respectively. “Correct” here means that non-default loans (observed
from the repayment performance of the approved loans) got approved. As such, equalized opportunity means
Pr(^Y=1|G=0,Y=1)=Pr(^Y=1|G=1,Y=1). Applying this criterion to our context, EOR describes the decision
biases between genders as follows: EOR = Appr(G=0)/NonD(G=0)
Appr(G=1)/NonD(G=1) , where Appr(G=0) and Appr(G=1) refer to the
approval rates for females and males, and NonD(G=0) and NonD(G=1) refer to the non-default rates for females
and males (calculated within female or male groups), respectively. The closer EOR is to 1, the greater the fairness
is between the genders. The larger the deviation from 1, the more bias there is toward females (EOR>1) or males
(EOR <1). Figure 6 plots the values of EOR across the different experimental groups.
We learned from Figure 6 that the human evaluators treated males and females equally in terms of fairness,
regardless of the volumes of information available (EOR = 1.012 (small amounts of information) and 0.987 (large
amounts of information), both close to 1). That is, the human evaluators tended to apply relatively similar standards
in evaluating the male and female applicants. The machines, however, significantly favored females when
they had large information volumes available for decision-making (EOR = 1.201). This was due mainly to the high
correlation between the most important features used by machines and the default indicator, as shown in Table 5.
This finding is consistent with previous studies (e.g., Fuster et al. 2022) and implies that whereas machines perform
much better in general with large-scale information, they return results that are gender-biased, notwithstanding

a Refer to Table D5 for complete values.
Figure 6 Equalized Opportunity Ratio on Gender
the literature’s demonstration of the value of large-scale information in alleviating certain forms of demographic
discrimination (Lu et al. 2023a). Further, we did not observe any significant change when human evaluators were
involved in making the final decisions with small information volumes (i.e.,EOR= 1.025 and 1.040 in Groups 5 and
6, respectively). However, we did observe a significant reduction in EOR when both large information volumes
and machine explanations were available (i.e., EOR = 1.056 in Group 8). In this scenario, the increase in final decision
accuracy could be attributed to human intervention in correcting the risk evaluations of female borrowers.
Similarly to our findings in Section 5.2, human evaluators associate certain observed features to others (i.e., “ATV
shopping virtual”). Fortunately, the “ATV shopping virtual” feature positively correlates with the feature gender
(refer toTable 5) and default probability. Hence, human evaluators helped mitigate gender biases successfully. This
again highlights the value and necessity of collaboration between humans and machines. It is essential to acknowledge
that the gender bias observed in our dataset and empirical context may be specific to our circumstances. Environments
with a more balanced interaction between genders could potentially avoid this gender-related issue.We
provided a comprehensive discussion about how our findings concerning gender biases could be extrapolated to
other contexts in Appendix D3.
7. Conclusions and Discussion
7.1. Simultaneous Needs of Both Conditions
In the emerging stream of human–machine collaboration literature, there is a dearth of systematic understanding
about when, with machines’ assistance, humans can actively contribute and how they can add extra value to task

outcomes.We dived into the information processing literature, the comprehension of which affords two prerequisites
for invoking humans’ deep thinking: information complexity initially draws humans’ attention to engaging
in the tasks, and useful external cues drive humans to perform active consideration. We applied these theoretical
implications to human–machine collaboration tasks, and accordingly, against the backdrop of the microloan
industry, we devised two treatments by manipulating information volumes and displays of machine explanations.
A unique two-stage field experiment helped us to explicitly quantify the corresponding performance.
Our empirical findings shed light on the significance and compatibility of the two theory-driven conditions,
and showed that neither can be dispensed with. First, although larger information volumes mean more potential
knowledge to help gauge decision-making performance (Hu et al. 2022), our empirical comparisons demonstrated
that humans tended to utilize what they have specialized in (i.e., small information volumes, Group 1 vs. Group
2), because learning is costly and instant feedback might be uncertain.Without effective extrinsic motivation, such
distortionwould further impede humans’ acceptance of machines’ recommendations (Group 4 vs. Group 7). This
is generally detrimental as humans’ insistence on their own decision rules is very likely to result in underfitted
decisions in different tasks (Song et al. 2021).
Second, it was also no surprise to find that offering machine explanations alone, without the presence of large
information volumes, could not inspire humans’ further contribution (Group 6 vs. Group 8). This is owed to
the fact that machines’ superiority in tackling prohibitively (for humans) complex tasks to achieve satisfactory
predictionswas constrained by information availability (also refer to the comparison between Group 3 vs. Group 4
in Figure 3).Ontop of limited information, humans could not becomesmarter than machines. Notably,we noticed
that a few recent studies have focused on the value of machine explanations to human–machine collaboration (e.g.,
Bauer et al. 2023, Jacobs et al. 2021).However, our study suggests contingent factors, such as task complexity,would
impact the effect of machine explanations. Although machine explanations provide humans with more reference
information, humans may not take advantage of them due to insufficient motivation to deeply involve themselves
in decision-making (Speier 2006). Instead, humans were found to involve more trust in machines by following
their recommendations with those “borderline” loans.
Hence, only with the simultaneous presence of large information volumes and machine explanations can
human–machine collaboration realize better performance than humans or machines alone (i.e., experimental

Group 8) via initiating active rethinking. This engagement results in further improvement of decision accuracy
and mitigation of the machines’ biased decisions. Our findings, therefore, confirm the validity of generalizing the
dual-process theories of reasoning from humans’ independent or interpersonal decision-making to the realm of
machine assistance.
7.2. Managerial Implications
This study, built on our unique experimental designs, also offers non-trivial insights to practitioners. Our findings
could inform companies’ future benefit-cost analyses in managing their efforts/investment and balancing among
human agents (human capacities), data purchasing/collecting, and adoption of AI techniques. Our experiments
probe diverse possible and manageable factors that could negatively affect the desired efficiency of human–machine
collaboration, andwe showempirical evidence of those factors’ roles in the overall decision-making process. What’s
more, this paper presents practitioners with a caveat to their prevalent preference for big data, AI techniques,
and/or human–machine collaboration. Specifically, if big data is available, this collaboration can achieve both satisfactory
decision-making efficiency and fairness. However, when faced with the threat of machines taking their
jobs or the possibility of over-domination by machine intelligence, human employees across companies and even
industries might resist machine assistance or begin to rely on it excessively. Thus, we provide a scheme of machine
interpretability to encourage human agents to rely less on machines and to create additional value. If only small
data is available (e.g., affordable), a machine alone seems enough. The involvement of human efforts, regardless of
whether machine explanations are present or absent, cannot add significant value in improving prediction accuracy
or addressing gender biases in this case.Moreover, our empirical analyses not only offer guidance to platforms
in designing efficient collaboration systems but also open pathways to gaining valuable insights into hiring decisions.
In particular, our heterogeneity analyses highlight that individuals with experience possess greater potential
to attain elevated levels of collaborative performance and amend machine biases through more systematic contributions.
Nevertheless, even with experienced employees, platforms should not neglect the importance of refining
their training approaches and procedures. This includes the implementation of comprehensive data literacy training
programs (Hvalshagen et al. 2023), providing valuable cues and timely feedback for decision-making improvement
loops (Proctor and Bonbright 2021). Additionally, it is essential to incorporate modules on ethics and bias
awareness into training programs (Sellier et al. 2019).

7.3. Discussions of Generalizability
Our theory-driven experimental design and empirical findings are highly generalizable to other contexts where
the decision-making task objectives are not excessively intricate for humans or machines and/or can be clearly formulated.
Moreover, the applicability extends to scenarios where opportunities exist to acquire additional information,
whether in terms of volume or type, to enhance overall performance (Amit and Sagiv 2013). Examples
of such tasks include job candidate screening in labor hiring, supplier evaluation in procurement, and medical
treatment decision-making. On a broader note, our study suggests that machines consistently outperform human
agents when tasked with objectives that are not particularly challenging, such as classification or prediction problems
involving structured objects and features. The availability of a large amount of information might stimulate
human agents to pay attention to their tasks, but it does not guarantee that they will aid machines. Additional cues,
such as machine explanations, are crucial for guiding human agents to perform an active rethinking of complex
information to deal with uncertainties, thereby producing better outcomes.
However, it is worth discussing some caveats to practical system designs as they relate to the generalizability
of our results. Our findings regarding the two conditions essential for stimulating active information processing
in humans are contingent on many surrounding factors. For example, humans should be responsible for their
decision-making performance to some degree, thereby preventing the complete delegation of decision-making to
machines. Humans’ loan-approval capabilities should also be associated with the ultimate collaboration performance.
Also, selected AI algorithms should be suitable for tackling the specific task objectives and models need to
bewell-trained. Regarding the two focal treatments, rich information is not a panacea; any newly acquired information
must be inherently valuable to bolster decision-making performance. In addition, machine explanations must
be delivered in a clear and compelling manner. As there might be disagreement between human (expert) knowledge
and machine explanations (Krishna et al. 2022), the explanations should be suitably displayed, understandable, and
able to stimulate cognitive reasoning (e.g., enabling easy comparisons). Lastly, as proposed byWang et al. (2023b),
humanworkers’ prior knowledge of AI and their responsibilities assigned are factors associated with their attitudes
toward AI. In our specific context, human evaluators generally had limited knowledge of machine learning, and
the compensation structure within the platform (as outlined in Section 3.1) did not encourage evaluators to proactively
enhance their understanding to achieve superior performance levels. In other settings where human workers
are more proficient in AI and have stronger motivation to consistently refine their task performance, the responses
to machine recommendations (with or without machine explanations) may exhibit variation.
From the technical perspective, our empirical results also reinforce our theory-guided design approach to some
extent, as our two proposed treatments did not explicitly rely on any specific form of machine interpretability.
As long as they could offer clear signals, we deemed them potentially valuable in encouraging humans to reassess
their perspectives.Moreover, while centered on the implementation of specific machine-learning algorithms, our
empirical analyses and findings can be extrapolated to diverse applications involving advanced and more intricate
AI techniques.Onthe one hand, our targeted interventionswere guided by theory and offered insights into human
behavioral responses to factors including task complexity and reference cues. Additionally, our experimental design
deliberately withheld information about the specific machine-learning algorithms from participants, making it
possible to extend our observations to other AI models, despite potential variations in actual performance and
opportunities for human contributions.
Additionally, it is worth noting that in our primary study, we cannot evaluate the value of AI identity explicitly.
Put differently, our empirical results do not conclusively discern whether the observed effects stem from the
additional information offered by machines (or senior managers) or from the direct attitudes of humans toward
AI or machines. However, it is crucial to acknowledge that the performance of senior managers in real-world scenarios
may not exhibit the same level of stability and efficiency as machines, especially given the vast amounts of
information involved. Considering the time constraints inherent in making accurate decisions, AI or machines
tend to outperform their experienced human counterparts.Moreover, several research papers have delved into the
difficulties faced by humans when attempting to articulate or summarize the rules guiding their decision-making
processes (Hu et al. 2022). Compared to reliance on machines, relying on senior managers to provide explicit decision
cues is more challenging. In contrast, machines offer the advantage of leveraging advanced techniques such as
feature importance extraction. This underscores the significance of fostering collaboration between humans and
machines.
Finally, our experimental design emphasized the efficacy of a two-stage decision-making process wherein where
human evaluators initially make independent loan approval decisions and subsequently determine their final decisions
by opting to adopt or reject the machine recommendations. While recognizing that two-stage designs may
be practically infeasible, we suggest the potential relevance of our findings in scenarios where only a single stage
is feasible–directly presenting machine recommendations to the original human-alone decision-making scenario.
However, this adjustment may influence decision-making outcomes. For example, without a distinct independent
decision-making stage, the direct provision of machine recommendations may lead to over-reliance on machines
or foster distrust due to the absence of a clear contrast to humans’ independent decisions. The lack of explicit comparisons
may further hinder rule identification, especially among less experienced individuals, resulting in more
significant heterogeneity in decision-making performance compared to a two-stage setting.
7.4. Limitations and Directions for Future Studies
Our paper has several limitations that provide promising opportunities for future research. First, our empirical
design focused on a static scenario without human learning. However, in a real-world environment, humans
and machines might learn from each other’s decision-making processes and adjust gradually over a relatively long
period. Future research can extend our analyses to disentangle learning behavior and thereby design an optimization
strategy for both sides using techniques such as reinforcement learning models. Second, our experimental
treatment considered a binary case between small and large information volumes. Future studies can relax this
constraint and explore a continuous level of information complexity, the insights from which could offer business
managers more practical conclusions and increased value. Third, in our empirical setup,we deliberately limited the
experimental period to one or twoweeks to establish a controlled environment, which helped us mitigate potential
biases introduced by human learning behaviors evolving over time.We acknowledge the temporal constraint as a
limitation in our study, paving the way for future investigations. Extending the experimental period would enable
researchers to explore how humans process and value information conditions over an extended period, offering
valuable insights into the dynamics of long-term interactions. Fourth, divergence in terms of cultural background
or industry domain might have affected our findings. Similar studies in other countries or industries can further
validate these findings and offer novel insights into human–machine collaboration designs.






----------------------------------------------------------------------------------







Ibrahim, Rouba, et al. “Eliciting human judgment for prediction algorithms.” Management
Science, https://doi.org/10.1287/mnsc.2020.3856.

Abstract
Even when human point forecasts are less accurate than data-based algorithm predictions, they can still help boost performance
by being used as algorithm inputs. Assuming one uses human judgment indirectly in this manner, we propose
changing the elicitation question from the traditional direct forecast (DF) to what we call the private information adjustment
(PIA): how much the human thinks the algorithm should adjust its forecast to account for information the human has
that is unused by the algorithm. Using stylized models with and without random error, we theoretically prove that human
random error makes eliciting the PIA lead to more accurate predictions than eliciting the DF. However, this DF-PIA gap
does not exist for perfectly consistent forecasters. The DF-PIA gap is increasing in the random error that people make
while incorporating public information (data that the algorithm uses) but is decreasing in the random error that people
make while incorporating private information (data that only the human can use). In controlled experiments with students
and Amazon Mechanical Turk workers, we find support for these hypotheses.

Key words: laboratory experiments, behavioral operations, random error, elicitation, forecasting, prediction, discretion,
expert input, private information, judgment, aggregation

1. Introduction
Because of increased access to data and advancements in machine-learning algorithms, a common operational
improvement initiative is to replace human forecasters with data-driven prediction algorithms. For
example, in our motivating setting, a hospital needs surgery duration forecasts to schedule operating room
use, which costs $2; 190 per hour on average (Childers and Maggard-Gibbons 2018). Using surgery duration
data from that hospital, Ibrahim and Kim (2019) show that physicians’ mean absolute percent forecast
error was 33%, whereas algorithms based on available patient and surgery data reduced that error to 29%.
Nevertheless, even if humans are not better than algorithms head-to-head, their judgments can still help.
In the above example, the hospital could improve predictive accuracy even further, to under 27%, by using
the physicians’ forecasts as an input (along with the other data) to the algorithm. In other words, the best
forecasts often come not from replacing humans with algorithms, but from combining them.
In this research, we ask the following question: If we know that we are going to use human judgment not
directly, but rather indirectly, in an algorithm, should we elicit something else besides point forecasts? If so,
what human judgment should we elicit, and why might it work better?
We theorize that the primary reason why humans add value to algorithms is that they have access to
private information that the algorithm does not use, for example, because it is not in the database or there are
not sufficient historical training data for the algorithm to effectively use it. Therefore, we consider whether,
rather than asking for a human’s direct forecast (DF), it may be better to instead ask about her judgment of
this private information’s impact (even if the system designer does not know what this private information
is ahead of time). Specifically, we propose the idea of eliciting the private information adjustment (PIA)—
how much the human thinks the algorithm should adjust its forecast to account for the information that only
the human has.
Using stylized models (x2), we theorize that the PIA leads to more accurate predictions than DF only if
there is human random error. That is, from a predictive accuracy perspective, there is no difference between
eliciting a DF or the PIA if people are perfectly consistent in how they use information to make a forecast.
However, if they are inconsistent, then the PIA should help algorithms more than the DF. Furthermore, the
models shed light on how random error creates this difference by predicting which environmental conditions
would lead to greater differences in performance. Namely, they show that the PIA’s advantage, relative
to DF, is larger when “public” data—the data that the algorithm uses—are complicated for the human to
process but smaller when the human’s private information is complicated to process instead.
To test these hypotheses regarding the difference in performance between DF and PIA, we conducted
controlled experiments in which we elicited human judgments for 50 simulated surgery durations based on
predictive data. We told our participants that the hospital’s algorithm had access to only some of the data
(“public information”) and that only the participant had access to the other data (“private information”).
In one condition, we elicited judgment by asking for the participant’s DF for each surgery, while in the
other, we elicited the participant’s PIA for each surgery. Then, for each condition, we calibrated prediction
algorithms using the first 35 surgeries and tested their predictive performance using the last 15 surgeries.
In Experiment 1 (x3), conducted with university students and replicated with Amazon Mechanical Turk
(MTurk) workers, we find that prediction algorithms performed significantly better when they had access
to the participants’ PIA as inputs as opposed to their DF: their average root mean squared error (RMSE)
in the test sets was 21% lower. In Experiment 2 (x4), we manipulate random error magnitudes by making
the public or private information more or less complex: subjects must aggregate multiple factors when
the information is complex but are provided one equivalent factor when the information is not complex.
Consistent with our theoretical development, we find that the RMSE for PIA is 48% lower than for DF
when the public information is complex but only 6% lower than for DF when the private information is
complex. Finally, in x5, we discuss why private information exists in practice, implications of our findings,
and opportunities for future research.
We contribute to four main bodies of research. Management science researchers have recognized the
potential value in integrating human judgment with forecasting algorithms (see Arvan et al. 2019 for a
review). Humans often possess so-called “domain knowledge”: better and more up-to-date information than
what statistical models use (see x3 of Lawrence et al. 2006). Such domain knowledge is the generally
accepted explanation for why human judgmental forecasting sometimes even outperforms statistical models
in practice (see Lawrence et al. 2000 for sales forecasting and Alexander Jr 1995 for earnings forecasting).
The two most common integration approaches are to make judgmental adjustments to an algorithm’s point
forecast (e.g., see Carbone et al. 1983, Fildes et al. 2009) or to combine separate human and algorithm
point forecasts (e.g., see Blattberg and Hoch 1990, Goodwin 2000). We contribute by examining a different
human elicitation question from the point forecast. Notably, our proposed method is not equivalent to
judgmental adjustments because we use the PIA as an input for the prediction algorithm. In fact, in our
experiments, using PIA responses to adjust algorithm forecast outputs yields poor predictive performance.
A stream of behavioral operations management research studies the system design implications of human
random error. For example, the best way to design contracts (Su 2008, Ho and Zhang 2008), queues (Huang
et al. 2013, Tong and Feiler 2017), or auctions (Davis et al. 2014) changes once the system designer considers
human random error. Most closely related to our paper is Kremer et al. (2016). They show that human
random error causes eliciting human forecasts in a top-down fashion to be more effective in some environments
but bottom-up forecasting to be more effective in others.We contribute by showing how a forecasting
system’s elicitation design impacts performance once one considers human random error, even if it has no
effect without human random error.
Researchers in judgment and decision making have made advancements in developing strategies to
improve human judgment accuracy. Perhaps the most well-known idea is to harness the “wisdom of crowds”
(e.g., see Surowiecki 2005) through averaging multiple people’s judgments. Interestingly, because people
are so inconsistent (Kahneman et al. 2016), even averaging multiple judgments by the same person separated
by time (Vul and Pashler 2008) or with a prompt to think differently (Herzog and Hertwig 2009) helps,
albeit only about half as much as averaging judgments by two different people (Mannes et al. 2012). Most
closely related to our work in this stream is Palley and Soll (2019), who develop a new elicitation method
that improves the “wisdom of crowds” strategy by estimating the amount of shared information between
individuals. Our elicitation strategy also seeks to improve an aggregation strategy by addressing the issue
of disentangling the shared information between the human and the algorithm.
Lastly, studying the benefit of incorporating discretion from humans with local knowledge in operational
decision making has been an emerging topic of study in operations management. Various application
domains have been considered, such as capacity decisions in service operations (Campbell and Frei 2011),
sales forecasting (Osadchiy et al. 2013), price setting (Phillips et al. 2015), hospital unit admission decisions
(Kim et al. 2015), and product removal decisions in retail stores (Kesavan and Kushwaha 2020).
2. Theory Development
In this section, we leverage simple mathematical models to compare the theoretical performance of a prediction
algorithm that uses human DF and one that uses human PIA. Specifically, our focus is on showing
that the difference in performance depends critically on whether or not we assume the forecaster suffers
from random error. Our theoretical development is agnostic about the exact sources of this random error
and does not attempt to provide a detailed description of the psychology of prediction. Rather, the point
of the models is to clearly demonstrate that including random error in the forecast is sufficient to generate
differences between DF and PIA. We use these results to motivate two hypotheses about whether and how
eliciting the PIA will be more effective than DF.
2.1. Surgery Duration Assumptions
We assume an actual surgery duration, Y , is a random variable defined by the linear model
Y =v +
X
i2P[I
wiXi +; (1)
where we separate the public factors, denoted by the index set P, from the private factors, denoted by the
index set I. In (1),  is an error term, with E[]=0, which represents true environmental random shocks, i.e.,
random variations that are impossible to predict even with all public and private information. We assume
that  and (Xi)i2P[I are mutually independent.
2.2. DF and PIA Are Equivalently Effective with Consistent Forecasters (No Random Error)
We define DF and PIA for the consistent forecaster who does not suffer from random error as follows:
DF =v +
X
i2P[I
w
iXi and PIA =
X
i2I
w
iXi: (2)
Here, we assume that v and w
i , for i 2 P [ I, are deterministic, though not necessarily known by the
algorithm a priori. (Note that they can be any constants and are not necessarily “optimal”. For example,
setting w
i =0 is equivalent to assuming humans do not use that information.) Define the best-fitting models
of Y given the public factors and DF or PIA using linear regression:
(Model-DF) MDF =0 +
X
i2P
iXi +DFDF; (3)
(Model-PIA) MPIA =
0 +
X
i2P

iXi +PIAPIA: (4)
Then, the following proposition holds. We relegate the proofs of all propositions to the Appendix.
PROPOSITION 1. Model-DF and Model-PIA yield the same predictions.
That is, with consistent forecasters, predicting surgery durations using DFs as model inputs yields the same
predictions as using PIAs as model inputs; the two elicitation methods are equivalent from the algorithm’s
perspective.
2.3. PIA Outperforms DF with Inconsistent Forecasters (Random Error)
Next, we define DF and PIA for the inconsistent forecaster who does suffer from random error:
DFb =vb +
X
i2P[I
Wb
i Xi and PIAb =
X
i2I
Wb
i Xi: (5)
Here, we assume that Wb
i are random variables with E[Wb
i ] =  wb
i and Var[Wb
i ] > 0. We also assume that
Wb
i and Xi are all mutually independent, for i 2 P [ I. Thus, in contrast to (2), (5) captures inconsistencies
or random error in assigning weights to each factor. For example, these random weights could reflect
inconsistencies in the encoding of information, memory retrieval, aggregation of multiple factors, or the
translation from one domain to another. Also, note that this random weights model can capture ideas such as
inconsistency in which factors humans take into account or adding a random term to DFb but not to PIAb.
For the purposes of this paper, we make no strong claim about the psychological source of this random
error—only that it exists (e.g., see Kahneman et al. 2016) and is greater when people are asked to account
for more factors.
Define the best-fitting models of Y given the public factors and DFb or PIAb using linear regression:
(Model-DFb) MDFb =0 +
X
i2P
iXi +DFbDFb; (6)
(Model-PIAb) MPIAb =
0 +
X
i2P

iXi +PIAbPIAb: (7)
In contrast to the equivalence result in Proposition 1 for the consistent-forecaster model, the following
proposition demonstrates the benefit of eliciting the PIA compared to eliciting the DF under the inconsistentforecaster
model. (Note that all propositions hold for both MSE and RMSE; we use RMSE to report our
experiment results.)
PROPOSITION 2. The mean squared error (MSE) for predictions under Model-DFb is strictly larger
than that under Model-PIAb, i.e., E[(Y 􀀀MDFb )2]>E[(Y 􀀀MPIAb )2].
6
The intuition is that from the algorithm’s perspective, the value of the human input is the private
information—the algorithm already has the public information. The algorithm can infer the private information
equally well from DF or PIA responses without human random error. However, when there is human
random error, the algorithm can more accurately infer the private information from the PIA. Based on this
result, we formulate our first hypothesis.
Hypothesis 1 All else equal, a prediction model that is calibrated using DF yields less accurate predictions
than a prediction model that is calibrated using PIA.
2.4. The DF-PIA Gap Magnitude Depends on Random Error Location
Proposition 2 establishes our main result that because of human random errors, using PIA yields more
accurate predictions than using DF. We now investigate how the “location” of these random errors (i.e.,
whether they occur incorporating public versus private factors) affects the performance difference between
DF and PIA. To do so, we study the behavior of the DF-PIA gap, which we define as the difference in the
MSEs from Proposition 2, E[(Y 􀀀MDFb )2]􀀀E[(Y 􀀀MPIAb )2].
Random Error Incorporating Public Factors. To examine the effect of random error on the DF-PIA
gap when incorporating public factors, we consider two cases that are identical except for the degree of
variability in Wb
i for i 2 P. Specifically, we define fWb
i to be a mean preserving spread of Wb
i (Rothschild
and Stiglitz 1970). The following result establishes that increasing the variability in how people incorporate
public factors increases the DF-PIA gap:
PROPOSITION 3. The DF-PIA gap is larger when fWb
i is used in (5), for i 2 P, instead of Wb
i .
The idea behind Proposition 3 is as follows. Model-PIAb remains the same when we add variability to
Wb
i ; i 2 P because PIA responses are unaffected by the random error incorporating public factors. However,
Model-DFb is less accurate when fWb
i is used instead of Wb
i ; i 2 P. DF responses are more variable when
fWb
i is used, which makes it harder for the algorithm to learn the private factors. Combining these two
observations implies that the DF-PIA gap increases.
Figure 1 shows the results from numerical simulations (see Appendix B for details). The left panel corresponds
to Proposition 3. It varies the standard deviation of the public-factor random weight, holding
constant the standard deviation of the private-factor random weight. Observe that the DF-PIA gap increases
with the variability in the public-factor weight because the RMSE associated with Model-PIAb remains
constant, while the RMSE associated with Model-DFb increases.
7
Figure 1 Numerical Simulation Illustrations of Propositions 3 and 4.
Random Error Incorporating Private Factors. We now turn to examining the effect of random error on
the DF-PIA gap when incorporating private factors.We proceed as above, by considering two cases that are
identical except for the degree of variability in Wb
i for i 2 I.
PROPOSITION 4. The DF-PIA gap is smaller when fWb
i is used in (5), for i 2 I, instead of Wb
i .
In contrast to Proposition 3, Proposition 4 shows that adding variability to how people incorporate private
factors reduces the DF-PIA gap. Both Model-PIAb and Model-DFb lose accuracy as we add variability to
Wb
i ; i 2 I. However, the loss is more dramatic for Model-PIAb. PIA’s advantage of more directly eliciting
the private information decreases as the random error incorporating private information increases.
The right panel of Figure 1 is the corresponding figure for Proposition 4. Observe that the DF-PIA gap
decreases in the standard deviation of the private-factor random error term because while random error
incorporating the private factor increases the RMSE under both Model-DFb and Model-PIAb, the increase
is steeper in the latter.
Summary and Hypothesis. Proposition 3 shows that the DF-PIA gap increases in the random error incorporating
public factors. Proposition 4 shows that the DF-PIA gap decreases in the random error incorporating
private factors. Combined, they imply that the DF-PIA will be greater when adding random error
incorporating public information than when adding the same amount of random error incorporating private
information. Based on these results, we formulate our second hypothesis:
Hypothesis 2 The location of random error moderates the DF-PIA gap. Specifically,
(a) Inducing greater random error incorporating public information increases the DF-PIA gap.
(b) Inducing greater random error incorporating private information decreases the DF-PIA gap.
8
(c) Random error incorporating public information increases the DF-PIA gap more than random error
incorporating private information.
3. Experiment 1: Elicitation via DF versus PIA
Experiment 1 is a simple direct test of Hypothesis 1.
3.1. Experimental Design
3.1.1. Task. Participants first reviewed 30 historical surgeries, each with information about the number
of procedures, the anesthesia complexity score, and the resulting surgery duration. They then completed 50
rounds of surgery duration prediction. In each round, they were shown a new surgery’s number of procedures
and anesthesia complexity score. Then, they were asked a question about predicting its duration.
3.1.2. Conditions. Subjects were randomly assigned to one of two conditions: direct forecast (“DF”)
or private information adjustment (“PIA”). The only difference between these two conditions is that in
each of the 50 rounds, DF participants answered the question “What is your forecast for the duration of
this surgery? I think this surgery duration will be minutes.”, whereas PIA participants answered the
question “The hospital system only has the first piece of information about this surgery—the number of
procedures. You have additional information. To account for your additional information, how would you
advise the hospital system to adjust its forecast for the duration of this surgery? I would advise the hospital
system to increase/decrease (choose one) its forecasted surgery duration by minutes.” See Appendix,
Figure E.1 for screenshots.
3.1.3. Simulating Surgery Duration. We used the following equation to simulate surgery duration:
Ys =60+20XP
s +10XI
s +s. Here, Ys is the duration of surgery s; XP
s denotes the number of procedures,
an integer-valued public factor that has a uniform distribution between 1 and 10, inclusive; and XI
s denotes
the anesthesia complexity score, a private factor that has a uniform distribution between 􀀀5 and 5. Finally,
s follows a normal distribution with mean 0 and standard deviation 5. All participants observed the same 30
simulated historical surgeries. However, each participant observed a unique sequence of randomly generated
surgeries for their 50 prediction rounds.
3.1.4. User Interface and Instructions. We programmed the user interface using the online software
SoPHIE (Hendriks 2012). After receiving written instructions describing the task, participants were required
to pass a three-question comprehension test before starting the experiment. They could review the instructions
and retake the test until they answered all questions correctly. See Appendix, Figure E.2 for full
instructions.
3.1.5. Pre-registration. For all experiments, we set our target sample sizes, exclusion criteria, and
analysis plans a priori. We pre-registered to exclude participants who (1) did not complete the experiment
or (2) put the same answer more than 90% of the time. We also pre-registered our dependent variable and
analyses. We calibrate prediction algorithms using data from the participants’ training set (first 35 rounds)
and then use the algorithms to generate predictions on the test set (last 15 rounds). Our performance criterion
is the RMSE of the predictions generated on the test set. The full pre-registration document for Experiment
1 is available at https://aspredicted.org/blind.php?x=3e427n.
3.2. Results
3.2.1. Participant and Response Summary Statistics. Undergraduate and graduate students at a large
research university in the US were invited to participate through a behavioral laboratory subject pool recruitment
system. Each participant received a $10 electronic gift card for completing the online study.
A total of 120 students participated. Following our exclusion criteria, we removed 8 participants who did
not complete the experiment, leaving 112 for analysis (56 in each condition). Among the 112 participants,
75% were female, 88% were 18 to 24 years old, and 12% were 25 to 34 years old. The average of mean
response to the question was 152:6 minutes (SD 30:8) in the DF condition and 12:1 minutes (SD 23:7) in
the PIA condition.
3.2.2. Algorithm Calibration and Prediction Accuracy Calculation. For each condition, we used the
number of procedures, actual surgery duration, and participant response from all participants’ first 35 rounds
to calibrate prediction algorithms for surgery duration. The pre-registered linear regression model included
participant dummies, number of procedures interacted with participant dummies, and participant response
interacted with participant dummies. Table E.1 in the Appendix summarizes the prediction algorithms calibrated
for each condition. We used the calibrated prediction algorithms to generate the predictions, ^ Ys, for
each surgery s in the last 15 rounds for each participant.We then computed RMSE =
q
1
15
P15
s=1(Ys 􀀀 ^ Ys)2
for each participant.
3.2.3. Testing Hypothesis 1. The average RMSE (averaged across all participants) was 22:4 (SD 6:3)
in the DF condition and 17:8 (SD 7:6) in the PIA condition; see Figure 2(a). This difference of 4:6 is
significant (p=0:0008) and represents a 21% decrease. This result supports Hypothesis 1.
3.2.4. Other Benchmarks. Figure 2(a) also shows the performance of three other benchmarks:
“DF-As is” corresponds to using participant DF condition responses without any algorithms. Doing so
results in an average RMSE of 46:8 (SD 24:5), significantly worse than when we use participant responses
as inputs to algorithms.
Figure 2 Experiment 1: Performance Comparison.
0 10 20 30 40 50 60
Root Mean Square Error (RMSE)
 
DF−As is XP only DF PIA XP and XI
(a) RMSE comparison. Means and standard errors are shown.
XP is the public factor, and XI is the private factor.
0 10 20 30 40
Root Mean Square Error (RMSE)
 
−.25 0 .25 .5 .75 1
 
Correlation(XI, R)
DF condition
PIA condition
(b) Correlation(XI , R) versus RMSE. Each dot is one participant.
R is defined as the residual of response after regressing it
on XP . Red x marks show the performance of the “XP and XI”
model.
“XP only” corresponds to using only the public information in the algorithm, without the use of any
participant responses. Across the 112 participants, such an algorithm leads to an average RMSE of 29:3
(SD 3:5)—an improvement over “DF-As is” even though participants had access to the private information
in addition to the public information. However, it is worse than the average RMSE of both the DF condition
(p < 0:0001) and the PIA condition (p < 0:0001). In other words, participant responses added predictive
value in the experiment.
Lastly, “XP and XI” corresponds to allowing prediction algorithms to directly observe the private information
XI and include it in prediction algorithms. In this experiment, it is equivalent to “consistent forecasters”
and is a benchmark for the best performance possible. This algorithm results in an average RMSE
across the 112 participants of 4:9 (SD 1:1).
3.2.5. Mechanistic Evidence. The theorized mechanism driving Hypothesis 1 is that PIA responses
help the algorithm account for the private information better than the responses from the DF condition. To
examine this mechanism, we calculate the correlation of the PIA responses with XI and compare them with
the correlation of the DF responses with XI .
Naturally, because the PIA asks directly about the private information, the correlation between XI and
response was lower in the DF condition than in the PIA condition (0:39 versus 0:74). Next, we consider the
correlation between XI and R, where we define R as the residual of participant response after regressing
it on XP . That is, we take out the effect of public factor from each response to construct R. Note that if
participants did not suffer from random error, then R would be perfectly correlated with XI in both DF
and PIA conditions. In contrast, we find that it is less than 1 in both conditions. However, it is significantly
higher in the PIA condition than in the DF condition (0:76 versus 0:62, p = 0:0008). In other words, PIA
responses provide better information about XI than DF responses.
Figure 2(b) illustrates the predictive accuracy versus the correlation value above for each participant.
As expected, higher correlation between XI and R leads to better prediction performance. There are more
participants with high correlation in the PIA condition than in the DF condition, which contributes to the
better performance of the PIA condition overall. The red “x” marks indicate the hypothetical perfectlyconsistent
benchmark, with no random error for each participant, which yields perfect correlation for both
DF and PIA conditions. The deviation of the PIA and DF dots from the red marks illustrates the effect of
human random error in participant responses.
3.3. MTurk Replication
We replicated the same experiment with MTurk workers. See https://aspredicted.org/blind.
php?x=yv2vs7 for the pre-registration document. While overall, the predictions from the experiment
with MTurk workers were less accurate, the between-condition results replicated, providing evidence of
robustness across different populations. Appendix C provides details on the replication as well as a comparison
between the performances of university students and MTurk workers.
3.4. Discussion
Consistent with Hypothesis 1, Experiment 1 provides evidence that eliciting the PIA information instead of
DF leads to better prediction algorithm performance. After the effect of public factor is taken out, participant
responses are more correlated with the private factor. This tighter relationship helps prediction algorithms
to incorporate private information, leading to better predictive performance. These results were replicated
across university students and MTurk workers.
4. Experiment 2: Manipulating Information Complexity of Public versus Private
Factors
Experiment 2 was designed to test Hypothesis 2, namely how the DF-PIA gap established in Experiment
1 is moderated by random error in incorporating public versus private factors. In addition, it provides a
replication test of Hypothesis 1 using different surgery duration equations.
4.1. Experimental Design
The task was similar to that in Experiment 1. However, we changed the surgery duration equation, and
we varied the number of factors by condition. We conjectured that greater information complexity induces
greater random error. Therefore, we created higher complexity to induce more human random error by
requiring that subjects aggregate multiple factors. Otherwise, to create lower complexity to induce less
random error, we automatically pre-aggregated multiple factors into a single representative factor for the
participant.
Specifically, in the Baseline case, we pre-aggregated information so that there was only one public and
one private factor, as in Experiment 1. However, we required that participants account for two public factors
in the Public Info Complex case or two private factors in the Private Info Complex case. Thus, the experiment
had a 2 (DF, PIA) by 3 (Baseline, Public Info Complex, Private Info Complex) between-subject experimental
design.
The equations below show the underlying model we used for all conditions and the pre-aggregations we
constructed to manipulate complexity by condition:
Ys = 150+10XP1
s +10XP2
s +10XI1
s +10XI2
s +s (Underlying Model)
= 150+10XP1
s +10XP2
s +(50+20XI
s )+s (Public Info Complex)
= 150 +(50+20XP
s )+10XI1
s +10XI2
s +s (Private Info Complex)
= 150 +(50+20XP
s ) +(50+20XI
s ) +s (Baseline).
Here, XP1
s and XP2
s represent the two public factors. In the experimental task, they are the “procedure
set-up requirements” and the “procedure complexity score,” respectively. Symmetrically, XI1
s and XI2
s represent
the two private factors. In the experimental task, they are the “anesthesia set-up requirements” and
the “anesthesia complexity score,” respectively. The random generation process for public and private factors
was symmetric. For every surgery s, XP1
s and XI1
s were uniform random integers between 0 and 10,
inclusive. XP2
s and XI2
s were uniform random numbers between 􀀀5 and 5 (rounded to the nearest tenth).
We set XP
s = (XP1
s 􀀀5)=2+XP2
s =2 and XI
s = (XI1
s 􀀀5)=2+XI2
s =2, which establishes the above equalities.
In the experimental task, they are a generic “procedure score” and “anesthesia score,” respectively.
See Appendix, Figure E.3 for screenshots. The pre-registration document for Experiment 2 is available at
https://aspredicted.org/blind.php?x=9uq8dw.
4.2. Results
4.2.1. Participants, Participant Responses, and Prediction Algorithm. MTurk workers who were
located in the US, had a Human Intelligence Task (HIT) approval rate of 99% or higher, and had 100 or more
HITs approved were qualified to participate in the experiment. Participants who completed the experiment
were paid $2 for participation. A total of 480 MTurk workers participated. Following the pre-registered
exclusion criteria, we removed 174 individuals who did not complete the experiment and 54 participants
who failed to correctly answer a four-question comprehension test on their first attempt. Among the 252
remaining participants, 42% were female, and 8% were 18 to 24 years old; 36%, 25 to 34; 31%, 35 to 44;
13%, 45 to 54; and 11%, 55 or over. Columns (1) and (2) of Table 1 provide the number of participants
and the average response in each condition. We developed prediction algorithms in the same way as in
Experiment 1 (see x3.2.2). Table E.2 in the Appendix summarizes the prediction algorithms.
Table 1 Experiment 2: Summary of Experiment Results.
Information Question (1) (2) (3) (4) (5) (6)
Type Type N Response Corr(XI , response) Corr(XI , R) RMSE of test set DF-PIA gap
Baseline DF 47 236.6 (30.7) 0.52 (0.21) 0.63 (0.25) 35.2 (13.9) 12.5***
PIA 42 4.0 (19.1) 0.76 (0.23) 0.79 (0.22) 22.8 (11.4)
Public Info DF 41 241.9 (24.5) 0.28 (0.18) 0.42 (0.26) 41.2 (11.2) 19.9***
Complex PIA 38 13.8 (33.9) 0.80 (0.27) 0.80 (0.27) 21.3 (14.0)
Private Info DF 47 237.7 (31.6) 0.66 (0.15) 0.70 (0.15) 30.6 (8.8) 1.7
Complex PIA 37 48.8 (46.1) 0.72 (0.17) 0.73 (0.17) 28.9 (7.0)
Note. Means and standard deviations (in parentheses) are shown. XP is the public factor, and XI is the private factor. In column
(4), R is defined as the residual of response after regressing it onXP . In column (6), DF-PIA gap is defined as the difference between
the mean RMSEs of DF and PIA conditions. Column (6) also shows DF-PIA gap’s statistical significance from a two-sample t-test
for difference of means. * p <0:05, ** p <0:01, *** p <0:001.
4.2.2. Robustness of Hypothesis 1. Columns (5) and (6) of Table 1 summarize the prediction performance
in each of the six conditions. Consistent with Hypothesis 1, the average RMSE of all PIA participants
was 32% lower than the average RMSE of all DF participants (35:4 versus 24:2, p < 0:0001). As shown
in column (6) of Table 1, the DF-PIA gap was statistically significant at the 5% level in two of the three
information conditions. In x3.2.5, we found that better performance is associated with higher correlation
between participant response and XI after the effect of XP in the responses is taken out. Columns (3) and
(4) of Table 1 provide the average correlation between XI and response and the average correlation between
XI and R, defined as the residual of response after regressing it on XP . As expected, the correlations are
higher in the PIA conditions than in the DF conditions, which again provides mechanistic evidence for
Hypothesis 1.
4.2.3. Testing Hypothesis 2. Hypothesis 2(a) predicts the DF-PIA gap to be greater under publicinformation-
complex conditions than under baseline conditions. Consistent with this hypothesis, the gap
was 19:9 under the public-information-complex conditions and 12:5 under the baseline conditions. This
difference of 7:4 was statistically significant (p=0:036; see Table 2).
Hypothesis 2(b) predicts the DF-PIA gap to be smaller under private-information-complex conditions
than under baseline conditions. Consistent with this hypothesis, the gap was 1:7 under the privateinformation-
complex conditions and 12:5 under the baseline conditions. This difference of 10:8 was statistically
significant (p=0:002; see Table 2).
Hypothesis 2(c) predicts the DF-PIA gap to be greater under public-information-complex conditions than
under private-information-complex conditions. Consistent with this hypothesis, the gap was 19:9 under
public-information-complex conditions and 1:7 under private-information-complex conditions. This difference
of 18:2 was statistically significant (p < 0:001; see Table 2). These findings provide evidence that the
benefit of PIA over DF is greater when public information is complex than when private information is
complex.
One unpredicted pattern is that the RMSE in DF is smaller under the private-information-complex condition
than under the baseline condition (30:6 versus 35:2, p <= 0:056). A plausible explanation is that, in
addition to inducing more random error, splitting the private information into two factors causes participants
to weight the private information more in general (see Fox and Clemen (2005)).
Table 2 Experiment 2: Performance Comparison.
(1)
Root Mean Squared Error
Information type (Base is Baseline conditions)
Public Info Complex conditions 5.96 (2.43)
Private Info Complex conditions -4.64 (2.35)
Question type (Base is DF conditions)
PIA conditions -12.49 (2.42)
Interaction effects (Base is Baseline conditions  PIA conditions)
Public Info Complex  PIA -7.42 (3.52)
Private Info Complex  PIA 10.77 (3.48)
Constant 35.25 (1.66)
N 252
R2 0.27
Note. Column (1) is a linear regression model with RMSE as the dependent variable. + p < 0:1, * p < 0:05,
** p <0:01, *** p <0:001.
4.3. Discussion
In addition to replicating Hypothesis 1 under different simulation parameters and information variables,
Experiment 2 provided evidence that the location of random error matters in a manner consistent with
Hypothesis 2. Specifically, eliciting human judgment via PIA instead of DF is helpful because of random
error incorporating public information. Increasing random error incorporating private information reduces
this benefit.
5. General Discussion
5.1. Summary
Our theoretical and experimental results suggest that in some situations, there is an opportunity to substantially
improve the way prediction algorithms incorporate human judgment by applying a new PIA elicitation
method, instead of the traditional method of DF. Under DF, humans contribute random error as they account
for public information that the algorithm can already access. This random error hinders the algorithm’s
ability to infer the humans’ private information. PIA avoids this hindrance by asking more directly about
how much to adjust for the human’s private information.
5.2. What Is Private Information and Why Does It Exist?
Pragmatically speaking, private information is any predictive information the human observes that the algorithm
does not use. Because private information is context specific, rather than attempting to discuss exactly
what it is, we find it constructive to discuss reasons why humans may have information that the algorithm
does not use (i.e., why private information exists).We discuss these reasons in the context of our motivating
example of predicting surgery durations: see Ibrahim and Kim (2019). At this hospital, the public information
is the patient’s electronic medical record as well as answers to specific questions from a standardized
booking slip for surgery. All other predictive information the surgeon has is private.
5.2.1. Identification Challenges. Some theoretically easy-to-input information may be private simply
because the system fails to request it. In our experiments, private information is easy to identify because
the researcher knows all the data that exist in the environment and what data the human can access that the
algorithm cannot. In practice, such an exercise is more difficult because if information is kept private from
the algorithm, it may also be kept from the system designer. In other words, you cannot ask for what you
do not know exists. For example, a surgeon may need special anesthesia equipment that requires additional
set-up time, but there may be no place to indicate this information on the booking slip because the system
designer was unaware of this special equipment.
5.2.2. Privacy Concerns and Integration Barriers. Even if the system designer knows that humans
have certain private information that should be used in the algorithm, humans may be unwilling to input
this information. For example, the surgeon may have a mental estimate of the likelihood of making a severe
mistake during the procedure, but he/she may be unwilling to record that information for liability reasons.
Similarly, certain information may be stored somewhere that is difficult to integrate. For example, a surgeon
may know which technician is scheduled to support the surgery, but that information is stored in another,
unintegrated database.
5.2.3. Codification Challenges. Despite advancements in “big data,” it often remains impractical to
input and store certain types or large quantities of predictive information into an organization’s system. For
example, one study reports that experienced doctors use nearly two million pieces of information to treat
their patients (Pauker et al. 1976). While some of the two million pieces of information may be explicit
knowledge—knowledge that can be easily articulated, codified, stored, and accessed—they are likely to be
tacit knowledge, or knowledge that is difficult transfer, such as intuitive judgment; see Cowan et al. (2000)
for a detailed discussion. As a result, inputting this information into the system will be costly and time
consuming (Pollack et al. 2014), if not impossible.
5.2.4. Insufficient Training Data. Even if information has been inputted into the system, it may remain
unused by the prediction algorithm due to a lack of sufficient historical data for training. Macario (2006)
reports that 50% of surgeries have less than five previous cases with the same procedure and same surgeon
during the preceding year. Zhou and Dexter (1998) report that only 32% of their surgeries had two or more
previous cases with the same procedure and same surgeon. Ibrahim and Kim (2019) remove about 60% of
the surgeries from their data collected over three years to keep only the surgeries that had 30 or more cases
with the same procedure and same surgeon. The fact that each specialty, or even each procedure, has its own
meaningful variables that are specific to the specialty or the procedure exacerbates the problem (Hosseini
et al. 2015). Thus, the algorithm designer may intentionally choose to leave certain information as “private”
because of lack of training data to make it useful.
5.3. What Should System Designers Do?
5.3.1. Consider Eliciting Human Input Even If Human Forecasts Are Inaccurate. Our study was
motivated by a hospital administrator who, concerned with poor human forecasting performance, was considering
using algorithms and fully eliminating surgeons from the surgery duration forecast process. Our
results highlight the fact that even when human forecasts are significantly worse than algorithms head-tohead,
system designers can potentially significantly boost the algorithms’ performance by seeking human
input. Thus, when implementing prediction algorithms, system designers should check to see whether
human judgment can boost algorithm performance before fully replacing humans.
5.3.2. Try Adding a PIA-Type Question. When using human judgment as an algorithm input, we suggest
adding a PIA-type question, especially when the public information is complex or the system designer
knows that there exists simple private information. Note also that DF and PIA are not mutually exclusive.
Thus, if the system already elicits the DF, one may choose to add the PIA and use both as inputs to the
algorithm. Prompting people to think differently via DF and PIA may, in fact, help people better communicate
private information to the algorithm (e.g., Herzog and Hertwig 2009), although doing so requires more
effort.
5.3.3. Identify and Convert Private Information into Public Information. The results of this paper
suggest that PIA mitigates the undesirable impact of human random error relative to DF, but not completely.
Directly converting private information to public information will be superior to eliciting the PIA (e.g., see
Figure 2) once enough training data have been collected. Thus, in addition to eliciting the PIA, we suggest
attempting to learn what the underlying private information is behind humans’ answers and altering the
system to collect or directly elicit it moving forward. While it is unlikely that one will be able to fully
eliminate private information in this manner, in some contexts, one may be able to eliminate enough private
information to render the improvement due to PIA or DF negligible.
5.3.4. Experiment with and Revise the PIA Elicitation Format. Because the type of private information
is context specific, the best way to write the PIA question is also likely to be context specific. Therefore,
we suggest experimenting with and periodically revising how to write the PIA question. In Appendix D,
we have made some limited progress on this issue via an experiment. We found that changing the format
of the PIA question to make it easier for the human to translate from the domain of the private information
to the domain of the question can enhance PIA’s performance. For example, on the one hand, if the private
information is a relative assessment of complexity, then structure the PIA question as an assessment relative
to an average patient with the same public information. On the other hand, if the private information is a
delay in minutes, then format the PIA question to be in minutes.
5.4. How Can Future Research Help Improve (or Disprove) This PIA Idea?
One limitation of our laboratory study approach is that it does not directly address the question of whether
PIA will outperform DF in practice. Certainly, field experiments or even more realistic laboratory experiments
can help address this question. Nevertheless, our initial studies suggest that more investigation into
how best to write the PIA question may be beneficial before one can confidently implement it and assess its
performance relative to DF. How should one decide whether to make the PIA question in a relative domain
(e.g., relative to an average case) or an absolute domain (e.g., in dollars)? What is the best way to describe
public information in specific practical contexts? Should one decompose the PIA into multiple parts based
on known categories of private information? Does showing the algorithm’s forecast before humans provide
their PIA help or hurt? Are there algorithm aversion or incentive issues that need to be addressed before
implementation?
We also recognize that there are several other open theoretical questions. We have assumed linear relationships
and linear models in this paper. Intuitively, we believe the main directional predictions apply to
other machine-learning algorithms and non-linear relationships. However, future work may verify whether
our results do indeed generalize, which may lead to further insight. Furthermore, we have assumed a simple
model of random error that does not capture detail in how humans turn cues into predictions, where exactly
the random error occurs, or how the format of the PIA question might matter. Another potentially fruitful
direction is to incorporate further psychologically descriptive detail into a behavioral model of prediction.
In conclusion, field work, laboratory experiments, and behavioral models are all important for enhancing
the understanding and use of PIA questions. It is our hope that by defining the PIA idea and documenting
its potential improvement experimentally, we stimulate research that drives improvement in practice.






----------------------------------------------------------------------------------







Yalcin, G., Lim, S., Puntoni, S., & van Osselaer, S. M. J. (2022). Thumbs Up or Down:
Consumer Reactions to Decisions by Algorithms Versus Humans. Journal of Marketing
Research, 59(4), 696-717. https://doi.org/10.1177/00222437211070016

Abstract
Although companies increasingly are adopting algorithms for consumer-facing tasks (e.g., application evaluations), little research
has compared consumers’ reactions to favorable decisions (e.g., acceptances) versus unfavorable decisions (e.g., rejections) about
themselves that are made by an algorithm versus a human. Ten studies reveal that, in contrast to managers’ predictions, consumers
react less positively when a favorable decision is made by an algorithmic (vs. a human) decision maker, whereas this difference
is mitigated for an unfavorable decision. The effect is driven by distinct attribution processes: it is easier for consumers to internalize
a favorable decision outcome that is rendered by a human than by an algorithm, but it is easy to externalize an unfavorable
decision outcome regardless of the decision maker type. The authors conclude by advising managers on how to limit the likelihood
of less positive reactions toward algorithmic (vs. human) acceptances.

Keywords
algorithms, decision making, decision outcome favorability, attribution theory

A growing number of companies are using algorithms to make
business decisions that directly affect potential and existing customers.
For example, algorithms are now used to decide which
applicants should be admitted to platforms (e.g., Raya) and who
should receive loans (e.g., Upstart; for more examples, see Web
Appendix A). As the prevalence of algorithms in consumerfacing
decisions increases, so does the managerial importance
of understanding consumers’ reactions to algorithmic versus
human decisions. We investigate consumers’ reactions toward
a company following a decision (favorable or unfavorable)
made by an algorithmic versus a human decision maker.
Specifically, we focus on contexts where the decision outcome
is considered diagnostic of the consumer’s qualifications, deservingness,
or merit, such as when consumers submit an application
to access a valued service or other benefits.
We demonstrate that consumers react less positively when a
favorable decision (e.g., the acceptance of an application) is
made by an algorithm rather than by a human. This difference,
however, is attenuated for an unfavorable decision (e.g., the
rejection of an application). We explain this interaction
between the decision maker type and decision outcome favorability
by drawing on attribution theory (Jones and Davis
1965; Kelley 1967). Consumers are motivated to internalize
favorable decisions, but internal attribution is more difficult
when the decisions are made by an algorithm (vs. a human),
so consumers react less positively (e.g., form less positive attitudes
toward the company). By contrast, consumers are motivated
to externalize unfavorable decisions, and this is
similarly easy with algorithmic and human decision makers,
so consumers’ subsequent reaction is relatively indifferent to
the decision maker type.
The current research makes three primary contributions (for
a comprehensive literature review, see Table 1). First, our
research addresses an underexplored question: How do consumers’
attitudes (and related constructs) change as a function
of a company’s use of algorithmic versus human decision
makers in consumer-facing tasks? Previous work has focused
on consumers’ choices, such as for advice, between an algorithmic
and a human decision maker (Dietvorst, Simmons, and
Massey 2015; Longoni, Bonezzi, and Morewedge 2019).

However, companies usually decide whether to rely on algorithms
or humans for a given task; consumers are more often
in the position of decision recipients. Unlike prior research,
the current research focuses on consumers’ reactions to algorithmic
versus human decisions about themselves. This distinction
is important because the two situations may elicit different
psychological processes. Decision recipients face the task of
interpreting a decision outcome reflective of one’s worth in
the eye of others. In such a context, one’s reaction to the decision
outcome often involves self-serving interpretations and
motivated reasoning (Taylor and Brown 1988), a topic that
has not been examined in prior research on algorithmic decisions.
More generally, as consumers’ choices often diverge
from their reactions to the given options (Botti and Iyengar
2006), we argue that it is unclear whether findings about consumers’
choice behavior (e.g., reluctance to rely on algorithmic
advice) are generalizable to the reactions of consumers as decision
recipients (e.g., negative reactions to algorithmic decisions
made about the consumers themselves).
Second, we examine an important factor that influences consumers’
reactions to different decision makers: the favorability of
decision outcomes, which is known to affect people’s attitudes
and behaviors (e.g., Barry, Chaplin, and Grafeman 2006;
Rhodewalt and Davison 1986). Both types of decision outcomes
are common; companies may deliver approvals or acceptances as
well as denials or rejections to existing or potential customers—
and yet, the consequences of decision outcome favorability are
underexplored in the research on algorithmic (vs. human) decision
making.We find that most managers believe that consumers
react more positively to decisions made by humans (vs. algorithms)
regardless of the decision outcome (see the managerial
intuitions study and Web Appendix B). We demonstrate,
however, that favorable decision outcomes elicit divergent reactions
to algorithmic versus human decisionmakers, whereas such
difference is attenuated for unfavorable decision outcomes.
Third, in examining the process underlying the proposed
effect, we elucidate how consumers interpret decisions made
by algorithms versus by humans. Unlike prior work that
focuses on consumers’ diverging perceptions of humans and
algorithms (e.g., moral authenticity [Jago 2019], trustworthiness
[Lee 2018]), the current work examines consumers’ differential
attribution of a given decision outcome. Specifically, we
demonstrate that for a favorable decision, a human (vs. an algorithmic)
decision maker facilitates stronger internal attribution
of the decision outcome, whereas for an unfavorable decision,
consumers readily engage in external attribution regardless of
the type of decision maker. The current research thus marries
the psychological literature on attribution (McFarland and
Ross 1982; Okten and Moskowitz 2018) with the marketing literature
on algorithms (Castelo, Bos, and Lehmann 2019;
Puntoni et al. 2021), offering a novel contribution to both.
In the following sections, we review the extant work on algorithmic
and human decision making. We draw on attribution
theory to make theoretical predictions about how consumers
respond to favorable and unfavorable decisions made by algorithms
versus humans.
Theoretical Background
An algorithm is “a set of steps that a computer can follow to
perform a task” (Castelo, Bos, and Lehmann 2019, p. 809). A
growing number of companies rely on algorithms; the market
for artificial intelligence is expected to be worth over $300
billion by 2026 (Markets and Markets 2021). The widespread
adoption of algorithms has encouraged researchers to investigate
how consumers perceive algorithms versus humans. Existing
work has demonstrated that consumers perceive algorithmic
and human decision makers to have different strengths and weaknesses.
For instance, compared with humans, algorithms are perceived
as more objective (Lee 2018; Sundar and Nass 2001) but
also as less authentic, less intuitive, and less moral (Bigman and
Gray 2018; Jago 2019; Yeomans et al. 2019).
Extant work has examined consumers’ choices between
algorithmic and human decision makers and has documented
an aversion to algorithms (for an exception, see Logg,
Minson, and Moore [2019]). For example, consumers are often
reluctant to use algorithms to predict stock prices (Onkal et al.
2009), solicit medical advice (Cadario, Longoni, and
Morewedge 2021; Longoni, Bonezzi, and Morewedge 2019;
Promberger and Baron 2006), and predict people’s performance
(Dietvorst, Simmons, and Massey 2015). In addition, algorithm
aversion varies with contextual factors such as the nature of the
task (subjective vs. objective; Castelo, Bos, and Lehmann 2019)
and the product (hedonic vs. utilitarian; Longoni and Cian 2022).
The current research is the first to examine consumers’ attitudes
toward a company in the context in which (1) a decision
maker (algorithm vs. human) is already chosen, (2) the decision
is made by the company about the consumers themselves (i.e.,
the decision is self-diagnostic), and (3) a decision outcome is
known (see Table 1). Our research context is of managerial
importance. Companies often deliver both types of decision
outcomes—favorable (e.g., approval, acceptance) and unfavorable
(e.g., denial, rejection)—to existing or potential customers.
Our in-depth interviews with practitioners confirm the prevalence
of algorithms in many consumer-facing tasks such as consumer
application evaluations (see Web Appendix C,
interviews #1, #5 and #11) and insurance premium decisions
(#1). Such decisions are often based on personal information
provided by the consumers, and decision outcomes are thus
reflective of consumers’ qualifications.
We posit that the self-diagnostic nature of many consumerfacing
decisions motivates consumers to make different attributions
for favorable and unfavorable outcomes. The type of
decision maker (algorithm vs. human) affects consumers’ internal
and external attributions, leading to an interaction effect
between the decision maker type and decision outcome favorability
on consumers’ attitudes toward the company.
Attribution of Favorable and Unfavorable Decisions as a
Function of the Decision Maker Type
Consumers often make inferences about the causes of events,
actions, and behaviors (Heider 1958; Jones and Davis 1965)

and attribute behaviors or outcomes to either internal or external
causes (Kelley 1967). Attribution theory proposes that people
are motivated to attribute self-relevant outcomes in a selfserving
way: to maintain or enhance their self-worth, people
are motivated to attribute favorable outcomes to themselves
(i.e., “internal attribution”; Baumeister 1999; Zuckerman
1979) and to attribute unfavorable outcomes to external
factors (i.e., “external attribution”; Kelley and Michela 1980;
Miller and Ross 1975). In marketing research, attribution
theory has been used to explain consumers’ perceptions of a
company’s performance (Dunn and Dahl 2012; Folkes 1984;
Wan and Wyer 2019), other consumers’ behavior (He and
Bond 2015; O’Laughlin and Malle 2002), and one’s own behavior
(Leung, Paolacci, and Puntoni 2018; Yoon and Simonson
2008). We contribute to this literature by demonstrating that
the decision maker type (algorithm vs. human) affects how consumers
attribute favorable versus unfavorable decision outcomes.
Consumers who receive a favorable decision are motivated to
make an internal attribution (Luginbuhl, Crowe, and Kahan
1975), and we argue that they find it easier to do so when the
decision is made by a human (vs. an algorithm). Consumers
often define themselves on the basis of personal characteristics
(e.g., abilities, attitudes) that make them feel unique (Brewer
1991; Fromkin and Snyder 1980). Human (vs. algorithmic) decision
makers are perceived as more adept at considering individuals’
unique characteristics and qualifications (Longoni,
Bonezzi, and Morewedge 2019). In contrast, algorithms
usually rely on a set of precoded categories of characteristics
and qualifications that are shared by many (note that an algorithm
probably would not recognize characteristics that are unique to a
single person) and reduce individuals into a number (Newman,
Fast, and Harmon 2020). Thus, we predict that consumers
view a favorable decision made by a human (vs. an algorithm)
as more reflective of their individuality (i.e., unique self) and
deservingness (e.g., “My application was accepted because of
who I am”), so they would more easily make strong internal attributions
for a favorable decision made by a human (vs. an algorithm).
It is easier to attribute a good outcome to “me” when
the decision maker relied on characteristics and achievements
that are “uniquely me.” Put differently, it is more difficult to attribute
a positive outcome to something about oneself if those qualities
or that something is shared with many others.
In contrast, consumers who receive an unfavorable decision
are motivated to make an external attribution, and we argue that
they would find no difference in difficulty to do so regardless of
whether the decision is made by a human or an algorithm. The
decision maker is easily blamed for making a bad decision
regardless of whether that decision maker is a human or an algorithm,
but for different reasons. For instance, an algorithm can
be easily blamed for ignoring consumers’ uniqueness (Longoni,
Bonezzi, and Morewedge 2019), while a human can easily be
blamed for not being objective (Lee 2018).
If the type of decision maker affects consumers’ ability to
make attributional inferences for different decision outcomes,
this should be expected to have repercussions for consumer attitudes.
Causal reasoning—reasoning about what or who is
responsible for a given outcome—is a key factor in attitude formation
and change (e.g., Forsyth 1980; Kelley 1973) and the
marketing literature contains many demonstrations that attributions
are an important determinant of attitudes toward companies
(e.g., Dunn and Dahl 2012). In the context of automation,
Leung, Paolacci, and Puntoni (2018) show that the extent to
which the consumption context enables people to make internal
or external attributions explains their product preferences. For
example, in their Study 6, the authors demonstrate that framing
an automated product in a way that makes it easier for people
to internally attribute favorable consumption outcomes leads to
more positive attitudes toward the product.
Summary of Key Predictions and Overview of Studies
We present ten studies that examine our theory (for a summary,
see Table 2). Whereas most managers predict (in interviews and
surveys) that consumers react more positively to human (vs. algorithmic)
decision makers regardless of decision outcome favorability,
we demonstrate a robust interaction effect on consumers’
attitudes toward the company (Studies 1a–8) and their
word-of-mouth (WoM) intentions (specifically, the net promoter
score measure, Study 1b). Furthermore, we examine the underlying
attribution processes through both mediation (Studies 4 and 6) and
moderation (Study 5). We also rule out alternative explanations
including attention (Study 2), social presence (Study 7), and perceived
fairness (a follow-up study in the “General Discussion”
section). Finally, we offer managerial insights into strategies for
improving reactions to favorable decisions made by algorithms
(Study 8). We report all conditions and all measures. Some
studies included an exploratory measure, for which we report analyses
in the Web Appendix. For some of our studies, we screened
participants beforethe study by using an attention check, such as
an instructional manipulation check (Oppenheimer, Meyvis, and
Davidenko 2009), and those who failed the attention check were
not allowed to proceed to the actual study. Our reports of these
studies include only those who participated in the actual study.
Sample sizes were determined prior to data collection. All data
and study materials are available on OSF (osf.io/3bnsz).
Managerial Intuitions
To evaluate the managerial importance of our findings, we
examined how practitioners would predict customers’ reactions
to favorable versus unfavorable decisions made by humans
versus algorithms. We started with a series of in-depth interviews
with 14 managers, and none correctly predicted our
hypothesized interaction effect. Motivated by this preliminary
result, we conducted a survey with a larger group of experienced
professionals. We report the results of the in-depth interviews
in Web Appendix C and the results of the survey next.
Method
We recruited 88 managers (Mage =35.05 years; 24 women;
Mwork experience =11 years) from an executive master of business administration program at a major European business
school.
We described a business situation involving consumer applications
(see Web Appendix B) and asked the managers to
predict how the type of decision maker would influence customer
satisfaction in response to an acceptance and in response
to a rejection (getting [accepted/rejected] by an algorithm
would be better than getting [accepted/rejected] by an employee
vs. getting [accepted/rejected] by an algorithm would be
equally good as getting [accepted/rejected] by an employee
vs. getting [accepted/rejected] by an employee would be
better than getting [accepted/rejected] by an algorithm).
Results and Discussion
Managers expected that an algorithmic (vs. a human) decision
maker would lead to lower satisfaction regardless of decision
outcome favorability (B=−.09, z=−. 31, p=.758). Specifically,
61% of the managers predicted that participants would be less satisfied
with an acceptance from an algorithm (vs. a human; see
Figure 1). Similarly, 59% of themanagers predicted that consumers
would be less satisfied with a rejection from an algorithm (vs. a
human).
Only 5% (i.e., four managers) generated our predicted interaction
effect: consumers would react more favorably to an
acceptance made by a human (vs. an algorithm) and would be
similarly satisfied with a rejection made by a human and by
an algorithm. Interestingly, managers also predicted that the
decision maker type would matter less for acceptance decisions
than for rejection decisions (choice share of “algorithm =
human”: Mfavorable =23.9% vs. Munfavorable=10.2%; B=
−1.39, z=−2.28, p=.023), the opposite of our hypothesized
pattern.
How Are Consumers’ Attitudes Toward the
Company Affected by the Decision Maker
Type and Decision Outcome Favorability?
The first set of studies tested the managers’ prediction (i.e., consumers
respond more positively to a human [vs. an algorithmic]
decision maker regardless of the outcome) against our own (an
interaction effect). In Studies 1a–b, we examined our hypothesized
interaction effect on two dependent variables: consumers’
attitudes toward the company and WoM intentions. We predicted
that consumers would react less positively when a favorable
decision was made by an algorithm (vs. a human); the
differential reaction would be mitigated for an unfavorable
decision.
Study 1a: Effect of the Decision Maker Type as a
Function of Decision Outcome Favorability: Attitudes
Toward the Company
Method. In this preregistered study (aspredicted.org/j7da3.pdf),
we randomly assigned 993 Amazon Mechanical Turk (MTurk)
workers (Mage=40.06 years; 531 women)1 to one of four conditions
in a 2 (decision maker type: algorithm vs. human) × 2
(decision outcome favorability: favorable vs. unfavorable)
between-participants design.
Participants read that they were applying for membership at
Violethall Country Club (see Web Appendix D). Participants
learned that their applications were either accepted (favorable
decision condition) or rejected (unfavorable decision condition);
we told participants that the decision was made by
either a country club algorithm (algorithm condition) or a
country club coordinator (human condition). We also told all
participants that the decision was final and could not be
appealed. After learning the outcome, participants indicated
their attitudes toward the country club (“What is your general
opinion about Violethall Country Club?”) on three bipolar
items (1=“dislike a great deal”/“very negative”/“not favorable
at all,” and 11=“like a great deal”/“very positive”/“very favorable”;
α=.99; adapted from Park et al. 2010).
Results. A 2 (decision maker type) ×2 (decision outcome favorability)
analysis of variance (ANOVA) revealed a significant
main effect of the decision maker type (Malgorithm=5.16, SD=
3.10 vs. Mhuman=5.38, SD=3.29; F(1, 989)=4.98, p=.026,
η2p
=.01) and of decision outcome favorability (Mfavorable=7.49,
SD=2.61 vs. Munfavorable=3.07, SD=1.96; F(1, 989)=924.46,
p < .001, η2p
=.48). Consistent with our theory and inconsistent
with the managers’ predictions, we found a significant interaction
effect (F(1, 989)=8.46, p=.004, η2p
=.01; see Figure 2): attitudes
toward the country club were less positive among participants
whose applications were accepted by the algorithm than among
participants whose applications were accepted by the club coordinator
(Malgorithm=7.13, SD=2.59 vs. Mhuman=7.88, SD=2.58;
F(1, 989)=13.15, p < .001, η2p
=.01).Meanwhile, the effect of the
decision maker type was significantly mitigated when participants’
applications were rejected (Malgorithm=3.12, SD=2.11
vs. Mhuman=3.02, SD=1.82; F(1, 989)=.23, p=.632).
Study 1b: Effect of Decision Maker Type as a Function of
Decision Outcome Favorability: WoM Intentions
With Study 1b, we aimed to replicate Study 1a with two key
changes. First, we tested whether our effect generalizes to a
nonsocial context: business loan applications. To further
remove social cues, we used “approved” and “denied” instead
of “accepted” and “rejected.” Second, we measured participants’
WoM intentions, another managerially important dependent
variable.

Method. We randomly assigned 500 Prolific workers (Mage=
33.97 years; 264 women) to one of four conditions in a 2 (decision
maker type: algorithm vs. human) × 2 (decision outcome
favorability: favorable vs. unfavorable) between-participants
design.
Participants read that they were applying for a business loan
(see Web Appendix E). We told participants that their loan
applications were either approved or denied by either a loan
algorithm or a loan officer. Next, participants indicated their
attitudes toward the bank (α=.99), as in Study 1a. We also
measured participants’ WoM intentions using the item made
famous by the net promoter score (“On a scale from 0–10,
how likely are you to recommend this bank to a friend or colleague?”;
0=“extremely unlikely,” and 10=“extremely likely”).
Results. We first conducted a 2 (decision maker type) × 2 (decision
outcome favorability) ANOVA on attitudes toward the
bank. We found a significant main effect of the decision
maker type (Malgorithm =5.74, SD =3.20 vs. Mhuman =6.51,
SD =3.35; F(1, 496)=22.17, p < .001, η2p
=.04) and of decision
outcome favorability (Mfavorable =8.72, SD=2.18 vs.
Munfavorable =3.55, SD=1.89; F(1, 496)=853.09, p < .001, η2p
=.63). Crucially, we replicated the significant interaction
effect on consumers’ attitudes (F(1, 496)=8.21, p=.004, η2p
=.02): attitudes toward the bank were less positive among participants
whose applications were approved by the algorithm
than among participants whose applications were approved by
the loan officer (Malgorithm =8.06, SD =2.51 vs. Mhuman =
9.40, SD=1.51; F(1, 496) =28.56, p < .001, η2p
=.05).
Meanwhile, the effect of the decision maker type was significantly
attenuated when the applications were denied
(Malgorithm =3.39, SD =1.79 vs. Mhuman =3.71, SD =1.97;
F(1, 496)=1.71, p=.192).
Next, we conducted an analogous ANOVA on WoM intentions.
We found a significant main effect of the decision maker
type (Malgorithm=4.34, SD=3.02 vs. Mhuman=5.10, SD=3.29;
F(1, 496)=21.06, p < .001, η2p
=.04) and of decision outcome
favorability (Mfavorable=7.15, SD=2.12 vs. Munfavorable=2.31,
SD=2.01; F(1, 496)=722.11, p < .001, η2p
=.59). More importantly,
we found a significant interaction effect (F(1, 496)=
5.04, p=.025, η2p
=.01; see Figure 3): the bank was less likely
to be recommended to others by participants whose applications
were approved by the algorithm than participants whose applications
were approved by the loan officer (Malgorithm=6.54, SD=
2.27 vs. Mhuman=7.77, SD=1.75; F(1, 496)=23.25, p < .001,
η2p
=.04). Again, however, the effect of the decision maker type
onWoMintentions was significantly mitigated when participants’
applications were denied (Malgorithm=2.10, SD=1.81 vs.Mhuman
=2.52, SD=2.18; F(1, 496)=2.76, p=.097, η2p
=.01).
Discussion of Studies 1a–b. Studies 1a–b demonstrated that the
effect of the decision maker type (algorithm vs. human) on consumers’
reactions is a function of decision outcome favorability.
When participants received a favorable decision outcome,
the algorithm (vs. human) decision maker led to less positive
reactions toward the company. However, this effect was significantly
mitigated when participants received an unfavorable
decision outcome.
We note the robustness of our effect thus far: it held in both
social (club membership application) and nonsocial (bank loan
application) contexts and with two managerially relevant measures
of consumers’ reactions (attitudes toward the company
and WoM intentions). In addition, we demonstrated that our
effect is not driven by an assumption that an algorithmic (vs.
human) decision would not be the final decision. We consistently
observed the key interaction effect regardless of
whether we explicitly emphasized that the decision is final.
Note that our findings contradict the managers’ intuitions,
so they are managerially informative. Furthermore, it is
noteworthy that our interaction effect cannot be explained by
the algorithm aversion literature (e.g., Longoni, Bonezzi, and
Morewedge 2019), which documents consumers’ avoidance
of algorithms (vs. humans) without consideration of decision
outcome favorability. The interaction effect is therefore distinct
from prior findings on general algorithm aversion.
Study 2: Replication with a Real Application Process
The purpose of Study 2 was twofold. First, we aimed to provide
a field test of the predicted effect. Participants applied to join a
research participant pool run by a research company, Johnson
Customer Insight. We examined participants’ attitudes toward
the research company when their applications were accepted
or rejected by either a human or an algorithm. Second, we
aimed to rule out an alternative account: inattention to unfavorable
information. People tend to avoid unfavorable information
that can hurt their self-esteem (Trope and Neter 1994), so they
may pay less attention to information (including the decision
maker type) that is related to an unfavorable decision
outcome. Accordingly, inattention may explain the apparent
indifference to the decision maker type for unfavorable decision
outcomes. To address this possibility, we directed participants’
attention to the decision maker type in all conditions before
measuring attitudes toward the company.
Method. We randomly assigned 303 Prolific workers (Mage=
34.19 years; 184 women) to one of four conditions in a 2 (decision
maker type: algorithm vs. human) × 2 (decision outcome
favorability: favorable vs. unfavorable) between-participants
design.
We created a Prolific researcher account under the name
Johnson Customer Insight and told Prolific workers (who are
essentially gig economy workers whose gig is to be a paid
research participant) that the company was creating a research
participant pool. Furthermore, we told participants that
Johnson Customer Insight was dedicating that particular day
to determining the eligibility of applicants for future surveys
with generous compensation (see Web Appendix F).
Participants were invited to complete an application form,
which included questions about their cognitive abilities and
their Prolific history; participants were told that the information
reflected their diligence and attractiveness as a research participant.
After submitting the application, each participant
received an application number and was asked to wait while
their applications were evaluated; after a few minutes, they
received either an acceptance (favorable decision outcome) or
a rejection (unfavorable decision outcome). Participants then
rated their overall attitude toward the research company
(“What is your overall evaluation of Johnson Customer
Insight?”) on a scale from one to ten stars.
On the next page, we informed participants of the type of
decision maker: either one of the coordinators or a computer
program designed by the information technology team.
Participants completed another measure of attitude: “How do
you feel about Johnson Customer Insight now?” (1=“less positive,”
and 7=“more positive”). Finally, we thanked and
debriefed participants (including telling them that Johnson
Customer Insight was a fictitious company) and paid the promised
bonus to all participants.
Attitudes before receiving information about the decision maker
type. As we expected, a 2 (decision maker type) ×2 (decision
outcome favorability) ANOVA on the initial rating of the
research company indicated a significant main effect of decision
outcome favorability (Mfavorable=8.30, SD=1.67 vs.Munfavorable
=4.20, SD=2.78; F(1, 299)=243.30, p < .001, η2p
=.45).
Unsurprisingly, as this measure was taken before the manipulation
of the decision maker type, we found neither a main effect
of the decision maker type (F(1, 299)=1.29, p=.256) nor an
interaction effect between the decision maker type and decision
outcome favorability (F(1, 299)=.53, p=.466), indicating successful
random assignment.
Core Results. Central to our hypothesis, we tested how the decision
maker type affected participants’ attitudes as a function of
decision outcome favorability. An ANOVA revealed a significant
main effect of the decision maker type (Malgorithm =3.72,
SD = 1.59 vs. Mhuman = 4.26, SD =1.85; F(1, 299) =11.04, p
= .001, η2p
=.04) and of decision outcome favorability
(Mfavorable =5.03, SD =1.29 vs. Munfavorable =2.95, SD=
1.50; F(1, 299) =175.69, p < .001, η2p
=.37). Crucially, we replicated
the key interaction effect (F(1, 299)=6.11, p=.014, η2p
=.02; Figure 4): attitudes toward the research company were
less positive among participants whose applications were
accepted by the algorithm than among participants whose applications
were accepted by the coordinator (Malgorithm=4.57,
SD =1.23 vs. Mhuman =5.47, SD =1.19; F(1, 299)=16.84, p <
.001, η2p
=.05). Meanwhile, the effect of the decision maker
type on the attitudes was significantly mitigated when participants’
applications were rejected (Malgorithm =2.88, SD =1.45
vs. Mhuman =3.01, SD =1.54; F(1, 299)=.36, p=.548). The
key interaction effect remained significant after controlling for
the initial rating of the research company (F(1, 298) =5.90, p
=.016, η2p
=.02).
Discussion. Study 2 replicated our key findings in a realistic
setting where participants ostensibly were applying to a
research company. Furthermore, Study 2 ruled out the alternative
account based on inattention to unfavorable information by
separating the decision outcome from the decision maker,
thereby ensuring attention to the latter.
Studies 3a and 3b: Effect of (Not) Disclosing the
Decision Maker
Studies 3a and 3b focused on favorable decision outcomes (as we
did not observe a significant effect of the decision maker type for
unfavorable decision outcomes in our previous studies). We
aimed to clarify whether the effect of the decision maker type
on reactions is driven by a positive effect of the human decision
maker, a negative effect of the algorithmic decision maker, or
both. The distinction is important from the perspectives of managers
and business ethics because it has implications for the consequences
of disclosing (vs. not disclosing) the decision maker
type. Studies 3a–b included a third condition in which consumers
are not informed of the decision maker, creating a baseline for
assessing the effect of the decision maker type.
Methods. We randomly assigned 403 (Study 3a: Mage=32.75
years; 251 women) and 402 (Study 3b: Mage =34.98 years;
259 women) Prolific workers to one of three conditions (decision
maker type: algorithm vs. human vs. unspecified) in a
between-participants design.
In Study 3a, participants were applying for membership at
Violethall Country Club (see Web Appendix G); depending
on the condition, participants learned that their applications
were accepted by a club algorithm (algorithm condition),
accepted by a club coordinator (human condition), or simply
accepted (unspecified decision maker condition). Participants
completed the same attitude items (a =.98) as in Study 1a.
Study 3b was a conceptual replication of Study 3a with one difference:
participants read that they were applying for a bank
loan (see Web Appendix G). Similar to Study 3a, participants
learned that their applications were accepted by a loan algorithm,
accepted by a loan officer, or accepted by an unspecified
decision maker. Participants rated their attitudes toward the
bank (a =.98).
Study 3a Results. We observed a significant effect of the decision
maker type (F(2, 400)=6.78, p=.001, η2p
=.03).
Replicating our previous findings, attitudes toward the
country club were less positive among participants whose applications
were accepted by the algorithm than among participants
whose applications were accepted by the club coordinator
(Malgorithm =6.04, SD =2.70 vs. Mhuman =7.21, SD =2.83;
F(1, 400)=12.14, p < .001, η2p
=.03). Attitudes were significantly
less positive in the algorithm condition than in the
unspecified condition (Malgorithm = 6.04, SD = 2.70 vs.
Munspecified = 6.97, SD = 2.69; F(1, 400) = 7.79, p = .006, η2p
= .02), but attitudes were similar in the human and unspecified
conditions (Mhuman = 7.21, SD = 2.83 vs. Munspecified = 6.97,
SD = 2.69; F < 1, p = .482; Figure 5).
Study 3b Results. We observed a significant effect of the decision
maker type (F(2, 399)=13.79, p < .001, η2p
=.06).
Participants whose loan applications were accepted by the algorithm
indicated less positive attitudes toward the bank than both
participants whose loan applications were accepted by the loan
officer (Malgorithm =7.38, SD =2.39 vs. Mhuman =8.50, SD =
1.94; F(1, 399) =18.85, p < .001, η2p
=.05) and participants
whose loan applications were accepted by an unspecified decision
maker (Malgorithm =7.38, SD =2.39 vs. Munspecified =8.59,
SD =1.98; F(1, 399)=22.29, p < .001, η2p
=.05). Again, the
difference between the human and unspecified conditions was
not significant (Mhuman =8.50, SD =1.94 vs. Munspecified =
8.59, SD =1.98; F < 1, p=.711; Figure 5).
Discussion of Studies 3a and 3b. Studies 3a and 3b clarify that
the effect of the decision maker type in favorable decisions
occurs because the disclosure of an algorithmic decision
maker hurts consumers’ attitudes relative to a baseline of an
undisclosed decision maker. These findings have implications
for decision transparency, which we discuss in the “General
Discussion” section.
What Psychological Mechanisms
Differentiate Consumers’ Reactions to
Algorithmic and Human Decision Makers?
We proposed that consumers react less positivelywhen their applications
are accepted by algorithms (vs. humans) because they find
it relatively more difficult to internalize an acceptance made by an
algorithm (vs. by a human). By contrast, when a decision outcome
is unfavorable, consumers readily externalize the decision
outcome, so they react similarly toward the company regardless
of the decision maker. We directly examined this attribution
mechanism through mediation (Studies 4 and 6) and moderation
(Study 5).
Study 4: Mediation by Internal Attribution
Study 4 examined the mediating role of attribution. We predicted
that algorithmic (vs. human) decision makers would
elicit distinct attributions as a function of decision outcome
favorability, and the attributions would mediate our key interaction
effect on attitudes toward the company.
Method. We randomly assigned 600 Prolific workers to one
of four conditions in a 2 (decision maker type: algorithm
vs. human) × 2 (decision outcome favorability: favorable
vs. unfavorable) between-participants design. Our final data
set consisted of 571 participants (Mage=33.84 years; 249
women) who passed our attention check.2
As in Study 3a, participants read that they were applying for
membership at a country club (seeWeb AppendixH); participants
learned that their applications were either accepted or rejected by
either the country club algorithm or the country club coordinator,
and they indicated their attitudes toward the country club (α=.99)
as in Study 3a. Next, we measured internal attributions
(adapted from Russell [1982]): “To what extent do you feel this
decision [reflects something about yourself/can be attributed to
something about yourself/is due to your personal qualities or
behaviors]?” (1=“not at all,” and 11=“very much”; α=.91).
Results. We conducted a 2 (decision maker type) ×2 (decision
outcome favorability) ANOVA on attitudes toward the country
club. We found a significant main effect of the decision maker
type (Malgorithm=5.01, SD=2.95 vs. Mhuman=5.91, SD=3.11;
F(1, 567)=8.62, p=.003, η2p
=.01) and of decision outcome
favorability (Mfavorable=7.06, SD=2.88 vs. Munfavorable=3.85,
SD=2.30; F(1, 567)=211.62, p < .001, η2p
=.27). Again, we
found a marginally significant interaction between the decision
maker type and decision outcome favorability (F(1, 567)=
3.66, p=.056, η2p
=.01; see Figure 6): attitudes toward the
country club were less positive among participants whose applications
were accepted by the algorithm than among participants
whose applications were accepted by the coordinator (Malgorithm
=6.49, SD=2.88 vs. Mhuman=7.54, SD=2.79; F(1, 567)=
11.82, p < .001, η2p
=.02). Meanwhile, this difference was significantly
mitigated among participants whose applications were
rejected (Malgorithm=3.74, SD=2.36 vs. Mhuman=3.97, SD=
2.23; F < 1, p=.471).
We conducted an analogous ANOVA on internal attributions.
We found a significant effect of the decision maker
type (Malgorithm =6.54, SD =2.79 vs. Mhuman =7.27, SD =
2.69; F(1, 567) =8.01, p=.005, η2p
=.01) and of decision
outcome favorability (Mfavorable =7.54, SD =2.47 vs.
Munfavorable =6.27, SD =2.90; F(1, 567)=30.15, p < .001, η2p
=.05). Importantly, we found a significant interaction effect
(F(1, 567)=10.11, p=.002, η2p
=.02; see Figure 6): the internal
attribution was weaker when the acceptance decision was made
by the algorithm than when it was made by the club coordinator
(Malgorithm =6.82, SD =2.54 vs. Mhuman =8.15, SD =2.24;
F(1, 567)=18.17, p < .001, η2p
=.03). The effect of the decision
maker type was significantly mitigated for the internal attribution
of the rejection decision (Malgorithm =6.30, SD =2.98 vs.
Mhuman =6.22, SD=2.81; F < 1, p=.806).
Finally, we ran a moderated mediation analysis (PROCESS
Model 8, 10,000 bootstrapped samples; Hayes 2013) with attitudes
toward the country club as the dependent variable, decision maker
type (−1=algorithm, 1=human) as the independent variable,
decision outcome favorability (−1=unfavorable, 1=favorable)
as the moderator, and internal attribution as the mediator (see
Figure 7).3 As we predicted, we found a significant moderated
mediation effect (B=.16, 95% confidence interval [CI] =
[.0536, .2780]). For a favorable decision outcome, the indirect
effect of the decision maker type through internal attribution
was significant (B=.15, 95% CI = [.0720, .2392]), suggesting
decision from an algorithm (vs. a human) was driven by the
weaker internal attribution of the favorable decision. For an unfavorable
decision outcome, however, the corresponding indirect
effect was not significant (B=−.01, 95% CI = [−.0864, .0694]).
In summary, Study 4 directly examined the proposed mechanism
and found evidence that decision outcome favorability
affects the internal attribution process of algorithmic versus
human decisions, thereby leading to divergent reactions to the
decisions made by the different decision makers.
Study 5: Moderated Mediation by Internal Attribution of
a Favorable Outcome
We proposed that consumers react more positively when a
favorable decision is made by a human (vs. an algorithm)
because a human decision maker facilitates the internal attribution
of the decision outcome more. If this is the case, this effect
should be mitigated when the decision outcome is not diagnostic
of consumers’ personal characteristics (e.g., the decision was
made at random), in which case there is little justification for
internal attribution regardless of the decision maker type.
Study 5 tested this prediction by manipulating selfdiagnosticity;
the decision was based on either an evaluation
of the consumer’s application or a raffle. Furthermore, Study
5 increased the generalizability of our effect by replicating it
in another managerially relevant context: networking platforms.
Method. We randomly assigned 501 Prolific workers to one of
four conditions in a 2 (decision maker type: algorithm vs.
human) ×2 (decision method: evaluation vs. raffle) betweenparticipants
design. Our final data set consisted of 443 participants
(Mage=39.33 years; 222 women) who passed our attention check.
Participants read that they were applying to join a business
networking community, NetWorkLink (see Web Appendix I).
Participants learned that their applications were accepted by
either the club algorithm or the club coordinator, and the decision
method involved either an evaluation of the applications or
a raffle (i.e., random selection). Finally, we measured participants’
attitudes toward the networking club (α=.98) and internal
attributions (α=.95) by using the same items as in Study 4.
Results. A 2 (decision maker type) ×2 (decision method)
ANOVA on attitudes revealed no significant main effect of the
decision maker type (Malgorithm=6.44, SD=2.59 vs. Mhuman=
6.55, SD=2.82; F(1, 439)=1.12, p=.290), but a significant
effect of the decision method (Mevaluation=7.30, SD=2.43 vs.
Mraffle=5.69, SD=2.73; F(1, 439)=44.54, p < .001, η2p
=.09).
Importantly, we found a marginally significant interaction
effect (F(1, 439)=3.44, p=.064, η2p
=.01; see Figure 8).
When the acceptance decision was based on an evaluation of
the applications (i.e., when the decision was self-diagnostic),
we replicated our previous findings: attitudes toward the networking
club were less positive among participants whose applications
were accepted by the algorithm than among participants
whose applications were accepted by the club coordinator
(Malgorithm=6.98, SD=2.34 vs. Mhuman=7.70, SD=2.49;
F(1, 439)=4.25, p=.040, η2p
=.01). However, when the acceptance
decision was based on a raffle (i.e., when the decision was
not self-diagnostic), the decision maker type did not significantly
affect participants’ attitudes (Malgorithm=5.80, SD=2.74 vs.
Mhuman=5.60, SD=2.73; F < 1, p=.574).
An analogous ANOVA on internal attribution revealed a significant
main effect of the decision maker type (Malgorithm=5.59,
SD=2.99 vs. Mhuman=6.07, SD=3.29; F(1, 439)=9.93, p=
.002, η2p
=.02) and of the decision method (Mevaluation=7.47,
SD=2.26 vs. Mraffle=4.17, SD=3.05; F(1, 439)=179.62, p <
.001, η2p
=.29). Crucially, we again found a significant interaction
effect (F(1, 439)=6.01, p=.015, η2p
=.01; Figure 8): when the
decision was based on an evaluation of the applications, the internal
attribution of the acceptance was weaker among participants
whose applications were accepted by the algorithm than among participants
whose applications were accepted by the club coordinator
(Malgorithm=6.84, SD=2.29 vs. Mhuman=8.25, SD=1.96; F(1,
439)=15.69, p < .001, η2p
=.03). However, when the acceptance
decision was based on a raffle, the decision maker type did not significantly
affect the internal attribution made by participants (Malgorithm
=4.08, SD=3.04 vs. Mhuman=4.25, SD=3.07; F < 1, p=.621).
To test whether our key effect ismediated by the internal attribution
of the favorable decision outcome, we conducted a moderated
mediation analysis (PROCESS Model 8, 95% CI, 10,000
bootstrapped samples; Hayes 2013) with attitudes toward the
networking club as the dependent variable, decision maker
type (−1=algorithm, 1=human) as the independent variable,
decision method (−1=raffle, 1=evaluation) as the moderator,
and internal attribution as the mediator. In line with our theory,
we found a significant moderated mediation effect (B=.26,
95% CI = [.0516, .4765]): when the decision was based on an
evaluation of the applications and thus self-diagnostic (such
that participants were motivated or able to internally attribute
the favorable outcome), the indirect effect through internal attribution
was significant (B=.29, 95% CI = [.1668, .4344]),
suggesting that the more positive attitude toward the networking
club after receiving a decision from the human (vs. algorithm)
was driven by the stronger internal attribution of the favorable
decision. When the decision was based on a raffle and thus
was not self-diagnostic, however, the indirect effect was not significant
(B=.04, 95% CI = [−.1327, .2088]).
In summary, Study 5 corroborates our attribution mechanism
by demonstrating moderation by the self-diagnosticity of the
decision. Together, the results of Studies 4 and 5 provide converging
evidence that supports our attribution mechanism.
Study 6: External Attribution of an Unfavorable Decision
Outcome
We proposed that the decision maker type has an attenuated effect
on consumers’ reactions following an unfavorable decision
outcome because consumers can readily engage in external attribution
of an unfavorable decision outcome regardless of the decision
maker. Consumers perceive both algorithmic and human decision
makers to have weaknesses: humans are less objective (Lee 2018),
and algorithms neglect the uniqueness of each individual (e.g.,
Longoni, Bonezzi, and Morewedge 2019). Accordingly, when
consumers receive an unfavorable decision outcome, they can
blame a human decision maker for a lack of objectivity and
blame an algorithmic decision maker for neglecting their individual
uniqueness. We argued that these countervailing effects cancel
each other out, resulting in consumers’ relative indifference to the
type of decision maker. In Study 6, we tested this proposition by
measuring consumers’ perceptions of the decision maker’s objectivity
and consideration of individual uniqueness.
Method. In this preregistered study (aspredicted.org/ah2sc.pdf),
we randomly assigned 626 MTurk workers (Mage=35.51
years; 332 women) to one of two conditions (decision maker
type: algorithm vs. human) in a between-participants design.4
Participants read that they were applying for membership at a
country club and their applications were rejected by either the
club algorithm or the club coordinator (see Web Appendix J).
Participants then assessed the decision maker’s objectivity and
consideration of the applicant’s uniqueness (the order of the measures
was randomized). Specifically, participants answered three
items about the decision maker’s objectivity: “To what extent do
you think [this algorithm/club coordinator] [made an unbiased
assessment of your application/made an unemotional assessment
of your application/assessed your application rationally]?” (1=
“not at all,” and 11=“very much”; α=.71). Participants also
answered three items about the decision maker’s consideration
of their application’s uniqueness: “To what extent do you think
this [algorithm/club coordinator] [recognized the uniqueness
of your application/considered the unique aspects of your application
/ tailored the decision to your unique case]?” (adapted 
from Longoni, Bonezzi, and Morewedge [2019]; 1=“not at all,”
and 11=“very much”; α=.93). Lastly, participants completed
the same attitude items as in Study 5 (α=.97).
Results. In line with our previous findings, and as preregistered,
there was no significant effect of the decision maker type on
attitudes toward the country club (Malgorithm =4.84, SD =2.62
vs. Mhuman =4.78, SD =2.61; F(1, 624) < 1, p=.782; see
Figure 9). Crucially, we found a significant effect of the decision
maker type on participants’ perceptions of the decision
maker’s objectivity and consideration of uniqueness: the club
coordinator (vs. algorithm) was perceived as less objective
(Mhuman=5.95, SD=2.47 vs. Malgorithm=7.07, SD=2.28;
F(1, 624)=34.76, p < .001, η2p
=.05), whereas the algorithm
(vs. club coordinator) was perceived as less sensitive to the applicant’s
uniqueness (Malgorithm=4.41, SD=2.81 vs. Mhuman=
5.35, SD=2.74; F(1, 624)=17.67, p < .001, η2p
=.03).
Finally, we conducted a mediation analysis (PROCESS
Model 4; 95% CI, 10,000 bootstrapped samples; Hayes 2013)
with attitudes toward the country club as the dependent variable,
the decision maker type (−1=algorithm, 1=human) as
the independent variable, and perceived objectivity and uniqueness
consideration as the two mediators. In line with the preregistered
prediction, the indirect effects of the decision maker type
via the two mediators were significant in opposite directions
(perceived objectivity: B=−. 19, 95% CI = [−.2905, −.1144];
uniqueness consideration: B=.17, 95% CI = [.0847, .2547]),
explaining the relative indifference to the decision maker type
for unfavorable decision outcomes. The direct effect was not significant
(B=.00, 95% CI = [−.1803, .1790]; see Figure 10).
In summary, Study 6 corroborates our theory that consumers
make external attributions about unfavorable decision outcomes
for both human and algorithmic decision makers, facilitated by
the perceived weakness of the decision maker—human decision
makers have poor objectivity, while algorithmic decision
makers do not consider each applicant’s unique characteristics.
In addition, these results contradict an alternative explanation
based on psychological numbness following a rejection, which
could plausibly lead to an indifference to the type of decision
maker for unfavorable decision outcomes. However, the psychological
numbness account predicts psychological deactivation (including
disengagement from attributional processes), which does not
explain the parallel mediation processes that we found in Study 6.
Study 7: Effect of Human Decision Making Versus Mere
Human Observation
One could argue that participants in our previous studies reacted
more positively to acceptance by humans due to social presence
(Argo, Dahl, and Manchanda 2005; McFerran and Argo 2014);
when an algorithm makes an acceptance decision, no social
agent is aware of the outcome. By contrast, the social presence
of the human decision maker might lead participants to feel
more positive about the outcome and thus react more positively
toward the company.
Although it cannot explain several findings in the previous
studies (e.g., the moderation in Study 5), we conducted Study
7 to directly test the alternative account of social presence by
adding a new condition in which a human monitored (but did
not interfere with) the algorithm’s decisions. If social presence
accounts for our effect, consumers should react similarly when
a human makes the decision versus merely observes the favorable
outcome. If our effect is due to distinct attributions under
human versus algorithmic decision makers, however, then reactions
should be similar when an algorithm makes the decision
with versus without a human monitoring the decision process.
Method. We randomly assigned 597 MTurk workers (Mage =
35.42 years; 318 women) to one of six conditions in a 3 (decision
maker type: algorithm only vs. human vs. algorithm with
human monitoring) × 2 (decision outcome favorability: favorable
vs. unfavorable) between-participants design. The procedure
of this study was similar to that of Study 1a with the
addition of the third decision maker condition, in which the
club coordinator ran and monitored the algorithm’s evaluation
of applications (see Web Appendix K). Participants completed
the same attitude scale as in Study 6 (α=.98).
Results. We found a significant main effect of the decision maker
type (Malgorithm only=5.50, SD=2.98 vs. Mhuman=5.76, SD=
3.17 vs. Malgorithm w/ human monitoring=5.17, SD=2.83; F(2, 591)
=4.52, p=.011, η2p
=.02) and of decision outcome favorability
(Mfavorable=7.18, SD=2.67 vs. Munfavorable=3.76, SD=2.24;
F(1, 591)=295.05, p < .001, η2p
=.33). The interaction effect
was marginally significant (F(2, 591)=2.45, p=.087, η2p
=.01;
see Figure 11).5
In the favorable decision outcome condition, the simple
effect of the decision maker type was significant (F(2, 591)=
5.67, p=.004, η2p
=.02). Replicating our previous studies, attitudes
toward the country club were less positive among participants
whose applications were accepted by the algorithm than
among participants whose applications were accepted by the
club coordinator (Malgorithm only=7.09, SD=2.68 vs. Mhuman
=7.82, SD =2.59; F(1, 591)=4.36, p=.037, η2p
=.01).
Moreover, we found a significant difference in attitudes
between the human condition and algorithm-with-humanmonitoring
conditions; attitudes toward the country club were
less positive in the latter condition (Mhuman =7.82, SD =2.59
vs. Malgorithm w/ human monitoring =6.68, SD=2.65; F(1, 591)=
11.13, p < .001, η2p
=.02). There was no significant difference
in attitudes between the algorithm-only and algorithm-withhuman
monitoring conditions (Malgorithm only=7.09, SD =2.68
vs. Malgorithm w/ human monitoring =6.68, SD=2.65; F(1, 591)=
1.40, p=.238). In the unfavorable decision outcome condition,
attitudes toward the country club were not influenced by the
decision maker type (F(2, 591) =1.37, p=.255).
Discussion. Consistent with our attribution account and inconsistent
with the social presence account, we found that consumers
react more positively when an acceptance decision is made
by a human than by an algorithm, regardless of whether a
human monitors the algorithm’s decisions. At first glance, our
findings may seem contradictory to those of Study 9 in
Longoni, Bonezzi, and Morewedge (2019), in which individuals
were more likely to use a medical algorithm if it was complemented
by a human dermatologist (i.e., a dermatologist reviewed
the algorithm’s diagnosis andmade a final decision). The studies,
however, have a key difference: a human was actively engaged in
the decision-making process in Longoni, Bonezzi, and
Morewedge’s study, whereas a human merely observed the algorithm
and could not alter its decisions in our study.
What Can Managers Do to Mitigate the
Negative Effects of Algorithms?
We consistently observed that consumers react less positively
when a favorable decision is made by an algorithm (vs. a
human). Study 8 examined a potential solution: anthropomorphizing
the algorithm. Extant work suggests that humanizing
a nonhuman agent (e.g., referring to an object with a personal
name) leads people to attribute human-like abilities to it
(Crolic et al. 2022; Epley 2018). We proposed that humanizing
an algorithm should more closely align consumers’ perceptions
of a human decision maker and an algorithmic decision maker,
enabling the human-like algorithm to lead to more positive
reactions than the non-human-like algorithm.
Study 8: Humanizing Algorithms to Mitigate Negative
Consequences: Attitudes Toward the Company
Method. We randomly assigned 601 Prolific workers (Mage=
33.52 years; 316 women) to one of three conditions (decision
maker type: algorithm vs. human vs. human-like algorithm)
in a between-participants design.
The procedure of Study 8 was similar to that of Study 1a.
Participants were told they were applying for membership at
a country club (see Web Appendix L); depending on the condition,
the decision maker was described as a country club algorithm
(depicted as a robot), a country club coordinator named
Sam (depicted as a woman), or a country club algorithm
named Sam (depicted as a cartoonized version of the picture
of the woman from the human condition). All participants
were informed that their applications were accepted. We
asked participants to indicate their attitudes toward the
country club using the same items as in Study 7 (α=.98).
Pretest. We conducted a separate pretest to examine whether a
human-like algorithm seems more human than a non-human-like
algorithm. We presented 100 Prolific workers (Mage=30.23
years; 41 women) with the information from the algorithm and
human-like algorithm conditions in the main study. We then
asked participants, “To what extent do you think that [the
country club algorithm/Sam] has some human-like qualities?”
and “To what extent do you think [the country club algorithm/
Sam] seems like a person?” (1=“not at all,” and 7=“very
much”; r=.80; adapted from Kim and McGill [2018]). Results
confirmed that our manipulation was successful: participants perceived
the human-like algorithm to be more human than the algorithm
was (Mhuman-like algorithm=3.92, SD=1.56 vs. Malgorithm=
2.65, SD=1.29; F(1, 598)=19.86, p < .001, η2p
=.17).
Results. In our main study, we conducted a one-way ANOVA on
participants’ attitudes toward the country club. Replicating our previous
findings, the decision maker type had a significant effect
(F(2, 598)=4.69, p=.009, η2p
=.02). Attitudes toward the club
were less positive among participants whose applications were
accepted by the algorithm than among participants whose applications
were accepted by the club coordinator (Malgorithm=7.07, SD
=2.67 vs. Mhuman=7.87, SD=2.52; F(1, 598)=8.88, p=.003,
η2p
=.01). Importantly, humanizing the algorithm led to significantly
more positive attitudes toward the country club (Malgorithm
=7.07, SD=2.67 vs. Mhuman-like algorithm=7.64, SD=2.82; F(1,
598)=4.46, p=.035, η2p
=.01) such that attitudes were similar
whether the decision maker was the human-like algorithm or the
club coordinator (Mhuman-like algorithm=7.64, SD=2.82 vs.
Mhuman=7.87, SD=2.52; F < 1, p=.384).
Discussion. Building on our prior studies’ finding that consumers
react less positively when a favorable decision is made by an
algorithm (vs. a human), Study 8 tested a potential solution:
anthropomorphizing the algorithm. Attitudes toward the
company were more positive when the favorable decision was
made by a human-like (vs. a non-human-like) algorithm.
General Discussion
The current research reveals that consumers react differently to
a company that uses algorithmic (vs. human) decision makers
as a function of decision outcome favorability: Consumers
react less positively toward a company when they receive a
favorable decision made by an algorithm than by a human;
however, this difference is significantly mitigated when the
decision outcome is unfavorable. The effect is driven by different
attributions: consumers find it relatively more difficult to
internalize a favorable decision made by an algorithm (vs. a
human), while it is similarly easy to externalize an unfavorable
decision made by either type of decision maker. Finally, we
demonstrate that humanizing the algorithm can mitigate the relatively
less positive reaction to an algorithmic (vs. a human)
decision maker in the setting of favorable decision outcomes.
Alternative Accounts
Several alternative accounts merit discussion. We review these
accounts and discuss how our findings and study design rule
them out. In addition, we have direct evidence, in the form of
both mediation and moderation, that supports attribution processes
(Studies 4–6).
First, one might argue that consumers care about a favorable
outcome being witnessed by (rather than made by) another
human and that it is this mere human presence that leads to
more positive reactions to human decision makers. Against
such a social presence account, however, participants in
Study 7 reacted more positively only when a human (vs. an
algorithm) made the favorable decision on them, but not
when a human merely observed the algorithm and thus knew
about the favorable decision outcome.
Second, our results might be explained by social cues. For
instance, being accepted by a human might create a sense of
social belonging, while being evaluated by an algorithm
might engender feelings of disrespect. However, we observed
the key interaction effect even in contexts in which social relationships
are less salient (i.e., business loan application, market
research participant panel). Moreover, if algorithmic (vs.
human) evaluation creates feelings of disrespect, we should
have found a main effect of the decision maker type, but not
necessarily the interaction effect between the decision maker
type and decision outcome favorability.
Third, consumers might pay less attention to unfavorable
information about the self because they inherently avoid information
that can hurt their self-esteem (Trope and Neter 1994).
In this regard, consumers might be inattentive to any unfavorable
information about the self, including the type of decision
maker that was involved in the unfavorable decision.
However, we replicated our interaction effect even when we
explicitly directed participants’ attention to the decision
maker type (Study 2), ruling out the inattention account. In
addition, the inattention account does not explain the two
opposing mediation processes for unfavorable decisions in
Study 6.
Fourth, one could argue that psychological numbness
explains the relative indifference to the decision maker type for
unfavorable decision outcomes. An experience of social exclusion
(e.g., ostracism) can impair people’s emotional sensitivity
and cognitive function (Williams 2007), and even social rejections
by nonhuman agents (e.g., robots) can lead to negative psychological
consequences (Nash et al. 2018). If psychological
numbness explains our effect, however, then it should be
limited to contexts in which social relationships are salient—
but our effect is significant in nonsocial contexts as well.
Moreover, the psychological numbness account would predict
that consumers who receive unfavorable decision outcomes
should be less likely to engage in any cognitive processes including
attributions, but in Study 6, participants’ reactions to unfavorable
decisions were due to external attribution processes.
Fifth, one could argue that the perceived fairness of algorithmic
versus human decision makers explains our results.
Consumers are known to perceive decisions made by algorithms
(vs. humans) as less fair (Lee 2018). Differential perceptions of
decision fairness should produce a main effect of the decision
maker type, but not necessarily the interaction effect that we
observed consistently. Nonetheless, we conducted a follow-up
study (seeWeb Appendix N) that measured the perceived fairness
of the decision.We found amain effect of the decision maker type:
participants perceived the human to be fairer than the algorithm
(Mhuman=4.12, SD=1.55 vs. Malgorithm=3.28, SD=1.57; F(1,
317)=23.63, p < .001, η2p
=.07). However, this effect was not
moderated by decision outcome favorability (F < 1, p=.578),
ruling out perceived fairness as a viable explanation for our effect.
Finally, one could be concerned about scale insensitivity as an
explanation for the interaction between the decision maker type
and decision outcome favorability. Specifically, one could
argue that there could be differences in consumers’ reactions to
unfavorable decision outcomes by different decision maker
types, but that our measures are not sensitive enough to
capture these differences (e.g., because such reactions are in
general quite negative). Study 2 rules out this concern. In
Study 2, we first elicited a response to the outcome (favorable
or unfavorable) and then provided information about the decision
maker to probe how the information of the decision maker type
changes participants’ attitudes toward the company. In this
study, we still observed that participants reacted to rejections
by humans and algorithms similarly.
Theoretical Implications
The current research makes several theoretical contributions.
Extending prior research on how consumers decide between algorithms
and humans (Dietvorst, Simmons, and Massey 2015;
Longoni, Bonezzi, and Morewedge 2019), we shed light on
how consumers’ reactions to a self-diagnostic decision (i.e., decisions
about the consumers themselves) are affected by the decision
maker type (human vs. algorithm). Second, our work identifies a
theoretically and managerially relevant moderator (decision
outcome favorability) that has been underexplored in the existing
literature on algorithmic decision making. Finally, we extend the
existing work on consumers’ perceptions of the different decision
makers (e.g., Lee 2018) by examining how algorithmic (vs.
human) decisions prompt different attributions as a function of
decision outcome favorability. In doing so, our research marries
the social psychology literature on attribution processes with the
marketing literature on algorithmic decision making.
Our article opens several avenues for future research. First,
future research could examine consumers’ perceptions of decisions
that are made through human–algorithm collaboration.
Consumers may react differently depending on the nature of
the collaboration (e.g., who conducts the first round of screening
vs. makes a final decision). Second, future research could
examine whether our interactive effect is influenced by the nature
of decision criteria. Companies use a variety of criteria to accept
or reject consumers (e.g., high/low performance, passing/failing
a threshold). In our studies, we did not specify why an application
was accepted or rejected.We encourage researchers to investigate
whether specific reasoning affects the interaction between
the decision maker type and decision outcome favorability.
Third, even though big data has improved the quality of decisions
made by both humans and algorithms, there are still concerns
about the representativeness of data used by firms (Bolukbasi
et al. 2016). Given that minority groups are often underrepresented
in data sets (Sheikh 2006), the effect of the decision
maker type on consumers’ reactions may differ for consumers
from a minority versus majority group. Future research can incorporate
consumer demographics to understand such differences.
Fourth, our work focuses on consumers’ attitudes toward the
company, but more research is needed to understand how algorithmic
decisionmaking impacts consumers’ psychological
security. For instance, future research can investigate how the
decision maker type affects consumers’ perceived threat and
anxiety (Mende et al. 2019). Fifth, future research could investigate
whether consumers’ reactions change depending on whether
the decision outcome is communicated by a person or through a
nonhuman medium (e.g., email). Although we manipulated only
the decision maker type and held all other communication about
the decision outcome constant, future research could examine the
effect of how decisions are communicated to consumers (e.g.,
Campbell 2007; price tag vs. store owner). Sixth, although the
current research primarily focuses on consumers’ attitudes
toward the company, which is a managerially important consumer
indicator, future research can extend our findings to
other behavioral measures.
Lastly, it is interesting to consider under which conditions
algorithmic (vs. human) decisions might be more likely to
facilitate internal attributions. Although we observed a consistent
pattern across different consumer contexts, responses,
and procedures, it is possible that in some situations algorithmic
acceptance might offer an especially salient cue of diagnosticity
and facilitate internal attributions to a larger extent.
In general, more research is needed to understand how our
effects can be moderated by the nature of the evaluation
context. For instance, if a decision process is based on a
simple objective criterion (e.g., if one’s grade point average
is above the 80th percentile), a favorable decision might facilitate
internal attributions regardless of whether the decision
maker is an algorithm or a human, mitigating the effect of
the decision maker type.
Managerial Implications
The current work has several managerial implications. First, our
results offer insights—perhaps surprising to many managers—
into how the adoption of algorithms for consumer-facing decisions
may affect consumers’ reactions toward the company. We
found that some managers hesitate to automate consumerfacing
decisions because they are concerned about exacerbating
consumers’ negative reactions to unfavorable decision outcomes
(see in-depth interviews #2, #3, and #12 in Web
Appendix C; Dietvorst, Simmons, and Massey 2015; Luo
et al. 2019). Our results, however, demonstrate that an algorithmic
(vs. a human) decision maker hurts consumers’ reactions
for favorable outcomes, not for unfavorable ones.
Second, in our interviews with managers, some managers
expected that consumers would respond more positively when
human and algorithmic decision makers collaborate, and
some mentioned that their companies are already using this
strategy (see in-depth interviews #2 and #9 in Web Appendix
C). Our results indicate that consumers may not necessarily
respond more positively to companies if humans are merely
observing the algorithms without active involvement in decision
making (Study 7). By showing this, we offer managerial
insights on how companies can design their evaluation processes.
In addition, we demonstrate that the effect of the decision
maker type is mitigated when the favorable decision
outcome is not self-diagnostic (i.e., when the decision was
based on a raffle; Study 5). Managers can leverage these findings
to improve consumers’ reactions to companies that use
algorithms for consumer-facing decisions.
Third, we explored a possible approach to mitigate the risk
of less positive reactions following algorithmic acceptance:
making the algorithm more human-like. In Study 8, the addition
of simple anthropomorphic cues eliminated the effect of the
decision maker type in the case of an acceptance decision.
We also observed a similar pattern in field data from a financial
services company. These data provide click-through rates on a
link to the company’s services after receiving financial feedback
from human-like algorithms (vs. non-human-like algorithms;
for details, see Web Appendix M). Once consumers
answered a questionnaire, the company provided feedback
based on an algorithmic assessment of the consumer’s financial
health. Some consumers received feedback that was highly
favorable (good financial health with just a check-up needed),
mimicking the favorable outcome condition of Study
8. Replicating the effect with a behavioral measure, these consumers
were more likely to seek information about the company’s
services when the favorable feedback came from a
human-like (vs. non-human-like) algorithm. These preliminary
findings mimic those in Study 8 and corroborate the conclusion
that negative consequences of algorithmic decision making may
be averted by making algorithms more human-like (using, e.g.,
a more conversational format, a human name, a human-like
photo).
Finally, we offer insights for policy makers. When the decision
maker type is not disclosed, consumers are likely to react
similarly as they do to a human decision maker (Studies 3a–
b), offering firms an incentive to avoid transparency, which is
not in the interest of consumers. Our results align with recent
movements calling practitioners to be more transparent about
their use of algorithms (Davenport et al. 2020; Rai 2020) and
laws in the United States and European Union that require companies
to disclose whether they use algorithms in consumerrelated
tasks (Castelluccia and Le Métayer 2019; Smith 2020).







----------------------------------------------------------------------------------










Fügener, A., Grahl, J., Gupta, A., & Ketter, W. (2022). Cognitive Challenges in
Human–Artificial Intelligence Collaboration: Investigating the Path Toward Productive
Delegation. Information Systems Research, 33(2), 678-696.
https://doi.org/10.1287/isre.2021.1079

Abstract
We study how humans make decisions when they collaborate with an artificial intelligence (AI) in a setting
where humans and the AI perform classification tasks. Our experimental results suggest that humans and AI
who work together can outperform the AI that outperforms humans when it works on its own. However, the
combined performance improves only when the AI delegates work to humans, but not when humans delegate
work to the AI. The AI's delegation performance improved even when it delegated to low-performing subjects;
in contrast, humans did not delegate well and did not benefit from delegation to the AI. This bad delegation
performance cannot be explained with some kind of algorithm aversion. On the contrary, subjects acted
rationally in an internally consistent manner by trying to follow a proven delegation strategy and appeared
to appreciate the AI support. However, human performance suffered due to a lack of metaknowledge, i.e.,
humans were not able to assess their own capabilities correctly, which in turn led to poor delegation decisions.
In contrast to reluctance to use AI, lacking metaknowledge is an unconscious trait. It fundamentally limits
how well human decision makers can collaborate with AI and other algorithms. The results have implications
for the future of work, the design of human-AI collaborative environments, and education in the digital age.

Key words : Future of work, Artificial Intelligence, Machine Learning, Delegation, Metaknowledge,
Human-AI Collaboration

1. Introduction
Early AI that tried to mimic human decision rules was only partly successful as it suered from
what Autor (2014) calls Polanyi's paradox: The fact that humans can often not accurately describe
the decision rules they use to solve a problem. Modern AI approaches such as deep neural nets seem
to overcome this limitation by learning 
exible models from large training sets instead of relying
on human-described rules (LeCun et al. 2015, Schmidhuber 2015). AI is now widely applicable and
eective, and perceived as a general purpose technology (McAfee and Brynjolfsson 2017) that fuels
innovation in diverse domains, such as medicine (Kononenko 2001, Esteva et al. 2017), and generic
perceptional tasks, such as processing images, text, and speech (Hinton et al. 2012, Deng and Yu
2013). We agree that AI performance will likely improve further and that AI will be embedded in
our day-to-day life. But, as Brynjolfsson et al. (2018) point out, not all decision making can be
automated completely as some tasks remain challenging for AI.
Perhaps the best prediction we can make today is that humans will remain integral to the
workplace, and they will work together with AI, algorithms, or intelligent machines. This is re
ected
in current IS research, having special issues on augmented intelligence in ISR and on managing
AI in MISQ, both issued in 2021. Baird and Maruping (2021) propose a theoretical framework for
next generation of IS research focusing on delegation from and to information systems, such as
AI. They explicitly state that both the information system and the human could be the delegating
unit. This mindset is in line with the concept of Human-in-the-loop AI (Zanzotto 2019), where
humans remain an integral part of AI decision making.
The critical question in delegation is how rms should distribute work between humans and AI.
Polanyi's work points to a facet of human decision making that is critical to the discussion, but
vastly ignored until now. When humans can solve a problem, but are unable to explain their decision
rules clearly, they should, nonetheless, be able to contribute complementarities to an algorithm
(Autor et al. 2003). Since humans have dierent experiences and education, decision models and
knowledge also dier between individual humans, and between humans and AI. Because we cannot
Electronic copy available at: https://ssrn.com/abstract=3368813

 3
articulate our idiosyncratic \decision rules" well, it is hard for learning algorithms to imitate them
precisely, even from large training sets. Humans' inability to tell what they know shields their
abilities from perfect digital imitation. The variety of human thought, only partially observable
through their actions, creates the possibility that humans have complementary knowledge with
respect to AI algorithms. The AI, on the other hand, may nd a way to solve a problem no human
being has thought about before. Thus, both humans and the AI potentially have complementary
knowledge, and the performance of humans working with an AI system may be better than that
of the AI system (or humans) working alone.
Our study focuses on the case where either an AI algorithm or a human is allowed to perform
a task by itself, or to delegate that task to the other actor. On a generic level, there are three
main boundary conditions that enable the combined performance to exceed the performance of the
better performing actor:
1. Existence of complementarities: In order to enhance performance through delegation between
two actors, complementary knowledge has to exist between the two actors, that is, a human and
an AI system. We claim that this should be the case for all tasks where Polanyi's paradox applies,
i.e., where humans cannot exactly specify their decision rules.
2. Recognition of complementarities: A delegating partner needs to recognize that complementarities
between the two partners exist and that the tasks should be performed by the better-suited
partner. While having information on the other partner's ability helps, the most important ability
is to estimate one's own ability. If an actor knows that they can perform a task, it is always wise
for them to complete the task themselves. On the other hand, it is wise to delegate if the actor
knows that they cannot perform the task but their partner potentially can. In line with Lories
et al. (1998) and Evans and Foster (2011), we denote this ability to assess own capabilities as
\metaknowledge." Therefore, we argue that metaknowledge is a crucial resource for recognizing
complementarities.
3. Execution of ecient delegation rules: Once a delegating partner recognizes complementarities,
they haves to delegate tasks to the better-suited actor. While this can be easily constructed
Electronic copy available at: https://ssrn.com/abstract=3368813

4 
and executed for an AI system, humans have to be willing to construct and follow such a delegation
rule.
In a series of behavioral experiments, we investigate how work is delegated between humans and
an AI algorithm. When designing the experiments, we aimed at making the results as generalizable
as possible. Central to the paper is a delegation rule that does not make assumptions about the
context in which it is used. Further, we conducted the experiments in a context where humans
make good decisions naturally, and modern AI performs equally well or even better. We chose
an environment where humans do not require any specic training. Contexts requiring specic
training make the results less generalizable, whereas abilities in more general settings can carry
over into more specialized tasks. Furthermore, expertise may not enable better decision making:
Metaknowledge seems to be only minimally increased by training (Hansson et al. 2008), and trained
experts could even show lower levels of metaknowledge compared to inexperienced subjects (Brezis
et al. 2018).
Therefore, we chose image classication as the focal task for our experiments. While humans are
naturally skilled at it, deep learning has improved AI algorithms beyond the human performance
level recently (Russakovsky et al. 2015). In our experiments, humans and an AI algorithm work
together on the same image classication tasks. In the rest of the paper, we refer to the image
classication AI algorithm as the \AI." Humans can delegate images to the AI, and the AI can
delegate images to humans in a condition called inversion.
Such collaboration between AI systems and humans is currently under-researched. A related eld
analyzes human decision makers' attitude toward algorithms. Several researchers have documented
the reluctance of human decision makers to use algorithms (Bazerman 1985, Dawes 1979, Kleinmuntz
1990), although some recent research has challenged this notion. In a recent seminal work,
Dietvorst et al. (2015) demonstrated that humans might react more strongly to errors made by
machines than to errors made by humans, even if the machine performs better, and if its errors are
smaller than those of human decision makers. The authors label this loss of condence \algorithm
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 
erm picked up willingly by the popular press. But neither Dietvorst et al. (2015) nor
a follow-up study (Dietvorst et al. 2018) document general human distrust towards algorithms.
Logg et al. (2019) study multiple prediction tasks and nd that humans are indeed willing to work
with machines, a tendency they label as \algorithm appreciation."
We are not aware of research that studies delegation between AI and humans in a setting with
complementary skills, and explores fundamental factors which might hinder or support collaboration.
The central questions that our study answers are:
 Study 1: Can delegation between humans and AI outperform humans or AI working alone,
and who can delegate better?
 Study 2: What factors limit human delegation performance? How can we overcome these
limitations?
Our experiments reveal that while the AI improves considerably by delegating to humans,
humans are naturally bad delegators to the AI. Even worse, and more interestingly, humans' performance
only slightly improves when they are taught a good delegation rule, even when they apply
it consistently and rationally. We observe little or no bias against the use of AI, in other words, our
results indicate that our subjects do not exhibit general algorithm aversion. Instead, humans try to
work with the AI to the best of their abilities but fail despite their best intentions. Humans seem to
be unable to judge their own capabilities and the diculty of the task, which in turn leads to bad
delegation decisions. Thus, humans delegating to the AI do not meet the boundary condition of a
sucient level of metaknowledge to enable a successful human-AI collaboration. We also conduct
additional robustness studies addressing the impact of continuous feedback and increasing task
diculty.
In the following, we summarize the theoretical underpinnings of our study and dierentiate our
study from the existing literature. We discuss theoretical antecedents in Section 2, describe the
experimental studies and the robustness checks in Sections 3 and 4, and conclude with a discussion
and an outlook on future research directions in Section 5.
Electronic copy available at: https://ssrn.com/abstract=3368813

6 
2.tant work and theories that inform our research questions and experimental design.
In the following subsections, we discuss contributions of our work. Specically, we shed light on
human attitude towards algorithms (Section 2.1), consider the role of delegation settings (Section
2.2), the role of complementarities on the task instances level (Section 2.3), and the role of feedback
(Section 2.4) to lay the foundation for our work.
2.1. Attitude towards AI
Research that studies how humans use computers for problem solving dates back decades and
includes works that compare human decisions with results from mathematical models (Meehl 1954).
Even during the infancy of computing environments in the 1950s, some computer models outperformed
human decision makers. It was observed that in many instances, humans were reluctant to
use algorithms, despite possible performance benets (Bazerman 1985, Dawes 1979, Kleinmuntz
1990). However other studies, such as Dijkstra (1999), demonstrated general willingness of humans
to use algorithms, even allowing machines to overrule their own inferior decisions. How humans
make decisions in concert with algorithms has been revisited lately and has been of great interest
with the surge in usage of AI techniques. Current applications look at investor usage of roboadvising
services in ntech (Ge et al. 2021), reactions to AI advice in health care (Jussupow et al.
2021), or the eect of similarity to human language used by chatbots (Schanke et al. 2021).
In a seminal work on attitude towards algorithms, Dietvorst et al. (2015) demonstrate that
humans react dierently to errors made by humans as compared to algorithms. In their experiments,
they let humans work with AI algorithms for prediction tasks. When the subjects saw the AI
perform, and err, they lost condence in it. Interestingly, this loss is much stronger than the loss
in condence in humans who made mistakes in the same task. The authors label this tendency
\algorithm aversion." Logg et al. (2019) studied whether human decision makers prefer advice from
other humans or algorithms and found that decision makers show a clear tendency for preference
for algorithmic advice over human advice. This preference holds for multiple prediction tasks, and
Electronic copy available at: https://ssrn.com/abstract=3368813

 7
(according to a survey the authors conducted) was not expected by most academics. In contrast
to Dietvorst's work, Logg et al. (2019) label their results as \algorithm appreciation." While this
seems like a contradiction, Dietvorst's work does not show a general aversion towards algorithms
but rather a dierent reaction towards errors made by algorithms and human decision makers.
2.2. Delegation
We consider a scenario, where a human has to perform a task without information about the
solution or performance of the AI for a specic task. We denote this as a \delegation" scenario. We
allow delegation to work in both directions. The human may delegate work to the machine, and the
machine may delegate work to the human. The latter approach is sometimes called inversion, or
using the human as an exit option (McAfee 2013). In delegation settings, a good decision heuristic
is the following: \If I am certain to know the correct answer, I should do the job. If I am uncertain,
I should delegate!" This rule works well because delegating a task that the decision maker is not
able to perform cannot decrease performance, independent of the other party's abilities.
When humans apply this rule, they have to rely on their metaknowledge. This is the ability to
assess one's own capabilities, that is, to know what you know (Lories et al. 1998, Evans and Foster
2011). A decision maker with strong metaknowledge can delegate well, as she will know whether
her answer is correct or not. If her level of metaknowledge is insucient, she might be certain that
incorrect answers are correct, and she might be uncertain about correct answers. In such cases, the
joint performance of a human and an algorithm will suer due to inappropriate delegations.
Note that this entire idea { to delegate tasks when you are uncertain { is particularly relevant if
delegation does not move an entire stack of tasks, or all the work, but when it occurs on the level
of task instances. We discuss this in the next section.
2.3. Complementarity on the instance level
Occupations typically consist of bundles of tasks (Autor et al. 2003), some of these tasks are suitable
for machine learning, others not (Brynjolfsson and Mitchell 2017). Because of this, many experts
do not expect that AI will automate entire bundles of tasks associated with a job but only specic
Electronic copy available at: https://ssrn.com/abstract=3368813

8 
parts of the bundle (Brynjolfsson et al. 2018). Therefore, a likely consequence of automation on
the task level is that some tasks are automated, while others are not. This leads to redesigned job
proles and work
ows based on the economic benets associated with such work arrangements.
For example, Agrawal et al. (2018) discuss ways to compute the economic value of automation on
the task level and present a related AI canvas.
We take this argument further and argue that structural complementarities between humans
and AI may exist even on the task instance level. Our reasoning builds on the design principles for
current AI systems. While traditional expert systems were built by humans who coded concrete
decision rules, current AI algorithms discover their own decision rules based on training data. Due
to structurally dierent decision rules, we argue that for each task there might be instances where
human decision rules work better than AI decision rules, and vice versa.
Sharing work between humans and AI algorithms on the task level can leverage these complementarities,
and joint performance of an AI working with humans may exceed the performance of
either of the parties individually. Even if the AI performance is better than the human performance,
the optimal allocation will assign some work to a human and some to the AI, and humans and AI
both can provide value. Therefore, it is important to conduct research in the area of human-AI
collaboration on the level of task instances. As we demonstrate in this paper, it oers signicantly
dierent implications for the future of work than the established paradigms have argued.
2.4. The role of feedback
While some extant research has used the eect of feedback on task level performance and accuracy,
we decided to not include such feedback in our main experiments to concentrate on our research
questions. The reason for our choice is manifold. For successful human-AI collaboration, several
factors have to be considered on the human side of the equation. When prior research considered
environments where humans make their decisions on the basis of observed AI performance and
errors, they found that receiving immediate feedback on AI performance might trigger behavioral
eects that undermine the collaborative setting. Examples include diminished trust in algorithms
Electronic copy available at: https://ssrn.com/abstract=3368813

 9
(lower adherence to algorithmic advice) when humans see the algorithm err (Dietvorst et al. 2015),
or the overweighting of signals from forecasting errors (Kremer et al. 2011). In that case, errors
resulting from random variation of data are misinterpreted as systematic errors.
There are many situations where AI feedback is not available or practical, for example when
predicting long-term eects, such as climate change (Logg et al. 2019). In other situations, decisions
have to be made quickly, such as in autonomous driving, or very frequently, such as in digital
markets. In these contexts, AI errors are either not available, it may not be economical to consider
them repeatedly, or there may simply be no time to integrate AI performance in decision making.
While we do not study the eects of feedback in our main study, we do explore whether continuous
feedback aects our ndings in a dedicated study in the robustness check section.
In the following sections, we provide details of our experimental studies.
3. Experimental Studies
After providing a rationale on the study context, we describe the hypotheses, designs and results
of two primary experimental studies with 902 subjects in total. We followed Nosek et al. (2018)
and pre-registered the experiments at the Open Science Foundation (Foster and Deardor 2017),
including the recruitment and data collection process, the initial hypotheses, and the statistical
analysis.
3.1. Study context
When designing the experimental studies, we aimed for a non-specialized setting, as we claim that
contexts that do require specic training make results less generalizable, while ndings in general
tasks can carry over to more specialized tasks. We also aimed for a task, where the three boundary
conditions for value-adding delegation between humans and AI are potentially met: 1) Existence
of complementarities, 2) recognition of complementarities, and 3) execution of ecient delegation
rules. We chose image classication as our experimental study context. Image classication is
the task of assigning a focal image to a class. A class can be thought of as a content group. A
classication is correct if the focal image is assigned to the right class (a focal picture with the
Electronic copy available at: https://ssrn.com/abstract=3368813

10 Information Systems Research 00(0), pp. 000{000, 
c 0poodle" is assigned to the \poodle" class, not to \husky" or \cat"). The research
design follows the logic of Fugener et al. (2021): We sampled 100 focal images with known class
labels from the ImageNet database. The ImageNet database consists of tens of millions of humanannotated
images that are used by current image recognition challenges, such as the ImageNet
Large Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al. 2015). We sample images
used in the ILSVRC image classication task that contain everyday objects and animals, and that
are assigned to one of 1,000 possible classes. The diculty of each task may be associated with
three main dimensions: 1) The image itself. This might include visibility of the object, size of the
object, or whether multiple objects are present. 2) Possible classes. A main driver for diculty of
image classication tasks is the denition of possible choices, for example, ne-grained recognition
between similar classes, such as breeds of dogs, is more dicult compared to more dierent classes,
such as between zebra, lion, or tiger. 3) The annotator. Familiarity with image and with possible
classes is a big driver of subjective diculty. This could depend on training data (in case of an AI)
or on personal experiences and interests (in case of a human). A diculty of human annotators is
to cope with a large number of classes, as they might not be aware of the existence of a specic
class (Russakovsky et al. 2015). To avoid this eect, we chose to display ten possible classes along
with the focal image. As in Russakovsky et al. (2015), we illustrated each possible answer class
by name and 13 example images. One answer was correct. A central performance measure was
classication accuracy, the percentage of correctly classied images.
We chose GoogLeNet Inception v3 (Szegedy et al. 2016) as AI. It is among the best AIs for image
classication and was trained on the ImageNet database with 1,000 classes. GoogLeNet assigns
a score to each class that can be interpreted as the likelihood of being correct. We obtained its
classication accuracy by applying it to the 100 images, and by comparing the image with the
highest score to the correct answer. As the AI is trained based on outcome data, the decision rules
dier from human decision rules, and complementarities between the AI and humans should exist.
We recruited human subjects via Amazon's Mechanical Turk (MTurk). We believe that using
MTurk to recruit subjects is particularly suitable for our study for the following reason: we are
Electronic copy available at: https://ssrn.com/abstract=3368813

 11
no specic training. Many tasks at MTurk relate to classication problems (Difallah
et al. 2015), thus, our experiment is a natural task for MTurk workers. We provide evidence that
subjects took the tasks seriously and performed them with a high degree of internal validity.
They made logical delegation decisions based on their internal assessment (see Figure 6 and the
subsequent discussion).
Study 1 \Delegation and Inversion" compares four dierent types of delegation: AI working alone,
humans working alone, humans who may delegate to AI, and an AI that may delegate to humans
(inversion). As expected, the AI outperforms humans. Surprisingly, the AI delegates better than
humans when it follows a simple rule. Study 2 \Explaining and Enforcing a Delegation Strategy"
explores the root cause for poor human delegation and analyzes the eects of teaching humans a
similar strategy to that of the AI.
3.2. Study 1: Delegation and Inversion
Hypotheses. Study 1 tackles our rst research question: can delegation outperform humans or AI
working alone, and who can delegate better? Thus, we compare four dierent settings: AI working
alone, humans working alone, humans who may delegate to AI, and an AI that may delegate to
humans (inversion). We chose image classication as focal task, and use an AI that is expected to
perform (slightly) above human performance. Our key measure is classication accuracy, that is, the
percentage of correctly classied images. We formulated and preregistered four initial hypotheses
considering the relation of accuracy between those options. In the following, we present three of
those hypotheses and theory that motivates them. One pre-registered hypothesis claimed that a
state-of-the-art AI outperforms human decision makers on average. While this is supported for our
specic setting (Szegedy et al. 2015, 2016), it lacks generality, and we decided to exclude it.
The rst two hypotheses motivate the value added through delegation. In the introduction, we
dened three boundary conditions: Existence of complementarities between the AI and humans,
recognition of complementarities, and execution of ecient delegation rules. We assume the rst
Electronic copy available at: https://ssrn.com/abstract=3368813

12 
property, existence of complementary knowledge, to hold for all tasks that follow Polanyi's paradox,
sucrities and the execution of ecient delegation rules, there is ample research
from the domain of humans working with decision support systems, ranging from seminal theoretical
work, such as Huber (1990), to recent experimental studies as carried out in Dietvorst et al.
(2018), conrming that humans can benet from working with advanced information technologies,
and that the second and third boundary conditions are at least partially given. This directly leads
to our second hypothesis:
Hypothesis 1.1 : Humans who can delegate tasks to the AI (after seeing the image to be classied)
perform better than humans who can not.
A more dicult question is to hypothesize on the eect of providing AI the possibility to delegate
to humans (inversion), given that human performance is potentially inferior to that of the AI. To
be able to improve accuracy, the AI has to delegate those tasks that AI is not able to execute, but
humans potentially are. For the second boundary condition, the recognition of complementarities,
it is important that the AI can assess its own certainty, that is, probability of success. Assessing its
own certainty is a main feature of modern AI that has gone through appropriate level of training,
and enables the AI to perform in a robust manner by using its certainty assessment to make the
nal choice. In our case of image classication, the AI score of the sample of 100 images estimated
an average likelihood of being correct of 0.769, and classied 77 images correctly. Using AI score as
an indicator for certainty and some benchmark for expected human certainty, we dene an ecient
delegation mechanism that the AI follows to leverage the potential of complementary knowledge.
We formulate our next hypothesis:
Hypothesis 1.2 : AI that can delegate image classication tasks to humans (after seeing the image)
performs better than an AI that can not.
In theory, both delegation and inversion could achieve the same accuracy: If the delegating actor
delegates all tasks that she or he is not able to perform, all tasks that at least one could perform
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 
dered correct. We denote this as ex-post optimal combination of humans and AI,
where all complementarities are realized. The AI in our inversion condition applies such a strategy
 or AI are better at delegating tasks might depend on the second and third
boundary conditions of successful delegation: humans need to have a sucient level of metaknowledge,
that is, correctly identifying tasks where they do not perform well, and humans have to come
up with an ecient delegation strategy and be willing to follow it through. We know that the AI
has a very high level of metaknowledge and will follow an ecient delegation strategy, whereas
both boundary conditions are uncertain for human delegators. This leads to our nal hypothesis
of Study 1:
Hypothesis 1.3 : AI that can delegate image classication tasks to humans performs better than
humans who can delegate to the AI.
In the following, we lay out the details of our study design before presenting our results.
Design. We compare classication accuracy between four conditions. In the \AI alone" condition
(1), GoogLeNet classied alone. In the \humans alone" condition (2), subjects classied alone.
Subjects in the delegation condition (3) could choose for each image to either classify alone, or
to delegate the image to the AI (subjects were informed about the AI accuracy measure). In the
inversion condition (4), the AI could choose for each image to classify alone or to delegate the
image to humans.
For conditions (2)-(4), we ran a between-subject design with 449 subjects in August 2018.We randomly
assigned subjects to the conditions humans alone (149 subjects), delegation (154 subjects),
and inversion (146 subjects). The humans alone (2) and inversion (4) conditions were identical.
Figure 1 shows a screenshot of the humans alone/inversion and delegation conditions.
In the delegation conditions, we added a button labeled \Delegate this question to the AI" at a
random position between the answers. If a subject clicked, she did not classify the image herself,
but delegated it to the AI. She would not see the AI's answer. The AI's answer was considered
Electronic copy available at: https://ssrn.com/abstract=3368813

14 
Figure 1 Screenshots of the humans alone/inversion condition (left) and the delegation condition (right).
hers, and she received her payment accordingly. We made it clear that each correct classication
earned a payment, regardless of whether the AI or the human classied the image. Subjects in
the
To ensure that the eects can be related to dierent delegation, and not to dierent human
classication behavior, subjects in the inversion condition had to classify all 100 images, like subjects
in the other conditions. We constructed the results for the inversion condition (4) after the
experiment. The AI classies images or delegates them to humans based on a simple rule: if the
score for the best answer was below a certain threshold, then GoogleNet delegated this image to
the humans. Otherwise, GoogLeNet classied the image. To simulate this mechanism we paired
GoogleNet with each subject from the inversion condition. The threshold was the average accuracy
of subjects in the humans alone condition (2). The AI thus delegated all images where the
estimated likelihood of being correct was below average human accuracy.
All subjects received instructions, had to pass a short quiz so that we could exclude bots, and
completed an example classication to ensure that they understood the task. They then had to
classify the 100 images in random order. Each subject received a base fee of 50 cents, and an
additional 5 cents for each correct answer. Afterwards, they were asked how many images they
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 
ssied correctly. They could earn 1 additional dollar if this estimation did not dier
from the actual number by more than ve images.
Average pay was $4.45, slightly above average pay on MTurk in general (Hara et al. 2018). The
average duration of the experiment was 57.7 minutes.
tic
Dep. Var.: Treatment N Min. Mean Max. St. Dev. Pctl(25) Median Pctl(75)
Accuracy
AI alone 0.770
Humans alone 149 0.310 0.717 1.000 0.132 0.650 0.740 0.810
Delegation 154 0.250 0.740 0.990 0.101 0.700 0.760 0.800
Inversion 146 0.710 0.870 0.980 0.042 0.850 0.870 0.898
Results. Descriptive statistics (Table 1) and visual evidence (Figure 2) suggest that the ability to
delegate aects classication accuracy. On average, accuracy is highest in the inversion condition
(87.0%), followed by the delegation condition (74.0%) and humans alone (71.7%). By itself, AI
accuracy is 77.0% (vertical dashed line in Figure 2). The standard deviation of accuracy (in number
of images) is highest when humans work alone (13.2), smaller when humans can delegate (10.1)
and smallest when the AI delegates to humans (4.2). We used .717 as the threshold in the inversion
condition. The results for inversion are robust for dierent threshold values (inversion accuracy is
above .840 for all thresholds between the 25th and 75th percentile of human performance, that is,
.650 and .810). The ex-post optimal combination of humans and AI of Treatment 1 would lead
to an upper bound of 89.9% average accuracy { that is, assuming that each image is classied
correctly, where either the AI or the (randomly picked) human classied the image correctly.
The variance of accuracy is signicantly dierent across experimental conditions (Levene test,
F(2, 446) = 36.752, p < :001; Hartleys Fmax test, Fmax = 9:962 > critical value), and means are
signicantly dierent as well (ANOVA with heterogeneous variances, F(2, 245.05) = 178.41, p <
:001, 2 = :315, which represents a large eect). Post-hoc tests with Tanhames T2 statistic for
Electronic copy available at: https://ssrn.com/abstract=3368813

16 
HumAI
Inversion
0.2 0.4 0.6 0.8 1
Accuracy
Treibution plots for accuracy per experimental condition (Study 1). The vertical dashed line is the AI
classication accuracy of 77%.
multiple comparisons show that most pairwise mean dierences are signicant (see Table 2 for a
summary of pairwise comparisons). Humans in the delegation condition seem to outperform humans
alone. However, this dierence (2.37 percentage points) is not signicant (p=:120) and represents
a relatively small eect (d=:2). Inversion clearly outperforms humans alone. This dierence (15.38
percentage points) is signicant (p < :001) and represents a large eect (d = 1:67). Inversion also
outperforms the delegation condition. This dierence (13 percentage points) is signicant (p<:001)
and represents a large eect (d=1:56).
Table 2 Summary of p-values for pairwise comparisons of accuracy (Study 1).
Humans alone Delegation Inversion
AI <0.001 <0.001 <0.001
Humans alone 0.120 <0.001
Delegation <0.001
Mean accuracies for the humans alone, delegation and inversion conditions are signicantly different
from AI alone (p<:001), and except for inversion, are all lower than AI alone. Performance
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 


0.3
0.4
0.5

0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Delegation rate
Accuracy
Delegation Rate (Study 1)
Figure 3 Scatter plot of accuracy per image (horizontal axis) against delegation rate per image (vertical axis).
The regression line is estimated from all images.
in the inversion condition is signicantly higher than when the AI is working alone, suggesting
that humans can improve the performance of an AI by providing their input. Not only is inversion
better than the other settings on average, we also notice that the AI benets from working with
almost all humans. In the inversion condition, only three of the 146 AI-human pairs had a performance
smaller than the AI itself; even the 25th accuracy percentile of inversion (85.0%) is much
larger than AI accuracy. To summarize, sharing work between humans and AI could outperform
humans and AI working alone. Inversion was highly eective, but human delegation was not.
To understand inferior human performance in the delegation condition, we investigate how
humans delegate. In Figures 3 and 4, image diculty is depicted on the horizontal axis. Image
diculty is the average accuracy in the humans alone condition of the respective image. A .2 dif-
culty/accuracy means 20% of subjects classied the image correctly. The vertical axis in both
gures shows the delegation rate, i.e., the ratio of subjects who delegated the image to the AI.
If we consider the entire data set (Figure 3), a weak trend can be detected where images with
higher accuracy (lower diculty) are delegated less often and vice versa. Thus, humans seem
to \rationally" delegate those images more often, which they are less able to classify correctly.
However, if we partition the data into images with less than 70% accuracy (these images are more
Electronic copy available at: https://ssrn.com/abstract=3368813

18 
0.0
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Delegation rate
Accuracy
Delegation Rate (Study 1)
Figure 4 Scatter plot of accuracy per image (horizontal axis) against delegation rate per image (vertical axis).
The two regression lines are estimated from two partitions of the data.
dicult than average human performance), and above 70% accuracy (these images are easier than
average human performance), the pattern changes (Figure 4): Human delegation is not in
uenced
by the diculty of an image if the image is relatively \dicult." Human delegation seems to follow
a \rational" pattern if the image was relatively \easy" to classify. Note that we chose the threshold
of 70% consistently over the following studies - as no image had an average accuracy between 0.7
and 0.717, setting the threshold to the precise average accuracy of 0.717 does not lead to any
changes. Eect sizes and signicance levels are in Table 3. Why did humans delegate randomly if
images were hard?
In the next section, we present Study 2 that explores possible explanations related to the boundary
conditions of successful delegation as potential mechanisms for the observed phenomenon.
3.3. Study 2: Explaining and Enforcing a Delegation Strategy
Hypotheses. This study seeks to analyze the cognitive challenges in human delegation and aims
at providing assistance to explore potential paths towards more productive delegation. In the
previous study, we observed random delegation patterns for hard images. Our boundary conditions
of successful delegation lead to dierent possible explanations that we test in this study:
1. Humans might not have a sucient level of metaknowledge. To be able to analyze this, we
asked humans to self-report their level of certainty for each image in all treatments.
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 
sion results for delegation (Study 1).
70% Accuracy
Accuracy 0:029 􀀀0:535
(0:104) (0:067)
Constant 0:169 0:557
(0:052) (0:059)
Observations 41 59
R2 0.002 0.529
Adjusted R2 -0.024 0.521
Residual Std. Error 0.090 (df = 39) 0.038 (df = 57)
F Statistic 0.080 (df = 1; 39) 64.138 (df = 1; 57)
Note: p<0.1; p<0.05; p<0.01
Standard errors in parentheses
2. Humans might not be able to come up with a good delegation strategy: We added a treatment
where we advised humans with a strategy that imitates the inversion logic.
3. Humans might not be willing to delegate suciently: We added a treatment where delegation
was enforced based on human certainty - this treatment applies inversion with humans.
Consequently, Study 2 contains three treatments, all allowing the option to delegate to the AI:
A \baseline" condition that replicates the delegation condition of Study 1 asking for self-reported
certainty for each image, a \strategy explained" condition, where we suggest a delegation strategy
that imitates the delegation logic of inversion based on human certainty, and a \strategy enforced"
condition, where we enforce a strategy by automatically delegating images to the AI if a human
reports low certainty. As was the case in Study 1, we preregistered two initial hypotheses based on
classication accuracy.
Assuming that all three explanations of inferior human delegation performance apply, we state
two hypotheses. First, if humans are simply unaware of good delegation strategies, they should
improve if such a delegation strategy is suggested. Thus, we pose our rst hypothesis:
Hypothesis 2.1 : Explaining the delegation strategy leads to a slight improvement in accuracy.
If humans are reluctant to delegate, then enforcing a good strategy should increase accuracy
even stronger than just suggesting it, which leads to our second hypothesis:
Electronic copy available at: https://ssrn.com/abstract=3368813

20 
Hypng, we present the detailed experimental design and the results of Study 2.
Desevel of certainty for each image on a scale from 1 (uncertain) to 4 (certain) in all
conditions. The \baseline" condition was set up like the delegation condition of Study 1. For the
remaining two conditions, we propose a simple delegation rule similar to that of the AI in the
inversion condition, where (1) it assessed its classication certainty and (2) delegates to humans
if the certainty score was below average human performance. Accordingly, we advised subjects
in the second condition, \strategy explained," to delegate images for which they were uncertain
(certainty levels between 1 and 3, average accuracy expected to be below the AI performance of
0.77). If subjects' certainty was high (certainty level 4, average accuracy expected to be above the
AI performance of 0.77), we advised them to classify the image themselves. In the third condition,
\strategy enforced," subjects could not delegate actively. We informed them that images will be
delegated automatically if their self-reported certainty was between 1 and 3. The human answer
was only considered if the reported score was a 4. This treatment represents most closely the human
version of our inversion condition of our rst experimental study.
We recruited 453 subjects via MTurk and randomly assigned them to experimental conditions.
Average pay was $5.19, average duration was 56.2 minutes. The assignment process and experimental
protocol was equivalent to that of Study 1.
Results. Table 5 shows summary statistics for accuracy and delegation rates. Accuracy improved
slightly and delegation rates increased strongly when the delegation strategy was explained or
enforced. Delegation rates in the strategy enforced condition look similar to the strategy explained
condition. This suggests that humans indeed followed the suggested delegation rule.
This is supported by statistical analysis. A Levene test reveals no signicant dierences between
the variances across experimental conditions (F(2, 450)=.849, p = :429), but means are dierent
(ANOVA, F(2, 450)=2.97, p=:052, 2 =:13 which represents a medium eect). All pairwise comparisons
are summarized in Table 4. Tukey's signicance test shows that humans in the strategy
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 
tion outperform humans in the baseline condition. This dierence (2.714 percentage
y explained condition is similar to that in the strategy enforced condition (p = :761).
 signicant (p = :207). It would represent a small eect (d=.185). When comparing
the condition's accuracies with AI performance, the baseline condition shows a signicantly lower
performance (p=:010), but there is no signicant dierence between AI and the strategy explained
(p = :727) or the strategy enforced condition (p = :484). In total, engaging with a good delegation
strategy led to more delegation, but accuracy did not increase proportionally. We also compute the
accuracy of humans per (self-reported) certainty score. From pre-tests, we expected the accuracy
for images with certainty scores between 1 and 3 to be below 0.77, and for images with a certainty
score of 4 to be above 0.77. Our results validate this assumption: For Treatment 1, the average
accuracies for non-delegated images were 0.43 (certainty score 1), 0.52 (certainty score 2), 0.68
(certainty score 3), and 0.87 (certainty score 4).
Table 4 Summary of p-values for pairwise comparisons of accuracy (Study 2).
Baseline Strategy explained Strategy enforced
AI 0.010 0.727 0.484
Baseline 0.207 0.048
Strategy explained 0.761
Figure 5 shows the delegation pattern. The horizontal axis depicts image diculty (average
human accuracy of Treatment 1, Study 1), the vertical axis delegation rates. The baseline condition
replicated the results of Study 1. Further, humans delegated more when the strategy was explained
or enforced. However, their behavior for dicult images was still random. The randomness just
centered around a higher average than in the baseline condition. Therefore, knowing a good delegation
strategy did not prohibit random delegation of dicult images. Hence, we can rule out the
second explanation from above: while providing a strategy helps to increase delegation, it could not
Electronic copy available at: https://ssrn.com/abstract=3368813

22 
Tabtic
Dep. Var.: Treatment N Min. Mean Max. St. Dev. Pctl(25) Median Pctl(75)
Accuracy
Basined 157 0.240 0.767 0.880 0.103 0.750 0.800 0.825
Strategy enforced 146 0.140 0.775 0.900 0.088 0.750 0.790 0.823
Delegation rate
Baseline 150 0.000 0.131 0.680 0.151 0.010 0.080 0.200
Strategy explained 157 0.000 0.342 0.950 0.203 0.185 0.330 0.475
Strategy enforced 146 0.010 0.335 0.960 0.183 0.190 0.315 0.463
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Delegation rate
Accuracy
Delegation rate (baseline)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Delegation rate
Accuracy
Delegation rate (strategy explained)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Delegation rate
Accuracy
Delegation rate (strategy enforced)
Figure 5 Scatter plots of accuracy against delegation rate per image and experimental conditions (Study 2).
The two regression lines are estimated from the two partitions of the data.
Table 6 Regressions per experimental condition (Study 2). The dependent variable is the images' delegation
rate. The data is partitioned into two regions.
Experimental condition
Baseline Strategy explained Strategy enforced
Dependent variable: Delegation rate for images with accuracy of
<70% 70% <70% 70% <70% 70%
Accuracy -0.041 -0.588 -0.230 -1.610 -0.131 -1.486
(0.121) (0.072) (0.182) (0.165) (0.180) (0.142)
Constant 0.233 0.592 0.650 1.626 0.579 1.520
(0.060) (0.064) (0.090) (0.146) (0.090) (0.126)
Observations 41 59 41 59 41 59
R2 0.003 0.537 0.039 0.625 0.013 0.657
Adjusted R2 -0.023 0.529 0.015 0.619 -0.012 0.651
Residual Sd. Error 0.105 0.041 0.158 0.094 0.157 0.081
F Statistic 0.117 66.08 1.59 95.14 0.531 109.30
Note: * p<:1; ** p<:05; *** p<:01
Standard errors in parentheses
Electronic copy available at: https://ssrn.com/abstract=3368813

 23
0.0

0.3
0.4

0.7
0.8
0.9
1.0
2.0 2.5 3.0 3.5 4.0
Delegation rate
Level of certainty
Delegation rate (baseline)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
2.0 2.5 3.0 3.5 4.0
Delegation rate
Level of certainty
Delegation rate (strategy explained)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
2.0 2.5 3.0 3.5 4.0
Delegation rate
Level of certainty
Delegation rate (strategy enforced)
Figure 6 Scatter plots of certainty against delegation rate per image and experimental conditions (Study 2).
x the random delegation pattern for dicult images. Table 6 shows the corresponding statistical
results.
We now address the third explanation (humans do not want to use the AI). We analyzed how
delegation rates changed with perceived image diculty (i.e., self-assessed certainty). Figure 6 plots
delegation rates (vertical axis) against self-assessed certainty (horizontal axis). The gure suggests
that humans delegated with great internal consistency. Images they perceived as more dicult were
delegated more often. This was true, independent of whether they knew the delegation strategy or
not. The subjects appeared to be aiming for a consistent delegation pattern. Once they learned a
good delegation strategy, delegation rates more than doubled. Therefore, we conclude that in our
experiments humans did not show reluctance towards using the AI.
In light of these ndings, the rst explanation seems likely. Humans might not be able to judge
the diculty of images when the images are hard. They may thus not be able to use the AI
systematically for these images, a problem associated with lack of metaknowledge.
To explore this explanation, we study how well humans can assess their own ability to classify
images. In Figure 7 we plot the average self-assessed certainty of an image (vertical axis) against
the average accuracy of the image (horizontal axis). The visual impression and the regression
results in Table 7 suggest that humans can assess their ability for relatively easy images (accuracy
above 70%), but they can not assess it for dicult images. An interesting side nding can be
observed for the strategy enforced condition. Here, the constant of the regression model is positive
and signicant for easy images. Thus, objective diculty explains perceived diculty (as in the
Electronic copy available at: https://ssrn.com/abstract=3368813

24 
2.0
2.2
2.4
2.8
3.0
3.2
3.6
3.8
4.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Level of certainty
Accuracy
Certainty (baseline)
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
3.8
4.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Level of certainty
Accuracy
Certainty (strategy explained)
2.0
2.2
2.4
2.6
2.8
3.0
3.2
3.4
3.6
3.8
4.0
0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Level of certainty
Accuracy
Certainty (strategy enforced)
Figure 7 Scatter plots of accuracy against certainty per image and experimental conditions (Study 2).
other conditions), but subjects seem to report higher certainty values independent of actual image
diculty. A possible explanation is that subjects avoided automated delegation by (mis)reporting
certainty values of four.
Table 7 Regressions per experimental condition (Study 2). The dependent variable is the subjects' certainty
per image. The data is partitioned into two regions.
Experimental condition
Baseline Strategy explained Strategy enforced
Dependent variable: Certainty, where images have accuracy of...
<70% 70% <70% 70% <70% 70%
Accuracy 0.249 3.524 0.368 3.505 0.109 2.363
(0.384) (0.344) (0.465) (0.374) (0.384) (0.238)
Constant 2.716 0.383 2.675 0.453 3.161 1.608
(0.191) (0.304) (0.231) (0.332) (0.191) (0.211)
Observations 41 59 41 59 41 59
R2 0.011 0.648 0.016 0.606 0.002 0.633
Adjusted R2 -0.015 0.642 -0.009 0.599 -0.024 0.626
Residual Sd. Error 0.334 0.196 0.403 0.214 0.334 0.136
F Statistic 0.419 105.10 0.626 87.64 0.081 98.25
Note: * p<:1; ** p<:05; *** p<:01
Standard errors in parentheses
Therefore, while humans delegate quite rationally based on their internal assessment (Figure 6),
this assessment is not precise for relatively dicult tasks (Figure 7). Put dierently, although human
delegation decisions are often misaligned with real problem diculty, they are not misaligned with
their perceived problem diculty. We conclude that lack of metaknowledge seems to drive the
inferior delegations. According to this explanation, humans did not know what they knew and
delegated the wrong images to the AI.
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 
Checks
In this section, we test the robustness of our ndings with two additional studies. In the rst
robustness check, we analyze whether continuous feedback on both human and AI performance
r an AI could realize complementarities with humans, even in cases, where the tasks
are more dicult than those the AI was trained with. We manipulate task diculty by scaling the
images to a lower resolution and test the eectiveness of inversion.
4.1. Study 3: The Role of Feedback
Purpose. Study 3 relaxes the assumption of receiving no feedback on task results to analyze the
eects of feedback on human delegation behavior and on human metaknowledge.
At the outset, we would like to point out that the potential eects are unclear, ex-ante, and the
literature provides no clear direction. We lay out possible eects in the following discussion. First,
metaknowledge might be increased by providing continuous feedback because it could improve the
human perception with regards to their own performance. However, we observe that even long-term
experience does not seem to prevent poor metaknowledge (Brezis et al. 2018). Thus, the eect
remains unclear. Second, feedback on human and AI performance might increase salience of AI
superiority, and could consequently lead to higher delegation rates. On the other side, Dietvorst
et al. (2015) demonstrated that humans relied less on algorithmic advice after seeing it err, even if
the algorithmic performance was superior. Thus, we do not state any directional hypotheses in this
study. In the following, we lay out the details of our study design before presenting our results.
Design. We compare classication accuracy and delegation rate between two conditions. The
\baseline" condition (1) replicates the \delegation" condition of Study 1 and the \baseline" condition
of Study 2. In the \feedback" condition (2), subjects received feedback after each classication
task, consisting of their own answer, the AI answer, and the correct answer. We ran a betweensubject
design with 289 subjects in February 2021, and randomly assigned subjects to the baseline
condition (148 subjects) and the feedback condition (141 subjects).
Electronic copy available at: https://ssrn.com/abstract=3368813

26 
Allan example classication to ensure they understood the task. They then had to
classify the 100 images in random order. Each subject received a base fee of $2 , and an additional
5 cents for each correct answer. Afterwards, they were asked how many images they think they
claby more than ve images. Average pay was $4.92, slightly above average pay on
MTurk in general (Hara et al. 2018). The average duration of the experiment was 62.7 minutes.
Results. Descriptive statistics (Table 8) show little dierence among the the experimental conditions,
regarding both accuracy (baseline: 53.4% feedback: 54.4%) and delegation rate (baseline:
12.0%, feedback: 12.4%). Accuracy does not signicantly dier in the baseline and the feedback
conditions (p=:697), neither does the delegation rate (p=:860). Thus, there is no indication that
continuous feedback on human and AI performance aects human delegation behavior. Note that
in line with literature on MTurk performance during COVID-19 (Arechar and Rand 2021), the
average accuracy values are below those of our previous studies, while delegation rates remain
similar.
Table 8 Summary statistics for accuracy and delegation rate (Study 3).
Summary statistic
Dep. Var.: Treatment N Min. Mean Max. St. Dev. Pctl(25) Median Pctl(75)
Accuracy
Baseline 149 0.070 0.534 0.880 0.221 0.373 0.540 0.758
Feedback 141 0.070 0.544 0.950 0.232 0.340 0.620 0.750
Delegation rate
Baseline 149 0.000 0.120 1.000 0.231 0.000 0.010 0.120
Feedback 141 0.000 0.124 0.990 0.212 0.000 0.010 0.155
Next, we analyze the eect of continuous feedback on metaknowledge by replicating the analysis
of the relationship between accuracy and certainty in Study 2. We illustrate the relationship of
delegation rate and diculty (average human accuracy of Treatment 1, Study 1) in Figure 8.
We further summarize the regression results in Table 9: as in Study 2, we only see a signicant
Electronic copy available at: https://ssrn.com/abstract=3368813

 27
0.0

0.3
0.4

0.7
0.8
0.9
1.0
0.0 0.2 0.4 0.6 0.8 1.0
Delegation Rate
Accuracy
Delegation Rate (baseline)
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
0.0 0.2 0.4 0.6 0.8 1.0
Delegation rate
Accuracy
Delegation Rate (feedback)
Figure 8 Scatter plots of accuracy against certainty per image and per experimental conditions (Study 3). The
two regression lines are estimated from the two partitions of the data.
in
uence of \diculty" (measured by the average accuracy of each image) for images with an
average accuracy of at least 70%. Note that we use the average accuracy of the rst treatment
of Study 1, where humans classied without AI delegation, to maintain a consistent denition of
\easy" and \dicult" tasks.
Table 9 Regressions per experimental condition (Study 3). The dependent variable is the subjects' certainty
per image. The data is partitioned into two regions.
Experimental condition
Baseline Feedback
DV: Certainty, images accuracy of...
<70% 70% <70% 70%
Accuracy 0.099 1.527 0.182 1.372
(0.179) (0.238) (0.134) (0.205)
Constant 3.130 2.067  3.083 2.148
(0.089) (0.210) (0.067) (0.205)
Observations 41 59 41 59
R2 0.008 0.420 0.045 0.439
Adjusted R2 -0.018 0.410 0.021 0.430
Residual Sd. Error 0.156 0.136 0.116 0.117
F Statistic 0.305 44.33 1.09 44.67
Note: * p<:1; ** p<:05; *** p<:01
Standard errors in parentheses
In the next section, we present a robustness check on the impact of more dicult tasks on the
eciency of delegation and inversion.
Electronic copy available at: https://ssrn.com/abstract=3368813

28 
4.2. Study 4: The Role of Diculty
Purpose. Inversion was the most eective condition in our rst experimental study. The key of
itsdicult than those it was trained with. In the case of image classication, this could
relate to images with a lower resolution. Thus, we replicate Study 1 with a higher task diculty
by applying a lower resolution to all images. We aim to analyze whether the AI would still be able
to pare classication accuracy between two conditions, \humans alone" (1) and
\delegation" (2). Those conditions mirror two conditions of Study 1. We further use condition (1)
to simulate dierent inversion strategies. We ran a between subject design with 299 subjects in
January 2021 and randomly assigned subjects to the \humans alone" condition (150 subjects) and
the \delegation" condition (148 subjects).
All subjects received instructions, had to pass a short quiz so that we could exclude robots,
and completed an example classication to ensure they understood the task. They then had to
classify the 100 images in random order. Each subject received a base fee of $2, and an additional
5 cents for each correct answer. Afterwards, they were asked how many images they think they
classied correctly. They could earn 1 additional dollar if this estimation did not dier from the
actual number by more than ve images. Average pay was $4.61, slightly above average pay on
MTurk in general (Hara et al. 2018). The average duration of the experiment was 65.9 minutes.
Table 10 Summary statistics for accuracy (Study 4).
Summary statistic
Dep. Var.: Treatment N Min. Mean Max. St. Dev. Pctl(25) Median Pctl(75)
Accuracy
Humans alone 150 0.120 0.481 0.790 0.168 0.360 0.480 0.633
Delegation 148 0.140 0.512 0.790 0.153 0.420 0.510 0.630
Results. Humans slightly improve by about three percentage points with the possibility to delegate
(p = :093), even though on average, only 7.6% of images were delegated. While no direct
Electronic copy available at: https://ssrn.com/abstract=3368813

Information Systems Research 00(0), pp. 000{000, 

0.5
0.6
3 0.4 0.5 0.6 0.7 0.8 0.9 1.0
Accuracy
Threshold
ns alone AI alone
Figure 9 Inversion accuracy by threshold value.
comparison is possible, we see that the human performance does not seem to be strongly aected
by lower resolution, as the total accuracy seems to be similar to those of Study 3. AI performance,
however, was decreased signicantly from 77% to 54%. Human performance still remains below AI
accuracy. Next, we analyze whether the AI is still able to improve by delegating to humans, even
though its own performance dropped strongly. In Figure 9, we simulate inversion accuracies based
on the humans alone condition with varying threshold values. A threshold of 0 symbolizes always
choosing the AI prediction resulting in the AI accuracy of 54%, while a threshold of 1 symbolizes
always choosing the human prediction resulting in the average human accuracy of 48.1%. With
every threshold value below 0.95, the inversion accuracy outperforms both human and AI accuracy,
with a maximum value at a threshold of 0.50, close to the average human accuracy. Using an
inversion delegation rule with a threshold of 0.50 improves the human accuracy by 15 percentage
points and the AI accuracy by nine percentage points. Thus, even in a situation where the AI
performs relatively poor, inversion seems to be a powerful delegation mechanism.
5. Discussion and Directions of Future Research
Our results demonstrate that humans and AI can work together on image classication, even if
there is no feedback about the AI performance and errors. In such a situation, it is benecial to
let the AI delegate work to humans in case the AI is uncertain. Humans were unable to delegate
Electronic copy available at: https://ssrn.com/abstract=3368813

30 
welss if they know the correct class of dicult images. This result is interesting because
it shows inferior human performance but no aversion to use the algorithm. Our data supports
the explanation that subjects were indeed motivated to work with the machine and were willing
to task diculty. We interpret this as a fundamental and latent limitation rather than
as an act of conscious reluctance. In this regard, our results are consistent with the general view
issued in Logg et al. (2019): humans do appreciate the help from AI. But we also show that they
migesults challenge the assumption that an entire task should be handed over to
an AI if the AI is better. We stated three boundary conditions where delegation and a good
distribution of work can outperform the assignment to one party. First, humans and AI have to have
complementary skills. We claim this should be the case for tasks where decision rules are not clearly
dened.We conrm this using image classication as an example: An optimal combination of the AI
and humans from the inversion condition would lead to an average accuracy of 89.9%, considerably
more than 77% accuracy for AI alone and 71.7% for humans alone. Second, complementarities have
to be recognized. We dene a sucient level of metaknowledge as a necessary condition. While the
AI seems to have a good perception of own abilities, humans are not able to dierentiate between
tasks they are able to do and those where this is not the case, especially for dicult images. Third,
an ecient delegation rule needs to be followed, where a task is moved to the actor that is better at
solving it. Under perfect information, a simple rule is eective: If you are able to do the task, do it
yourself; if you are not, then delegate. We demonstrate that such a rule can easily be implemented
for AI, and that humans can potentially be trained to follow such a rule.
When AI delegates to humans: Inversion. If AI would be responsible to delegate to humans,
several interesting things could happen. First, in our experiment the resulting performance was
higher than that of the AI alone. This makes inversion economically desirable. Second, humans
Electronic copy available at: https://ssrn.com/abstract=3368813

 31
would do some of the work. They contribute to the superior result, without them we would not
timulating environment (Pink 2011). In our example, classifying easily identiable
images is perhaps routine and boring, whereas the classication of dicult images could be an
interesting challenge. Inversion might enable humans to spend less time on mundane tasks and more
 could be interpreted not only as a delegation to humans but also as freeing humans
some valuable time. The AI would not be the humans' boss but rather an assistant who swipes
away distractions from the real work. However, inversion comes with a loss of human control.
The AI decides about the delegations, it asks the human for support only if it is required. It
does this without emotions, only to leverage complementarities that exist as foreseen by Polanyi's
groundbreaking work.
When humans delegate to AI: Metaknowledge and the quest for good delegation. Our research
points to a fundamental characteristic of human behavior that needs to be understood in order to
design more eective human-AI collaborative environments: Humans did not perform well in delegating
tasks to the AI. We can design and teach simple delegation rules, especially in modularized
tasks. However, even when humans diligently and rationally apply a delegation rule (Figure 5) that
is internally consistent with their perception of task diculty, humans that delegate to AI do not
perform as well as they should. The reason for this (Table 9) is an apparent lack of understanding
what is dicult for them, and what is not. This phenomenon is not isolated to working with AI or
computers. Humans tend to misjudge their certainty when dealing with high diculty questions
as compared to medium-diculty or easier questions (Pulford and Colman 1997). In our research
context, this translates into humans making more arbitrary delegation decisions when dealing with
dicult tasks, which worsens their overall performance.
More generally, the phenomenon to not understand the diculty of a task at hand relates to a
lack of metaknowledge in terms of \appreciation of what we do know and what we do not know"
Electronic copy available at: https://ssrn.com/abstract=3368813

32 Information Systems Research 00(0), pp. 000{000, 
c 0oemaker 1992, p.8). In our formal education, we do not emphasize this higher
level of learning to recognize our own strengths and shortcomings. Often, the impact of lacking
metout alternatives, logical and procedural inconsistencies and/or errors. One way to
interpret group discussions is to try and achieve the best compromise based on metaknowledge
and primary knowledge based on facts, concepts, models, relationships and solution techniques.
Howre specically where AI engages humans in a dialogue to resolve issues related
to metaknowledge. If we want to produce students that will be eective in the future workforce,
improving metaknowledge should be a central tenet of higher education.
Limitations. Our study informs on delegation between humans and AI. To ensure a certain
degree of generalizability of results, we aimed for a generic, non-specialized task and non-specialized
workers relying on image classication and MTurk workers. While we think that our ndings carry
over to many other settings, there might be additional eects in any specialized environment, that
strengthen or weaken our ndings. While relying on non-specialized situations is a limitation of
this study, it also creates an opportunity for future research, and to test whether our ndings can
be replicated in specialized environments.
There has been a lot of discussion on the suitability of MTurk workers for behavioral experiments.
These discussions concentrate on three main criticisms of using MTurk in behavioral experiments:
First, non-naivete, that is, subjects might be experienced in similar experiments and behave strategically
(Chandler et al. 2019). Second, carelessness, that is, subjects act with a lower degree of rigor
leading to noisy and partially inconsistent results (Aruguete et al. 2019). Third, representativeness,
that is, the MTurk population does not re
ect the composition of society in general. However, it
should be noted that MTurk subjects are better representatives of the general population compared
to typical student subjects used in a large number of academic studies (Chandler et al. 2019).
How could we safeguard against these issues? We contend that non-naivete does not apply in this
instance since our study is unique in character, compared to, for example, potentially hundreds of
Electronic copy available at: https://ssrn.com/abstract=3368813

 33
studies looking at newsvendor problems or dictator games. A potential solution could be to rely on
new MTurk workers or to exclude workers who participated in similar studies if this information
is available. We excluded all subjects who participated in our own related studies or pre-tests. In
higher spread in data quality in MTurk samples compared to traditional student
samples and recommend measures to ensure validity of results. Following these suggestions, we
decided to: a) restrict our subject pool to subjects with a positive track record and at least 90%
 errors in order to participate in the study. Please note that for our set of robustness
experiments (Study 3 and 4), we had to conduct the experiments during the COVID-19 pandemic.
This led to an increased level of subjects' carelessness and lower performance compared to the other
studies. This nding is in line with the literature (Arechar and Rand 2021), and we refrain from
direct comparisons of the specic results between the rst set of experiments and the robustness
checks. We admit that our study does not claim to represent a general population. Thus, we
do not make any claims regarding absolute results of our study, such as \we expect humans to
delegate 13% of tasks to an AI," rather we compare dierences in behavior between conditions.
Replicating several studies from dierent subject samples with MTurk samples, Coppock (2019)
conclude that MTurk samples can be compared to other national samples. Many other studies
validate the appropriateness of MTurk samples for experimental studies in social sciences, such as
Buhrmester et al. (2016), Horton et al. (2011), or Lee et al. (2018).
Future research. As laid out above, a potentially relevant limitation of our sample lies in an
expected low performance, especially for the samples drawn during the COVID-19 pandemic. In
concert with focusing on non-specialized tasks, this limits the generalizability of the results regarding
the absolute performance of our experiment. While we do not expect that those limitations
have aected our main ndings regarding dierent congurations of delegation schemes, or the
mechanisms we observed, we believe that analyzing similar settings with high-impact decisions and
dedicated workers is a fruitful avenue for future studies.
Electronic copy available at: https://ssrn.com/abstract=3368813

34 
In addition to addressing potential limitations of our study, a key research area should be about
making humans better delegators in order to develop eective human-AI collaborative environments.
This requires research on three fronts:
a) Research on human-AI dialogue and decision authority. How should an AI engine communicate
and adapt when working with humans that have dierent levels of metaknowledge, and how should
it develop an appropriate framework for decision making in these environments? For example, an
AI engine can delegate decision authority to individuals with high levels of metaknowledge, whereas
it may simply receive inputs from highly competent individuals lacking metaknowledge.
b) Research on system feedback to increase metaknowledge. Prior research (Pulford and Colman
1997) has shown that feedback may not aect metaknowledge, especially when the task is di-
cult. No concerted eort has been made to design feedback environments that lead to improved
individual metaknowledge when other options are available.
c) Research on improving metaknowledge. Laboratory studies have shown that experience only
partially impacts metaknowledge (Hansson et al. 2008), and our robustness check in Study 3 showed
no eect of providing continuous feedback on metaknowledge. We still cannot rule out that longterm
debrieng, for example as it is common with airline pilots (Kikkawa and Mavin 2017), might
improve metaknowledge by providing a better understanding of our own strengths, weaknesses and
boundaries. This may lead to better appreciation of alternative sources that can help in decisions. In
human-only environments, providing long-term feedback and intensive debrieng is costly. Human
feedback may show internal consistency problems and may be intrusive at the task level. However,
modern technology, including realistic simulations (see Ketter et al. (2016) as an example), can
potentially provide innovative solutions that help improve our metaknowledge.








----------------------------------------------------------------------------------








Expl(AI)ned: The Impact of Explainable Artificial Intelligence on
Users’ Information Processing

Abstract
Because of a growing number of initiatives and regulations, predictions of modern
artificial intelligence (AI) systems increasingly come with explanations about why they
behave the way they do. In this paper, we explore the impact of feature-based explanations
on users’ information processing. We designed two complementary empirical studies
where participants either made incentivized decisions on their own, with the aid of opaque
predictions, or with explained predictions. In Study 1, laypeople engaged in the deliberately
abstract investment game task. In Study 2, experts from the real estate industry estimated
listing prices for real German apartments. Our results indicate that the provision of featurebased
explanations paves the way for AI systems to reshape users’ sense making of information
and understanding of the world around them. Specifically, explanations change
users’ situational weighting of available information and evoke mental model adjustments.
Crucially, mental model adjustments are subject to the confirmation bias so that misconceptions
can persist and even accumulate, possibly leading to suboptimal or biased decisions.
Additionally, mental model adjustments create spillover effects that alter user behavior in
related yet disparate domains. Overall, this paper provides important insights into potential
downstream consequences of the broad employment of modern explainable AI methods. In
particular, side effects of mental model adjustments present a potential risk of manipulating
user behavior, promoting discriminatory inclinations, and increasing noise in decision making.
Our findings may inform the refinement of current efforts of companies building AI
systems and regulators that aim to mitigate problems associated with the black-box nature
of many modern AI systems.

Keywords: explainable artificial intelligence, user behavior, information processing, mental models

1. Introduction
Contemporary artificial intelligence (AI) systems’ high
predictive performance frequently comes at the expense
of users’ understanding of why systems produce a certain
output (Gunning et al. 2019, Meske et al. 2022). For AI systems
that provide predictions to augment highly consequential
processes such as hiring decisions (Hoffman
et al. 2018), investment decisions (Ban et al. 2018), or medical
diagnosing (Jussupow et al. 2021), this “black box”
nature can create considerable downsides. These issues
include impaired user trust, reduced error safeguarding,
restricted contestability, and limited accountability (see
Rosenfeld and Richardson 2019 for a review). Having recognized
these problems, organizations developing AI
and governments increasingly adopt principles and regulations
(EU 2016, 2021; Google AI 2019; Meta AI 2021)
effectively stipulating that AI systems need to provide
meaningful explanations about why they make certain
predictions (Goodman and Flaxman 2017, Cabral 2021).
In light of these developments, the implementation and
use of explainable AI (XAI) methods are becoming more
widespread and mandated by law.

The purpose of XAI methods is to make AI systems’ hidden logic intelligible to humans by answering the question: Why does an AI system make the predictions it does? Thereby, XAI methods aim to achieve high predictive performance and interpretability at the same time. Many state-of-the-art XAI techniques convey insights into AI systems’ logic after training and explain behaviors by depicting the contribution of individual input features to the outputted prediction (Doshi-Velez and Kim 2017). Although there is reason to believe that XAI can mitigate black-box problems (Bauer et al. 2021), the pivotal question is how users respond to modern explanations, given that the human factor frequently creates unanticipated, unintended consequences even in well- designed information systems (Willison and Warkentin 2013, Chatterjee et al. 2015).
Nascent research on human-XAI interaction examines how explainability affects humans’ perceptions, attitudes, and use of the system, for example, trust (Erlei et al. 2020), detection of malfunctioning (Poursabzi-Sangdeh et al. 2021), (over)reliance (Bussone et al. 2015), and task performance (Senoner et al. 2021). Prior research, however, does not consider the potential consequences of providing explanations for users’ situational information processing (the use of currently available information in the given situation) and mental models (cognitive representations that encode beliefs, facts, and knowledge). By depicting the contribution of individual features to specific predictions, feature-based XAI enables users to recognize previously unknown relationships between features and ground truth labels that the AI system autonomously learned from complex data structures. In that sense, XAI may constitute the channel through which AI systems impact humans’ conceptualization and understanding of their environment. This effect could reinforce the already considerable influence contemporary AI systems have on human societies (Rahwan et al. 2019) by, for better or worse, allowing human users to adopt systems’ inner logic and problem-solving strategies. Despite the increasing (legally required) implementation of XAI methods, a systematic study of these effects is yet missing. The paper at hand aims to fill this important gap.
We ask three research questions. Does the additional provision of feature-based explanations affect AI system users’ situational processing of observed information? Does it affect users’ underlying mental models? What are important moderating factors? Consider, for instance, a loan officer who works with an AI system to predict an applicant’s risk parameters and determine the credit approval. Because of legal requirements (e.g., Artificial Intelligence Act; EU 2021), the AI system recently started to provide feature-based explanations, showing that it strongly relies on people’s smartphone charging behavior to predict creditworthiness.1Although previous research examines how this explanation may affect the loan officer’s perceptions of the system, we conjecture that the explanation also, and maybe more importantly, affects his processing of currently available information and his underlying mental models of the determinants of creditworthiness. By changing mental models, explanations may even reshape the loan officer’s behaviors in related domains beyond the loan approval decision, for example, assessing the faithfulness of his daughter’s new boyfriend based on the smartphone charging behavior.2
Considerable challenges arise when trying to answer our research questions. First, measuring how XAI methods affect users’ situational processing of information and mental models is extremely difficult because these cognitive processes are typically unobserved. Second, we need to control for possible external cues, unintended stimuli, additionally attainable information, and preferences that may affect these cognitive processes in any given situation. Third, whether people interact with an (X)AI system, let alone rely on it, is highly endogenous and depends on factors such as culture, technological literacy, and the socio-technological environment. Thus, isolating effects associated with the provision of explanations in addition to predictions is particularly demanding, if not outright impracticable, in a natural (organizational) setting. To address these challenges, we rely on two complementary, incentivized experimental studies.
In Study 1 (n �607), laypeople played a series of investment games (Berg et al. 1995), making sequential economic transaction decisions in an intentionally abstract setting. In Study 2 (n �153), experts from the real-estate industry predicted listing prices for real apartments located in Germany. Study 2 extends Study 1 by testing the generalizability of our findings and elaborating on mechanisms driving the results. In both studies, conditional on the treatment, participants either received no decision support, support from an AI system in the form of opaque predictions or an XAI system with predictions plus feature-based explanations. We answer our research questions by eliciting and comparing changes in both participants’ decision-making patterns and their beliefs about feature-label relationships.
The two studies strongly complement each other for three reasons. First, laypeople (Study 1) and experts (Study 2) are the two diametrical archetypes of AI system users affected by growing explainability requirements. Studying both types’ responses to XAI methods enables us to identify possibly differential effects and make inferences about the generalizability of our findings. Second, we consider two fundamental types of prediction problems where AI systems are frequently in use: transaction outcome predictions (Study 1) and price predictions (Study 2) (Ban et al. 2018, Rico-Juan and de La Paz 2021). Examining the two settings allows us to understand better whether the interplay between XAI and cognitive processes is task specific. Third, using local interpretable model-agnostic explanations (LIME) (Study 1)

and SHapley Additive exPlanations (SHAP) explanations (Study 2), the two most popular feature-based XAI methods (Gramegna and Giudici 2021), allows us to draw more general conclusions about the interplay between feature-based explainability and cognitive processes.
Our findings paint a consistent picture: Providing explanations is the critical factor that enables AI systems to influence the way people make sense of and leverage information, both situationally and more permanently. Crucially, we find an asymmetric enduring effect that can foster preconceptions and spill over to other decisions, thereby promoting certain (possibly biased) behaviors.
Our paper proceeds as follows. Section 2presents theoretical foundations, whereas Section 3explains our experimental studies and results. Section 4concludes by discussing our results, the limitations of our work, and directions for future research.
2.Theory
In this section, we first discuss modern XAI methods (Section 2.1). Subsequently, we outline the relation between providing explanations and cognitive processes (Section 2.2) and discuss our work’s contribution to the literature (Section 2.3).
2.1.Explainable AI
Following Doshi-Velez and Kim (2017), we conceptualize XAI as methods that possess the ability to present in understandable terms to a human why an AI system makes certain predictions. Over the last couple of years, researchers developed ample XAI methods that help elucidate the opaque logic of machine learning (ML)- based AI systems (Ribeiro et al. 2016, Lundberg and Lee 2017, Koh and Liang 2017, Lakkaraju et al. 2019). Very generally, XAI methods aim to alleviate problems associated with the black-box nature (e.g., distrust, lack of accountability, and error safeguarding) while maintaining a high level of prediction accuracy (Bauer et al. 2021).
Our study focuses on feature-based XAI methods, hereafter XAI methods, that can explain the behavior of any ML-based AI system by showing the contribution of individual features to the prediction. We do so for several reasons. First, these explanations are the most widespread in practice (Bhatt et al. 2020, Senoner et al. 2021, Gramegna and Giudici 2021). Second, they are highly intuitive and straightforward to interpret as they satisfy most requirements for human-friendly explanations (Molnar 2020). Third, they are typically applicable to systems using structured and unstructured data (Garreau and Luxburg 2020). Fourth, these methods can explain individual predictions, local explainability, which might be the only method legally compliant with (upcoming) regulations (Goodman and Flaxman 2017).
Many researchers recognize two related XAI methods as state-of-the-art: LIME and SHAP (Gramegna and Giudici 2021, Molnar 2020). LIME (Ribeiro et al. 2016) and SHAP (Lundberg and Lee 2017) provide explanations through additive feature attributions, that is, linear models that depict the numeric contribution of each feature value to the overall black box model prediction. Both approaches learn these interpretable “surrogate models” on input- prediction pairs of the black box model and are applicable to virtually all classes of ML models, that is, are model agnostic. On the individual level, SHAP and LIME provide contrastive explanations that inform users why predictions for a specific instance diverge from the prediction for an average instance (Molnar 2020). For example, if the SHAP value for the feature Balcony equals +500 (�200), it indicates that having a balcony marginally increases (decreases) the current apartment’s listing price prediction by $500 ($200). The big difference between LIME and SHAP is the way of estimating the additive feature attributions. LIME creates synthetic, perturbed data points in the local neighborhood of the observation of interest and fits a weighted linear model to explain the relationship between the synthetic data and the relevant black box predictions. Importantly, LIME weights synthetic instances based on their proximity to the original data point. By contrast, SHAP is inspired by coalitional game theory and treats input features as a team of players that cooperate to generate a payoff (the prediction). The method essentially estimates the marginal contribution of each player to the overall payoff, Shapley values (Shapley 1953), using a linear model that weights instances based on characteristics of coalitions. Given these mathematical differences, the two methods can produce (slightly) different feature attributions for the same instance. However, from the perspective of a user who is not familiar with these details, the intuition and interpretation of the two methods’ explanations are reasonably similar (Molnar 2020). Notably, LIME and SHAP closely relate to the seminal description of Gregor and Benbasat (1999) of “why and why not explanations” in the context of knowledge-based expert systems.
With the development of modern explainability methods, research on the impact of contemporary XAI on user behavior has become increasingly essential (Vilone and Longo 2021). Nascent research in this domain typically focuses on how explanations affect user attitudes and reliance on the AI system (Lu and Yin 2021). These studies produce mixed evidence on the consequences of XAI on decision performance, user trust, perception, and decision- making performance. Several studies depict that explanations can enhance trust in and positive perceptions of the system (Rader et al. 2018, Dodge et al. 2019, Yang et al. 2020), whereas others provide reversed evidence (Erlei et al. 2020, Poursabzi-Sangdeh et al. 2021). Although prior studies produce important insights regarding the interplay between XAI and user perceptions, none of them considers that the additional provision of explanations may also reshape users’ information processing,

both situationally and more permanently. For instance, using SHAP to show the contribution of input features to a creditworthiness prediction may not only affect a loan officer’s perception of the AI system in use. Instead, she may process currently available information about the applicant differently and develop a novel understanding of the determinants of creditworthiness, that is, adjust her mental model. With the increasing adoption of explainability principles by organizations (Google AI 2019, Meta AI 2021) and the growing number of regulatory transparency requirements (EU 2016, 2021), it is pivotal to understand how contemporary XAI methods influence cognitive processes that lie at the heart of people’s knowledge, behavior, and problem-solving capabilities.
2.2.Cognitive Perspective on XAI Employment
Through feature-based explanations about an AI system’s prediction, human users can observe possibly unknown feature-label relationships that the system learned from complex data structures by itself (Agarwal and Dhar 2014, Berente et al. 2021). Although providing explanations, in general, can have a variety of cognitive effects, researchers across disciplines generally agree that they primarily enhance people’s understanding of someone or something, improve reasoning, and facilitate learning (Gregor 2006, Malle 2006). From a cognitive perspective, obtaining explanations can entail two effects: First, it may change people’s situational processing of available information: their use of available information while observing explanations. Second, it can lead to an adjustment of their beliefs about feature- label relationships the AI system inherently models: their mental representation of real-world processes. In this paper, we follow previous work in information systems and rely on the “Mental Models Framework” to conceptualize relevant cognitive processes (Vandenbosch and Higgins 1996, Lim et al. 1997, Alavi et al. 2002).
Mental models are “all forms of mental representation, general or specific, from any domain, causal, intentional or spatial” (Brewer 1987, p. 193), encoding beliefs, facts, and knowledge (Jones et al. 2011). Through imaginary manipulations of model components, people can reason and make inferences about how to solve problems (Rouse and Morris 1986). Much of the people’s decision making is based on these simulations that figuratively create informal algorithms for carrying out specific tasks (Johnson-Laird et al. 2017). For instance, real estate agents can mentally simulate how listing prices might change if an apartment for sale had a balcony.
When people perform tasks, they draw on relevant mental models that guide their processing of incoming information to form expectations and make (expectedly) optimal decisions. Working with an AI system that provides black box predictions, that is, information relevant to the task, allows people to reflect on their own expectations and compare it to the machine prediction (Sch¨on 2017). This mental process might entice people to revise their expectations and thus make different decisions because the machine prediction effectively substitutes for people’s own mental model driven formation of expectations (Agrawal et al. 2019). However, the black box nature does not allow users to directly compare their underlying beliefs and logic with that of the AI system. This comparison can only occur when they learn how the system combines available information to arrive at a prediction. In the previous example, the real estate agent may have access to an XAI system that provides a listing price prediction together with an explanation of how specific apartment attributes contribute to it. The agent can compare the explanation to her own initial perception of the individual attribute contributions to the listing price. As a result, the agent may detect inconsistencies that prompt her to revise her logic by putting more or less emphasis on specific information currently available to evaluate the apartment. This explanation- enabled situational process (Sch¨on 2017) can reconcile the distinct logic that humans and machines apply to arrive at a certain assessment. From this perspective, providing explanations on top of predictions may constitute a pivotal factor in allowing users to reflect on how they leverage information to solve a problem and adapt it according to the AI system’s logic for the given task.
Apart from situationally changing cognitive processes that shape the current decision, the interaction between mental models and explanations may also yield lasting effects because mental models possess the dynamic capacity to change (Jones et al. 2011). Repeatedly observing explanations about how feature X contributes to prediction ˆY and engaging in reflection processes may evoke adjustments of the underlying mental model in use. Following Vandenbosch and Higgins (1996), exposure to external stimuli, here explanations, can lead to two mental model adjustment processes: maintenance and building. Under mental model maintenance, people feel encouraged to maintain or reinforce current beliefs and decision- making rules. This process occurs when they perceive or select new information to fit into their current beliefs and routines. Under mental model building, individuals profoundly restructure or build new mental models in response to handling novel, disconfirming information. As a result of these processes, individuals may adopt different beliefs about how X contributes to the real label Y, enticing them to process information differently even when explanations are no longer present. Put differently: users may not merely combine situationally observed explanations with their own logic to solve a given task. Instead, observing the system’s logic may more fundamentally reshape users’ way of solving problems in general, that is, evoke learning. Therefore, users may exhibit different problem-solving strategies whenever they draw on the explanation-adjusted mental model, even in situations where they do not observe explanations anymore.

In sum, cognitive theories give reason to believe that providing explanations in addition to predictions can influence users’ processing of information about feature X, both situationally and more fundamentally. Because of the latter effect, modern XAI methods may constitute a cornerstone of effective knowledge transfers from ML-based AI systems to human users, helping them to learn from the AI how X relates to Y. Hence, explanations could facilitate learning machine knowledge: new knowledge AI systems autonomously learned from Big Data and previously missed by domain experts (Teodorescu et al. 2021, van den Broek et al. 2021).
2.3.Contribution to the Literature
Our study complements three different streams of literature. The first and most closely related line of work studies the interplay between XAI techniques and user behavior (see Rosenfeld and Richardson (2019) and Vilone and Longo (2021) for an overview). About two decades ago, several studies found that suitably designed explanations about the functioning and purpose of legacy knowledge-based expert systems can increase users’ trust in the systems, improve users’ perceptions of the system, and enhance decision-making performance (Dhaliwal and Benbasat 1996, Gregor and Benbasat 1999, Ji-Ye Mao 2000, Wang and Benbasat 2007). However, these expert systems codify knowledge from human experts as explicit procedures, instructions, rules, and constraints in a digital format. They do not represent machine knowledge that modern ML-based AI systems learn independently of domain experts by training on large data sets (van den Broek et al. 2021). Given the inherent distinctions between expert systems and ML- based AI systems in terms of encoded knowledge, contemporary explainability methods present an entirely different form of reasoning to users, namely that of machines (Vilone and Longo 2021, Meske et al. 2022). More recent research on the impact of explainability on user behavior mainly focuses on how contemporary XAI methods impact users’ perceptions of the AI system. This nascent literature shows that explainability often improves reliance on and trust in the system (Bussone et al. 2015), fairness perceptions (Dodge et al. 2019), human-AI collaboration (Yang et al. 2020), task efficiency (Senoner et al. 2021), and users’ understanding of the system’s malfunctions (Rader et al. 2018). However, there is also evidence of disadvantages relating to informational overload (Poursabzi-Sangdeh et al. 2021), reduced user trust (Erlei et al. 2020), and overreliance (Bussone et al. 2015). Moreover, explanations that are unstable and sensitive even to small perturbations to inputs have the potential to mislead human users into trusting a problematic black box, for example, by selectively providing explanations that conceal biased behaviors and malfunctions (Kaur et al. 2020, Lakkaraju and Bastani 2020). Hence, explanations may be a security concern if adversaries use perturbations of inputs and model attributes to produce intentionally misleading explanations that manipulate users’ trust and behaviors (Ghorbani et al. 2019). We complement this pivotal and insightful work by examining the impact of contemporary XAI on users’ situational information processing and mental models. Understanding how the provision of explanations about the workings of ML-based AI systems may reshape these cognitive processes is pivotal for anticipating the downstream consequences of this technology on human societies and designing effective transparency and explainability regulations.
The second set of literature we complement explores the mechanisms of learning in socio-technological environments. A common theoretical foundation builds on Bayes rule as a rational benchmark of how humans accommodate new information (Holt and Smith 2009). However, research has shown systematic deviations from Bayes’ rule. Reasons include over- or underweighting of new information (Rabin and Schrag 1999) and a general tendency to asymmetrically discount information conflicting with prior beliefs while readily internalizing confirming information (Yin et al. 2016). We complement this research stream by showing how human users deviate from Bayes rule in the context of learning from modern AI systems. Notably, there exists a limited number of prior research examining how black box predictions change users’ decision-making habits (Abdel-Karim et al. 2020, 2022; F¨ugener et al. 2021a, b; Jussupow et al. 2021). Relatedly, in a formal model, Agrawal et al. (2019) show that the predictions of black box AI systems can alter users’ abilities by providing them with incentives to learn to assess the (negative) consequences of their actions for the task supported by the AI.3None of these studies, however, examines the role of feature-based explanations in learning, which could pave the way for more fundamental changes in the way users understand real-world processes. Our paper intends to fill this gap. We study how the provision of explanations about how an AI system solves prediction tasks allows users to integrate the presented machine knowledge into their mental models, that is, learn from XAI. A better understanding of how explainability may contribute to machine teaching, the notion that AI systems first learn novel knowledge that experts neither conceive nor anticipate from data and then transfer this knowledge to human users (Abdel- Karim et al. 2020), is particularly significant given the growing requirements to implement explainability methods when using AI systems.
The third stream of literature we add to studies how humans collaborate with computerized systems to solve

problems. Previous research in this area dates back decades. Several studies document that humans resist using computerized decision aids, despite possible performance benefits (Kleinmuntz 1990), whereas others find that humans possess a strong preference for using them (Dijkstra 1999). With the growing employment of modern AI systems in a broad range of domains, the examination of human-machine collaboration has seen a considerable resurgence, for example, in the domain of finance (Ge et al. 2021), medicine (Jussupow et al. 2021), customer service (Schanke et al. 2021), and on- demand tasks (F¨ugener et al. 2021a). Research on “centaur” systems (Goldstein et al. 2017, Case 2018) documents how hybrid human-AI systems (i.e., centaur systems) achieve superior results in comparison with the entities operating independently (Dellermann et al. 2019, Tschandl et al. 2020), promising considerable benefits from successful human-AI collaboration. Several factors moderate the interaction of humans and AI systems including the perceived subjectivity of the task (Castelo et al. 2019, Logg et al. 2019), seeing the system err (Dietvorst et al. 2015), being able to modify predictions (Dietvorst et al. 2018), the divergence between actual and expected predictive performance (Jussupow et al. 2020), and, most importantly for our research, understanding the system’s internal logic (Gregor and Benbasat 1999, Hemmer et al. 2021). Following our conjecture that explanations pave the way for AI systems to affect people’s cognitive processes, contemporary XAI methods introduce another layer of complexity in human-AI interaction and its success: an interaction between machine and human problem- solving strategies. Our work provides novel insights into whether and under what circumstances people prefer to rely on their own way of leveraging information or willingly adjust it according to machine explanations. In this sense, our work contributes to the literature on (hybrid) human-AI collaboration by analyzing the underlying cognitive processes that may facilitate or hinder the realization of the promise of this technology.
3.Empirical Studies
We now present the design and results of Studies 1 and 2. In both studies, participants made decisions under uncertainty (providing loans and predicting apartment listing prices) either with the aid of an opaque AI, an explainable AI, or without any support. We paid participants according to their decision-making performance to reveal actual preferences and beliefs.4We implemented both studies using oTree, Python, and HTML and ran them online. In Study 1, we recruited 607 participants on Prolific and let them engage in deliberately abstract investment games (Berg et al. 1995). Results allow us to observe how the provision of explanations on top of predictions shapes information processing and mental models for laypeople in a very general sequential transaction domain. Study 2 extends the first study by testing the generalizability of mental model adjustments regarding the task domain (listing price predictions), decision-maker expertise, and the explanation presentation, and elaborates on important asymmetric effects. With the help of our industry partner, the Real Estate Association Germany (IVD), we recruited 153 experts from the real estate industry to participate in Study 2. We report the designs and results of the two studies consecutively. Figure 1portrays an overview of the experimental designs.
3.1.Study 1
3.1.1.Design.
In Study 1, participants repeatedly engaged in one-shot investment games (Berg et al. 1995) that possess the following structure. An investor receives 10 monetary units (MU). The investor initially observes 10 deliberately abstract borrower characteristics and decides whether to invest her 10 MU with the borrower. If she does not invest, the game ends without the borrower making a decision, and both the investor and borrower earn a payoff of 10 MU. If she invests, the borrower possesses 30 MU and can keep the whole amount without repercussions. Crucially, the borrower can repay the

investor 10 MU, thereby reciprocating the investor’s initial trust. In case of repayment, the investor receives 20 MU (we double the amount); otherwise, the investor earns 0 MU while the borrower gets 30 MU. The borrower, in the absence of sufficiently strong social motives, for example, altruism, egalitarian concerns, or moral preferences (Miettinen et al. 2020), will not make a repayment and maximize his personal income. As a result, the payoff structure of the investment game is of an adversarial nature from the investor’s perspective because her material well-being is at the mercy of the borrower if she invests. The investor loses her initial investment of 10 MU whenever the borrower pursues pure income- maximizing or adversarial motives like wanting to minimize the investors’ payoffs. Given this payoff structure, an income-maximizing investor in the experiment will only invest if (i) her belief that the borrower’s motive leads him to repay her is sufficiently strong, and (ii) she ultimately judges that the prospect of doubling her income is worth risking the loss of her investment.5Study 1 participants always played as investors. Borrowers are subjects from a previous incentivized field study who had to decide on repayment assuming an initial investment; that is, they have already committed to a repayment decision and cannot strategically change this choice ex post. We did not provide intermediary feedback to prevent the development of idiosyncratic expertise, experience, or investment strategies that may confound our results. We randomly matched investor and borrower decisions to determine game outcomes at the end of the study and pay both according to the earned MU.
Study 1 comprised a baseline (AI) and a treatment (XAI) condition, each with three stages.6In Stage I, each participant made 10 investment decisions for distinct, randomly drawn borrowers without intermediary feedback. They always observed the 10 characteristics of a borrower and did not obtain any aid. The idea is that the 10 borrower characteristics allow investors to get an idea of the likelihood that an individual borrower will make a repayment, for whatever motives, and to assess whether it is worth taking the risk of losing their investment. We deliberately chose 10 unintuitive traits correlated with a person’s repayment inclination so that participants did not possess strong prior beliefs about the informativeness of characteristics for someone’s repayment behavior (see Table 4 in the online appendix). The main reason for choosing just these characteristics is that previous empirical tests have shown that they are appropriate features for developing an AI system that accurately predicts repayment with which participants interact in Stage II. Importantly, participants learned that the AI system makes predictions based on the same 10 borrower characteristics they also observe, mitigating concerns that they believed the AI system to have access to more information.
Stage II introduced our treatment variation. Participants made 20 decisions for new random borrowers observing all 10 borrower traits. Additionally, baseline participants saw an AI system’s prediction about whether borrowers will repay an initial investment. Again, we did not provide intermediary feedback. We trained the AI system on 1,054 distinct data points collected in a previous field study, the same data set that the borrowers that participants encounter in the experiment stem from (see the online appendix for details).7The system did not continue to learn during the experiment. Treatment participants, on top of predictions, observed LIME explanations (Ribeiro et al. 2016) for each borrower characteristic, informing them of its contribution to the repayment prediction. Revealing LIME values on top of identical predictions constituted the treatment variation. As is often the case, we depicted LIME values graphically using colored bars of different lengths. Participants received detailed information about the model, input features, performance on a representative test set, and how to interpret LIME explanations.
Stage III perfectly mirrored Stage I. Importantly, participants engaged with the same borrowers from Stage I in random order. We did not draw participants’ attention to this fact to alleviate concerns about the experimenter’s demand effect. The study concluded with a brief questionnaire on socio-economic control variables.
3.1.2.Results.
Throughout our analyses of Study 1, we mainly rely on the following regression model:Yijs�β1·Xj+β2·(Xj×Is)+β3·(Xj×Expli)+β4·(Xj×Expli×Is)+γijs+ɛ:(1)
Yijs is a dummy indicating whether participant i invested with borrower j in Stage s. Hence, β coefficients measure variation in the probability to invest with a borrower, and Xj is a vector reflecting the 10 observed borrower traits, the overall prediction, and LIME values.8Most relevant to our analyses, Is and Expli are dummy variables, respectively, indicating whether a decision takes place in Stage s compared with Stage I (i.e., Stage I serves as the reference category) and whether participant i is in the XAI treatment (observes explanations on top of predictions in Stage II), and γis represents individual-state fixed effects. We report standardized regression coefficients with robust standard errors. Our main interest lies in the interaction terms β3 and β4, respectively, capturing the isolated effects of observing the prediction and additionally observing LIME explanations. As β4 constitutes a difference-in-difference (DiD) estimator, it is pivotal to check that before the intervention, there are no treatment differences (parallel trends assumption). Regression analyses reveal that baseline and treatment participants in Stage I did not place significantly different weight on any trait; hence, the use of a

ertheless, because participants placed significant weight on Gender, Conscientiousness, Neuroticism, and Younger Siblings in only one of the two conditions, there is still some concern about the appropriate interpretation of DiD estimates for these traits.9To avoid drawing incorrect conclusions, we conservatively refrain from interpreting these traits’ estimates while still including them as controls in the model.
3.1.2.1.Situational Information Processing.
We start analyzing how participants’ weighting of borrower characteristics changed from Stage I to II, that is, changes in participants’ situational information processing. Figure 2illustrates our results. Figure 2(a) depicts the average LIME values (color saturation) participants observed for different feature values (y and x axis). Higher positive (negative) LIME values depict a higher positive (negative) contribution of a given feature value to the predicted probability that a borrower makes a repayment. Figure 2(b) portrays how the provision of predictions and explanations affected the weighting of a given borrower trait. The diamond marker represents the original weighting in Stage I (β1). The dashed and solid arrows, respectively, illustrate the isolated effects of observing predictions (β3) and additional explanations (β4). Depicted results stem from regressions reported in Table 9 in the online appendix.
There are two main insights. First, prediction effects in Figure 2(b) suggest that the provision of opaque predictions generally decreased the weight participants placed on observed borrower traits. On average, the absolute magnitude of coefficients changed by 63.6%. Although only the estimates for Agreeableness, Patience, and Older Siblings are significant, predictions reduced the absolute magnitude of all variables. Second, the provision of explanations on top of predictions entailed significant weight changes that mirror the relationship between borrower traits and repayment behavior as depicted by the LIME values. Here, the average magnitude of absolute weight changes equals 73.9%. Figure 2(a) shows that the predicted repayment probability markedly decreases (increases) with a borrower’s level of Competitiveness (Patience). Figure 2(b) reveals that these are the two traits whose weighting the provision of explanations significantly fostered: observing explanations rendered the relationship between a borrower’s Competitiveness (Patience) and a participant’s investment likelihood significantly more negative (positive). Notably, explanations as such increased the absolute magnitude of the coefficient for Competitiveness (Patience) by 240.0% (94.6%). LIME values reveal that Agreeableness, the trait participants initially weighted the most, has almost no impact on the repayment prediction. Accordingly, we find that the provision of explanations led to a significant decrease in the magnitude of the weight participants placed on this trait (�44.7%). Additional analyses confirm that LIME values for these three characteristics had a significantly positive influence on participants’ investment decisions, corroborating the notion that participants paid attention to and adjusted their weighting of traits according to observed explanations (see Table 11 in the online appendix). Taken together, participants significantly adjusted their weighting of information in the direction of observed explanations for (i) the trait they initially perceived as most important and (ii) the traits LIME highlighted as most important.10Finally, although not shown in the Figure 2for ease of interpretation, regression analyses further reveal that explanations significantly reduced the weight participants placed on the prediction as such (magnitude of coefficient decreased

by 26.8%); that is, they were less likely to follow a prediction that a borrower makes a repayment.11
Result 1.1.
Observing explanations changed participants’ situational processing of the overall prediction and borrower traits that explanations or they themselves consider most important. The direction of adjustments mirrors explanations.
Result 1.1agrees with our theoretical elaborations: People adjust their situational information processing in response and according to explanations they currently observe. Notably, elicited expectations about the prediction accuracy did not differ significantly for predictions with or without explanations (71.8% and 70.6%, respectively; p �0.751, Wilcoxon rank-sum test). Therefore, changes in the weighting of predictions do not seem to result from lower performance expectations. Next, we test the conjecture that explanations affect beliefs about the relationship between borrower characteristics and repayment behavior, that is, mental models.
3.1.2.2.Mental Model Adjustments.
We compare participants’ information weighting across Stages I and III to test the conjecture that explanations affect mental models about the relationship between borrower traits and repayment behavior. We rely on the regression model (1), setting s �3 and excluding controls for the prediction and LIME values. Figure 3illustrates regression results that we report in Table 12 in the online appendix.
Figure 3portrays how the provision of predictions and explanations lastingly changed the weighting of a given borrower trait across Stages I and III, where participants had no (X)AI aid. The diamond marker depicts the original weighting in Stage I (β1). The dashed and solid arrows, respectively, show how having observed predictions (β3) and explanations on top of predictions (β4) did fundamentally alter participants’ information processing, that is, mental models.
Observing opaque predictions did not result in a significant change in participants’ weighting of borrower traits. By contrast, depicted results suggest that providing explanations did entail an adjustment of mental models with the absolute magnitude of coefficients changing by 61.8% on average. Importantly, this adjustment was asymmetric. Observing explanations led participants to place significantly more weight on borrowers’ Competitiveness (+148.6%) and Patience (+59.4%) in Stage III than in Stage I. The weight changes again mirror the observed LIME explanations. After observing explanations that the AI system places the most weight on borrowers’ Competitiveness and Patience, participants increased their weighting of these attributes even for investment decisions where they no longer observed explanations. Intriguingly, we do not find that explanations about the low relevance of Agreeableness led participants to adjust their marked weighting of this trait significantly. Although participants weighted Agreeableness significantly less while observing explanations, they returned to their original weighting of it once they lost access to the XAI system. Naturally, one may wonder about this asymmetry’s origins. One plausible interpretation is that explanations are less likely to evoke pronounced mental model adjustments when they conflict with strong preconceptions. Put differently, people are more inclined to engage in mental model maintenance rather than building because it is less cognitively demanding and creates less psychological distress (Vandenbosch and Higgins 1996). In Stage I, participants put by far the most emphasis on a borrower’s Agreeableness to decide on investing. LIME values, however, suggested that this conception is incorrect because it is among the least relevant predictors for borrowers’ repayment inclination. Although one would expect that participants engaged in mental model building to reshape their beliefs about the relationship between Agreeableness and repayment behavior, we do not find significant adjustments. For Competitiveness (Patience), explanations depicted an important negative (positive) influence, which, given their initial weighting of it, confirmed participants’ prior beliefs. Following the Mental Models framework, confirming explanations should evoke the maintenance or reinforcement of prior beliefs. Given the significant explanation effects, it seems that participants willingly engaged in this process. This inclination to engage in mental model maintenance rather than building more generally concurs with the frequently documented confirmation bias (Yin et al. 2016), that is, the tendency to selectively process information in a way that allows for the continuation or strengthening of beliefs. We elaborate on this issue in Study 2 and the discussion

Result 1.2.
Machine explanations entailed asymmetric mental model adjustments. Participants reinforced priors that explanations confirmed but did not abandon priors that explanations markedly contradicted.
3.1.2.3.Investment Performance.
Thus far, it remains open how providing explanations on top of predictions affected participants’ decision-making performance in our setting. Table 1summarizes participants’ performance measured by the accuracy (share of payoff maximizing decisions) and recall (share of investments with repaying borrowers). We also report p values of F tests to illustrate significant treatment differences.13
Although there are no differences in Stage I, treatment participants performed significantly worse than baseline ones in Stage II (�8.9% and �11.0% for accuracy and recall, respectively).14Treatment participants’ relatively lower performance in Stage II stems from not investing with the most competitive borrowers (with most negative LIME values), whereas the overall prediction implies doing so, that is, from overruling positive predictions.15
They overruled positive predictions and refrained from investing in 46.5% of these cases, resulting in a decision accuracy of merely 53.5%. Baseline participants, for most competitive borrowers, overruled positive predictions only in 21.2% of the cases and achieved a decision accuracy of 78.9%; that is, they are 47.5% more likely to make an income maximizing decision than treatment participants. For all other borrowers, treatment (baseline) participants overruled positive predictions and made optimal decisions in 23% (19.4%) and 69.6% (71.1%) of the cases, respectively. Hence, treatment participants seem to have placed too much weight on very high competitiveness, leading them to overrule the overall prediction inefficiently often.
Examining Stage III, we find that this overweighting of the highest competitiveness level persisted even when participants did not observe explanations anymore (see Table 13 in the online appendix). In Stage III, treatment (baseline) participants invested with most competitive borrowers in 44.7% (54.7%, p <0.01, F test) of the cases and with other borrowers in 68.2% (67.6%, p <0.7, F test) of the cases. As a result, treatment and baseline participants respectively achieved a decision accuracy of 51.7% and 57.2% (�9.6%, p <0.01, F test) for most competitive borrowers and 59.5% and 62.8% (�5.3%, p <0.05, F test) for other borrowers. Notably, participants already associated very high competitiveness with a low repayment likelihood in Stage I: Most competitive borrowers received an investment in 56.3% of the cases, whereas all others did so in 69.5% of the cases (there do not exist treatment differences). Against this background, explanations seem to have exacerbated this inaccurate pattern16to an extent that treatment participants made significantly worse decisions than before. Put differently, confirming explanations inappropriately reinforced preconceptions about most competitive borrowers not repaying an investment in our setting.
Result 1.3.
Participants excessively increased the isolated weighting of a trait they already believe to be evidence against repayment. This reaction inefficiently decreased participants’ likelihood to invest with repaying borrowers that were highly competitive.
In sum, the results for Study 1 are highly consistent with the notion that the provision of explanations creates a novel channel through which AI systems may reshape users’ way of processing information, both situationally and more permanently. For the latter effect, we observe an asymmetry that is reminiscent of a confirmation bias and, in our setting, decreased participants’ decision-making performance by excessively reinforcing inaccurate preconceptions.
3.2.Study 2
The goal of Study 2 is twofold. First, we extend Study 1 results by testing the generalizability of mental model adjustment findings regarding the task domain, user expertise, and explanation presentation and examining whether the asymmetry we found for explanation-driven mental model adjustments in Study 1 is indeed a manifestation of the confirmation bias. Second, we explore if mental model adjustments spill over to related but disparate domains.

3.2.1.Design.
Study 2 comprises four consecutive stages, where recruited real estate experts estimated the listing price per square meter in Euros of apartments that we previously collected from a large online platform.17Participants saw 10 apartment characteristics to make an informed guess and did not receive intermediate feedback. To reduce the task complexity and avoid informational overload, we fixed seven apartment characteristics across all stages, that is, apartments only differed regarding the same three characteristics: Location (Frankfurt/Cologne), Balcony (Yes/No), and Green voter share in the district (Below city average/City average/Above city average).18We provide screenshots of the interfaces from each stage in the online appendix.
In Stage I, we elicited participants’ initial beliefs about the relationship between the three variable apartment characteristics and listing prices. Participants estimated the listing price of four random apartments with different combinations of the variable attributes by entering their marginal contributions to the price using a slider. Sliders ranged from minus to plus 2.500e in steps of 50e. We initially set the marginal contributions and overall price estimation to 0e and the average listing price (9,600e), respectively. Participants additionally stated their confidence in the entered marginal contributions and the resulting price estimation on a five-point scale.
Stage II introduced our treatment variations. In all variations, participants estimated listing prices for eight random apartments with different combinations of variable attributes they did not encounter in Stage I. In contrast to Stage I, participants directly entered the estimated listing price. As a reference point, they again observed the average listing price for an apartment. Participants stated their confidence on a five-point scale. In our baseline condition (NoAid), participants estimated the price without any aid. Participants in the AI condition observed opaque listing price predictions of a steady, that is, nonlearning, AI system trained on 4,975 collected observations.19In our XAI condition, in addition to observing these predictions, participants also saw numerically presented SHAP values for the three variable apartment characteristics, that is, marginal contributions to the prediction in Euros. After they entered all eight listing price estimates, participants in treatments with decision support filled out a survey containing items on their trust, degree of reliance, and perceived transparency of the AI system (and explanations).
Stage III replicated Stage I to measure posterior beliefs. Independent of the condition, participants again made decisions without any aid for the same apartments.
Finally, in Stage IV, participants estimated the listing price for one last apartment without any decision aid. Across participants, we varied the balcony and green voter attribute of the apartment, whereas the seven fixed attributes were identical to the previous listings. Most importantly, the apartment was in a midsize city in eastern Germany (Chemnitz). For historical, demographic, and socioeconomic reasons, Chemnitz is very different from “A-cities” such as Frankfurt and Cologne, so the housing market is also very different. Germans in general and real estate agents in particular are usually aware of this East-West disparity.20The study concluded with a questionnaire on participants’ socio- demographics.
3.2.2.Results.
We report our results in three steps. First, we outline the experts’ belief adjustments from Stage I to Stage III. Second, we examine the occurrence of confirmation bias in these adjustment processes. Finally, we analyze experts’ listing price estimates in Stage IV.
3.2.2.1.Mental Model Adjustments.
Figure 4shows the distribution of absolute differences between experts’ beliefs about the marginal contribution of the three variable attributes before and after the treatment intervention. We show results for the NoAid, AI, and XAI conditions. The distributions for the NoAid and AI conditions are remarkably similar and skewed toward zero, indicating that experts frequently did not adjust beliefs. The distribution for XAI participants is considerably less right-skewed; that is, they adjusted their beliefs across Stages I and III more. On average, NoAid, AI, and XAI participants’ absolute belief adjustments equaled 166.4e, 165.4e, and 299.1e, respectively. Only the differences between NoAid versus XAI (+79.7%, p <0.01, F test), and AI versus XAI (+80.8%, p <0.01, F test) conditions are statistically significant (see Table 24 in the online appendix), that is, observing explanations led to remarkably stronger adjustments of beliefs. Our notion is that real estate experts updated initially held mental models about the relationship between apartment attributes and listing prices as they encountered SHAP explanations. Contrasting our first study, we directly measure participants’ prior and posterior beliefs about the contribution of distinct apartment characteristics to listing prices in Study 2. This design facet enables us to estimate mental model adjustments directly, leveraging the accepted framework by DeGroot (1974). Specifically, we assume that agent i’s posterior belief about the relationship of characteristic j and the listing price Posti,j�ai,j·Priori,j+(1�ai,j)·Expli,j is a weighted combination of the corresponding prior belief Priori,j and the personally observed explanation Expli,j; 1�ai,j represents the extent of belief adaptation in the direction of the explanation, whereas ai,j describes the anchoring of the previous belief. For instance, in the extreme case of 1�ai,j�1, individual i completely abandons her prior mental model and adopts the observed explanation as her new one. We estimate the weights (1�ai)and ai for our three study conditions using a
regression model comprising treatment interactions that has the following form:Posijk�β1·Priijk+β2·(AIi×Priijk)+β3·(Expli×Priijk)+β4·SVij+β5·(AIi×SVij)+β6·(Expli×SVij)+γi+δk+ɛ:(2)
The variables Posijk and Priijk, respectively, represent expert i’s posterior and prior beliefs about attribute j’s contribution to apartment k’s listing price in Euros. Most importantly, AIi is a dummy variable indicating that expert i observed a prediction, whereas the dummy Expli equals one if a participant additionally observed explanations; SVij represents the average SHAP value for apartment attribute j of the apartments participant i encountered in Stage II; and γi and δk are expert and apartment controls, respectively.
On an individual level, Model (2) estimates how observed SHAP values affected participants’ adjustments of beliefs about the relationship between a given characteristic and the listing price. It enables us to quantify the “stickiness” of prior beliefs (β1�β3) and “gravitational pull” of explanations (β4�β6) and directly test the occurrence of confirmation bias. Importantly, this estimation is only possible for Study 2, where we elicited prior and posterior beliefs about distinct feature-label relationships. In Study 1, we measured the ultimate investment decisions only and observed belief changes indirectly through changes in those decisions. As a result, we cannot individually quantify the impact of observed explanations on specific beliefs nor can we analyze confirmation bias: a key contribution of our second study.
Table 2depicts regression results for Model (2). Results show that in our NoAid and AI conditions where participants did not observe explanations, SHAP values (unsurprisingly) have no significant explanatory power regarding posterior beliefs (see β4 and β5).21When participants did not obtain machine aid or only observed predictions, their prior and posterior beliefs were more than 60% positively correlated (β1 and β2); that is, participants barely adjusted their beliefs. Only when participants observed explanations in addition to predictions did the displayed SHAP values have positive, statistically significant effects. β6 reveals that XAI participants significantly adjusted their beliefs in the direction of observed explanations. According to the estimate, posterior beliefs resembled SHAP values more closely in the XAI treatment condition compared with the NoAid and AI conditions (approximately +25 percentage points). Observing explanations also caused XAI participants’ posterior beliefs to resemble their prior significantly less (β3), that is, prior beliefs became less “sticky” compared with the NoAid and AI conditions (approximately �25 percentage points). In sum, these results suggest that observing SHAP explanations led participants to adjust their beliefs in the direction of explanations and abandon their priors. This insight corroborates our Result 1.2in Study 1 on an individual level, revealing that explanation-driven mental model adjustments also occur for experienced experts, who are arguably familiar with apartment traits and listing price predictions.22
3.2.2.2.Confirmation Bias.
In Study 1, we observed asymmetric mental model adjustments that are reminiscent of the confirmation bias. The design of Study 2 allows us to test for confirmation bias in mental model adjustment processes more directly by examining whether XAI participants’ adjustments depended on the alignment of explanations and prior beliefs.
We define that explanations confirmed an expert’s preconception about the price contribution of a specific apartment attribute if the prior and the observed average SHAP value for the corresponding attribute have the same sign. With this definition, observed explanations confirm prior beliefs in 49.6% of the cases.23We analyze differences in belief adjustments with respect to confirming and conflicting explanations using a modified version of Model (2). Specifically, we are interested in whether the convergence of XAI participants’ posterior beliefs toward observed SHAP values only occurred when explanations confirmed prior beliefs. Therefore, we focus on the subsample of XAI participants allowing us to omit treatment dummies and interaction terms which facilitates the interpretation of results. Along the lines of Model (2), we regress XAI participants’ posterior beliefs about the relationship between apartment characteristics and the listing price on their prior beliefs and observed SHAP values. Most importantly, we now add a dummy variable (Confirm) indicating whether explanations confirmed prior beliefs and its interaction with average SHAP values and prior beliefs as independent variables. The interaction Avg. SHAP ×Confirm will provide insights into whether the influence of observed SHAP values on belief adjustments depended on the alignment of explanations and prior beliefs, which are insights we cannot obtain from Study 1 using Model (1).
Corroborating our interpretation of Result 1.2from Study 1, we find that explanation-driven belief adjustment processes depended on whether explanations confirmed or conflicted with prior beliefs. The estimate for the interaction term Avg. SHAP ×Confirm is positive and statistically significant (see column (1) in Table 3). Following the estimate, posterior beliefs resembled observed SHAP values significantly more closely (about 50% more) if they confirmed their prior beliefs. Hence, consistent with confirmation bias, the belief adjustment was asymmetric regarding the confirmatory nature of explanations. If participants had updated beliefs rationally according to Bayes rule, the interaction term should be insignificant as Bayesian observers would not weight explanations conditional on their alignment with prior beliefs (Rabin and Schrag 1999).
To elaborate on the notion that these asymmetric belief adjustments are a manifestation of confirmation bias, we further consider the role of experts’ confidence in their prior beliefs. Prior research shows that confirmation bias is strongest for entrenched beliefs (Pyszczynski and Greenberg 1987, Knobloch-Westerwick and Meng 2009). To test the existence of such heterogeneity, we consider experts’ reported confidence in prior beliefs and define that an expert possessed low (high) confidence in a prior, if, on a five-point scale, they reported a confidence level of less than 4 (at least 4). In columns (2) and (3) of Table 3, we, respectively, repeat the regression analysis reported in column (1) for the subsamples of low- and high-confidence prior beliefs.
Reported estimates provide further evidence that explanation-enabled mental model adjustments were subject to confirmation bias. According to the estimated coefficient of Avg. SHAP ×Confirm, for low-confidence priors, the influence of observed SHAP values on posterior beliefs did not depend on whether explanations confirmed prior beliefs (see column (2)). Considering the positive and significant estimate of Avg. SHAP, the belief updating was in line with Bayes rule. By contrast, for high-confidence priors, belief adjustments were highly sensitive to whether SHAP values confirmed priors (see column (3)). The estimate for Avg. SHAP ×Confirm suggests that the magnitude of the adjustment of high-confidence priors was about two times larger when observed explanations were in line with them.
Result 2.1.
Study 1 findings extend to expert users, SHAP explanations, and the domain of apartment price predictions: SHAP explanations led real estate experts to adjust prior beliefs about the relation between apartment attributes
and listing prices. Adjustment processes were subject to the confirmation bias.
3.2.2.3.Spillover Effects.
Although we observe that real estate experts (asymmetrically) adjusted prior beliefs, all previously reported results pertain to the same market: Participants observed SHAP explanations for the same two A-cities in Western Germany, for which we elicited prior and posterior beliefs. What remains open is whether explanation-driven belief adjustments spilled over to the listing price estimation for apartments in different markets. We put this idea to the test by examining the distribution of participants’ final price predictions for an apartment in a medium-sized eastern German city that is not an “A city”: Chemnitz.24
Figure 5shows the distribution of listing price estimates conditional on the share of green voters in the district for NoAid, AI, and XAI participants. The results indicate that observing explanations impacted participants’ price estimates for Chemnitz apartments in neighborhoods with high and low proportions of green voters. Figure 5(a) shows that the distribution of listing prices for an apartment in a district with a low green voter share is considerably more right-skewed for XAI than NoAid or AI participants; that is, they estimate relatively low prices more frequently. NoAid, AI, and XAI participants on average estimated a listing price of 4,752e, 5,141e, and 3,140e, respectively. Only the differences between NoAid versus XAI and AI versus XAI are statistically significant in regression analyses (p <0.05, F test, for both). The distribution of price estimates in districts with high shares of green voters has a stronger left-skew for XAI participants than their NoAid and AI counterparts (Figure 5(b)). On average, NoAid, AI, and XAI participants estimated a listing price of 5,231e, 4,600e, and 6,092e, respectively, for an apartment in a district with a high percentage of green voters. Again, we only find significant explanation effects (p <0.1, F test, for both). These results reveal the economic significance in the changes of price distributions. Specifically, compared with observing no predictions (opaque predictions), observing explained predictions decreased Chemnitz price estimates by 33.9% (38.9%) if the share of green voters was low and increased price estimates by 16.5% (32.4%) if the share of green voters was high. As one might expect, the direction of the difference in experts’ evaluation of the green voter share attribute is in line with explanations observed in Stage II: SHAP values indicated that in Frankfurt and Cologne, a high (low) share of green voters marginally contributes to listing prices by about +652e (�613e). We do not find any effect for experts who only observed opaque predictions in Stage II.
To elaborate on these findings, we also perform a median split and analyze the subsamples of experts whose average absolute belief adjustment for the attribute “Green voter” is below and above the median. Consistent with the idea that belief spillover effects drive differences in listing price estimates in Chemnitz, experts who strongly adjusted their beliefs about the relevance of “Green voters” from Stage I to III drive our aggregate-level results. We do not find significant treatment differences in the accuracy of participants’ listing price estimates as measured by the absolute deviation from actual prices. Nevertheless, our results show that using XAI as a decision support tool in one market can affect aggregate listing prices in another market in an economically considerably way (average absolute change: approximately 20%), which is not the case for opaque systems. This result demonstrates that XAI methods can link disparate decision-making tasks.
Result 2.2.
Pronounced explanation-driven belief adjustments spill over to experts’ listing price estimation in a fundamentally different market.
In summary, our results from Study 2 (i) demonstrate the robustness of our results from Study 1 on mental model adjustments in terms of system user expertise, explanation representation, and decision domain; (ii)
provide strong evidence that explanation-driven mental model adjustments are subject to confirmation bias; and (iii) show that explanation-driven mental model adjustments generate significant spillover effects.
4.Discussion and Conclusion
We report results from two empirical studies that provide novel insights into the interplay between the use of feature-based XAI methods and users’ cognitive processes. Our main contribution is the identification of considerable side effects of providing feature-based explanations, the most popular form of XAI methods, on users’ situational information processing and mental models. We find that the latter effect (i) is subject to the confirmation bias so that misconceptions can persist and even accumulate, possibly leading to suboptimal decisions, and (ii) can create spillover effects into other decision domains. These overarching results suggest that the growing, partially legally required, use of feature- based XAI methods opens a new channel through which AI systems may fundamentally reshape the way humans understand real-world relationships between features X and target variables Y. In the following, we discuss our results, present implications for organizations and society, and, based on the limitations of our studies, provide directions for future research.
4.1.Discussion of Results
Study 1 demonstrates that the provision of explanations can situationally lead lay users to adjust their weighing of features accordingly, the average absolute change in estimates equals 73.9%, and to put less emphasis on the overall prediction (�26.8%). Explanations also evoked asymmetric changes in lay users’ conceptions about the relationship between borrower traits and repayment inclinations that influence behaviors even when they do not observe explanations anymore, the average absolute change in estimated coefficients equals 61.8%; that is, explanations affect mental models. Explanation-driven effects decreased lay users’ decision-making performance in our setting. Compared with opaque predictions, explanations decreased participants investment performance by 8.9% while observing them and by 9.8% even when not observing explanations anymore. Study 2 extended these results in three ways. First, we find that even expert users in a considerably more applied domain adjusted mental models by about 25 percentage points. Second, results indicate that asymmetric mental model adjustments were a manifestation of the confirmation bias because posterior beliefs resembled observed explanations about 50% more closely if explanations confirmed prior beliefs. Third, Study 2 reveals that mental model adjustments created spillover effects leading to an average absolute change in apartment price estimates for a different market by approximately 20%.
From a theoretical perspective, our results contribute to our understanding of the role of popular XAI methods in effective knowledge transfers from ML-based AI systems to human users. A key promise of modern AI systems is that the application of ML techniques will discover new knowledge from Big Data that has previously eluded even experienced experts (Berente et al. 2021, van den Broek et al. 2021). This “machine knowledge” is typically codified in the form of a complex predictive model that outperforms humans. We show that providing predictions alone is insufficient to achieve systematic knowledge transfers from AI systems to human users. In both our studies, neither laymen nor experts adapted their understanding of the relationships between features X and label Y according to “machine knowledge” when observing only opaque predictions. Merely in treatments where users also had access to explanations, they began to adapt their approach to solving the task so that it more closely matched the strategy of the AI system. Therefore, XAI methods appear to be a pivotal factor contributing to an effective channel through which AI systems can pass on their self-learned knowledge to human users. Crucially, feature-based XAI methods seem to induce an asymmetry in mental model adjustments: users adjust their beliefs more in the direction of observed explanations if they confirm rather than disconfirm their priors. This asymmetry contradicts with the updating behavior of a Bayesian observer who would neither over- nor underweight explanations conditional on them confirming or disconfirming prior beliefs. This asymmetry occurred regardless of whether we provide graphically visualized LIME or numerically represented SHAP explanations. It therefore seems as if additive feature-based explanations more generally evoke cognitive processes leading users to learn from the machine selectively. Researchers across disciplines commonly refer to such an asymmetry as confirmation bias (Yin et al. 2016). Study 2 provides consistent evidence that explanation-driven knowledge transfers from an AI to a human similarly suffer from confirmation bias as knowledge transfers in the human-to-human domain. For example, confidence in prior conceptions and their difference from the new information moderate confirmation bias (Pyszczynski and Greenberg 1987). Similar to learning from other humans, users seem unwilling to internalize potentially helpful, XAI-channeled machine knowledge if it is inconsistent with what they already, perhaps incorrectly, believe to be true. From the perspective of the Mental Models framework, individuals more frequently engage in maintaining rather than in building mental models of the relationships between features and labels. One reason for this effect could be the need to attain or maintain a high level of self-esteem (Klayman 1995), leading users to focus inappropriately on explanations that make them feel competent. In other words, they may derive
(Gilad et al. 1987). From this perspective, people may misuse the XAI as a tool to enhance their self-esteem. If left unaddressed, the asymmetric adaptation of mental models by humans may prevent modern (X)AI applications from fulfilling their promise of making humans smarter, which (ironically) may also hinder the further development of AI applications by humans.
Interpreting our results in the light of the model by Agrawal et al. (2019) yields another theoretical insight regarding the ramifications of XAI. Our results indicate that users’ willingness to follow XAI predictions depends on whether the explanations conform with their mental models. One way to rationalize this behavior is that their objective function includes a component that accounts for experiencing some positive (negative) intrinsic utility when obtaining a signal that their mental model may (not) be accurate (Festinger 1962, Gilad et al. 1987, Harmon-Jones 2019). In the model by Agrawal et al. (2019), AI systems make predictions about uncertain states of the world that relate to the profitability of taking specific actions. Human users, in turn, assess the expected payoffs associated with specific actions, that is, make judgments. Our results suggest that human judgment in this model encompasses not only the material consequences of an action but also the psychological impact of receiving a signal that implicitly shows whether current mental models are correct. If explanations reveal that the AI system arrived at a prediction in a way that contradicts their held mental models, taking an action that follows this prediction effectively constitutes a signal to oneself that the current mental model is incorrect, creating psychological distress, for example, in the form of a cognitive dissonance (Harmon-Jones 2019). This mental toll may lead users not to follow the prediction in the first place. Conversely, users may follow unreliable predictions more often if the explanations are consistent with their current mental models because doing so provides a psychologically valuable self-signal that they are in the right (Gilad et al. 1987). Against this background, users’ inclination to follow predictions of an XAI system, and thus their ultimate decisions and gains, is subject to greater variance than with a black-box AI. That is because users’ propensity to follow predictions depends on the consistency of the explanations with their mental models.
Another theoretical contribution of our work is to show the potential of feature-based XAI to link different decision domains by influencing users’ beliefs about the feature-label relationship. Study 2 results show that observing explanations for listing price predictions for apartments in Market A influenced the price estimation of experts in a different Market B, where the learned pattern does not exist, and they did not have access to XAI decision support. We find that listing prices estimated by experts who observed explanations differed significantly from those estimated by experts who either had no decision aid or only observed opaque predictions. This spillover effect seems to occur because of the adjustment of mental models that experts draw on in both situations. Therefore, as an unintended side effect, increasing public and private efforts to promote the use of XAI methods may extend the already significant influence of AI systems from areas where we interact with them (Rahwan et al. 2019) to areas where such systems are not in use. Feature-based XAI methods’ potential to link different domains is particularly concerning given recent evidence on their susceptibility to intentional manipulation and adversarial attacks (Lipton 2018). Many modern XAI methods, including LIME and SHAP, optimize fidelity, that is, ensure that explanations accurately mimic the predictions of the black box model. However, even small perturbations of the input data (e.g., deliberate manipulation and measurement errors) can lead to considerably different explanations for identical predictions, that is, depict different feature-label relations (Ghorbani et al. 2019, Lakkaraju and Bastani 2020). The potential instability of explanations allows manipulating user behaviors. Following our results, the creation of misleading explanations may not only affect users’ trust in the AI system (Lakkaraju and Bastani 2020) but also lead to an (asymmetric) adjustment of mental models that affect users’ decision making beyond the XAI augmented decision at hand. Specifically, the depiction of certain feature-label relationships that are not present can evoke inappropriate mental model adjustments that, given the documented asymmetry, will cause users who already believe these patterns to be true, to feel vindicated and reinforce these beliefs. In general, the documented spillover effects may magnify the reach and impact of intentional manipulations of explanations, increasing deceiving parties’ incentive to do so.
4.2.Implications
Reported results have important practical implications for organizations and policymakers. Our finding that XAI can change human thinking points to potential pitfalls for companies that want, or have to, use XAI. Consider a company that plans to implement XAI methods to explain to its employees why an AI system makes certain predictions. As Study 1 shows, providing explanations in addition to predictions may draw users’ attention excessively to the explanations, to the detriment of the prediction itself. Users may place too much emphasis on individual explanations that confirm their prior beliefs, rather than adhering to the overall prediction. As a result, employees’ decision-making performance for the task at hand may deteriorate, which is in line with evidence from related research (Poursabzi- Sangdeh et al. 2021). In domains where explanations are becoming a regulatory standard, managers need to take such potential downsides into account and contemplate
the ramifications of implementing explainability measures. Following our results, managers who, in the future, are obliged to put XAI methods in place, should not take these steps too lightly. From a business perspective, our documented downsides of explainability could render the continued use of AI-based decision support systems unattractive. Considering that AI systems are often deeply interwoven with business processes, this XAI-driven discontinuance may entail considerable organizational change. As a result, managers may be well advised to assess potential inconsistencies between the AI system’s internal logic and employees’ understanding of the task it supports before rolling out explainability measures. This puts managers in a position to evaluate the magnitude of the potential downside of explainability and use countermeasures. For example, managers may obviate confirmation bias by openly discussing explanations that conflict with employees’ mental models and showcasing arguments in support of the explanation.
Another pitfall for companies concerns the transfer of knowledge from AI systems to human users. As Study 2 shows, even experts can overgeneralize learned feature- label relationships that are only applicable in the context in which they interact with the system. With the confirmatory learning from explanations, existing differences in employees’ initial conceptions may lead to differences in how they collaborate with and what they learn from the XAI, for example, fostering the biased weighting of certain information. From this perspective, providing explanations might decrease individual level noise in the decision-making process (Kahneman et al. 2021) because individuals’ decisions become more consistent. This is in line with F¨ugener et al. (2021b), who find decisions to be increasingly consistent among users engaging with opaque predictions. On a more aggregate level, however, our results suggest that explained predictions may additionally foster differences in the decision-making process across subgroups of users that possess heterogeneous priors. As a consequence, the variation of decisions on a group level can grow. As pointed out by Kahneman et al. (2021), variation in decisions can substantially contribute to errors and ultimately harm business performance. Consider our previous example of loan officers. XAI may cause loan approval decisions to increasingly depend on the particular employee, with idiosyncratic mental models, assessing the applicant’s creditworthiness. This increase in loan approval variation may create considerable business, legal, and reputational risks. Against this background, managers should closely monitor the introduction of XAI to identify a possible increase in decision variance. For instance, managers could complement XAI with “noise audits” and the development of “reasoned rules” (as proposed by Kahneman et al. 2021) to overcome the hidden costs of XAI-driven increases in inconsistent decision making.
From a societal perspective, our results indicate that broad, indiscriminate implementation of XAI methods may create unintended downstream ramifications. Our finding that XAI can lead users to adjust mental models in a confirmatory way and carry over learned patterns to other domains may, in an extreme case, foster discrimination and social divisions. Assume all recruiters start to collaborate with an XAI system to support hiring decisions. For example, a subgroup of recruiters may discriminate against women because they believe female applicants to be less productive on the job. If the XAI (occasionally) provides local explanations that depict being female as negative evidence for high future performance, the subgroup that statistically discriminates based on gender will readily reinforce its prior belief, that is, engage in mental model maintenance. As a result, these recruiters may become more biased and less noisy in their behavior as they hire female applicants consistently less. Given the spillover effects we find, they may even carry over their strengthened conceptions about women’s productivity to other jobs, further reinforcing discriminatory patterns. Additionally, because nondiscriminating recruiters will most likely refrain from adjusting their mental model, that is, not engage in mental model building, social divisions among recruiters may develop and accumulate along the lines of gender biases. Hence, without any malicious intent, the broad use of XAI may ironically foster human discriminatory tendencies and divide social groups. Notably, with the possibility to manipulate explanations, deceiving third parties could also intentionally cause explanations to exhibit specific prediction contributions for sensitive attributes such as race, gender, or age. This effect could lead human users who already hold prejudices, stereotypes, or discriminatory tendencies to reinforce their views, which could promote certain political agendas.
4.3.Limitations and Future Research
As with any other research study, ours is not without limitations. In light of increasing regulatory requirements and private initiatives, we believe that these limitations open up fruitful avenues for future research. One limitation of our work concerns the lack of feedback on the decision outcomes and thus the performance of the AI system. In both our studies, we did not provide feedback for two reasons. First, it adds a considerable layer of complexity that impedes the measurement and interpretation of isolated explanation-driven effects on users’ cognitive processes. Second, in practice, many AI-supported decisions do not yield immediate feedback, or only yield feedback for some of the predictions. Hence, users have to interact with the system without learning its prediction accuracy, at least for a certain period. Examples include hiring decisions supported by an on-the-job performance predicting AI
system, investment decisions supported by a return predicting AI system, and drug treatment decisions supported by an effectiveness predicting AI system. Consequently, explanations may alter users’ situational information processing and mental models before feedback on system performance arrives. Nonetheless, we strongly encourage future research to examine the role of feedback as it may introduce unexpected dynamics in the cognitive effects we document. For instance, the (selective) reinforcement of their mental models through explanations, may lead users to be more forgiving and maintain trust in the AI system, even if they eventually see it making mistakes. In this way, the interaction between feedback and explanations might constitute a factor contributing to unwarranted algorithm appreciation (Logg et al. 2019), leading users to rely on incorrect outcomes blindly. Additionally, people’s adjustments of the situational information processing and existing mental models possibly depend on the extent to which the XAI system’s predictions outperform their own. If users learn that an XAI system’s predictions perform considerably better than their subjective ones, the magnitude of reported confirmation biases may vary. Conversely, when users’ predictions are better than the XAI, their confirmation bias might be even stronger. Future research could examine to what extent our reported effects, at the intensive margin, depend on users’ perceptions about differences in their own and the XAI system’s predictive performance.
Another limitation of our work originates from letting participants interact with local, feature-based XAI methods. We opted to use these explanations because they are already widely in use in practice and because there are arguments that feature-based explanations on an individual level are necessary to comply with (upcoming) regulatory requirements (Goodman and Flaxman 2017). Yet, there exist other forms of explanations, for example, global feature-based explanations or even example-based explanations. Although an investigation and comparison of the interplay between different forms of explanations and cognitive processes are beyond the scope of this paper, it is worthwhile for future research to explore whether, and if so why, the effects we document would change if users (additionally) obtain other forms of explanations. Consider, for instance, global explanations. Although local explanations help understand why an AI system produces a prediction on a case-by-case basis, global explanations reveal important high-level patterns and nonlinearities in the system’s logic. Such global explanations effectively aggregate individual-level information for the user and help to understand the system’s overall logic. By taking over this information aggregation task, global explainability could mitigate concerns about the selective processing of isolated local explanations that arguably contribute to the occurrence of confirmation bias. Additionally, the global representation may facilitate comparison and reflection processes that ultimately improves the transfer of knowledge from the AI system to the user.
4.4.Conclusion
A concluding remark is worth making. Of course, our work is not meant to be an argument, let alone a plea, against making “black box” AI systems more explainable or transparent. Instead, we comprehend our findings as a warning that the indiscriminate use of modern XAI methods as an isolated measure may lead to unintended, unforeseen problems because it creates a new channel through which AI systems can affect human behaviors across domains. The pervasive human inclination to process information in a way that confirms their preconceptions while ignoring potentially helpful yet conflicting information needs addressing if explainability is to become an effective means to combat accountability, transparency, and fairness issues without creating adverse second-order effects. For instance, one might restrict the provision of explanations of sensitive features for end users of the system and only use them to ensure the proper and unbiased functioning of the AI system during the development process. Additionally, it might be important to provide developers and data scientists with cognitive awareness trainings to make them more sensitive to their own biased mental processes.






----------------------------------------------------------------------------------






Gnewuch, U., Morana, S., Hinz, O., Kellner, R., & Maedche, A. (2023). More Than a Bot?
The Impact of Disclosing Human Involvement on Customer Interactions with Hybrid Service
Agents. Information Systems Research. https://doi.org/10.1287/isre.2022.0152

Abstract
The proliferation of hybrid service agents—combinations of artificial intelligence
(AI) and human employees behind a single interface—further blurs the line between
humans and technology in online service encounters. While much of the current debate
focuses on disclosing the nonhuman identity of AI-based technologies (e.g., chatbots), the
question of whether to also disclose the involvement of human employees working behind
the scenes has received little attention. We address this gap by examining how such a disclosure
affects customer interactions with a hybrid service agent consisting of an AI-based
chatbot and human employees. Results from a randomized field experiment and a controlled
online experiment show that disclosing human involvement before or during an
interaction with the hybrid service agent leads customers to adopt a more human-oriented
communication style. This effect is driven by impression management concerns that are
activated when customers become aware of humans working in tandem with the chatbot.
The more human-oriented communication style ultimately increases employee workload
because fewer customer requests can be handled automatically by the chatbot and must be
delegated to a human. These findings provide novel insights into how and why disclosing
human involvement affects customer communication behavior, shed light on its negative
consequences for employees working in tandem with a chatbot, and help managers understand
the potential costs and benefits of providing transparency in customer–hybrid service
agent interactions.

Keywords: hybrid service agent, artificial intelligence, human involvement disclosure, communication style, chatbot, 
human–AI collaboration, customer service

1. Introduction
With the rapid proliferation of artificial intelligence (AI)
in online service encounters, customers increasingly find
themselves interacting with chatbots instead of human
service employees (Schanke et al. 2021). However,
despite advances in AI, chatbots frequently struggle with
nonroutine questions and complex requests, causing
frustration and poor customer experience (Schuetzler
et al. 2021). To avoid these issues, firms have begun to
employ hybrid service agents: combinations of AI agents
(e.g., chatbots) and human agents (e.g., service employees)
that function as an integrated unit with a single
interface to the customer (Rai et al. 2019, Schuetzler et al.
2021).1 Their fundamental idea is to balance the complementary
strengths and weaknesses of artificial and
human intelligence by combining them such that common
questions and requests are handled by AI, whereas
the rest are delegated to a human (De Keyser et al. 2019).
Although hybrid service agents offer several advantages
over service channels operated by either humans
(e.g., live chat) or AI alone (e.g., chatbots), they further
blur the line between humans and technology in online
service encounters. Customers already struggle to determine
whether they are interacting with a human or a
chatbot (Mozafari et al. 2022); this is likely to be exacerbated in interactions with hybrid service agents, which may involve a chatbot, a human employee, or both (Grimes et al. 2021). Not only does this cause confusion and annoyance to customers, but it has also become a serious concern for firms. Recent policy initiatives aiming to protect customers from counterfeit service encounters, such as California’s “BOT bill” (State of California 2018) and the European “Ethics Guidelines for Trustworthy AI” (AI HLEG 2019), have put pressure on firms to disclose that their chatbots are not real people (Robinson et al. 2020). However, above and beyond making the chatbot identity transparent, firms that use hybrid service agents (rather than fully automated chatbots) must also decide whether to disclose human involvement or avoid transparency about the behind-the-scenes employees who step in if the chatbot is unable to respond. Although anecdotal evidence suggests that not disclosing human involvement can cause backlash from the firm’s customer base and the general public (Forbes 2020), research on the impact of disclosing it is scarce. Prior studies have focused on the impact of chatbot identity disclosure in customer–chatbot interactions (Luo et al. 2019, Mozafari et al. 2022), with little attention being paid to hybrid service agents in general (Adam et al. 2022) and the disclosure of human involvement in customer– hybrid service agent interactions in particular. Hence, an important yet largely unanswered question is whether firms should disclose human involvement in customer– hybrid service agent interactions and, if so, how such a disclosure affects not only customers but also the employees who work in tandem with the chatbot.
Against this backdrop, our aim is to empirically examine the impact of human involvement disclosure (HID) on customer interactions with hybrid service agents. Specifically, we focus on understanding whether, how, and why customers communicate differently with a hybrid service agent when human involvement is disclosed (versus not disclosed). Although communication is a fundamental part of customer–firm interactions, how customers use language to express themselves is an often overlooked aspect of customer behavior (Berger et al. 2020). What customers say and how they say it (their communication style) cannot only provide marketing insights but also affect or even disrupt service operations (Altman et al. 2021). This is therefore a crucial concern for firms that use AI-based technologies designed to communicate with customers using natural language. We also examine an important downstream consequence: how customer communication style influences the workload of human employees. A higher workload could not only affect service operations but also imply that firms are unable to fully realize the benefits of employing hybrid service agents. To this end, we conducted two experiments in which customers interacted with a hybrid service agent via chat in a customer service context. The results of our randomized field experiment indicate that customers exhibit a more human-oriented communication style when human involvement is disclosed (versus not disclosed), which in turn increases employee workload. Our controlled online experiment replicated these findings and, in addition, revealed that the effect of HID on customer communication style is driven by customers’ impression management concerns. That is, customers are more concerned about making a good impression when they know that humans are working in tandem with the chatbot, which leads them to adopt a more human-oriented communication style. We conducted various robustness checks to rule out alternative explanations for our results, such as customer perceptions of the chatbot’s capabilities and variation in employee language. Furthermore, our results were robust across different analytical approaches, across subsamples, and to the inclusion of several control variables. Finally, we performed a set of additional analyses to generate further insights into the impact of HID on other business-related outcomes, such as customer tendency to seek out human involvement and customer sentiment.
Our work offers three main contributions to information systems (IS) research. First, it contributes to the literature on the role of information technology (IT) in customer service encounters. While prior research has primarily focused on IT-based self-service (e.g., web portals, chatbots) and human-based service (e.g., phone, live chat), our study sheds light on a novel hybrid approach that combines humans and chatbot technology behind a single interface. In particular, our empirical investigation of customer–hybrid service agent interaction provides important insights into how customers respond to hybrid service interfaces that further blur the once clear lines between humans and technology in customer service encounters. Second, our research adds to the emerging literature on the use of AI in service automation by revealing how transparency about human involvement in predominantly AI-based service encounters can place additional workload on employees working in tandem with AI and ultimately undermine AI’s ability to free up employees from mundane customer service work. Third, our research extends human–computer interaction (HCI) literature by identifying impression management concerns as a key psychological mechanism in customer– hybrid service agent interactions.
2.Related Literature
Our work is informed by three research streams. First, it relates to the established stream of IS research on the role of IT in service encounters. Because of the transformation from face-to-face service encounters to IT-based self- service (e.g., web portals) and IT-mediated service (e.g., live chat), seeking to understand customer usage of online service channels, the drivers of channel choice,
and the interactions among different channels has a long tradition in IS research (Ba et al. 2010, Kumar and Telang 2012). This line of work also introduced the notion of “hybrid services” to describe combinations of IT-based self-service and human-based service (Xu et al. 2014). However, little research has examined hybrid service interfaces and how customers respond to them (Adam et al. 2022).
Second, particularly relevant to our work is the emerging stream of IS research on the use of AI to automate repetitive customer interactions and free up service employees for more value-adding tasks. One of the most prominent technologies in this domain is that of AI- based chatbots (Adam et al. 2020, Schanke et al. 2021, Han et al. 2022). Chatbots are software applications designed to interact with customers using natural language and answer their questions around the clock (Gnewuch et al. 2022). However, as chatbots often struggle with complex questions and nonroutine requests, many firms have recognized the need to keep a human in the loop (Schuetzler et al. 2021). The idea is that chatbots and employees work together in a symbiotic fashion (Jain et al. 2021), with simple questions and requests handled automatically by the chatbot while more difficult ones are delegated to a human. This has given rise to a class of human–AI hybrids (Rai et al. 2019) that we refer to as hybrid service agents: combinations of AI agents (e.g., chatbots) and human agents (e.g., service employees) that function as an integrated unit and serve customers via a single interface.
Third, our work is closely related to prior research in the field of HCI. Although little research has focused on human–AI hybrids, several studies have investigated how and why people interact differently with human versus nonhuman counterparts (e.g., chatbots). By comparing human�human and human�chatbot interactions, researchers have found that people invest greater time and effort into interactions with another human (Shechtman and Horowitz 2003), use a simpler vocabulary and more profanity when interacting with a chatbot (Hill et al. 2015), and demonstrate different personality traits depending on the identity of their counterparts (Mou and Xu 2017). The literature has identified several psychological mechanisms related to perceptions of humans versus chatbots that may help explain these differences. One key set of mechanisms is linked to the two major dimensions of social cognition: competence and warmth. Compared with chatbots, humans are generally perceived as more competent (e.g., having more expertise and knowledge) and warmer (e.g., being more empathetic, friendly, and trustworthy) (Go and Sundar 2019, Luo et al. 2019, Cheng et al. 2022, Mozafari et al. 2022). A related mechanism involves the expectations that people form before or during interactions with a human or chatbot. For example, people tend to have higher competence-related expectations of humans (e.g., regarding conversational engagement) than they do of chatbots (Grimes et al. 2021). In light of AI advances that make distinguishing between humans and chatbots increasingly difficult, recent research in this stream has focused on the impact of chatbot identity disclosure— that is, informing customers that they are interacting with a chatbot and not a human. On the one hand, studies have determined that such disclosure reduces purchase rates (Luo et al. 2019), customer trust (Mozafari et al. 2022), and service evaluation (Castelo et al. 2023). On the other hand, research has found that customers feel fooled when made to believe they were interacting with a human but find out that it was actually a chatbot (Castillo et al. 2021). Table 1presents a summary of previous studies on disclosure in customer interactions with AI-based service agents (e.g., chatbots).
Despite the considerable amount of existing research, two important gaps remain. First, empirical investigation of service encounters has predominantly focused on service channels either operated by humans alone or enabled by technology alone (Ba et al. 2010, Kumar and Telang 2012, Schanke et al. 2021, Han et al. 2022). Therefore, we know little about hybrid approaches combining employees and technology not only within the same channel but also behind a single interface (Adam et al. 2022). The growing prevalence of hybrid service agents underscores the need to investigate customer interactions with them and better understand the potential consequences for employees who step in if the chatbot is unable to respond. Second, while prior research has demonstrated the largely negative impact of chatbot identity disclosure in service encounters (Go and Sundar 2019, Luo et al. 2019, Mozafari et al. 2022), little attention has been paid to the disclosure of human involvement. This is particularly important in the context of hybrid service agents, as the more elusive nature of their interactions with customers may involve a chatbot, a human employee, or both. As summarized in Table 1, our work addresses these gaps by investigating the impact of HID on customer interactions with hybrid service agents.
3.Theory Development
Our primary focus in this research is to understand whether, how, and why customers communicate differently with a hybrid service agent when human involvement is disclosed (versus not disclosed), and how these differences affect the workload of employees working in tandem with the chatbot. Drawing on research from interpersonal communication and human–AI collaboration, we focus our theorizing on the impact of HID on one direct customer outcome—communication style— and one indirect employee outcome—workload. Furthermore, we investigate customers’ impression management concerns as the psychological mechanism underlying the effect of HID on communication style. Finally, given that
Gnewuch et al.: Customer Interactions with Hybrid Service Agents
a disclosure can occur at different points in time (Luo et al. 2019), we differentiate between disclosing human involvement before the interaction starts (up-front HID) and during the interaction when an employee steps in (step-in HID). Figure 1illustrates our research model.
3.1.Human Involvement Disclosure and Customer Communication Style
Communication style can be understood as one’s pattern of communication when interacting with another person. In text-based communication, it specifically refers to how people form a message beyond its content (Brown et al. 2016). The same content can be expressed in different ways by, for example, using fewer or more words and less or more complex language. According to audience design theory (Bell 1984), people tailor their communication style to fit the audience. For example, when addressing a child, adults tend to avoid complex formulation and vocabulary. Importantly, people use such audience design strategies not only when communicating with other humans but also when interacting with technology. For example, when addressing a chatbot, people tend to write short messages with simple sentence structures (Shechtman and Horowitz 2003, Hill et al. 2015). Furthermore, their messages often consist of incomplete sentences or only keywords, resembling search engine queries rather than natural conversation (Castillo et al. 2021). Drawing on these empirical insights, we argue that without HID, customers interacting with a hybrid service agent would assume their counterpart is an automated chatbot2and therefore tailor their messages in a similar fashion by adopting a rather task-oriented, command-like communication style.
Against this backdrop, we draw on audience design theory to propose that a customer’s communication style will differ from the one described above if human involvement in the interaction with a hybrid service agent is disclosed. Specifically, we argue that HID makes customers aware that their audience includes not only a chatbot but also an unseen employee working behind the scenes. When human involvement is disclosed up front—that is, before customers start interacting with a hybrid service agent—the disclosure typically indicates the possibility of human involvement during the course of the interaction but does not guarantee that an employee will step in. Nevertheless, revealing the presence of an employee working in tandem with the chatbot signals to customers that their audience includes not only a chatbot but also a human who might monitor, read, and eventually become involved in the interaction. According to audience design theory, people also tailor their communication style to so-called auditors—those who are not directly involved with but are present and listen to a conversation. For example, Youssef (1993) observed that the presence of an auditor, such as a child’s mother, has a greater influence on how children speak than the actual identity of their counterpart does. Similarly, research has shown that the mere presence of another human being can substantially influence customer behavior in both offline and online settings (Argo and Dahl 2020). Although research on audiences consisting of both humans and nonhumans is scarce, prior studies have revealed notable differences in how people communicate with another human versus a chatbot. More specifically, people tend to use longer, more vocabulary-rich messages with complete, grammatically correct sentences when interacting with a human (Hill et al. 2015). Based on these considerations, we argue that after an up-front HID, customers will tailor their communication style to take into account the employee’s presence. Consequently, they will exhibit a more human-oriented communication style that is characterized by longer and more natural sentences (e.g., “Hello, I just received my monthly bill, but it must be wrong. Can you help me?”) instead of simple keywords (e.g., “wrong bill”). We therefore pose the following hypothesis.
Hypothesis 1.
After an up-front human involvement disclosure (versus no up-front disclosure), customers exhibit a more human-oriented communication style in interacting with the hybrid service agent.
Besides up front, human involvement can also be disclosed during an interaction when an employee actually steps in for the chatbot. Such step-in HID may be particularly salient to customers as it signals that they are in fact engaging with two immediate addressees (a chatbot and an employee) rather than having a dyadic conversation with a chatbot. Audience design theory states that when audiences consist of more than one person, people use a communication style that appeals to all members of the audience. For example, in conversations with multiple participants, people tailor their communication style to take the perspectives of all addressees into consideration (Yoon and Brown-Schmidt 2019). Given that research has shown that people communicate differently with a human than with a chatbot (Hill et al. 2015), we argue that after a step-in HID, customers will tailor their communication style in a way that takes into account the employee who just stepped in and might continue to be involved in the interaction. As a result, their communication style will be more human-oriented than when there is no step-in HID. Therefore, we propose a second hypothesis.
Hypothesis 2.
After a step-in human involvement disclosure (versus no step-in disclosure), customers exhibit a more human-oriented communication style in interacting with the hybrid service agent.
3.2.Customer Communication Style and Employee Workload
Human–AI collaboration can occur not only on the job level but also on the level of tasks and task instances
(F¨ugener et al. 2022). To distribute work between humans and AI at the task-instance level, human–AI collaborative environments require effective delegation mechanisms (Baird and Maruping 2021, F¨ugener et al. 2022). Hybrid service agents build on this idea by leveraging a chatbot to handle common questions and requests and delegating more difficult ones to an employee. The underlying delegation mechanism typically follows a simple rule: If the chatbot is unable to respond, delegate the message to a human. Consequently, the workload of employees working in tandem with a chatbot is not fixed but depends heavily on the delegation rate of customer messages.
Against this backdrop, we argue that all else being equal, the style in which customers communicate with a hybrid service agent influences how much work will be delegated to an employee. Although the field of natural language processing (NLP) has made significant progress in enabling AI-based chatbots to understand natural language input, research has shown that how customers formulate their questions and requests can negatively affect NLP performance (Beaver et al. 2020). For instance, even small amounts of noise in a message can make it more difficult for a chatbot to understand what exactly the customer seeks (Beaver et al. 2020). Based on these observations, we argue that when customers adopt a more human-oriented communication style characterized by longer and more natural sentences instead of simpler keyword requests, the chatbot is less well equipped to deal with these messages and more likely to delegate them to a human. In contrast, when customer messages resemble short and simple information requests similar to search engine queries, the chatbot is more likely to handle them automatically without human involvement. Hence, we expect that a more human-oriented communication style plays to the weaknesses of a chatbot and thus leads to higher delegation rates. Employees’ workloads then increase, as they need to step in for the chatbot more often and spend more time responding to customer messages themselves. This leads to our third hypothesis.
Hypothesis 3.
A more human-oriented communication style of customers in interacting with a hybrid service agent increases employee workload.
3.3.Mediating Role of Impression Management Concerns
Up to this point, we have focused on how customers adapt their communication style depending on whether human involvement is disclosed in the interaction with a hybrid service agent. To theorize the underlying psychological mechanism at work, we draw on social psychology research that suggests that tailoring one’s communication style is an important component of impression management. As we elaborate below, we propose that disclosing human involvement activates impression management concerns—that is, concerns about making a good impression on others—which in turn lead customers to adopt a more human-oriented communication style.
A wealth of research in social psychology has shown that people are deeply concerned with how others perceive them (Leary and Kowalski 1990). Even when no immediate or future outcomes depend on the impressions they make, people have a strong desire to be viewed in a positive light and do their best to make a good impression. As a result, people follow social standards and norms (i.e., act more desirably) when others are present (Leary and Kowalski 1990). Compared with human–human interactions, impression management concerns are less likely to arise when people interact with machines such as robots or chatbots (Glikson and Woolley 2020). Although this offers a clear advantage in healthcare contexts where patients can open up about sensitive topics, reduced impression management concerns have also proven to be beneficial in customer service. For example, Følstad et al. (2018) found that people were less concerned when seeking answers from a customer service chatbot instead of a human employee because they did not feel judged for their potentially naive questions. Taken together, this evidence suggests
that impression management concerns are less pronounced when communicating with a chatbot.
Against this backdrop, we argue that disclosing (versus not disclosing) human involvement increases customers’ impression management concerns when interacting with a hybrid service agent, which in turn influences their communication style. More specifically, when customers are made aware of the employee working in tandem with the chatbot, they become concerned about making a good impression and presenting themselves in a positive light to the unseen employee. Following this line of thought, we argue that this effect occurs not only when an employee actually steps in during an interaction (step- in HID) but also when there is no guarantee that an employee will do so (up-front HID); just the existence of another human who might read and monitor their conversation with the chatbot should be enough to activate impression management concerns. This argument is in line with prior research indicating that the mere presence of another human, for example in a store, triggers impression management concerns (Argo and Dahl 2020). As social psychology research has shown that such concerns lead people to act in socially desirable ways, we further argue that customers concerned about making a good impression will adopt a human-oriented communication style that is more in line with social standards and norms of communication. Although an in-depth discussion of communication norms is beyond our scope, it is reasonable to assume that talking to someone as if they were a search engine (e.g., with simple keyword requests instead of natural sentences) would not leave a good impression but rather be considered awkward and rude. Hence, customers who are concerned about making a good impression due to a HID in their interaction with the hybrid service agent will exhibit a more human-oriented communication style than they would if they believed their counterpart to be just a chatbot.
Hypothesis 4.
The effect of an up-front human involvement disclosure on customer communication style is mediated by impression management concerns.
Hypothesis 5.
The effect of a step-in human involvement disclosure on customer communication style is mediated by impression management concerns.
4.Randomized Field Experiment: Impact of Human Involvement Disclosure on Customer Communication Style and Employee Workload
To test our hypotheses on how up-front and step-in HID affect customer communication style (Hypotheses 1and 2) and employee workload (Hypothesis 3), we conducted a randomized field experiment in cooperation with a multinational telecommunications company. The company had implemented a hybrid service agent on their website to answer common questions (e.g., contract extension, Internet connection breakdown) and provide information about their products and services (e.g., mobile phone plans). The hybrid service agent could be accessed via a chat interface and introduced itself as “Lisa, an automated chatbot” so that customers knew from the start that they were not interacting with a real person. A team of human employees worked behind the scenes and stepped in when customer messages were not handled automatically. Before our experiment, the hybrid service agent had not disclosed any human involvement.
4.1.Method
4.1.1.Experimental Design and Treatments.
The field experiment consisted of a 2 (up-front HID: disclosure versus no disclosure)×2 (step-in HID: disclosure versus no disclosure) between-subjects factorial design, resulting in four separate experimental conditions. In the conditions with up-front HID, possible human involvement was disclosed at the outset of the interaction with the customer. This was operationalized by inserting the statement “If I don’t know the answer to your question, my human colleague will read your message and give you an answer” at the end of the hybrid service agent’s welcome message to indicate that, under certain conditions, a human employee could step in. In the conditions without up-front HID, this statement was not included.
In the conditions with step-in HID, human involvement was disclosed at the point in time when a human employee stepped in during the customer’s interaction with the hybrid service agent. This was operationalized by sending the message “One moment, please. Unfortunately, I don’t know the answer to your question. I will pass it on to my human colleague who will give you an answer shortly” when the chatbot was unable to respond automatically. In the conditions without step-in HID, the hybrid service agent did not disclose the actual human involvement and customers received the message “One moment, please. Currently looking for an answer to your question” instead. Online Appendix A.1 provides screenshots of both treatments.
4.1.2.Manipulation Check.
Following prior studies (Schanke et al. 2021), we conducted a separate manipulation check to confirm the effectiveness of our treatments. Based on the scenario of a customer receiving a higher- than-usual mobile phone bill, we designed a typical interaction between a customer and the hybrid service agent of a fictitious telecommunications company (see Online Appendix A.2 for details). We then created four different versions of a video playing out the interaction based on the four experimental conditions. To carry
out the manipulation check, we recruited 120 participants via the online platform Clickworker who received e0.50 in exchange for their participation. They were randomly assigned to watch one of the four video-recorded interactions and then report the extent to which they believed that a human had been involved in the interaction, using a seven-point Likert scale (1�strongly disagree to 7�strongly agree). A one-way analysis of variance (ANOVA) revealed significant variation in perceived human involvement across conditions (F(3,116)�64.00, p<0.001). Planned contrasts confirmed that perceived human involvement was significantly lower in the condition without disclosure (mean (M)�1.46, standard deviation (SD)�0.88) than in the conditions with up-front HID (M�3.76, SD�2.29; t(116)�5.73, p<0.001), step-in HID (M�6.17, SD�1.12; t(116)�11.86, p<0.001), or both up-front and step-in HID (M�6.06, SD�1.37; t(116)�11.85, p<0.001). Hence, our experimental manipulations performed as intended.
4.1.3.Procedure and Hybrid Service Agent Design.
The field experiment proceeded as follows. When customers clicked on the “Chat” button on the company’s website, a chat window opened, and they were randomly assigned to one of the four experimental conditions. In the window, customers were greeted by the hybrid service agent in accordance with their designated experimental condition and then entered their questions or requests. During the interaction, each customer message was processed by the chatbot’s NLP algorithm to identify the customer’s intent (i.e., the action or goal the customer hoped to achieve by sending the message). For example, the message “I want to cancel my contract” resulted in the intent “churn.” If an intent was recognized with a confidence score greater than 0.95, the chatbot automatically sent a predefined response mapped to the intent. For the intent “churn,” this was “We’re sorry to hear that you want to cancel your contract. Please select which product or service you want to cancel.” The intents and the predefined responses had been refined by the company over several months and remained the same during the experiment. If no intent was recognized with a confidence score greater than 0.95, the message was delegated to a human. In such cases, employees received a notification in the employee user interface of the hybrid service agent indicating that they needed to step in for the chatbot in a customer interaction (see Online Appendix A.1 for a screenshot). Employees could then either (1) confirm a response suggested by the chatbot (with a confidence score of less than 0.95), (2) edit the suggested response, or (3) enter a completely new response. When a response was finalized, the employee would send it to the customer. The delegation mechanism was the same for all customer messages.
4.1.4.Data Collection and Measures.
Data collection took place over a two-week period in February 2019. During this period, a total of 8,966 customers were randomly assigned to one of the four experimental conditions, resulting in a roughly equal number of customers across conditions. For each customer–hybrid service agent interaction (hereafter referred to as chat), we collected chat data (e.g., text content of all messages, timestamps, sender), basic customer characteristics (e.g., device, location), and information about the activity of employees working in tandem with the chatbot (e.g., number of customer messages handled by an employee, response times). Sensitive customer data, such as names, phone numbers, and addresses, were anonymized before the analysis. To assess the efficacy of our randomization procedure, we compared the experimental groups on two baseline customer characteristics (device and location) and assessed the number of customers in each group across different time periods (day of the week and part of the day). The results showed that customers were randomly assigned to the experimental groups based on device and location and across weekdays and within a day (see Online Appendix A.3 for details). On average, the chats in our sample lasted 7.82minutes (SD�57.77) and included 4.71 customer messages (SD�2.37), 6.06 chatbot messages (SD�2.16), and 1.02 employee messages (SD�1.28). Human employees stepped in for the chatbot in 60.8% of chats.
Our focal constructs were customer communication style and employee workload. Both were modeled as latent constructs using three well-selected indicators. To operationalize customer communication style and specifically measure the degree of human orientation, we drew on research that examined how people communicate with a chatbot as opposed to with another human (Shechtman and Horowitz 2003, Hill et al. 2015, Knijnenburg and Willemsen 2016). Based on these studies, we identified three key structural characteristics of communication style that differ between human–chatbot and human–human communication: verbosity, complexity, and density. First, verbosity can be understood as the use of many or too many words. Prior studies have found that people use significantly fewer words when interacting with a chatbot as opposed to with another human (Shechtman and Horowitz 2003, Hill et al. 2015). We calculated verbosity as the average number of words per message from a customer in a chat. Second, complexity (also called linguistic complexity or readability) refers to the ease or difficulty with which people can read and understand a message. Prior studies have found that when interacting with a chatbot, people tend to avoid complex sentences and prefer command-style language with only keywords (Knijnenburg and Willemsen 2016, Castillo et al. 2021). We calculated the average complexity of customers’ messages in a chat using the readability formula of Coleman (1971). When we use an alternative complexity measure, the results of our main analyses are qualitatively similar. Third, density (also
called functional density) is a characteristic that differentiates between function words (e.g., conjunctions, pronouns, prepositions) and content words (e.g., nouns, verbs, adjectives). Whereas content words carry meaning and information, function words have little meaning and only serve to make sentences grammatically correct by binding content words together. Prior studies have found that people use more grammatically correct sentences when interacting with another human compared with when they interact with a chatbot (Hill et al. 2015, Knijnenburg and Willemsen 2016). We calculated density as the average ratio of function words to the total number of words in each of a customer’s messages in a chat. As in previous research, we used the linguistic inquiry and word count (LIWC) text analysis program to classify words into these predefined categories (Pennebaker et al. 2015). Taken together, we modeled customer communication style as a latent construct with three structural characteristics—verbosity, complexity, and density—as reflective indicators. Online Appendix A.4 provides further details.
Employee workload in hybrid service agents is inextricably linked to the chatbot’s ability to handle customer messages automatically rather than delegating them to a human. From our discussions with the company, we learned that employees’ overall workloads depend not only on how often they are required to step in (frequency) but also on how much time they spend responding to a customer’s messages (duration) and whether they can simply confirm or edit the chatbot’s suggested response or must enter a completely new one (intensity). We therefore adopted these factors as three key indicators of employee workload. First, we measured frequency as the total number of responses sent by an employee instead of the chatbot in a chat. Second, we calculated duration as the total time that employees spent responding to customer messages in a chat. We created this variable by computing the elapsed time between each customer message and the employee response (Altman et al. 2021) and taking the sum of these response times. Third, we calculated intensity using a weighted average of the specific actions that employees took when they stepped in. Naturally, confirming a suggested response required less work than editing it before sending, which in turn required less work than entering a new response. Taken together, we modeled employee workload as a latent construct with three reflective indicators: frequency, duration, and intensity. Online Appendix A.4 provides further details. Table 2presents descriptive statistics for the indicators of both constructs in each experimental condition.
4.2.Results
4.2.1.Measurement Model Evaluation.
We first conducted a confirmatory factor analysis using the lavaan package in R (Rosseel 2012) to assess the validity and reliability of our latent constructs (1) customer communication style and (2) employee workload. The results indicate that the proposed measurement model exhibits a good fit to the data (comparative fit index (CFI)�0.99, Tucker–Lewis index (TLI)�0.98, root mean square error of approximation (RMSEA)�0.04, standardized root mean square residual (SRMR)�0.02), providing support for construct validity. We further assessed convergent and discriminant validity by examining factor loadings, composite reliability scores, interconstruct correlations, and average variances extracted (AVE) for each construct. All indicators loaded strongly on their constructs with loadings ranging from 0.68 to 0.90. Moreover, all composite reliability scores were above the recommended level of 0.70, all AVE values exceeded 0.50, and the square root of each construct’s AVE was greater than the interconstruct correlations, thus indicating convergent and discriminant validity. Online Appendix A.5 provides an overview of correlations and psychometric properties of both constructs.
4.2.2.Structural Model and Hypothesis Testing.
To estimate the structural model and test our hypotheses, we used structural equation modeling (SEM) using the lavaan package in R. We created two dummy variables for up-front and step-in HID (0�without disclosure, 1�with disclosure) and used them as independent variables in our model. Furthermore, we included customer communication style as an (unobserved) latent construct with three (observed) indicators: verbosity, complexity, and density. Employee workload was also modeled as a latent construct with three indicators: frequency, duration, and intensity. In addition, we included a set of variables to control for the type of service (e.g., sales, churn) and customer characteristics (device and location; see Online Appendix A.3). Finally, we added employee fixed effects to account for unobserved heterogeneity across individual employees and weekday and part-of-day fixed effects to control for time-specific trends (Altman et al. 2021). We used full information maximum likelihood estimation to use all data available. The overall fit indices of the model indicated an adequate fit to the data (CFI�0.91, TLI�0.86, RMSEA�0.05, SRMR�0.02).
The results in Table 3show that the effect of up-front HID on customer communication style is significant (b�0.143, p<0.001), supporting Hypothesis 1. We also find a significant effect of step-in HID on customer communication style (b�0.066, p�0.010), providing support for Hypothesis 2. Unlike the two main effects, the interaction effect is not significant (b�0.018, p�0.553), indicating that combining up-front and step-in HID does not yield a synergistic effect that is greater than the sum of their independent effects. Comparing the standardized coefficients of the two treatments reveals that up-front HID has a stronger effect on communication style than step-in HID does. Finally, the results show a significant
effect of customer communication style on employee workload (b�0.380, p<0.001), supporting Hypothesis 3. All reported effects are consistent in magnitude, direction, and significance in both the model with controls (Model 1) and the one without (Model 2). To provide a fairer comparison, we also analyzed the structural model with only the subsample of customers who had a human step in during their chat (Model 3), which yielded similar results. Finally, to examine the causal chain in our theoretical model, we performed a SEM-based mediation analysis (Cheung and Lau 2008) to formally test the indirect effects of up-front and step-in HID on employee workload through customer communication style. The results, presented in Online Appendix A.6, show that both indirect effects are significant, providing further support for our theoretical model. Taken together, our results provide evidence that customers exhibit a more human-oriented communication style when human involvement is disclosed (versus not disclosed), which in turn increases employee workload.
4.2.3.Robustness Checks.
We conducted a series of robustness checks to validate our findings. Specifically, we repeated our analysis using a different analytical approach and carried out various analyses to rule out possible alternative explanations. First, to verify the robustness of our SEM-based findings, we reanalyzed the data using an econometric regression-based approach. In this analysis, we also accounted for potential estimation biases caused by the fact that a proportion of customers assigned to receive the step-in HID treatment did not actually receive it (38.5%), as an employee never had to step in during their chat. Therefore, we used the local average treatment effects (LATE) model (Imbens and Angrist 1994) to correct for the potential biases arising from this form of one-sided treatment noncompliance. Online Appendix A.7 provides details on our model specification and estimation. Consistent with our SEM-based findings and in support of Hypotheses 1–3, the results in Tables A7 and A8 show that both up-front and step-in HID have significant positive effects on customer communication style, which in turn has a significant positive effect on employee workload. Hence, we can conclude that our findings on the impact of HID are robust and not due to the particular analytical approach chosen.
Second, a potential alternative explanation for our findings could be that it is not the disclosure of human involvement in itself that caused the observed effects on customer communication style; instead, the results could be due to the inclusion of text signaling the chatbot’s possible inferiority to humans in the treatment conditions (e.g., “If I don’t know the answer”). To rule this out, we conducted a follow-up controlled online experiment (N�410) in which we added treatment conditions with neutral wording (e.g., “If the situation requires”) and compared them to our original treatment conditions. Online Appendix A.8 provides details on the experimental design and analyses. Overall, the results in Table A10 indicate no significant differences in customer communication styles between original and neutral treatment versions (all p>0.415). Furthermore, we can fully replicate the results from our field experiment using our original HID treatments and the neutral versions in the additional online experiment. Collectively, these findings suggest that the effects observed in the field experiment are indeed caused by the disclosure of human involvement in itself and not by the inclusion of text signaling that the chatbot might be inferior to humans.
Third, another plausible alternative explanation for the observed results is that the differences in customer communication style were driven by differences in employee behavior when stepping in for the chatbot. With respect to this, it is important to note that all employees had received the same training and were provided with the same broad set of canned responses that were in fact the same responses that the chatbot used. Nonetheless, as a robustness check, we analyzed employees’ actions and language across experimental conditions
and individuals and found no systematic differences in their behavior (see Online Appendix A.9 for details). In addition, we created two new control variables— measuring the similarity of the text between messages from the bot and from employees and between individual employees for each chat—to account for potential variation in employees’ language in our analysis. After adding both controls to our main analysis, the results remained qualitatively similar. Taking this evidence together, it seems unlikely that our findings could be explained by variation in the language used by employees.
Fourth, a final potential alternative explanation is that when human involvement was not disclosed, customers could have been more likely to end the chat without their problems being solved. If this were the case, the differences in customer communication style could be explained by the fact that getting a problem solved in the chat might have required a more human-like communication style with longer, more complex, and more natural messages. To rule this out, we manually coded each chat to score the extent to which the customer’s problem was resolved through the interaction on a five-point scale from 1�not at all resolved to 5�completely resolved (see Online Appendix A.10 for details). We then compared problem resolution scores across conditions and found no significant difference (all p>0.468), suggesting that problem resolution did not depend on whether human involvement was disclosed. Furthermore, we added the problem resolution score as a control variable in our main analysis and the results remained qualitatively similar. In sum, these findings allow us to rule out the possibility that the observed effects are due to customers ending the chat without their problems being solved when human involvement was not disclosed.
Finally, in our analysis, we included the step-in HID treatment as a dichotomous variable in line with our two-level treatment design (0�without step-in HID, 1�with step-in HID). However, treated customers could receive the step-in HID treatment more than once because it was shown every time a human employee stepped in for the chatbot. We therefore repeated our main SEM analysis using step-in HID treatment intensity— operationalized as the number of times a customer received the treatment—instead of the dichotomous treatment level. The results, presented in Online Appendix A.11, show that the impact of step-in HID on customer communication style increases with the number of times the disclosure is shown to customers (b�0.280, p<0.001), further corroborating our main findings.
4.2.4.Additional Analyses.
Although the focus of our field experiment was to investigate the impact of HID on customer communication style and employee workload, we supplemented our core results with additional analyses exploring several other important business-related outcomes. First, an important question to examine is whether HID increased customers’ tendency to actively seek out human involvement. To explore this, we manually reviewed all chats to identify customer messages with an explicit request for an employee and then compared the occurrence of such a request across conditions (see Online Appendix A.13 for details). Although there is no widespread tendency to seek out human involvement (96.7% of chats contain no such request), our results do suggest that customers are more likely to ask for an employee when human involvement is disclosed up front (p<0.001). This finding, while preliminary, may indicate that customers are less willing to interact with a chatbot when they know from the beginning that humans are also around to help. It would be interesting for future research to study whether this aspect of customer behavior is another example of algorithm aversion (Dietvorst et al. 2015) and, if so, how a customer’s preference for humans over chatbots could be overcome when human involvement is disclosed.
Second, we extended our analysis to investigate how HID affects overall communication length. Prior literature has suggested that communication is longer when human involvement is disclosed (versus not disclosed; Luo et al. 2019). At the same time, greater efficiency in terms of addressing customer requests as quickly as possible is generally desired in customer service operations. The findings of our additional analysis on communication length, presented in Online Appendix A.14, suggest that customer–hybrid service agent interactions tend to be one minute longer on average when there is an up-front or step-in HID (both p<0.001). One plausible explanation—consistent with our findings on the mediating role of customers’ impression management concerns (see next section)—is that customers may spend more time crafting messages and reading responses when they feel they are being taken care of by a human instead of a bot.
Third, customer satisfaction with a service encounter is of particular importance to firms. Because our data set did not include explicit information on customer satisfaction, we used an automated sentiment analysis approach to assess customers’ evaluations of their interactions with the hybrid service agent. The results of our additional analysis on customer sentiment, presented in Online Appendix A.15, suggest a significant negative effect of up-front HID on customer sentiment (b��0.011, p�0.018). There are several explanations for this finding. For example, it could be that customers are generally more likely to vent their frustration and negative emotions to a human working in tandem with a bot than to a fully automated chatbot or, alternatively, that they become frustrated more easily when being served by the chatbot while knowing that humans are also around to help. Although our data do not allow us to uncover the underlying reasons, we believe that these alternative explanations call for future research to study customer
satisfaction regarding service encounters with hybrid service agents that involve (un)disclosed employees.
Finally, to provide additional managerial insights, we analyzed the downstream financial impact of the additional workload caused by disclosing human involvement. Although exploratory in nature, our results suggest that the additional workload increases costs associated with actual human involvement in a hybrid service agent by 38–70%—potentially even more if firms need to hire and train additional employees (see Online Appendix A.16 for details).
5.Controlled Online Experiment: Mediating Role of Impression Management Concerns
Our field experiment provides real-world evidence that HID influences customer communication style in the interaction with a hybrid service agent. To reveal the underlying psychological mechanism and assess the robustness of our findings, we conducted a controlled online experiment. The objective of this experiment was threefold: (1) replicate the findings of the field experiment in a controlled setting, (2) examine impression management concerns as a mediator to the effects of up-front and step-in HID on communication style (Hypotheses 4and 5), and (3) rule out potential alternative explanations.
5.1.Method
5.1.1.Experimental Design and Treatments.
The online experiment resembled the field experiment in applying a 2 (up-front HID: disclosure versus no disclosure)×2 (step-in HID: disclosure versus no disclosure) between-subjects factorial design. We used the same verbal statements as in the field experiment for the up-front HID (“If I don’t know the answer to your question, my human colleague will read your message and give you an answer”) and step-in HID treatments (“One moment, please. Unfortunately, I don’t know the answer to your question. I will pass it on to my human colleague who will give you an answer shortly”). To keep the text length equal across all groups, we formulated statements of the same length for the corresponding conditions without disclosure (see Online Appendix B.1).
5.1.2.Procedure and Hybrid Service Agent Design.
To enhance experimental realism, we framed the experiment as a real-world test of the new customer service platform of a telecommunications company. More specifically, we told participants that they were invited to test and provide feedback on the new platform before it would be released to the company’s customers. To make the experiment appear authentic, we explained to participants that they would receive a randomly selected service issue and that they should use this information as a starting point to contact customer service. In reality, however, all participants received the same service issue (an unexpectedly high bill) to ensure a high level of comparability across their interactions with the hybrid service agent.
In the experiment, we first asked participants to imagine that they had just received their monthly mobile phone bill. The bill appeared to be higher than usual but did not provide an explanation for the higher costs (see Online Appendix B.1 for a full description). We chose this scenario to offer better comparison with the results of the field experiment, as billing questions and complaints are typical customer service issues for telecommunications companies. After reading the scenario description, participants summarized the problem at hand in their own words in an open-ended text box. In addition to serving as a control variable for customers’ baseline communication styles, this question also functioned as an attention check, allowing us to filter out participants who showed clear signs of inattentiveness (e.g., nonsensical answers). Next, we told participants to contact customer service to find out the reason for their unexpectedly high bill.
Subsequently, participants were randomly assigned to one of the four experimental conditions. In each condition, they interacted with a hybrid service agent in the form of a custom chatbot that was developed for this experiment using the Microsoft Bot Framework and pretested extensively (see Online Appendix B.2 for details). We did not involve human confederates in the experiment, aiming to avoid any potential confounding effects caused by their behavior. This helped ensure high internal validity and identical interactions across conditions. Participants entered and sent their own messages to reflect a natural and realistic interaction, also allowing us to measure their communication styles as done in the field experiment based on the text content of their messages. The chatbot used NLP to analyze text input (i.e., detect keywords in customer messages) and automatically provide predetermined responses. To ensure that chats were comparable across participants, we implemented a guided dialog that participants entered once they had clearly described their reason for contacting customer service. This dialog contained six messages with general information about the causes of an unusually high bill and two follow-up questions on details of the bill (e.g., “Which item on your bill seems to be incorrect?”). At the end of the dialog, participants learned that their unexpectedly high bill was likely due to an expired discount and the chat ended.
As in the field experiment, the welcome message at the beginning of the chat contained the up-front HID. The step-in HID was displayed once during the chat before the start of the guided dialog. To make it appear as if there were actual human involvement at this point, we delayed the subsequent message by 50seconds, which corresponds to the average response time of
employees in the field experiment. After the experiment, participants completed a survey with measures, demographic questions, and manipulation checks. At the end of the survey, we included an open-ended question asking participants to share their opinions about HID (e.g., “Do you want to know if human employees are involved when you talk to a chatbot?”). Finally, we debriefed participants, including about the fact that they interacted only with a chatbot and that they took part in a research study.
5.1.3.Participants.
We recruited participants via the online platform Clickworker. We chose this platform because it is widely used by companies to conduct usability testing and user research, making it ideal for our experiment framed as a real-world test of a new customer service platform. Of the 300 participants who completed both the chat and the survey, we excluded 12 for failing the attention check (i.e., entering a nonsensical response when asked to summarize the problem at hand). The analysis then included responses from 288 participants (61% male, Mage�40; see Online Appendix B.3 for sample characteristics) who received e2.50 for their time, which was 15minutes on average.
5.1.4.Measures.
To assess customer communication style, we used the same measurement as in the field experiment—that is, we calculated its three indicators verbosity, complexity, and density from the text messages entered by participants during the chat. In the survey, we measured impression management concerns using four items adapted from the public self-consciousness scales of Fenigstein et al. (1975) and Govern and Marsch (2001) on a seven-point Likert scale (1�strongly disagree to 7�strongly agree). To control for individual differences, we collected demographic information (gender, age, education), assessed participants’ prior chatbot experience (i.e., how often they contact customer service via a chatbot) and need for human interaction (Dabholkar 1996) and calculated their baseline communication styles from their summary of the problem in the open-ended question before the chat. Online Appendix B.3 presents all measurement items and reliabilities.
5.1.5.Manipulation Check.
We included two manipulation check questions at the end of the survey. First, we asked participants to indicate whether they thought that a human employee was involved in their interaction with customer service (seven-point Likert scale: 1�strongly disagree to 7�strongly agree). A one-way ANOVA revealed significant variation in perceived human involvement across conditions (F(3, 284)�51.39, p<0.001). Planned contrasts confirmed that perceived human involvement was significantly lower in the condition without HID (M�2.01, SD�1.54) than in the conditions with up-front HID (M�3.50, SD�2.22; t (284)�4.74, p<0.001), step-in HID (M�5.36, SD�1.79; t(284)�10.76, p<0.001), or both up-front and step-in HID (M�5.23, SD�1.90; t(284)�10.18, p<0.001). The second manipulation check was to verify that participants knew that their direct counterpart was a chatbot and did not falsely assume they were chatting with a human masquerading as a bot. Therefore, we asked participants to indicate whether they thought that their direct counterpart was a chatbot or a human (seven-point Likert-type scale: 1�automated chatbot to 7�human employee; adapted from Mozafari et al. 2022). There were significant differences across the conditions (F(3, 284)�6.13, p<0.001). Consistent with the first manipulation check, planned contrasts showed that participants in the condition without HID perceived their counterpart significantly more as an automated chatbot (M�2.24, SD�1.61) than did participants in the conditions with step-in HID (M�3.16, SD�1.55; t(284)�3.28, p�0.001) or both up-front and step-in HID (M�3.27, SD�1.90; t(284)�3.62, p<0.001). The difference in the up-front HID condition was nonsignificant (M�2.53, SD�1.75; t(284)�1.03, p�0.306). Most importantly, however, all four scores were significantly lower than the scale midpoint (all p<0.01), indicating that participants assumed their direct counterpart to be a chatbot (Crolic et al. 2022). In sum, these results demonstrate that our manipulations worked as intended.
5.2.Results
As in the field experiment, we employed SEM using the lavaan package in R to analyze the data and test our hypotheses. We first conducted a confirmatory factor analysis, which showed that the measurement model exhibited a good fit to the data (CFI�0.96, TLI�0.94, RMSEA�0.06, SRMR�0.06). All indicators loaded strongly on their constructs with loadings ranging from 0.60 to 0.98. Convergent validity was satisfactory, as the composite reliabilities and AVE for each construct exceeded the suggested thresholds (>0.70 and >0.50, respectively). The square root of each AVE value was greater than all individual correlations, supporting discriminant validity. Table 4presents descriptive statistics for each condition.
5.2.1.Effects of Human Involvement Disclosure on Customer Communication Style.
To replicate the findings of the field experiment, we first constructed a baseline model. We again modeled customer communication style as a latent construct with verbosity, complexity, and density as observed indicators. We entered up-front and step-in HID as dummy-coded independent variables (0�without disclosure, 1�with disclosure) and included chatbot experience, need for human interaction, age, gender, and baseline communication style as control variables. The model indicated a good fit to the data
(CFI�0.94, TLI�0.92, RMSEA�0.06, SRMR�0.06). Furthermore, the results showed significant effects of up-front HID (b�0.177, p�0.027) and step-in HID (b�0.222, p�0.005) on customer communication style. As in the field experiment, the interaction effect was not significant (b��0.099, p�0.314). All results were qualitatively similar when control variables were not included (see Online Appendix B.4). By replicating the direct effects of HID observed in the field experiment, these results demonstrate the robustness of our findings.
5.2.2.Mediating Effect of Impression Management Concerns.
To examine the mediating role of impression management concerns as proposed in Hypotheses 4and 5, we performed a SEM-based mediation analysis using the bias-corrected bootstrapping procedure with 5,000 samples (Cheung and Lau 2008). We constructed a mediation model with communication style as the dependent variable, up-front and step-in HID as independent variables, impression management concerns as the mediator, and the same set of control variables. Overall, the model indicated a good fit to the data (CFI�0.94, TLI�0.92, RMSEA�0.06, SRMR�0.06). The results in Table 5show that both up-front and step-in HID have significant positive effects on impression management concerns (b�0.310, p<0.001 and b�0.299, p<0.001, respectively). Moreover, the cumulative effect of up-front and step- in HID is less than the sum of their independent effects, as the interaction is negative (b��0.291, p<0.001). This suggests that both disclosures together do not lead to greater impression management concerns than either alone. Furthermore, impression management concerns positively influence customer communication style (b�0.209, p�0.004). Most importantly, the indirect effects of up-front HID (b�0.065, SE�0.030) and step-in HID (b�0.063, SE�0.028) on customer communication style through impression management concerns are significant, as the confidence intervals (CIs) of these effects do not include zero (up-front HID: 95% CI [0.019, 0.140]; step-in HID: 95% CI [0.018, 0.131]). Furthermore, the direct effect of up-front HID on communication style becomes nonsignificant when the mediator is included (p�0.162, indicating full mediation), whereas the direct effect of step-in HID remains significant (p�0.044, indicating partial mediation; Zhao et al. 2010). One possible explanation for this is that once a (disclosed) human employee steps in for the chatbot (i.e., step-in HID), other mechanisms may also come into play (e.g., the employee’s helpfulness). All results were qualitatively similar when control variables were not included (see Online Appendix B.4). In sum, these results provide evidence for the mediating effect of impression management concerns, supporting Hypotheses 4and 5.
5.2.3.Additional Analyses.
We conducted several additional analyses to examine potential alternative mechanisms. However, none of these mechanisms, including customers’ tendencies to actively seek out human involvement and social cognition perceptions (i.e., perceived competence and warmth), could explain our findings (see Online Appendix B.5). This provides further support for the role of impression management concerns as the psychological mechanism underlying the effect of HID. We also performed a content analysis of participants’ responses to the open-ended question on whether they would want to know about employees working in tandem with a chatbot (see Online Appendix B.6); most participants indeed indicated that they would want to be informed about human involvement. Furthermore, participants reported that they would feel fooled by and lose trust in a firm if they found out later that they were deliberately kept in the dark.
6.Discussion
The proliferation of hybrid service agents adds a new layer of complexity to the already blurred line between humans and technology in online service encounters. While ethical and legal reasons have pushed most firms to disclose their chatbots’ nonhuman identities, it is less clear whether firms should also be transparent about behind-the-scenes employees who step in if a chatbot is unable to respond. Against this backdrop,
we investigated the impact of HID on customer interactions with hybrid service agents. Consistent evidence from a randomized field experiment and a controlled online experiment suggests that HID substantially influences how customers communicate with a hybrid service agent. In line with audience design theory, we find that when human involvement is disclosed, customers exhibit a more human-oriented communication style, characterized by longer, more complex, and more natural messages rather than simple keyword-style queries. Interestingly, our results suggest that a disclosure before the interaction (up-front HID) has a similar or even stronger effect than a disclosure during the interaction when an employee steps in (step-in HID), even though the up-front HID does not guarantee that an employee will eventually become involved. Furthermore, we reveal that the effect of HID on customer communication style is driven by customers’ impression management concerns. That is, customers are more concerned about making a good impression when they know that humans are working in tandem with the chatbot, which then affects how they communicate with the hybrid service agent. Finally, our results demonstrate that these changes in communication style have negative downstream consequences, as they increase the workload of employees. In addition to our main results, we also find that when human involvement is disclosed before the interaction, customers are more likely to actively seek out human involvement and tend to express more negative sentiment. Furthermore, customer–hybrid service agent interactions tend to be longer when human involvement is disclosed. Taken together, our findings highlight the major impact that HID has on customer– hybrid service agent interactions.
6.1.Implications for Research
Our work makes three contributions to research. First, we contribute to the literature on the role of IT in customer service encounters. While prior research has primarily focused on IT-based self-service (e.g., web portals, chatbots) and human-based service (e.g., phone, live chat) (Ba et al. 2010, Kumar and Telang 2012, Schanke et al. 2021, Han et al. 2022), our study sheds light on a novel approach that combines humans and chatbot technology behind a single service interface: hybrid service agents. In particular, our empirical investigation of customer–hybrid service agent interaction adds to prior literature by revealing notable differences in customer behavior when customers know that their counterpart is not just a chatbot but that there are also humans around to help. Even when there is no guarantee that a human will eventually step in for the chatbot, the mere disclosure of potential human involvement can alter customer behavior. Our research thus offers important insights into how customers respond to hybrid service interfaces that blur the once clear lines between humans and technology in customer service encounters.
Second, we contribute to the emerging stream of IS research on the use of AI in service automation. While current research highlights the various benefits of using AI to augment, rather than replace, service employees (Jain et al. 2021, Schuetzler et al. 2021), there is limited understanding of the obstacles that can hamper effective human–AI collaboration (Jussupow et al. 2021). Against this backdrop, our study sheds light on how unexpected changes in customer behavior can affect AI-to-human delegation mechanisms, place additional workload on human employees working in tandem with AI, and create additional costs for firms. More broadly, these insights contribute to IS delegation research (Baird and Maruping 2021, F¨ugener et al. 2022) by showing that delegation mechanisms are not only shaped by the human and the AI agent themselves but can also be deliberately or accidently influenced by external actors (here, customers). Overall, our key addition to the literature is the notion that being transparent about human involvement in predominantly AI-based service encounters may ultimately lead to a lower degree of automation and therefore undermine AI’s ability to free up employees from mundane and repetitive customer service work.
Third, we contribute to the HCI literature on understanding the psychological mechanisms involved in customer interactions with human, nonhuman, and hybrid agents. Previous research has primarily focused on competence- and warmth-related perceptions and expectations as key drivers of customer behavior (Go and Sundar 2019, Luo et al. 2019, Grimes et al. 2021, Mozafari et al. 2022). However, these mechanisms were unable to explain our results. Instead, our study reveals a different psychological mechanism behind customer behavior: impression management concerns. Although such concerns are well known from traditional human–human interactions (Argo and Dahl 2020), our study provides novel insights into how similar concerns arise when customers become aware of human employees working in tandem with a chatbot. These insights contribute to HCI literature by revealing that customers’ behavior may sometimes depend more on how they perceive themselves than how they perceive their counterparts (e.g., a chatbot, human, or both).
6.2.Implications for Practice
Our research also provides several implications for managers and policymakers. First, our findings suggest that firms face somewhat of a dilemma when deciding whether to disclose human involvement in customer– hybrid service agent interactions. On the one hand, a disclosure triggers undesirable changes in customer behavior and adversely affects employees’ workloads. This not only undermines the hybrid service agent’s effectiveness in automating routine customer interactions and creates additional costs (see Online Appendix A.16) but could also disrupt customer service operations if too much work is delegated to a human. On the other hand, our qualitative findings indicate that customers demand transparency from firms employing hybrid service agents. Many customers want to know about the involvement of human employees and some even report feeling betrayed and losing trust in a firm if they only find out at a later stage (see Online Appendix B.6). These findings resonate with criticism surrounding the use of so-called pseudo-AI in the popular press (Forbes 2020) and suggest that firms should be aware of the negative backlash that may stem from deliberately avoiding being transparent about human involvement. Given that disclosing human involvement may follow chatbot identity disclosure to become law at some point, we suggest two actions that managers can take to mitigate its negative consequences. Initially, managers should improve the ability of their hybrid service agents to handle a human-oriented customer communication style by training their chatbots to better understand the nuances of human language, such as semantics and pragmatics. Furthermore, before going live with the disclosure, they should temporarily increase the headcount of employees who can work with the core team to absorb higher workloads during the transition phase. As these solutions may not always be feasible or economically viable, managers could also consider some easy-to-implement strategies. For instance, including a statement such as “Don’t worry, my human colleagues don’t mind if you communicate with them like a robot” in the welcome message of the hybrid service agent could prevent impression management concerns from arising. Another strategy could be to limit the length of customer messages to a maximum number of characters or words, encouraging customers to use simpler keyword queries instead of more human-oriented language.
In addition to these managerial insights, our findings carry implications for policymakers. As the previous discussion suggests, firms must currently decide for themselves what is right and wrong in terms of disclosure when using hybrid service agents. Will they disclose human involvement even if it leads to higher employee workload and costs? Maybe not. What is missing is a regulatory framework that balances economic interests and ethical concerns and provides guidance for firms on what is and is not allowed. Although research has emphasized that the blurring line between humans and AI creates counterfeit service encounters and raises serious transparency issues (Robinson et al. 2020), most policy initiatives to date exist only as nonlegally binding principles and guidelines. The one notable exception (California’s BOT bill) only requires the disclosure of chatbot identity and thus does not regulate whether, when, and to what extent firms should disclose that humans are working in tandem with their chatbot. There is therefore a need for policymakers to review and extend existing regulatory frameworks to protect
customers from counterfeit service encounters while also minimizing potential negative impacts for firms (e.g., additional costs). Given the clear parallels to mandatory call recording disclosures (“This call may be monitored or recorded”), existing telephone call recording laws may serve as a source of inspiration for future policy initiatives.
6.3.Limitations and Future Research
This work has certain limitations that offer several opportunities for future research. First, we primarily focused on understanding hybrid service agent interactions from the customer perspective. Although we examined how customer behavior affects the workload of employees working in tandem with the chatbot, future research should place greater emphasis on the employee perspective. It would be particularly valuable to better understand how employees feel about the (non)disclosure of their involvement, as their work may remain completely hidden from customers. In addition, future research should investigate the nature and consequences of human–AI collaboration in hybrid service agents on a more general level. For example, such studies could draw from emerging research on algorithmic management (M¨ohlmann et al. 2021) and AI-to-human delegation (F¨ugener et al. 2022) to examine how employees cope with the loss of control when an AI algorithm decides whether and when they have to step in during customer interactions.
A second fruitful avenue for further research could be to extend our work with a broader investigation of customer communication behavior. As our analysis focused on structural characteristics of communication style in text-based customer service interactions (i.e., verbosity, complexity, and density), additional research is needed to investigate other relevant language aspects (e.g., level of concreteness), service contexts (e.g., medical consultations), and communication channels (e.g., voice). For instance, it could be interesting to explore whether different types of service interactions (e.g., enquiry-based versus complaint-based conversations) activate stronger impression management concerns and consequently also lead to a more human-oriented communication style. In addition, as chatbots are often enriched with social cues, such as human names and humanlike appearances (Feine et al. 2019), it would be valuable to explore how these cues affect customer communication behavior.
Third, our research opens the door to further investigation of customer perceptions of the service encounter, the firm, and the hybrid service agent itself. Although we provide a detailed understanding of customer communication style and some additional insights into other customer outcomes (e.g., tendency to seek out human involvement, sentiment), there remains a need for research on how customers evaluate service encounters with hybrid service agents and how the disclosure of human involvement can affect their evaluations. For example, our results could serve as a starting point for investigating whether such disclosure causes or exacerbates algorithm aversion (Dietvorst et al. 2015) and whether customers are even more frustrated by failures during an interaction when they know that humans are working in tandem with the chatbot. Moreover, as our qualitative insights suggest that customers want to know about human involvement (see Online Appendix B.6), future research could more systematically investigate the role of nontransparency and how customers react when they find out that a firm has deliberately kept them in the dark.
Finally, it is important to discuss our findings in light of the continuous advances in AI. As the technology behind chatbots is constantly improving, there may be a point in time when human involvement is no longer needed to provide quality customer service. Although the chatbot in our field experiment was based on a commercial product from one of the market leaders in this field, used state-of-the-art intent recognition algorithms, and was extensively trained and continuously optimized by the telecommunications company, we acknowledge that the observed effects of customer communication style on employee workload may fade as the technology improves. With the gradual emergence of chatbots based on large language models such as ChatGPT or Bard, it would be interesting to explore how they might reduce the need for human involvement in certain areas while increasing it in others. For example, as large language model–based chatbots may generate incorrect but plausible-looking answers (“hallucinations”), future research could investigate how and when employees should step in to review an answer and verify the accuracy of the information before sending it to the customer.
In conclusion, our research takes a first step toward understanding customer–hybrid service agent interactions and the impact of disclosing human involvement to customers. We hope it offers thought-provoking insights for researchers, managers, and policymakers alike and can stimulate more work in this promising area of research.






----------------------------------------------------------------------------------






Cui, R., Li, M., & Zhang, S. (2022). AI and procurement. Manufacturing & Service
Operations Management, 24(2), 691-706.

Abstract
In a world advancing toward Articial Intelligence (AI), we explore how AI creates and delivers value
in procurement. AI has two unique abilities: automation and smartness, which are associated with phys-
ical machines or software that enable us to operate more eciently and eectively. In this research, we
study how buyers' usage of AI aects suppliers' price quoting strategies. Specically, we study the impact
of automation|i.e., the buyer uses a chatbot to automatically inquire about prices instead of asking in
person|and the impact of smartness|i.e., the buyer signals the usage of a smart AI algorithm in selecting
the supplier. We collaborate with a trading company to run a eld experiment on an online platform in
which we compare suppliers' wholesale price quotes across female, male, and chatbot buyer types under AI
and no recommendation conditions. We nd that, when not equipped with a smart control, there is price
discrimination against chatbot buyers who receive a higher wholesale price quote than human buyers. In fact,
without smartness, automation alone receives the highest quoted wholesale price. However, signaling the
use of a smart recommendation system can eectively reduce suppliers' price quote for chatbot buyers. We
also show that AI delivers the most value when buyers adopt automation and smartness simultaneously in
procurement. Our results imply that automation is not very valuable when implemented without smartness,
which in turn suggests that building smartness is necessary before considering high levels of autonomy. Our
study unlocks the optimal steps that buyers could adopt to develop AI in procurement processes.

Keywords: articial intelligence, procurement, wholesale pricing, automation, smartness

1. Introduction
Articial intelligence (AI) is related to making machines or software mimic human behavior and
intelligence, and eventually supersede or augment human work. AI is becoming the new operational
foundation of business, and has transformed the very nature of companies|how they
operate and how they compete (Iansiti and Lakhani, 2020). AI has two unique capabilities: automa-
tion and smartness, which are associated with physical machines or software that replace manual
work through automated processes or augment human work through smart decisions (Boute and
1

2
Van Mieghem, 2020). AI can automate simple, tedious, and repetitive tasks to perform them faster
and cheaper. AI can also facilitate smarter control rules by continuously learning, reasoning, deciding,
and acting to drive a business outcome. As AI enables companies to reach unprecedented levels
of scale, scope, and learning speed, organizations around the world are eager to participate in this
AI transformation. However, the rise of AI is posing new challenges for organizations to understand
how it works, when it is the most powerful, and how to optimize their AI strategies.
AI has created new business opportunities and delivered value to organizations in numerous ways.
For example, a chatbot is an AI application that can automate basic, repeatable, standardized
interactions between customers and sellers. Specically, chatbots such as IKEA's Anna use voice
or texts to automate communications and create personalized customer experiences. The chatbot
market size is predicted to expand from $250 million in 2017 to $1.34 billion in 2024 (Pise, 2018),
and the adoption of chatbot feature is predicted to save businesses $11 billion annually by 2023
(Hampshire, 2018).
AI has also been applied to automate procurement tasks and assist strategic sourcing in businessto-
business (B2B) markets, which is referred to as cognitive procurement (Loo and Santhiram,
2018). Surveys reveal that 60% of companies use AI to automate the request-for-quotation process
and 50% of companies use AI to recommend new suppliers (Tata Consultancy Services, 2016).
There are two ways in which AI can be used for smarter sourcing in procurement. The rst is
the automation [...] For example, AI-powered [...] bots [...] The second|and more important|
use relates to AI-powered tools helping to rapidly collect, present and even analyse commodity,
market, and supply intelligence to inform market strategies.
|Nicholas Walden, Senior Director at The Hackett Group (HICX Solutions, 2018).
On one hand, chatbots have been used to automate the request-for-quotation process in procurement
by mimicking human interactions, thereby relieving workers from tedious and repeatable
tasks. For example, SAP Ariba|an information technology services company in the US|utilizes
a procurement AI assistant to request price quotations and draft contracts. Chatbots have been
shown to reduce labor costs by 39% for a global energy company by automating procurement
processes (Papa et al., 2019). On the other hand, procurement managers can also utilize AI to
identify potential suppliers, which is referred to as AI recommendation. Traditionally, procurement
companies often identify potential suppliers based on their colleagues' recommendation, which
is referred to as human recommendation. AI adds the component of smartness to procurement
manager's supplier selection decisions by using its extraordinary capability to collect and analyze
market information. To summarize, in the procurement context, automation helps buyers automatically
inquire about prices instead of asking in person, and smartness aids buyers by using an
AI algorithm to recommend suppliers.

3
Given that procurement is the core decision in B2B businesses, it is critical to study how AI
creates and delivers value along its automation and smartness capabilities. We investigate how
buyers' AI strategies aect suppliers' wholesale pricing decisions. Specically, we study the eect
of automation|that is, whether the buyer inquires about prices using an autonomous chatbot or
in person. We also study the eect of smartness|that is, whether the buyer signals the use of AI
recommendations in selecting suppliers.
In this study, we run a eld experiment by collaborating with a trading company that operates
on Alibaba's trading platform 1688 (1688.com), which is the largest domestic trading platform
in China. It serves millions of buyers and suppliers who use an integrated chat program called
Aliwangwang to communicate with each other. The trading company's procurement managers
are required to quote prices from suppliers on 1688. We design a 3  3 eld experiment. The
procurement representatives are (1) a female human, (2) a male human, or (3) a chatbot, where the
chatbot automatically sends inquiry messages without human involvement. The quoting messages
indicate that the supplier is (1) not informed of any recommendation, (2) recommended by a
peer, or (3) recommended by an AI algorithm. We test the eect of automation and smartness in
procurement by comparing suppliers' wholesale price quotes across the aforementioned three buyer
types and three recommendation conditions.
We nd that when automation is not equipped with a smart control, it negatively aects the
quoted wholesale price. Specically, chatbot buyers receive a higher price quote than human buyers.
This is because a supplier might believe that a chatbot buyer lacks the expertise in product specics,
and in turn, has a higher reservation price and a higher willingness to pay than human buyers.
Moreover, a supplier does not have to lower his wholesale price in order to develop a professional
relationship with a chatbot buyer. Consequently, the supplier prices discriminate against chatbot
buyers. In addition, as a side nding, our results reveal that the wholesale prices quoted to male
and female buyers are not signicantly dierent.
We nd that signaling the use of AI algorithms in selecting the supplier reduces the wholesale
price for chatbot buyers, but it cannot reduce the price for human buyers. This is because, for chatbot
buyers, suppliers believe in AI's capability to collect and learn from market information and in
AI's complete in
uence on chatbot buyers' decisions, thereby changing their perception of chatbot
buyers' reservation price and willingness to pay. However, human buyers are not machines. They
are susceptible to their own judgment and heuristics, thereby making them reluctant to strictly follow
algorithm-suggested decisions (Cui et al., 2015; Dietvorst et al., 2018; Ibanez et al., 2018; Sun
et al., 2020; Tan and Staats, 2020). Due to such decision deviations, suppliers may perceive that
human buyers do not follow AI's recommendations, thereby ignoring these buyers' use of AI and

4
not altering the wholesale price accordingly. In contrast, we nd that the traditional recommendation
without smart controls|that is, human recommendation|cannot reduce the price quotes for
either chatbot buyers or human buyers. This allows us to tease out the eect of recommendation
and attribute the overall eect of AI recommendation to the eect of smartness.
In summary, in the absence of smart controls, the buyer suers from automation by receiving
a higher wholesale price, whereas having smart controls leads to a lower wholesale price for these
autonomous buyers. This implies that when automation is implemented without smart controls, it
is not very valuable, which suggests that building smartness is necessary before implementing high
levels of autonomy.
Last, we study the combined value of automation and smartness. We nd that chatbot buyers
aided by a smart recommendation system receive the lowest price quote among all conditions. In
other words, AI delivers the most value when buyers use both automation and recommendation
in price inquiry. This nding highlights the value of using autonomous agents aided by a smart
recommendation system in procurement.
2. Literature Review
AI Automation and Recommendation. Prior research indicates that automation creates value
in inventory replenishment (Van Donselaar et al., 2010) and nancial services (Acimovic et al.,
2020; Kohler et al., 2011; Luo et al., 2019). An application of automation is a chatbot, which helps
human workers automate communications with consumers. Extant literature reveals that consumers
often dislike communicating with a chatbot, despite the fact that automation can improve
agents' productivity. We complement this literature by investigating suppliers' reactions to the
procurement managers' usage of chatbot.
Prior research has also shown that AI's recommendations add value in various contexts, such as
disease diagnosis (Leachman and Merlino, 2017), wholesale pricing (Karlinsky-Shichor and Netzer,
2019), and product recommendations (Haubl and Trifts, 2000). For example, Karlinsky-Shichor
and Netzer (2019) create an AI version of B2B salespersons' pricing decisions that mimics their past
pricing behavior, which improves prots by 10%. However, human decision-makers often choose to
rely on their own judgment, making them reluctant to strictly follow algorithm-instructed decisions.
Such decision deviation behavior has been widely documented in the literature. For example,
managers tend to use their own demand forecasts rather than forecasts provided by machines (Cui
et al., 2015; Dietvorst et al., 2018); doctors prioritize tasks in a manner that deviates from system
recommendations (Ibanez et al., 2018); workers pack orders in boxes larger than the size suggested
by the system (Sun et al., 2020); and restaurant managers deviate from the routing rules that they
are instructed to follow (Tan and Staats, 2020). We add to this literature by studying suppliers'
reactions when B2B buyers tell them that they (the suppliers) are recommended by AI algorithms.

5
Our paper is closely related to Boute and Van Mieghem (2020). The authors propose a framework
that synthesizes automation and smartness for companies who transform operations digitally.
They argue that having a smart control is necessary before high levels of autonomy are considered.
Our paper follows this framework to study the value and synergies between automation and
smartness in procurement processes. Our ndings echo Boute and Van Mieghem's insights in that
we empirically show that automation, when implemented without smart controls, does not bring
value and can even backre, whereas smartness is valuable. Specically, we nd that automation
causes suppliers to increase their wholesale prices, but AI recommendations can eectively lower
the price quotes. Consequently, AI delivers the most value when automation and smartness are
adopted in combination with each other.
Procurement and Wholesale Pricing. Procurement is a critical business decision. The literature
has studied various mechanisms, such as inventory investment (Jain et al., 2014), information
provision (Engelbrecht-Wiggans and Katok, 2008), timing of auctions (Bimpikis et al., 2020), and
trust (Fugger et al., 2019) to improve procurement eectiveness. We follow suit to study the integration
of AI as a market mechanism to aect request-for-quotation outcomes.
The procurement outcome that we measure is the wholesale price charged by sellers. Wholesale
pricing is one of the central topics of supply chain management (Cachon, 2003; Cachon and Netessine,
2006). In supply chains, the wholesale price that suppliers charge for downstream buyers is
an important determinant of suppliers' prot margins and buyers' prices, which in turn aects
protability. A supplier may charge dierent wholesale prices to retailers based on, for example,
buyer intermediation (Tunca and Zhu, 2018), supplier{buyer relationships (Zhang et al., 2014), or
buyers' race (Cui et al., 2020). We contribute to the literature by studying whether suppliers price
against or in favor of chatbot buyers and, if so, which features of AI allow it to deliver the most
value.
3. Research Hypotheses
We study how suppliers vary their wholesale prices to buyers with and without the use of AI on
an online B2B platform. Before purchasing a product, buyers research its market price by asking
for price quotes from suppliers. Suppliers then provide a price quote to buyers based on buyers'
characteristics and the inquiry message. In this section, we develop a framework that predicts the
eect of automation and smartness in procurement. We discuss whether suppliers price against
or in favor of (1) buyers' autonomous characteristic|whether the buyer is a chatbot or human,
(2) buyers' smartness characteristic|whether the buyer signals the use of AI recommendations in
selecting suppliers, and (3) buyers' autonomous and smartness characteristics|whether the buyer
is a chatbot with a smart control or a human without a smart control.

6
3.1. Eect of Automation
When deciding on a wholesale price to charge buyers in a B2B setting, a supplier's most pivotal
consideration is the buyer's best alternative to a negotiated agreement (BATNA). BATNA refers
to the most advantageous alternative action that the buyer can take if the negotiation reaches an
impasse (Fisher and Ury, 1981; Fisher et al., 2011; Pinkley et al., 1994). Consequently, the buyers'
BATNA determines the suppliers' pricing strategy: buyers with a stronger BATNA have better
outside options, and in turn, they have a lower reservation price and a lower willingness to pay
(Korobkin, 2014), which results in a lower wholesale price charged by suppliers.
We consider the scenario that the chatbot or human buyer asks for prices without providing
any recommendation information to the supplier, i.e., automation without smartness. Autonomous
chatbots are an eective tool to automate repeated inquiries and preprogrammed responses. In
our research context, a chatbot is used to automatically send inquiry messages to a group of suppliers,
saving buyers' time spent in sending messages to each supplier personally. However, these
traditional chatbots, when their main objective is to repeat tasks without smart controls, are not
equipped to address the complex requirements of B2B suppliers, who expect in-depth communications
and negotiations with buyers (Swanson, 2015). We interviewed several highly experienced
trading managers who conrm that procurement requires a signicant level of professional knowledge
in product specics, such as product materials, size, functionality, and after-sales service,
which preprogrammed chatbots might be less knowledgeable in.1 Consequently, suppliers may
believe that chatbots lack expertise in product specics and, in turn, have a worse BATNA and
thus a higher reservation price than human buyers.
Further, because chatbots lack personal feelings and empathy, suppliers do not need to lower
the wholesale price in order to develop a serious relationship with chatbot buyers (Dietvorst et al.,
2018; Luo et al., 2019). Therefore, we expect that without smart controls, chatbot buyers will be
price discriminated against and receive a higher price quote than human buyers.
Hypothesis 1 (Automation). Without smart controls, chatbot buyers receive a higher whole-
sale price quote than human buyers.
3.2. Eect of Smartness
In this section, we study the eect of having smartness in the process of wholesale price inquiries.
Smartness means that the buyer uses AI recommendations to select suppliers. Without claiming
the use of AI recommendations, suppliers would not be able to know and react to this. Therefore,
in our research context, smartness is signaled to the suppliers. Specically, when asking for a price
quote, the buyer tells the supplier that the company was recommended to the buyer by AI's market
1We discuss our interviews in detail in Section 5.1.

7
search and data analysis. The supplier can use this information to update beliefs about the buyer
and alter the oered wholesale price accordingly.
The information on the use of AI recommendations can be a key determinant for suppliers,
because AI has an extraordinary capability to collect and learn from market information (Haubl
and Trifts, 2000). Knowing that the buyer has comprehensive knowledge of the market aided by AI,
the supplier would believe that the buyer has a stronger BATNA|i.e., a better walk-away option|
and in turn would consider lowering the price. In addition to the capability of AI, suppliers would
also evaluate whether the AI recommendations have a complete in
uence on buyers' decisions. If
a buyer does not follow AI-generated recommendations, then the buyer's decisions will not heavily
rely on AI, which suggests that the supplier could ignore the buyer's AI usage.
We rst consider the scenario where the procurement manager uses autonomous chatbots to
ask for prices and signal that the supplier was selected by AI's market search. Because chatbots
are machines programmed to follow the AI's recommendations, the supplier would believe in the
thorough knowledge of the market gained by AI and the in
uence that AI has on the buyer.
Therefore, we expect that chatbot buyers' use of AI recommendations will improve the supplier's
perception of their BATNA, thereby reducing the supplier's wholesale price.
Next, we consider the scenario where the human buyer asks for prices in person and informs the
supplier that the company was recommended by AI. Humans are not machines. They are susceptible
to their own judgment and heuristics, thereby making them reluctant to strictly follow algorithms
and rules. This is known as decision deviation (Cui et al., 2015; Dietvorst et al., 2018; Ibanez et al.,
2018; Sun et al., 2020; Tan and Staats, 2020). Such deviation behavior from algorithm-instructed
decisions has been widely documented in the literature. For example, managers are shown to use
human forecasts rather than algorithmic forecasts (Cui et al., 2015; Deloitte, 2018); doctors are
shown to prioritize tasks in a manner that deviates from system recommendations (Ibanez et al.,
2018); workers are shown to pack orders in boxes larger than the system-recommended size (Sun
et al., 2020); and restaurant managers are shown to deviate from the routing rules that machines
instruct them to follow (Tan and Staats, 2020). Given this widespread recognition that humans
often deviate from algorithmic recommendations, we expect that suppliers would anticipate human
buyers to not strictly follow AI recommendations.
To further conrm that decision deviations exist in procurement, we interviewed nine professional
B2B suppliers with an average of 12 years' trading experience. In the interviews, we asked them to
share whether they believe in AI recommendations' in
uence on a human buyer or a chatbot buyer.
We summarize their quotes in Table 8 in the appendix. Most suppliers indicate that they believe
that such an AI recommendation can help chatbot buyers learn about market knowledge and can
dictate their sourcing choices. However, eight out of nine suppliers do not believe that a human

8
buyer will follow AI recommendations because they think that human buyers would make their
own judgment about the market and are likely to deviate from algorithms. Therefore, we expect
that suppliers would assume that human buyers do not follow the AI's recommendations, thereby
ignoring buyers' usage of AI and not altering their perception on the human buyers' BATNA. In
other words, the use of AI becomes ineective in reducing the wholesale price quote for human
buyers.
Hypothesis 2 (Smartness). (a) Chatbot buyers, when informing suppliers that they are
selected by smart AI algorithms, receive a lower wholesale price quote than chatbot buyers without
AI recommendations. (b) Human buyers, when informing suppliers that they are selected by smart
AI algorithms, receive a similar wholesale price quote as human buyers without AI recommenda-
tions.
3.3. Automation and Smartness
In this section, we study the eect of having both automation and smartness. We rst discuss the
eect of automation under smart controls. That is, we compare the dierence between chatbot
buyers with AI recommendations and human buyers with AI recommendations. When both human
and chatbots are equipped with AI recommendations, the eect boils down to who would follow
the AI's recommendations. According to Hypothesis 2(a), chatbots are programmed to follow the
AI's recommendations. And according to Hypothesis 2(b), human buyers may not fully follow the
AI's recommendations due to their tendency to deviate from AI-instructed decisions. Therefore,
suppliers will react to chatbot buyers' and ignore human buyers' usage of AI recommendations.
Taken together, when equipped with a smart control, suppliers would perceive that chatbot buyers
have more comprehensive market knowledge, thereby a stronger BATNA with a lower reservation
price than human buyers. We hypothesize this relation in Hypothesis 3(a).
Next, we study the dierence between chatbot buyers with AI recommendations and human
buyers without any recommendation. According to the above theories, AI enables buyers to have
comprehensive knowledge about the market and exerts a full in
uence on chatbot buyers. As a
result, suppliers would perceive chatbot buyers with smartness to have a stronger BATNA and
thus would oer them a lower wholesale price than human buyers without smartness. Therefore,
we expect that the eect of AI is the strongest when both automation and smartness are in place.
Hypothesis 3 (Automation and Smartness). (a) When informing suppliers that they are
selected by smart AI algorithms, chatbot buyers receive a lower wholesale price quote than human
buyers. (b) Chatbot buyers, when informing suppliers that they are selected by smart AI algorithms,
receive a lower wholesale price quote than human buyers without AI recommendations.
Electronic copy available at: https://ssrn.com/abstract=3570967

Figure 1: Framework of Automation and Smartness in Procurement
Human Buyer
Automated
Smart
+
=
H2 (b)
H1
H2 (a)
Automated
Smart
Smart
Automated
_
_
H3 (b)
_ H3 (a)
Notes. +, 􀀀, and = represent higher, lower, and similar price quotes, respectively. H1{H3 represent Hypotheses 1{3,
respectively.
We summarize the eect of automation and smartness on suppliers' price decisions in Figure 1.
We follow Boute and Van Mieghem's (2020) framework to classify buyers' AI strategies into four
groups: human buyer without the help of AI, automation enabled by chatbot buyers, smart control
enabled by AI recommendations, and the joint application of automation and smartness. In this
framework, Hypothesis 1 describes the pure eect of automation when we move from the \Human
Buyer" zone to the \Automated" zone; Hypothesis 2 describes the eect of smartness on human
buyers and chatbot buyers separately when we move from the \Human Buyer" zone to the \Smart"
zone, and from the \Automated" zone to the \Automated+Smart" zone, respectively; Hypothesis 3
describes the eect of automation under smartness when we move from the \Smart" zone to the
\Automated+Smart" zone and the joint eect of automation and smartness when we move from
the \Human Buyer" zone to the \Automated+Smart" zone.
4. Research Context
Alibaba Group launched 1688 in 1999, which is the largest domestic online B2B platform in China
(Alibaba, 2020a). This platform connects 30 million enterprise buyers and suppliers (China Daily,
2019); the suppliers provide products in 49 major categories, including apparel, general merchandise,
electronics, and car accessories (CNXtrans, 2020). The 1688 platform has a built-in instant
chat program called Aliwangwang that enables buyers to contact suppliers for product specics and
prices. Buying companies are permitted to embed autonomous chatbot features in Aliwangwang
in order to automate communications.
On 1688, a supplier introduces company information on a prole page and lists product information
on a product page. Figure 4 in the appendix illustrates an example of a supplier's prole and
product pages. The supplier's prole page displays basic company information (e.g., name, location,
membership status, and credibility) and trading performance on the platform (e.g., number of

peat purchase rate, and refund rate). Suppliers can pay to have
an elite membership in order to obtain advantages in product promotion and exposure. A supplier's
credibility has ve levels. The product page displays product characteristics|for example,
description, picture, price, and options|and transaction details|for example, number of reviews,
review rating, and transaction volume in the past 30 days.
A buyer also has a personal prole that includes the buyer's name, gender, date of birth, location,
photo, phone number, and email address. Buyers can search for a specic product and choose one
from a list of suppliers displayed by the platform. The buyer can then view the supplier's prole
and product details, as depicted in Figure 4. The buyer sends a price quote to the supplier on
Aliwangwang either through a personal message or using autonomous chatbots to automate the
inquiry process. After receiving an inquiry from a buyer, the supplier chooses whether to follow
up and how much to quote. After transaction details are settled, the buyer makes a payment, the
supplier ships the order, and the transaction is completed.
5. Identication Design
We aim to study the eect of the buyer's usage of automation and smartness on the suppliers'
price quoting strategy. We collaborate with a trading company that operates on 1688 to conduct
a eld experiment.
5.1. Study Design
In order to study the eect of automation, we design the sender who asks for the price quote to be a
female human, a male human, or an autonomous chatbot. We identify the value of pure automation
by comparing the price quote received by a chatbot buyer and a human buyer. In order to study the
eect of smartness, we design the sender to signal that the supplier is recommended by AI or human
peers, or to not signal any recommendation at all. We identify the value of AI recommendations by
comparing the price quote received with AI recommendations and without any recommendation.
We also introduce a treatment with human recommendations, in which the buyer signals that
the supplier was recommended by a (human) peer, in order to disentangle the pure impact of
having recommendations and the pure impact of having smart controls. If the eect of human
recommendations is weak, we can attribute the overall eect of AI recommendations to smartness.
Consequently, we employ a 33 experiment design by considering three types of buyers (female
buyer, male buyer, and chatbot buyer) and three recommendation conditions (no recommendation,
human recommendation, and AI recommendation). We outline how our experiment design matches
our AI framework in Figure 6 in the appendix.
The company has multiple procurement representatives whose routine job is to keep track of
market dynamics by collecting wholesale price information. The company also uses chatbots to
Electronic copy available at: https://ssrn.com/abstract=3570967

 suppliers. The trading company asks for price quotes via three
buying representatives: a female buyer, a male buyer, and a chatbot buyer. We tailor the messages
to incorporate dierent recommendation conditions. Thereafter, we record and compare suppliers'
responses. Table 1 summarizes the study design.
Table 1: Field Experiment Design
Design
Automation  Recommendation Condition
No Recommendation Human Recommendation AI Recommendation
Chatbot Female Male Chatbot Female Male Chatbot Female Male
Planned Sample Size 440 440 440 440 440 440 440 440 440
Actual Sample Size 440 439 437 435 436 439 440 439 439
Notes. The planned sample size was 3,960|that is, 440 suppliers per treatment arm. The actual sample size is 3,944 after
excluding unavailable listings.
We select a sample of 3,960 products from 3,960 suppliers in the car accessories sector.2 This
sector, which is the backbone of China's industrial ascent (Hong and Einhorn, 2018), has a large
number of suppliers on 1688. Car-related products have also been studied to test price discrimination
behavior in previous literature (Busse et al., 2017). In our sample, there are 14 product
subcategories including, for example, automobile data recorders, car cameras, car MP3, vehicle
displays, vehicle bluetooth headsets, vehicle bluetooth speakers, vehicle-mounted mobile holders,
vehicle chargers, vehicle lockers, car vacuum cleaners, GPS locators, vehicle air puriers, vehicle
refrigerators, and vehicle-mounted inverters.3 Each supplier usually sells a wide selection of products
(e.g., a vehicle refrigerator in capacities of 6, 12, or 20 liters). From each supplier's listed
products, we select a product model that is the most common and standard in the market. Suppliers
are randomly assigned to one of the nine (33) treatment arms. Consequently, we obtain
1,320 suppliers per buyer type, 1,320 suppliers per recommendation condition, and 440 suppliers
per treatment arm. This means that each supplier is quoted only once.
Note that all of our studied products are commodity products. Relative to non-commodities
which are custom and unique, commodities are standard and basic goods. One might question that
whether procuring standard commodities requires buyers to have extensive expertise in product
2 The sample size is determined by the statistics power calculation. By running a pilot experiment with 40 chatbot
buyers, 40 female buyers, and 40 male buyers under the no recommendation, human recommendation, and AI recom-
mendation conditions, respectively, we compare the price discounts across treatment arms and obtain their eect size.
Based on a two-sided t-test with a power level of 0.8 and a signicance level of 0.05, we require 99 observations with
a 0.40 eect size between chatbot and female buyers under the \no recommendation" condition, 38 observations with
a 0.65 eect size between chatbot and male buyers under the \no recommendation" condition, 393 observations with
a 0.20 eect size between the \no recommendation" and \human recommendation" conditions, and 164 observations
with a 0.31 eect size between the \no recommendation" and \AI recommendation" conditions. We determined the
sample size per treatment arm to be 440 (>393) to further ensure the validity of the experiment.
3 In order to explore new markets, the trading company species these 14 product categories from which our research
team independently selects the supplier and product sample. We validate with the company that there is no previous
supplier in the sample.

ral highly experienced trading managers conrm that buying
commodity products also requires signicant professional knowledge such as product materials,
size, functionality, and after-sales service, which enables suppliers to exert in-depth communications
and negotiations. Their exact interview quotes are summarized in Table 9 in the appendix. On
the other hand, when procuring non-commodity products, chatbots might be less knowledgeable
in product specics due to their uniqueness. Therefore, the estimated eect of automation for
non-commodities products might be larger than the eect identied in our study.
In order to ensure that suppliers are randomly assigned to treatment arms, we conduct a randomization
check across the following supplier characteristics: (1) membership status (i.e., the number
of years that the supplier has been an elite member), (2) credibility (i.e., the supplier's credibility
based on the Alibaba credit system), (3) number of transactions in the past 90 days, (4) number
of buyers in the past 90 days, (5) repeat purchase rate in the past 90 days, (6) refund rate in the
past 90 days, (7) listed price of the selected product, (8) trading volume of the selected product
in the past 30 days; (9) number of reviews for the selected product, and (10) review rating for the
selected product. Table 2 presents the summary statistics of these variables. Further, the p-values
in Table 3 are all larger than 0.05, which ensures the randomized assignment.
5.2. Study Procedure
Buyers' characteristics (male, female, or chatbot) are signaled by their names and prole pictures.4
The buyers sent price inquiries to the selected suppliers during the period December 18, 2019,
to January 20, 2020.5 Each message asks for a price quote per unit for 1,000 units of the preselected
product. The message content varies based on the recommendation conditions. In the \no
recommendation" condition, the buyer includes the most basic information in the inquiry message
without indicating any human or AI recommendation. In particular, all buyers in this condition sent
a message that said, \Hello, I am [a procurement manager or a chatbot buyer]. We are interested
in your product: [the specic product name and link of this product]. Could you please quote
us your best price per piece for an order of 1,000 units?" The AI chatbot buyers disclose their
machine identity in order to comply with China's regulation regarding AI (Laskai and Webster,
2019). Quoting a price including the packaging fee is the industry norm. In order to ensure that the
quoted prices are not confounded by the value-added tax or shipping fee, the buyers ask suppliers
4 The chatbot buyer has a standard robotic prole picture. We edit the photos of human buyers using Photoshop to
ensure their photos have a similar attractiveness.
5 Our experiment (which was from December 18, 2019, to January 20, 2020) was conducted before the outbreak of
COVID-19 (which caused the rst lockdown measure to take place on January 23, 2020) and before the Chinese New
Year (which was from January 24, 2020, to January 30, 2020). As a result, our experiment was not aected by the
pandemic or the holiday.
Electronic copy available at: https://ssrn.com/abstract=3570967


bership
Cred-
ibility
No. of
Trans.
No. of
Buyers
Repeat
Purchase
Rate
Refund
Rate
Listed
Price
No. of
Reviews
Review
Rating
Trading
Volume
Obs.
Chatbot
4.62
(3.28)
3.26
(0.90)
503.8
(1,759)
170.0
(482.2)
28.63
(18.19)
5.88
(10.01)
140.9
(202.3)
25.89
(155.5)
2.46
(2.47)
205.9
(1,940) 1,320
Female
4.47
(3.08)
3.24
(0.93)
597.4
(2,959)
174.9
(529.6)
27.47
(17.52)
6.50
(11.57)
142.0
(198.4)
18.54
(102.4)
2.41
(2.47)
166.3
(1,625) 1,320
Male
4.44
(3.12)
3.20
(0.92)
536.6
(2,144)
171.6
(495.6)
27.58
(16.96)
6.50
(14.28)
140.2
(197.6)
33.96
(391.1)
2.48
(2.47)
140.1
(1,590) 1,320
N
4.79
(3.46)
3.29
(0.91)
502.1
(1,940)
163.7
(428.3)
29.01
(18.40)
5.62
(9.17)
139.7
(205.7)
32.24
(194.4)
2.51
(2.48)
340.5
(3,132)
440
Chatbot
H
4.62
(3.39)
3.26
(0.93)
494.1
(1,391)
169.7
(477.7)
29.07
(18.68)
6.08
(11.25)
133.9
(191.8)
25.24
(157.7)
2.35
(2.47)
121.5
(650.4)
440
A
4.45
(2.98)
3.23
(0.85)
515.3
(1,896)
176.4
(535.6)
27.80
(17.48)
5.93
(9.51)
149.1
(209.1)
20.20
(99.68)
2.51
(2.46)
155.8
(1,020)
440
N
4.47
(3.05)
3.25
(0.95)
492.0
(1,799)
167.4
(456.1)
26.12
(16.88)
7.09
(12.6)
132.7
(173.6)
21.84
(148.6)
2.42
(2.48)
145.6
(1,057)
440
Female
H
4.50
(3.20)
3.23
(0.97)
683.3
(3,516)
174.2
(514.6)
28.40
(18.55)
5.95
(10.13)
153.3
(215.2)
20.96
(81.47)
2.41
(2.47)
121.2
(743.8)
440
A
4.44
(2.98)
3.23
(0.87)
617.0
(3,269)
183.2
(608.0)
27.90
(17.05)
6.45
(11.77)
140.0
(204.0)
12.83
(52.30)
2.40
(2.47)
232.1
(2,501)
440
N
4.40
(2.82)
3.18
(0.94)
523.1
(1,743)
165.2
(439.1)
27.97
(16.50)
6.57
(13.11)
141.1
(193.9)
30.97
(181.7)
2.58
(2.48)
106.2
(824.3)
440
Male
H
4.43
(3.14)
3.19
(0.95)
632.2
(2,953)
171.5
(489.7)
27.50
(17.52)
6.29
(10.60)
140.1
(197.3)
41.95
(636.1)
2.34
(2.47)
105.9
(892.2)
440
A
4.51
(3.02)
3.24
(0.87)
454.3
(1,429)
178.2
(552.6)
27.28
(16.88)
6.64
(18.13)
139.5
(202.0)
28.97
(147.7)
2.53
(2.47)
208.2
(1,443)
440
Notes. \No. of Trans." indicates the numbers of transactions. \Obs." represents the number of observations. N, H, and A
represent no recommendation, human recommendation, and AI recommendation, respectively.
Table 3: Randomization Check (p-value)
C vs F C vs M F vs M
Chatbot Female Male
N vs H N vs A H vs A N vs H N vs A H vs A N vs H N vs A H vs A
Membership 0.22 0.15 0.83 0.48 0.13 0.43 0.89 0.88 0.77 0.87 0.56 0.70
Credibility 0.51 0.09 0.30 0.66 0.34 0.62 0.86 0.82 0.97 0.89 0.34 0.42
No. of Trans. 0.32 0.67 0.55 0.94 0.92 0.85 0.31 0.48 0.77 0.51 0.52 0.26
No. of Buyers 0.80 0.93 0.87 0.84 0.70 0.85 0.84 0.67 0.81 0.84 0.70 0.85
Repeat
Purchase Rate
0.10 0.13 0.87 0.96 0.32 0.30 0.06 0.12 0.68 0.68 0.54 0.85
Refund Rate 0.14 0.20 1.00 0.51 0.62 0.83 0.14 0.44 0.50 0.73 0.94 0.72
Listed Price 0.89 0.93 0.82 0.67 0.50 0.26 0.12 0.57 0.35 0.94 0.90 0.96
No. of Reviews 0.15 0.49 0.17 0.56 0.25 0.57 0.91 0.23 0.08 0.73 0.86 0.68
Review Rating 0.65 0.78 0.46 0.34 1.00 0.33 0.97 0.91 0.94 0.17 0.80 0.26
Trading Volume 0.57 0.28 0.63 0.15 0.24 0.55 0.69 0.51 0.37 1.00 0.20 0.21
Notes. \No. of Trans." indicates the numbers of transactions. C, F, and M represent chatbot buyer, female buyer, and male
buyer, respectively. N, H, and A represent no recommendation, human recommendation, and AI recommendation, respectively.
to quote a price excluding these fees. The original inquiry messages in the eld experiment are in
Chinese and are carefully translated and presented in Figure 5 in the appendix.

ition, the buyer discloses that the supplier is recommended
 recommended to us by a peer." We follow the common
practice and the industry norm to not include the peers' name in the inquiry message.6 In the \AI
recommendation" condition, the buyer reveals that the supplier is recommended by AI's market
search and data analysis: \Your company was recommended to us by an AI system's market
information collection and data processing."
Within a week after the inquiry, we record and compare the initial price quotes.7 We received
1,807 responses that included a price quote from the 3,944 suppliers that we sent messages to.
6. Results
In this section, we study the eect of automation and smartness on suppliers' price quoting strategy.
We examine the eect of automation in Section 6.1, the eect of smartness in Section 6.2, and the
joint eect of automation and smartness in Section 6.3.
6.1. Eect of Automation
In a B2B setting, it is an industry norm and a common practice that suppliers privately quote a
lower price than their publicly listed prices (Cui et al., 2020). In order to conduct a fair comparison
of the amount of price discount oered by suppliers, we follow previous literature (Cui et al., 2020)
to compare the price discount percentage relative to the listed price:
Discount=100%

Listed Price􀀀Supplier's Quoted Price
Listed Price

: (1)
Automation without Smartness. We rst focus on the \no recommendation" condition and
investigate the eect of automation on suppliers' price quoting strategy. Panel A of Table 10 in
the appendix presents the summary statistics of the suppliers' price discounts for chatbot, female,
and male buyers. Figure 2 presents a visual illustration. In particular, chatbot, female, and male
buyers receive an average price discount of 18.01%, 19.15%, and 20.96%, respectively|that is, both
female and male buyers receive a lower price quote than chatbot buyers. Moreover, the dierence
of the price discount between male and chatbot buyers is statistically signicant (p-value =0:07).
6 In our \human recommendation" message design, a buyer does not provide the name of the peer who recommended
the supplier, and it has been validated that such a design format conforms to norms regarding both condentiality
and industry practice (Cui et al., 2020).
7 Following the literature (Ayres and Siegelman, 1995; Busse et al., 2017; Cui et al., 2020), our study focuses on the
initial price quote because (1) the initial price quote re
ects the supplier's perception of the buyer's willingness to
pay; (2) suppliers could easily lose customers to competitors if they do not oer an attractive initial price in an online
trading platform; and (3) the initial price quote, unlike a second price quote or price concession, is not confounded
by any bargaining or negotiation techniques.
Electronic copy available at: https://ssrn.com/abstract=3570967


ble that represents whether a buyer is a chatbot, female, or male;
Controlsi is a vector of control variables regarding supplier characteristics, including membership
status, number of transactions, listed price, repeat purchase rate, and number of reviews.
Figure 2: Eect of Automation and Smartness
18.01
19.15
20.96
19.36
17.39
19.46
18.01 18.28
22.57
18.76
21.04 20.65
16
18
20
22
24
Chatbot Female Male All Data
Discount (%)
Buyers' Type
No Recommendation Human Recommendation AI Recommendation
The estimation results are presented in the rst column of Table 4, where the omitted buyers'
type is the chatbot group. The coecients of Female (Male) represent the additional price
discounts oered to female (male) buyers relative to chatbot buyers. The coecient of Male is
weakly positively signicant (p-value < 0:1), which implies that the supplier quotes a signicantly
lower wholesale price to human|particularly male|buyers than chatbot buyers, which weakly
supports Hypothesis 1. Note that we conduct several analyses in order to conrm the robustness
of this coecient: a combined regression with all the observations in Section 7.1 and an analysis
with time xed eects in Section 7.2. All these analyses support Hypothesis 1. In other words, the
implementation of pure automation does not help buyers and can even backre in a procurement
setting. This is because a chatbot buyer, due to its autonomous and unsmart nature, signals a
higher willingness to pay than human buyers, and human suppliers are less interested in building
a professional relationship with a chatbot buyer.
Automation under Smartness. Next, we discuss the eect of automation on suppliers' pricing
strategy in the presence of smartness. Panel C of Table 10 in the appendix presents the summary
statistics of the suppliers' price discounts for chatbot, female, and male buyers under the \AI
recommendation" condition. In particular, chatbot, female, and male buyers, when equipped with
smart recommendations, receive a price discount of 22.57%, 18.76%, and 21.04%, respectively. The

having automation and smartness) and human buyers (only
 4. We can see that chatbot buyers receive a signicantly lower
er words, automation is helpful in the presence of smartness.
This nding echoes the conjecture of Boute and Van Mieghem (2020): in the presence of smart
controls, it is conceivable that trust in the algorithm increases and risk is contained, which opens
up the possibility of higher levels of autonomy.
Automation under Human Recommendation. In addition, from column II of Table 4, we can
observe that under the \human recommendation" condition, the coecient of Female is weakly
positively signicant (p-value < 0:1), which implies that the supplier quotes a signicantly lower
wholesale price to human|particularly female|buyers than chatbot buyers. In other words, the
implementation of automation still results in a higher price even when human recommendations are
adopted. This highlights the importance of having smart controls when implementing automation
in a procurement setting.
Table 4: Eect of Automation on Price Quote
Dependent Variable: Discount
No Recommendation
I
Human Recommendation
II
AI Recommendation
III
All Data
IV
Male 0.028
(0.016)
0.009
(0.014)
-0.015
(0.016)
0.009
(0.009)
Female 0.010
(0.016)
0.026
(0.014)
-0.037
(0.016)
0.004
(0.009)
Supplier Controls Yes Yes Yes Yes
Observations 595 665 547 1,807
R2 0.047 0.054 0.031 0.033
Notes. This table tests the eect of automation on the price discount for four dierent samples. Results from columns I{
III are based on the sample under the \no recommendation" condition, under the \human recommendation" condition,
and under the \AI recommendation" condition, respectively. Results from column IV are based on the full sample.
p <0:1; p <0:05.
Gender. A natural extension that we can study is whether suppliers price discriminate based
on buyers' gender. Table 10 in the appendix and Figure 2 summarize the price discounts for
female and male buyers under dierent recommendation conditions. In the \no recommendation"
condition, we nd that female and male buyers receive an average price discount of 19.15% and
20.96%, respectively; there is no statistically signicant dierence between male and female buyers
(p-value=0:26). This result also holds under the \human recommendation" condition and the \AI
recommendation" condition. We also formally test the price dierence based on buyers' gender,
Discounti =+Genderi +
Controlsi +i; (3)

17
re presented in Table 5, where the omitted variable is Female;
nicant.

No Recommendation
I
Human Recommendation
II
AI Recommendation
III
All Data
IV
Male 0.020
(0.016)
-0.017
(0.014)
0.022
(0.015)
0.006
(0.009)
Supplier Controls Yes Yes Yes Yes
Observations 410 453 395 1,258
R2 0.046 0.040 0.045 0.033
Notes. This table tests the eect of gender on the price discount for four dierent samples. Results from columns I{III
are based on the sample under the \no recommendation" condition, under the \human recommendation" condition,
and under the \AI recommendation" condition, respectively. Results from column IV are based on the full sample.
We show that there is no gender discrimination in the B2B procurement context. This result differs
from the ndings in the business-to-consumer (B2C) settings|that female consumers receive
a higher price than male consumers because they are perceived to be less knowledgeable (Busse
et al., 2017; Mejia and Parker, 2019). Unlike B2C consumers whose purchasing decisions are often
emotional and irrational, B2B buyers are professional procurement managers whose job is to negotiate
with suppliers. Consequently, male and female procurement managers are perceived to have
a similar willingness to pay (Goldberg, 2018).
6.2. Eect of Smartness
AI recommendation. We investigate how AI recommendation aects suppliers' price quoting
strategy for chatbot, female, and male buyers, respectively. Table 11 in the appendix summarizes
the suppliers' price discounts for chatbot, female, and male buyers. Figure 2 presents an
illustration. In particular, for chatbot buyers, the average price discount is 18.01% under the \no
recommendation" condition and 22.57% under the \AI recommendation" condition, respectively.
This implies that, compared to the \no recommendation" condition, AI recommendation signi-
cantly reduces the wholesale price quoted for chatbot buyers (p-value = 0:01). For female (male)
buyers, the average price discount is 19.15% (20.96%) under the \no recommendation" condition
and 18.76% (21.04%) under the \AI recommendation" condition, respectively. This implies that,
compared to the \no recommendation" condition, AI recommendation cannot reduce the wholesale
price quoted for female or male buyers.
We also formally examine the impact of recommendation conditions on price discounts,
Discounti =+Conditioni +
Controlsi +i; (4)

18
where Conditioni is a binary variable that represents the \no recommendation" condition or \AI
" condition.


Female
II
Male
III
All Data
IV
AI Recommendation 0.042
(0.017)
-0.003
(0.015)
0.000
(0.015)
0.012
(0.009)
Supplier Controls Yes Yes Yes Yes
Observations 337 421 384 1,142
R2 0.040 0.032 0.058 0.033
Notes. This table tests the eect of smartness on price discounts for four dierent samples. Results from columns
I{III are based on the sample of chatbot, female, and male buyers, respectively. Results from column IV are based
on the full sample. p <0:05.
The coecient of AI Recommendation represents the additional price discount that a buyer can
obtain when signaling that the supplier is recommended by an AI algorithm, compared to the
\no recommendation" condition. The coecient of AI Recommendation is signicant and positive
(p-value < 0:05) for a chatbot buyer, but not signicant for female or male buyers. These results
conrm that a smart recommendation is eective for lowering prices for chatbot buyers but not
for human buyers, thereby supporting Hypothesis 2. Because of AI's ability to search and learn
about market information, suppliers believe that chatbot buyers have a lower reservation price and
a lower willingness to pay, and therefore reduce their wholesale price. However, human buyers are
deemed to not fully follow algorithms' recommendations and would not be able to benet from
claiming the use of AI recommendations.
In summary, having a purely autonomous process leads to a higher wholesale price, putting
buyers in a disadvantageous position, whereas having a smart control leads to a lower wholesale
price. In other words, automation is not very valuable when implemented without smart controls,
which suggests that building smartness is necessary before high levels of autonomy are to be
considered.
Human Recommendation. Recall that we introduced a treatment with human recommendation
in order to disentangle the pure impact of having any recommendation at all and the pure impact
of having smart controls. Next, we study this human recommendation eect. If this eect is weak,
then we can conclude that the eect of AI recommendation stems from having smart controls.
Table 11 in the appendix and Figure 2 indicate that the average price discount for chatbot buyers
is 18.01% under the \no recommendation" condition and 17.39% under the \human recommendation"
condition, respectively. We perform a t-test and nd that the dierence is insignicant
(p-value = 0:68). We nd a similar result for human buyers that human recommendations cannot

19
help human buyers lower the wholesale price received from suppliers. We conjecture that this is
driven by two reasons. First, due to humans' limitations in information processing (Payne, 1982;
often distrust buyers when they present soft social information
hanging suppliers' belief regarding buyers' willingness to pay and
their pricing strategy.
6.3. Joint Eect of Automation and Smartness
Thus far, we have demonstrated that automation brings a negative eect to buyers while smartness
brings a positive eect. Next, we study the joint value of automation and smartness. Table 12 in
the appendix summarizes the price discounts received with and without automation and smartness.
This table reveals that autonomous chatbot buyers, when informing suppliers that they are selected
by smart algorithms, receive a lower wholesale price quote than human (particularly female) buyers
without any recommendation (p-value =0:05). We also formally test this joint eect,
Discounti =+Jointi +
Controlsi +i; (5)
where Jointi is a categorical variable that represents a chatbot buyer aided by AI recommendations,
a female buyer without recommendations, or a male buyer without recommendations. The
estimation results are presented in Table 7, where the omitted variable is when a buyer is equipped
with both automation and smartness. Table 7 reveals that having both automation and smartness
can eectively reduce the price for (particularly female) buyers (p-value<0:05). This implies that
we should improve the levels of autonomy and smartness simultaneously.
Table 7: Joint Eect of Automation and Smartness on Price Quote
Dependent Variable: Discount
Male -0.014
(0.018)
Female -0.034
(0.017)
Supplier Controls Yes
Observations 562
R2 0.034
Notes. This table tests the joint eect of automation and smartness on price
discount. Results are based on the sample of chatbot buyers under AI recommen-
dation pooled with human buyers without recommendation. p <0:05.
We summarize the results of buyers' AI strategies in our framework in Figure 3. First, when a
buyer adopts pure automation but without smartness by moving from the \Human Buyer" zone
to the \Automated" zone, the buyer suers from automation by receiving a higher price. However,
Electronic copy available at: https://ssrn.com/abstract=3570967

Figure 3: Framework and Results of Automation and Smartness in Procurement
Human Buyer
Automated


Automated
Smart
Smart
Automated
_ _
Notes. +, 􀀀, and = represent higher, lower, and similar price quotes, respectively.
when the buyer adopts automation in the presence of smartness by moving from the \Smart" zone
to the \Automated+Smart" zone, the buyer benets from automation by receiving a lower price.
Second, when a human buyer is equipped with a smart algorithm by moving from the \Human
Buyer" zone to the \Smart" zone, smartness does not change the price. However, when a chatbot
buyer incorporates a smart recommendation system by moving from the \Automated" zone to
the \Automated+Smart" zone, smartness becomes helpful in reducing the price. Last, when the
buyer adopts both automation and smartness by moving from the \Human Buyer" zone to the
\Automated+Smart" zone, the buyer can also benet from receiving a lower price quote.
7. Robustness Check
In this section, we conduct additional analysis to check the robustness of our key insights regarding
the individual and joint eects of automation and smartness.
7.1. Combined Regression
In our main analysis, we studied the eect of automation and the eect of smartness separately.
We now combine all observations into a single regression:
Discounti =+0Typei +1Recommendationi
+2Typei Recommendationi +
Controlsi +i; (6)
where Recommendationi is a categorical variable that represents the \no recommendation" condition,
\human recommendation" condition, or \AI recommendation" condition.
Table 13 in the appendix reports the estimation results, where the omitted buyers' type is the
chatbot buyer and the omitted recommendation condition is the \no recommendation" condition.
This table shows three observations. First, the coecient of Male is weakly positively signicant
(p-value<0:1). That is, chatbot buyers receive a signicantly higher price quote than human buyers
without recommendations; this is consistent with our result on the eect of automation without

thesis 1. Second, the coecient of AI Recommendation is positively
signicant (p-value < 0:05). That is, AI recommendations help chatbot buyers receive a
lower price quote; this is consistent with our result on the eect of smartness on chatbot buyers,
thereby supporting Hypothesis 2(a). Third, the coecients of Male  Human Recommendation
 negatively signicant (p-value < 0:1). That
e discrimination; these results are consistent with
our main result on the eect of automation under smartness, thereby supporting Hypothesis 3(a).
7.2. Time Fixed Eects
We test our key results by including the time xed eects at two levels: the inquiries' request date
and the inquiries' quote date. Because dierent suppliers may take dierent amounts of time to
respond to a price inquiry, the quote dates for the same batch of inquiries might dier. To ensure
rigor and robustness, we test for both time xed eects.
The estimation results with time xed eects are shown in Tables 14 and 15 in the appendix. As
shown in column I of Panel A in Tables 14 and 15, the coecients of Male are positively signicant
(p-value<0:1), which implies that suppliers quote a signicantly lower wholesale price to human|
particularly male|buyers than chatbot buyers in the absence of smartness. However, column III
of Panel A in Tables 14 and 15 shows that the coecients of Female are negatively signicant
(p-value < 0:05), which implies that chatbot buyers receive a signicantly lower price quote than
(particularly female) buyers when smartness is adopted. This is consistent with our main results
regarding the eect of automation, thereby supporting Hypothesis 1 and Hypothesis 3(a).
As shown in Panel B of Tables 14 and 15, the coecients of AI recommendation are positively
signicant (p-value < 0:05) for a chatbot buyer, but not signicant for female or male buyers.
This is consistent with our main results regarding the eect of smartness, thereby supporting
Hypothesis 2(a) and Hypothesis 2(b).
As shown in Panel C of Tables 14 and 15, having both automation and smartness can eectively
reduce the price for (particularly female) buyers (p-value<0:05). This is consistent with our main
results regarding the joint eect of automation and smartness, thereby supporting Hypothesis 3(b).
7.3. Simulated AI Recommendation
In our design, the signal that the supplier is recommended by AI is randomly assigned to each
supplier. In practice, it may be true that only some (high-quality) suppliers would receive such
signals. In order to simulate such a scenario, we follow our collaborative company's guide to score
10 supplier/product characteristics (as shown in Section 5) according to how much they determine

22
 suppliers above the average score as high-quality suppliers
and the rest as low-quality suppliers. We then simulate an AI recommendation condition where
buyers equipped with AI recommendation only contact the high-quality suppliers. In this way, we
can simulate the situation where only high-quality suppliers are selected by AI algorithms and
tness in practice by comparing suppliers' wholesale prices
0.25 under the \no recommendation" condition, which is lower
than 0.31 under the \simulated AI recommendation" condition, conrming that only high-quality
suppliers are included in the sample.
Table 16 in the appendix summarizes the suppliers' price discounts for chatbot, female, and
male buyers under the \simulated AI recommendation" condition and the \no recommendation"
condition. In particular, for chatbot buyers, the average price discount is 18.01% without recommendations
and 23.91% with the simulated AI recommendation. This means that the simulated
AI recommendation signicantly reduces the wholesale price quoted for chatbot buyers (p-value=
0:01). However, consistent with our main result, the simulated AI recommendation cannot reduce
the wholesale price quoted for human buyers; for female (male) buyers, the average price discount
is 19.15% (20.96%) without AI recommendation and 18.76% (21.34%) with simulated AI
recommendation, respectively.
We also formally examine the impact of the \simulated AI recommendation" on price by
Discounti =+AIRecommendationi +
Controlsi +i; (7)
where AIRecommendationi is a binary variable that represents the \no recommendation" condition
or the \simulated AI recommendation" condition. The estimation results are presented in
Table 17 in the appendix, where the coecient of Simulated AI Recommendation is signicant (p-
value < 0:05) and positive for chatbot buyers, but not signicant for human buyers. These results
again conrm that a smart recommendation is eective in lowering prices for chatbot buyers but
not for human buyers.
7.4. Heterogeneous Treatment Eect
We next test whether any supplier or product characteristics (i.e., the number of transactions, listed
price, review rating, and trading volume) could change the eect of automation and smartness.
For the eect of automation, we use the following estimation:
Discounti =+1Typei +2Moderatori +3Moderatori Typei +
Controlsi +i; (8)

23
where 2 represents how a supplier or product characteristic moderates the eect of automation
on the wholesale price quotes. Moderatori represents the number of transactions for the supplier,
product's listed price, review rating, or trading volume. Controlsi includes all other control variables
except for the tested moderator. Table 18 in the appendix presents the estimation results.
For the eect of smartness, we use the estimation below. Table 19 in the appendix presents the
estimation results.
Discounti =+1Conditioni +2Moderatori
+3Moderatori Conditioni +
Controlsi +i: (9)
For the joint eect of automation and smartness, we use the estimation below. Table 20 in the
appendix presents the estimation results.
Discounti =+1Jointi +2Moderatori +3Moderatori Jointi +
Controlsi +i: (10)
Overall, none of the studied characteristics (except for the listed price) has an impact on the
individual and joint eects of automation and smartness. A higher listed price weakens the eectiveness
of smartness for chatbot buyers, probably because suppliers are more prudent when selling
expensive products and are less likely to regard AI-driven price quotations as a serious negotiation.
8. Conclusion
AI is transforming the very nature of procurement|how to operate and how to interact with
supply chain partners. According to the Roland Berger's survey on Fortune Global 500 companies,
67% of chief procurement managers rank AI among their top three priorities for the next 10 years
(Marlinghaus, 2018). Thus, we explore how a buyer's AI strategy would aect the wholesale price
received from suppliers. By designing and conducting a randomized eld experiment, we nd that
having a purely autonomous request-for-quotation process results in a higher price quote|that
is, suppliers price-discriminate a not-so-smart chatbot buyer. Further, we nd that introducing a
smart control|signaling that the supplier is recommended by a smart system|can reduce the
price quoted for chatbot buyers. Last, we show that automation and smartness can jointly reduce
the wholesale price quoted by suppliers, thereby highlighting the potential of a smart automation
in procurement.
Our study provides guidance for companies moving toward automating their standard and routine
processes, such as price requests and new supplier selection. In fact, excessive and duplicated
processes can comprise up to 40%{60% of a procurement company's capacity (Papa et al., 2019).
AI is capable of unlocking employees' workload for more strategic pursuits, thereby transforming
the transaction-oriented procurement toward the strategy-oriented procurement, which is known

24
as Procurement 4.0 (Marlinghaus, 2018; Loo and Santhiram, 2018). Our results indicate the great
potential of these AI-related initiatives in procurement.
Our ndings further shed light on how to implement AI strategies in procurement. In the absence
of AI smartness, automation alone can backre. This implies that a company should rst initiate
and strengthen its smart control algorithms, such as improving the prediction accuracy of its
recommendation systems, before considering a high level of autonomy. In order to ensure the eectiveness
of smartness, companies should help their employees get along with AI|that is, reduce
their biases and enhance their trust in algorithms. Our work also provides managerial implications
for online trading platforms aiming to embrace AI. Platforms such as Alibaba have initiated
the automatic request-for-quotation systems as a premium service provided for buyers (Alibaba,
2020b). Our study suggests that such automatic systems should be facilitated with a smart supplieridenti
cation system in order to reduce the wholesale price charged to downstream buyers as well
as reducing the ineciencies of supply chains arising from the double marginalization issue.
AI has become the universal engine of execution, driving the explosive growth of new business
models, but there is limited empirical research to understand and quantify how AI works and when
it is the most powerful (Terwiesch et al., 2020; Terwiesch, 2019). Our study is among the rst to
research how AI creates and delivers value in a critical business process, namely, procurement. We
hope that our paper will serve as a stepping stone for future AI-related business research.







----------------------------------------------------------------------------------






Han, E., Yin, D., & Zhang, H. (2023). Bots with feelings: Should AI agents express positive
emotion in customer service?. Information Systems Research, 34(3), 1296-1311.

Abstract
Customer service employees are generally advised to express positive emotion in their interactions
with customers. The rise and maturity of artificial intelligence (AI) powered conversational agents, also
known as chatbots, beg the question: should AI agents be equipped with the ability to express positive
emotion in customer service? This research explores how, when, and why an AI agent’s expression of
positive emotion affects customers’ service evaluations. We argue that AI-expressed positive emotion can
influence customers via dual pathways: an affective pathway of emotional contagion and a cognitive
pathway of expectation-disconfirmation. We propose that positive emotion expressed by an AI agent (vs.
a human employee) is less effective in facilitating service evaluations because of a heightened level of
expectation-disconfirmation. We further introduce customers’ relationship norm orientation as a novel
individual difference variable that affects their expectations toward the AI agent and moderates the
cognitive pathway of expectation-disconfirmation. Results from three laboratory experiments substantiate
our claims. By revealing a distinctive impact of positive emotion expressed by an AI agent compared with
a human employee, these findings deepen our understanding of customers’ reactions to emotional AIs and
offer valuable insights for the deployment of AIs in customer service.

Keywords: emotional artificial intelligence, conversation agent, chatbot, customer service, emotional
contagion, expectation-disconfirmation, relationship norm orientation

Introduction
With the surge of technological innovations such as machine learning and deep learning, artificial
intelligence (AI) has become a major interest for researchers, practitioners, and the public. In 2020, 56%
of businesses adopted AI in at least one function, and more than 50% of the AI use cases were related to
service operations (McKinsey 2021). Because of the cost efficiency and growing capabilities of AIpowered
conversational agents (‘AI agents’ for brevity) in the form of chatbots or voice-based AIs, they
have been increasingly deployed in customer service to reduce the burden of human labor and often
replace customer service employees (Larivière et al. 2017). Financial Digest (2017) predicted that AIs
would handle 95% of customer service interactions by 2025. Recognizing the popularity and importance
of using AIs in customer service, researchers have started exploring how to maximize the value of AI
service agents through means such as controlling their identity disclosure or humanizing AIs through
visual, auditory, and communication cues (Lucas et al. 2014; Luo et al. 2019; Schanke et al. 2021; Yuan
and Dennis 2019).
While prior research has examined several aspects of AI service agents and their impact on service
outcomes (Araujo 2018; Luo et al. 2019; Schanke et al. 2021), less attention has been paid to the AI
agents’ expressed emotion. Emotional expression is regarded as one of the foundational attributes that
define human nature (Haslam 2006). However, the recent debate about the emergence of a sentient AI
gaining consciousness and feelings raises the possibility that AIs can also possess the primary attributes
of human beings, such as the ability to perceive, think, and feel (Tiku 2022). The emerging emotional
AIs, which can recognize, interpret, process, and simulate human emotions (Huang and Rust 2018, 2021),
further underscore the need to investigate how people make sense of and react to the emotional
capabilities of an AI. Indeed, the global affective computing market, which develops technologies for
emotional AIs, is projected to reach $100 billion by 2024 and $200 billion by 2026 at a compounded
annual growth rate of over 30% (Global Industry Analysts 2021; Reports and Data 2021). Such emotional
AI technologies can be critical for the development and deployment of AI service agents because human
employees’ positive emotions are a key driver of customer service evaluations in firm-customer

3
encounters (Kranzbühler et al. 2020). As AI service agents grow more popular, equipping them with the
capability of expressing positive emotion (e.g., being cheerful and happy) is expected to benefit
businesses and enhance customer experience.
However, equipping AI service agents with this ability should be planned and rolled out cautiously
because the positive effect of human-expressed positive emotion may not apply to an AI agent (Gray and
Wegner 2012). Prior studies from HCI and psychology provided conflicting evidence for the effectiveness
of AIs expressing emotion in non-business contexts (Creed et al. 2014; Stein and Ohler 2017). In the
customer service setting, however, little research has examined the impact of AI-expressed emotion. We
focus on AI service agents in the form of text-based chatbots increasingly deployed in customer service
departments and explore the impact of their expressed positive emotion on service evaluations.
Our research question is the following: how, when, and why does an AI agent’s expression of positive
emotion influence customers’ service evaluations? Our primary goal is to examine the unique impact of
AI-expressed emotion that might be different from the impact of human-expressed emotion. Since human
service employees typically display positive emotion during a service encounter, we also restrict our
focus to positive emotion that is deemed appropriate as a first step toward achieving our primary goal.
Drawing on emotional contagion and expectation-disconfirmation literature (Hatfield et al. 1993; Oliver
1977), we argue that positive emotion expressed by an AI agent can influence customers’ service
evaluations through dual pathways: one affective and the other cognitive. On the one hand, the affective
pathway of emotional contagion that underlies the positive effect of human-expressed positive emotion,
as repeatedly confirmed in the prior customer service literature (Pugh 2001; Tsai and Huang 2002), may
also apply to an AI service agent. On the other hand, an emotion-expressing AI agent might violate a
customer’s expectation that it is not capable of feeling emotion (Gray et al. 2007; Haslam 2006). This
negative, cognitive pathway may cancel out the positive, affective pathway of emotional contagion,
resulting in a weakened effect of positive emotion on service evaluations. We further explore individual
differences in people’s norms toward their relationship with an agent—termed “relationship norm
orientation”—that can be distinguished into communal-oriented and exchange-oriented relationship

4
norms (Clark and Mils 1993). We propose that variations in these norms lead to different expectations
toward an AI service agent and subsequently affect the potency of the negative pathway.
To test these hypotheses, we present three experimental studies in which participants engaged in a
hypothetical customer service scenario and chatted with an agent to resolve a service-related issue. We
find consistent evidence for our predictions. Our theoretical framework and findings provide three
primary contributions to the literature on expressed emotion in customer service and human-AI
interactions. First, this paper is among the first to investigate the role of emotion expressed by an AI
service agent. Our findings extend the customer service literature by exploring the implications of
expressed emotion when the service is provided by an AI rather than a human. Second, we illuminate the
effect of expressed emotion on observers in human-AI interactions, which is a nascent area of research.
Third, we unravel the dual pathways of expressed emotion’s impact and reveal a boundary condition for
the cognitive pathway, deepening our understanding of a critical but understudied phenomenon.
Theoretical Development and Hypotheses
Expressed Emotion in Customer Service
In traditional customer service settings where humans are service providers, the role of their displayed
emotion has been an important area of scholarly inquiry (Pugh 2001; Rafaeli and Sutton 1990). The
display of positive emotion by service employees is generally desirable as it enhances service outcomes
(Kranzbühler et al. 2020). For example, displaying a smile to customers can lead to higher service
evaluations in both face-to-face and online interactions because of emotional contagion (Barger and
Grandey 2006; Pugh 2001; Tsai and Huang 2002; Verhagen et al. 2014). Emotional contagion refers to
the process in which an individual’s emotional state is transferred to an observer (Hatfield et al. 1993).
The means through which emotional contagion occurs is not confined to nonverbal behaviors, such as
facial, postural, or vocal expressions, and it also includes text-based computer-mediated communication
(Goldenberg and Gross 2020). Thus, if a customer perceives positive emotion from a service agent, he or
she can experience the same emotion and evaluate the service more positively as a result.

5
However, expressing positive emotion might not always be beneficial. For example, expressed
emotion could backfire when it is perceived as inappropriate or inauthentic (Cheshin et al. 2018). Also, Li
et al. (2018) investigated the effect of positive emotion expressed through emoticons during online
service interactions and found that expressing positive emotion can enhance the perceptions of a service
agent’s warmth but not competence. These findings suggest a need to explore the consequences of
expressing positive emotion when the service is provided by an AI agent.
AI-Expressed Emotion
While prior studies provided extensive evidence for the effect of emotion expressed by a human
service agent, little research has examined the applicability of these findings when an AI provides the
service. AIs have been rapidly replacing human service agents in the recent decade (Oracle 2016).
Moreover, we are witnessing the development of emotional AIs that are increasingly able to recognize
human emotions and simulate human’s emotional responses (Somers 2019). Thus, it is crucial to
understand how, when, and why the positive emotion expressed by an AI service agent can influence
customers’ service evaluations.
As the history of developing emotional AIs is short, research on the effect of AI-expressed emotion is
nascent. The very few studies examining the effects of AIs’ simulated emotions, mostly in non-business
contexts, provided mixed evidence, partly because the contexts of the studies varied substantially.
Machines displaying emotions were preferred over their neutral counterparts in certain contexts (Creed et
al. 2014), but they also elicited people’s negative feelings in other contexts (Kim et al. 2019; Stein and
Ohler 2017). These mixed findings suggest that insights from earlier customer service studies based on
humans expressing positive emotion may not apply to AIs equipped to mimic human emotions.
AI-Expressed Positive Emotion and Dual Pathways
First, we believe that the impact of a service agent’s expressed positive emotion in service encounters
depends on the agent’s identity as a human or an AI. A possible reason is that emotion-related capabilities
are deemed unique capabilities of humans, such as experiencing and expressing one’s own emotions as
well as sharing others’ emotions (i.e., empathy) (Haslam 2006). Thus, customers should have different

6
expectations about these capabilities from a human versus an AI agent. As explained in more depth later,
an AI agent is less expected to express positive emotion than a human employee because machines are
generally believed to lack consciousness or feelings (Gray et al. 2007; The Economist 2022). A violation
of this expectation in the case of an AI agent should weaken the positive impact of expressed positive
emotion revealed in prior literature studying human agents. Thus, we propose the following:
H1: The positive effect of positive emotion expressed by an agent on service evaluations depends on the
agent’s identity, such that the effect is greater for a human agent than for an AI agent.
Because the focus of our paper is positive emotion expressed by AI agents, we limit our attention in
the rest of theory development to AI-expressed positive emotion and discuss how it influences service
evaluations through dual, opposing processes: one affective and the other cognitive. First, one’s expressed
emotion can lead an observer to feel the same emotion through emotional contagion (Hatfield et al. 1993).
Prior literature in customer service showed that the display of a human employee’s positive emotion
provokes the positive affect of a customer, thus enhancing service evaluations (Pugh 2001). In addition,
the likelihood and extent of emotional contagion may depend on various factors, such as the expresser’s
characteristics, the expresser-perceiver relationship, and the perceiver’s susceptibility to others’ emotions
(Doherty 1997; van der Schalk et al. 2011).
Emotional contagion might be weakened when the expresser is an AI rather than a human agent.
However, we argue that the affective process of emotional contagion can still underlie the impact of AIexpressed
positive emotion. After observing another person’s emotional expression, one’s affective states
can be automatically and subconsciously evoked without involving any cognitive resources and often,
even without being aware of the origin (Neumann and Strack 2000). Moreover, prior literature on
computer-mediated communication suggested that textual cues suffice for eliciting emotional contagion
because affective words prime an observer with the emotion conveyed in those words (Cheshin et al.
2011; Hancock et al. 2008). This finding also implies that emotional contagion may occur through IT
artifacts in digital environments that lack human presence, such as on social media (Ferrara and Yang
2015; Kramer et al. 2014).

7
In our context, if an AI service agent expresses positive emotion during a service interaction, the
textual cues of positive emotion can prime a customer with the same emotion, thus automatically
triggering positive emotion of the customer before they form any cognitive judgment towards the agent’s
identity. The triggered positive emotion will then serve as information for judging the service encounter.
According to affect-as-information theory, one’s affective states provide information about an event he or
she is involved in (Schwarz and Clore 1983). Specifically, affective valence can be attributed to the value
judgment of an event, such that positive (negative) emotion leads to a perception that the event is pleasant
(unpleasant) (Clore et al. 2001). Thus, a customer’s positive emotion triggered by emotional contagion
will lead to a positive evaluation of a service encounter (Pugh 2001). Taken together, we propose that a
customer’s felt positive emotion can mediate the impact of AI-expressed positive emotion.
H2a (positive mediation through emotional contagion): An AI agent’s expressed positive emotion
increases a customer’s positive emotion, which in turn enhances service evaluations.
In addition to the affective pathway of emotional contagion, we also propose a cognitive pathway such
that AI-expressed positive emotion increases the magnitude of expectation-disconfirmation, which refers
to the extent to which an individual’s prior expectation does not align with the actual experience (Oliver
1977). Expectation-disconfirmation is known to influence various consumer behaviors, such as product or
service evaluations, post-purchase behavior, and continuous use of information systems (Bhattacherjee
2001; Oliver 1993). During a service interaction, customers compare their expectations and the actual
service experience when evaluating a service (Oliver 1993; Parasuraman et al. 1985). The impact of
expectation is especially salient for interpersonal communication that involves emotion, as individuals
have strong expectations toward others’ emotional expressions (Burgoon 1993). Beyond interpersonal
communication, an expectation has also been revealed to play an important role in the context of
communication through technological artifacts (Jensen et al. 2013; Jin 2012; Kalman and Rafaeli 2011;
Ramirez and Wang 2008). Overall, when the expectation is violated, especially if the observed behavior is
inferior to the expected behavior (i.e., negative violation), the resulting disconfirmation and cognitive
dissonance often lead people to develop negative attitudes or behaviors (Festinger 1957).

8
While several factors can determine the impact of expectation, one factor is a communicator’s
characteristics (Burgoon 1993), and we focus on the identity of a service agent as such a characteristic in
our context. For an AI agent, customers should have prior expectations regarding its capability of feeling
(and subsequently expressing) emotion, which should be different from that of a human agent. One of the
core characteristics that define human nature and differentiate humans from machines is related to
emotion, such as emotionality (i.e., experiencing or expressing one’s own emotions) and emotional
responsiveness (i.e., understanding or sharing others’ emotions and responding accordingly) (Haslam
2006). Different from humans, machines are commonly believed to lack the mental capability of feeling
various emotions (e.g., joy, fear, rage) (Gray et al. 2007; Gray and Wegner 2012), which is a necessary
step before emotional display. Due to this fundamental difference in emotional capabilities between
humans and machines, customers should have different expectations for the agent’s emotional display,
such that a human agent can and should express (supposedly positive) emotion, while an AI agent cannot.
Thus, when an AI agent expresses emotion during an actual interaction, customers’ expectations about its
emotional expression should be disconfirmed.
While the violation of expectation can be either positive or negative, we argue that an emotionexpressing
AI agent will result in a negative violation because emotionally capable machines can evoke a
sense of threat to human uniqueness and lead to strong eeriness and aversion toward the machines (Stein
and Ohler 2017). Such a negative violation of expectation will lead to lower service evaluations (Brady
and Cronin 2001; Oliver 1993). Thus, expectation-disconfirmation can also mediate the impact of an AI
agent’s expressed positive emotion on service evaluations.
H2b (negative mediation through expectation-disconfirmation): An AI agent’s expressed positive
emotion increases the extent of expectation-disconfirmation, which in turn reduces service evaluations.
Accordingly, when an AI agent expresses positive emotion, the negative indirect effect through
expectation-disconfirmation may cancel out the positive indirect effect through emotional contagion. The

9
co-occurrence of these two opposing processes may explain the weaker effect of an AI agent’s expressed
positive emotion compared to a human agent’s expressed positive emotion, as proposed in H11.
The Moderating Effect of Relationship Norm Orientation
While two opposing processes might underlie the impact of AI-expressed positive emotion, the
pathway of expectation-disconfirmation may vary based on an individual’s exact expectation. We suggest
relationship norm orientation as an individual difference variable to capture the natural variation in
customers’ expectations. Relationship norm is used in social psychology to explain people’s varying
norms about two distinct types of relationships—exchange and communal—based on economic and
social factors (Clark and Mils 1993). An exchange relationship is a quid pro quo relationship of
exchanging a similar level of benefits. In communal relationships, however, such quid pro quo is not
obligatory. Instead, benefits are given in response to a person’s need or to demonstrate a general concern
for another. Because this distinction is based on a rule or a norm about giving and receiving benefits, the
two relationships generate different norms of behavior which, in turn, influence expectations toward
another’s behavior in an interpersonal relationship (Clark and Taraban 1991). Thus, the same behavior
might lead to different interpersonal outcomes depending on the observer’s relationship norm orientation.
Relationship norm orientation has been found to be influential beyond interpersonal relationships. For
example, customers tend to form different expectations toward a brand depending on their relationship
norm orientation, ultimately influencing their evaluations of the brand or its product (Aggarwal 2004; Liu
and Gal 2011). These studies provide converging evidence that violating the relationship norm leads to a
negative evaluation because of cognitive dissonance between expectations and actual observations.
Similarly, customers’ relationship norm orientation may influence how they interpret certain cues from a
service agent during a service encounter (Scott et al. 2013), which in turn can alter the subsequent
likelihood of expectation-disconfirmation.
1 Note that the two proposed pathways may be interdependent due to the intertwining of affect and cognition (Izard 2011; Phelps
2006). While we acknowledge that the two processes can be mutually influential, we still treat the two pathways as distinct
processes because a) such a model is more parsimonious and b) this treatment is consistent with similar theories such as the
emotions as social information theory (Van Kleef 2009) and dual-process theories (Evans 2003; Petty and Cacioppo 1986).

10
In our context, customers can evaluate AI agents’ expression of positive emotion differently
depending on their relationship norm orientation. Customers with a communal relationship norm—
communal-oriented customers—will expect a service agent to show a genuine concern and care like a
friend or a family member (Scott et al. 2013). Because the expression of positive emotion insinuates such
care and attention, it will confirm communal-oriented customers’ expectations derived from their
relationship norm, even if the source is an AI agent. Thus, the positive effect of AI-expressed positive
emotion on expectation-disconfirmation will be weaker for communal-oriented customers.
In contrast, customers with an exchange relationship norm—exchange-oriented customers—will
expect a service agent to be more transaction-focused, providing a professional and exact service (Scott et
al. 2013). Because the expression of positive emotion does not satisfy such a transaction-focused norm, it
will not confirm exchange-oriented customers’ expectations derived from their relationship norm. As
exchange-oriented customers are more likely to treat an AI agent as a machine (which is not supposed to
have emotion) than a friend or family member, the positive effect of AI-expressed positive emotion on
expectation-disconfirmation should be greater for them than for communal-oriented customers. Taken
together, an AI agent’s expression of positive emotion should enhance the service evaluations when the
customers are communal-oriented (because of emotional contagion and weaker expectationdisconfirmation),
but this effect should weaken or even reverse when the customers are exchange-oriented
(because of emotional contagion and expectation-disconfirmation operating in opposite directions). We
propose our last hypothesis below. Figure 1 depicts the complete research framework.
H3 (moderation by relationship norm orientation): For communal-oriented customers, an AI agent’s
expressed positive emotion has a positive effect on service evaluations, but for exchange-oriented
customers, such an effect is non-existent or even reversed.
To test these hypotheses, we conducted three laboratory experiments in which participants were asked
to interact with a customer service agent in a hypothetical scenario. In the first study, we tested H1 by
manipulating the agent’s (human vs. AI) identity and the presence of positive emotional expression
during the interaction. In Study 2, we focused only on the AI agent and explored the moderating role of

11
participants’ relationship norm orientations as proposed in H3. In the final study, we tested H3 as well as
the underlying mechanisms as proposed in H2a and H2b.
Figure 1. Research Framework
Pretest
Before the main experiments, we conducted a pretest to verify the effectiveness and validity of our key
emotion manipulation in a customer service context. To achieve this goal, we varied an AI service agent’s
expressed positive emotion at multiple levels in a between-subjects design and kept all other aspects of
the interaction identical across conditions. We focused only on the AI agent in this pretest because our
primary interest is the effectiveness of AI agents expressing emotion. During the study, participants took
part in a hypothetical customer service task and interacted with an AI agent via virtual chat to resolve a
service-related issue. After the chat, participants evaluated the expressed emotion of the AI agent.
Stimulus Materials
To ensure that participants across conditions receive the same messages from the AI agent during the
chat except for the level of expressed emotion, we used a predesigned script. The script included four
messages from the agent, with two to four sentences within each message. The script was devised based
on examples of best practices and canned responses for live chat from livechat.com, a popular platform
that provides live chat software. Messages at the beginning (for greetings) and end of the chat followed
the exact examples from the platform. The rest of the messages also followed the best practice examples
from the platform but were slightly modified to fit our setting.
We manipulated expressed positive emotion at three levels by selecting one sentence from each
message and varying the presence of emotional adjectives or exclamation marks in the sentence. We
focused only on the positive emotion to avoid the possible confound of valence. For the low emotion

12
condition, there were neither emotional adjectives nor exclamation marks throughout the interaction. For
the intermediate emotion condition, following Yin et al. (2017), we added exclamation marks and
emotional adjectives to every manipulated sentence. For the high emotion condition, we added both
exclamation marks and emotional adjectives to every manipulated sentence. Furthermore, to strengthen
participants’ belief that they are interacting with an AI agent, we showed an introductory message of
“being connected to a bot created by the customer service department” before the chat started. We also
inserted a robot icon under the introductory message and next to each message from the agent. The three
versions of the entire script can be found in Appendix A.
Procedure
One hundred and five subjects from Amazon Mechanical Turk (53 female) participated in the pretest.
Participants were randomly assigned to one of the three conditions with different levels of expressed
positive emotion. The cover story involved a hypothetical but realistic scenario that described a servicerelated
issue. We chose the online retail industry as the setting because virtual chat is commonly deployed
to communicate with customers, and this industry is at the forefront of rapidly replacing human agents
with AI agents. For the service-related issue, we used one of the most common complaints in the online
retail industry: a missing item from a delivery. The scenario described a recent delivery in which one of
the items was missing. Participants were asked to chat with a service agent and request delivery of the
missing item (see Appendix B for details). Then participants saw the introductory message that they were
being connected to a customer service bot, and the chat started on a new screen.
When the chat started, the first message was displayed. Participants had to type in their response
below the first message before moving on to the next screen and seeing the agent’s next message.
Participants were instructed to provide a response to the agent based on the cover story. Furthermore, on
each screen, we provided a reminder of the facts from the cover story that pertained to the agent’s
question so that the chat would not go off topic, and the subsequent message from the agent would appear
logical. On each screen, participants could also see the chat history up to that point. To further enhance
the live chat experience, each of the agent’s messages was presented with a slight delay.

13
To verify the effectiveness of our affect intensity manipulation (Jensen et al. 2013), we asked the
participants to rate the intensity of the agent’s expressed emotion after the chat concluded. Emotional
intensity was measured using three items from Puntoni et al. (2008) (e.g., “very little emotion / a great
deal of emotion”). We also asked participants to report the appropriateness of expressed emotion to
ensure that they are similarly appropriate across conditions (Van Kleef and Côté 2007). Emotion
appropriateness was measured using four items from Cheshin et al. (2018) (e.g., “The emotions the
service agent expressed were appropriate.”). All these questions were measured on a seven-point semantic
differential scale. To identify outliers and ensure subject quality, we also asked participants to answer two
attention check questions about the content of the service issue and the solution provided by the agent. All
measurement items are listed in Appendix C.
Results
Out of 105 subjects, 84 subjects passed both attention check questions and were used in our analysis.
We first conducted a manipulation check for the perceived intensity of the agent’s expressed emotion.
Analysis revealed that participants perceived the emotional intensity of the agent differently across the
three conditions (F(2, 81) = 17.324, p < .001). According to a Tukey post-hoc test, the low emotion agent
was perceived as less emotionally intense than the intermediate emotion agent (Mlow = 2.36 vs. Mintermediate
= 4.01, SDs = 1.43 and 1.53, t(54) = 4.16, p < .001) or high emotion agent (Mhigh = 4.48, SDhigh = 1.22,
t(53) = 5.92, p < .001). However, the intermediate emotion agent and the high emotion agent were not
perceived differently in terms of emotional intensity (p = .4). Thus, our manipulations indeed varied
emotional intensity successfully between low and higher levels but not between intermediate and high
levels.
Next, we evaluated the appropriateness of expressed emotion to rule out this possible confound.
Results revealed that subjects did not evaluate the appropriateness of emotion differently across
conditions (F(2, 81) = .878, p = .4). The pairwise comparisons further confirmed that the participants did
not perceive a difference in emotional appropriateness between low versus intermediate (p = .4), low
versus high (p = .6), or intermediate versus high (p = 1) emotion conditions.

14
Discussion
This pretest manipulated the level of emotion expressed by a service agent and validated this key
manipulation. Among the three levels, we picked the low and high levels for use in the main studies for
two reasons. First, the perceived intensity of the agent’s expressed emotion was the lowest in the low
emotion condition and the highest in the high emotion condition, and this difference was significant. We
did not choose the intermediate level of expressed emotion because we intended to strengthen the
manipulation as much as possible. Second, we verified that perceived appropriateness did not differ
across intensity levels. For simplicity, we will refer to the low and high levels as “emotion-absent” and
“emotion-present,” and the presence of positive emotion as “positive emotion” henceforth.
Study 1
In Study 1, we investigated whether the effect of expressed positive emotion depends on the service
agent’s identity, as suggested in H1. To do so, we varied both the presence of the expressed positive
emotion and the agent’s (human versus AI) identity in a between-subjects design.
Procedure and Measures
To manipulate the agent’s identity, we varied the icons that appeared next to each of the agent’s
messages from the chat (see Figure 2). For those assigned to the human condition, the employee was
either male or female (randomly determined) to reduce a possible gender effect. For manipulating the
presence of emotion, we used the low and high emotional intensity scripts verified in the pretest (see
Figure 3).
AI Agent
Human Agent
Figure 2. Agent Icons
or

15
Emotion-absent Condition Emotion-present Condition
Figure 3. Chat Scripts (in AI conditions)
One hundred and fifty-eight undergraduate students (86 female) from a U.S. university participated in
the study in exchange for course credit. Participants were randomly assigned to one of the four treatment
conditions. The cover story and procedure were identical to that of the pretest, except that we asked the
outcome variables right after participants finished their chat with the agent.
We focused on two important service evaluation outcomes: perceived service quality and satisfaction
with the service. Perceived service quality is an overall evaluation of the service outcome and interaction,
and it is associated with key organizational outcomes such as customer loyalty, market share, and
purchase intention (Brady and Cronin 2001). Satisfaction with the service is another essential evaluation
metric, as it is a key predictor of customers’ intention to continue using the service (Oliva et al. 1992).
Although the two have been revealed to jointly influence more downstream consequences (Cronin et al.

16
2000; Gotlieb et al. 1994), they are distinct constructs at the theoretical level (Anderson and Sullivan
1993; Cronin et al. 2000; Taylor and Baker 1994). To measure perceived service quality and satisfaction
with the service, we adapted existing scales from the customer service literature (Cronin et al. 2000).
Perceived service quality was measured using three items (e.g., “poor / excellent”). Satisfaction with the
service was measured using three questions (e.g., “Overall, how satisfied or dissatisfied did your
experience with the service agent leave you feeling?”, “extremely dissatisfied / extremely satisfied”).
After the measures for service evaluations, we asked two attention check questions as in the pretest,
followed by the manipulation check questions. As a manipulation check for the presence of emotion, we
used the same measure of emotional intensity from the pretest. As a manipulation check for the agent’s
identity, we measured the perceived human-likeness of the agent on a seven-point, semantic differential
scale, using three items from MacDorman (2006) and Lankton et al. (2015) (e.g., “very mechanical / very
humanlike”). All measurement items of this study and the later studies are listed in Appendix C.
Results
We used 155 subjects who passed attention checks. The analysis of the manipulation checks revealed
that participants perceived the emotion-present agent as more emotionally intense than the emotion-absent
agent (Mpresent = 4.04 vs. Mabsent = 2.52, SDs = 1.35 and 1.47, t(153) = 6.703, p < .001). Also, participants
perceived the human agent as more human-like than the AI agent (Mhuman = 3.23 vs. MAI = 2.68, SDs =
1.79 and 1.27, t(153) = 2.208, p = .029). Therefore, both of our manipulations were deemed successful.
To test H1, we conducted a two-way ANCOVA with positive emotion and the agent’s identity as
between-subjects factors and gender as a covariate. We used gender as a covariate because of the prior
literature indicating gender differences in emotion recognition and perception (Brody and Hall 2008;
Fischer et al. 2018). Results revealed a main effect of positive emotion, such that overall, expressing
positive emotion led to a more positive evaluation of service quality (Mabsent = 5.67 vs. Mpresent = 6.13, SDs
= 1.45 and 1.07, F(1, 150) = 5.650, p = .019) and greater satisfaction (Mabsent = 6.04 vs. Mpresent = 6.41,
SDs = 1.21 and .94, F(1, 150) = 4.601, p = .034). However, the main effect of agent identity was not
observed (ps = .8), nor the main effect of gender (ps = .2 and .6).

17
Most importantly, agent identity significantly moderated the positive effect of positive emotion on
perceived service quality (F(1, 150) = 5.451, p = .021) and on satisfaction (F(1, 150) = 3.606, p = .059).
Pairwise comparisons showed that positive emotion from a human agent significantly increased perceived
service quality (Mhuman_absent = 5.42 vs. Mhuman_present = 6.37, SDs = 1.25 and 1.29, t(75) = 3.282, p = .001)
and satisfaction (Mhuman_absent = 5.86 vs. Mhuman_present = 6.57, SDs = 1.06 and 1.11, t(75) = 2.871, p = .005).
In the case of an AI agent, however, the effects of positive emotion did not reach significance for service
quality (MAI_absent = 5.94 vs. MAI_present = 5.93, SDs = 1.25, t(76) = .035, p = 1) or satisfaction (MAI_absent =
6.27 vs. MAI_present = 6.23, SDs = 1.06, t(76) = .167, p = .9) (see Figure 4). These results confirmed H1.
Figure 4. Interaction Effect of Positive Emotion and Agent Identity in Study 1
Note: ns, not significant; ** p < .05
Discussion
This study provides direct evidence that positive emotion expressed by a human agent can increase
perceived service quality and satisfaction with the service, but such effects are absent when the emotion is
expressed by an AI agent. As discussed before, prior literature on customer service has shown that
positive emotional expressions by a human service agent positively influence customers’ service
evaluations (Kranzbühler et al. 2020). However, this study suggests that the positive impact of human’s
positive emotional displays is not directly applicable when AI agents replace human agents.
A reason for this lack of effect in the case of an AI agent might be that customers differ in perceived
norms regarding their relationships with the AI agent and thus have different expectations toward the AI
ns
** **
ns
Emotion-absent Emotion-present

18
agent’s expressed emotion. Such different expectations may lead to different reactions, as we proposed in
H3. Thus, we focused only on AI agents in the next study and tested this hypothesis.
Study 2
The goal of Study 2 was to investigate whether the effect of AI-expressed positive emotion is
dependent on customers’ individual differences in their relationship norm orientation as proposed in H3.
Because we shifted our focus to only the AI agent, we varied the presence of positive emotion as a single
between-subjects factor and measured participants’ relationship norm orientation.
Stimulus Materials, Procedure, and Measures
We changed our predesigned script by switching to a different service-related issue and extending the
length of the conversation. We asked participants to request an exchange for a textbook they had already
ordered, as this scenario is more relevant to student subjects. We also added one more message to the
conversation to enhance participant engagement. This additional message, which was inserted after the
greetings message, asked why a participant wanted an exchange. Manipulation of emotional intensity was
also implemented in this additional message and all other messages as in the first study.
Ninety-two undergraduate students (49 female) from a U.S. university participated in this study in
exchange for course credit. Participants were randomly assigned to either the emotion-absent or the
emotion-present condition. The cover story and procedure were identical to those of Study 1. In addition
to the measures used in Study 1, we added a new scale measuring participants’ individual differences in
relationship norm orientation. We used a seven-point, semantic differential scale with three items,
describing the kind of relationship a participant would want with an online customer service agent (e.g.,
“strictly for business / bonded like family and friends”) (Aggarwal 2004; Li et al. 2018).
Results
We used the responses from 88 subjects who passed both attention checks. Analysis of the
manipulation check for emotional intensity revealed that participants perceived the emotion-present agent
as more emotionally intense than the emotion-absent agent (Mpresent = 4.22 vs. Mabsent = 2.86, SDs = 1.27
and 1.39, t(86) = 4.791, p < .001). Therefore, this manipulation was deemed successful.

19
To test the moderation effect proposed in H3, we conducted a one-way ANCOVA with positive
emotion as a between-subjects factor, relationship norm orientation as a continuous moderator, and
gender as a covariate. First, replicating the AI-related findings from Study 1, we did not find any
significant main effect of positive emotion on perceived service quality (Mabsent = 5.98 versus Mpresent =
6.02, SDs = .93 and .94, F(1, 83) = .667, p = .4) or satisfaction (Mabsent = 6.25 versus Mpresent = 6.33, SDs
= .96 and .73, F(1, 83) = 1.836, p = .2). Meanwhile, we observed a significant effect of gender on
satisfaction, such that females tended to be more satisfied with the service than males (F(1, 83) = 6.140, p
= .015), but not on service quality (F(1, 83) = 1.426, p = .2).
Most importantly, we discovered that relationship norm orientation significantly moderated the effect
of positive emotion on perceived service quality (F(1, 83) = 12.744, p = .001) and on satisfaction (F(1,
83) = 14.066, p < .001). In order to probe the pattern of the interaction, we conducted a simple slope
analysis and examined the marginal effect of positive emotion at one standard deviation above and below
the mean of relationship norm orientation. For exchange-oriented individuals (relationship norm
orientation = 1.10, -1 SD), AI-expressed positive emotion has a significant, negative effect on perceived
service quality (b = -.57, t(86) = -2.12, p = .037) and satisfaction (b = -.44, t(86) = -1.88, p = .06). On the
other hand, for communal-oriented individuals (relationship norm orientation = 3.95, +1 SD), AIexpressed
positive emotion had a significant, positive effect on perceived service quality (b = .89, t(86) =
3.04, p = .003) and satisfaction (b = .89, t(86) = 3.52, p < .001). Figure 5 illustrates the simple slope
analyses. Taken together, these results indicate that the effect of positive emotion from an AI agent on
service evaluations depends on an individual’s relationship norm orientation, thus confirming H3.

20
Figure 5. Moderating Effect of Relationship Norm Orientation in Study 2
Note: * p < .1; ** p < .05; *** p < .001
Discussion
Study 2 extends our previous findings by revealing the moderating role of a theoretically relevant
individual difference variable, relationship norm orientation. Individuals with a communal-oriented norm
evaluated an AI agent’s service more positively when the agent expressed positive emotion than when it
did not. Conversely, individuals with an exchange-oriented norm evaluated an AI agent’s service more
negatively when the agent expressed positive emotion than when it did not. Despite the revelation of the
moderating role of relationship norm orientation in this study, we have not explored the underlying
mechanisms, which we turn to in the final study.
Study 3
In Study 3, we delved into the mechanisms proposed in H2a and H2b. Similar to Study 2, we focused
only on AI agents and manipulated the presence of positive emotion as a single between-subjects factor.
To test the proposed mechanisms, we added new measures for the subject’s felt positive emotion and the
extent of expectation-disconfirmation to capture the opposing pathways.
Procedure and Measures
One hundred and eighty-six undergraduate students (93 female) from a U.S. university participated in
this study in exchange for course credit. Similar to Study 2, participants were randomly assigned to either
the emotion-absent or emotion-present condition. We used the predesigned script from Study 1 to vary the
presence of positive emotion. The cover story and procedure were similar to those of prior studies. After
**
** ***
*
Emotion-absent Emotion-present

21
the service interaction, participants reported service evaluations, followed by attention checks, mechanism
measures, manipulation checks, and individual difference measures of relationship norm orientation.
To measure the mechanisms, we asked participants’ felt positive emotions to quantify emotional
contagion because measuring one’s emotion right after an emotion-invoking stimulus can capture
affective transfer (Hasford et al. 2015). We used five items from Pham (1998) to measure participants’
felt emotions (e.g., “sad / joyful”). We also measured the extent to which participants confirmed their
expectations toward the service agent, using three items from Bhattacherjee (2001). We modified the
original items to tailor to our need to capture the specific expectations about the level of emotion
expressed by the service agent (e.g., “The level of the chatbot’s emotional display was exactly what I
expected”). In data analysis, we reversed these items’ scores to represent expectation-disconfirmation.
Results
One hundred and seventy-seven subjects passed the attention checks and thus were used in the
following analyses. We first analyzed the perceived emotional intensity of the service agent as a
manipulation check. We found that participants perceived the emotion-present agent as more emotionally
intense than the emotion-absent agent (Mabsent = 3.11 vs. Mpresent = 5.19, SDs = 1.25 and 1.22, t(175) =
11.194, p < .001), indicating that our manipulation of the presence of positive emotions was successful.
Next, we conducted a one-way ANCOVA to replicate prior findings, with positive emotion included
as a between-subjects factor, relationship norm orientation as a continuous moderator, and gender as a
covariate. Results revealed that AI-expressed positive emotion did not significantly influence perceived
service quality (Mabsent = 6.13 vs. Mpresent = 6.26, SDs = 1.02 and .82, F(1, 172) = .726, p = .4) or
satisfaction with the service (Mabsent = 6.33 vs. Mpresent = 6.44, SDs = .93 and .75, F(1, 172) = .404, p = .5).
We did not find any significant effect of gender on service evaluations (ps = .4 and .9). These results
replicated the lack of effect of AI-expressed positive emotion in the earlier studies.
We also discovered a significant moderation by relationship norm orientation for the effect of positive
emotion on perceived service quality (F(1, 172) = 3.738, p = .055) and on satisfaction (F(1, 172) = 6.683,
p = .011). Simple slope analysis showed that, for communal-oriented individuals (relationship norm

22
orientation = 4.54, 1 SD above the mean), AI-expressed positive emotion significantly increased
perceived service quality (b = .41, t(172) = 1.99, p = .049) and satisfaction (b = .43, t(172) = 2.30, p
= .023). However, for exchange-oriented individuals (relationship norm orientation = 1.67, 1 SD below
the mean), positive emotion did not have any effect on perceived service quality (b = -.16, t(172) = -.76, p
= .45) or on satisfaction (b = -.26, t(172) = -1.37, p = .17). Figure 6 illustrates the simple slope analyses.
These results, once again, confirmed H3.
Figure 6. Moderating Effect of Relationship Norm Orientation in Study 3
Note: ns, not significant; ** p < .05
To determine if the effect of AI-expressed positive emotion on service evaluations is mediated by
emotional contagion and expectation-disconfirmation, we used PROCESS Model 4 (parallel mediation
model) with gender as a covariate and a bootstrapped sample of 5,000 (Hayes 2013). Results revealed the
lack of total effects and direct effects of AI-expressed positive emotion on perceived service quality (ps
= .3 and 1) and satisfaction (ps = .4 and .9). However, AI-expressed positive emotion increased
customers’ positive emotions (b = .26, t(175) = 1.737, p = .084), implying emotional contagion. An
increase in felt positive emotion further led to greater perceived service quality (b = .62, t(173) = 11.498,
p < .001) and greater satisfaction (b = .52, t(173) = 10.362, p < .001). The test of indirect effects revealed
a marginally significant, positive indirect effect of AI-expressed positive emotion through participants’
felt positive emotion on perceived service quality (b = .16, SE = .097, 90% CI = [.006, .332]) and on
ns **
ns **
Emotion-absent Emotion-present

23
satisfaction (b = .14, SE = .082, 90% CI = [.007, .277]). These results provide suggestive evidence for the
positive, affective pathway of emotional contagion as hypothesized in H2a.
On the other hand, positive emotion increased expectation-disconfirmation (b = .32, t(175) = 1.859, p
= .065), which further reduced perceived service quality (b = -.083, t(173) = -1.759, p = .080) and
satisfaction (b = -.13, t(173) = -3.074, p = .003). The test of indirect effects confirmed a marginally
significant, negative indirect effect of AI-expressed positive emotion through expectation-disconfirmation
on satisfaction (b = -.043, SE = .033, 90% CI = [-.106, -.002]), but not on perceived service quality (b =
-.026, SE = .023, 90% CI = [-.074, .001]). These results partially support the negative, cognitive pathway
of expectation-disconfirmation proposed in H2b. Overall, our results suggest that the two opposing
pathways may explain the lack of total effects of AI-expressed positive emotion on service evaluations.2
Figure 7 shows the summary of the mediation model along with the results.
Study 3 unraveled how individuals might react to AI agent’s expressed positive emotion affectively
and cognitively, thus illuminating the potential reasons for the lack of effect of AI-expressed positive
emotion on service evaluations. Although positive emotion expressed by an AI agent could be transferred
to customers through emotional contagion, it violated the customers’ expectations toward the agent (e.g.,
machines are not supposed to have emotions). Therefore, the positive affective pathway and negative
cognitive pathway may have canceled out each other’s effects.
However, our hypotheses regarding the indirect effects obtained only marginal statistical support, as
the effects of AI-expressed positive emotion on the two mediators were marginally significant. First, the
marginally significant indirect effect through expectation disconfirmation is not unexpected. The reason is
that based on findings from Studies 2-3, the impact of positive emotion on expectation disconfirmation
was revealed to depend on participants’ relationship norm orientation. In addition, as revealed in footnote
2, the indirect effect through expectation-disconfirmation was present and significant for exchangeoriented
individuals, but such an indirect effect was absent for communal-oriented individuals, exactly as
we expected. Thus, the overall indirect effect through expectation disconfirmation is expected to be weak
if we disregard this interaction in a pure-mediation model. Second, the marginal support for the indirect
effect through emotional contagion may arise from different reasons, including the relatively subtle
manipulation of expressed positive emotion, our focus on measuring the valence (but not other aspects) of
felt emotion, and the presence of other mechanisms not captured in our dual-pathway model.
General Discussion
Extending the concept of expectation-disconfirmation (Oliver 1977), we propose that positive
emotional expressions of AI service agents may not be as effective as those of human service employees
in enhancing customers’ service evaluations. Despite customers’ increased positive feelings triggered by
emotional contagion, there is also a risk of emotion-expressing AI service agents violating customers’
expectations, thus weakening the positive effect of positive emotion. We further propose relationship
norm orientation as a moderator because it might influence the likelihood of customers’ expectation-

25
disconfirmation as customers hold different norms regarding their relationship with service agents. Three
experimental studies provided converging evidence for our predictions. Table 1 summarizes our findings.
Table 1. Summary of Findings
Study 1 Study 2 Study 3
H1: The positive effect of positive emotion expressed by an agent on service
evaluations depends on the agent’s identity, such that the effect is greater for a
human agent than for an AI agent.
Supported - -
H2a (positive mediation through emotional contagion): An AI agent’s expressed
positive emotion increases a customer’s positive emotion, which in turn
enhances service evaluations.
- - Supported
H2b (negative mediation through expectation-disconfirmation): An AI agent’s
expressed positive emotion increases the extent of expectation-disconfirmation,
which in turn reduces service evaluations.
- - Partially
supported
H3 (moderation by relationship norm orientation): For communal-oriented
customers, an AI agent’s expressed positive emotion has a positive effect on
service evaluations, but for exchange-oriented customers, such an effect is nonexistent
or even reversed.
- Supported Supported
Note: “-” indicates that the hypothesis was not explored in that study.
Theoretical Implications
Prior investigations of the effect of emotional expressions by a customer service agent have focused
entirely on human employees (Barger and Grandey 2006; Cheshin et al. 2018; Kranzbühler et al. 2020; Li
et al. 2018). However, the rapid deployment of AIs for handling a service encounter calls for extending
the study of emotions to AI service agents. Addressing this emerging phenomenon, we discover that the
commonly observed positive effect of positive emotion from human service employees is not directly
applicable to AI service agents. To the best of our knowledge, this paper is the first in the customer
service literature to examine the role of emotion expressed by an AI service agent, illustrating the need to
study the unique impacts of AI-expressed emotion in service encounters.
This research also contributes to the burgeoning human-AI interaction literature, in which the
exploration of interactions between emotional AIs and humans has just started to emerge (Creed et al.
2014; Melo et al. 2013; Stein and Ohler 2017). Most of the research examining factors that influence the
effectiveness of human-AI interactions focused on the transparency of an AI’s decision-making process
and an AI’s behaviors that can enhance its social presence or conformity to the norms (Amershi et al.
2019; Velez et al. 2019). On the other hand, emotional AIs have been increasingly popular in automated
chatbots or conversational agents, and their expressed emotions can potentially influence various business
outcomes. However, the impact of AI-expressed emotion, especially in business domains, has not

26
received much attention from scholars studying human-AI interactions. Our research underscores the
importance of incorporating emotional factors in future investigations of human-AI interactions.
At a broader level, we supplement the emotion literature by delving into how, when, and why
emotions from an AI, a new entity, are perceived by the observers. Emotion has been known to serve an
important role in interpersonal relationships (Van Kleef et al. 2010). Prior research has extensively
documented how various aspects of emotion influence interpersonal outcomes (Lazarus 2006; van Kleef
and Côté 2022). As emotion is universally considered a unique capability of human beings, emotion
scholars rarely acknowledged the possibility of AI agents or machines expressing emotions. However, the
latest technological innovations have enabled AI agents to mimic a human’s emotion-related capabilities,
raising the need to study emotions in human-AI relationships. Our study addresses this need by
discovering the distinct role of emotion expressed by human vs. non-human agents. Thus, this research
opens up exciting opportunities for further studies to explore the impact of emotion in novel contexts.
Also, our finding that emotional expressions from an AI agent may trigger emotional contagion
extends this well-documented phenomenon beyond interpersonal relationships. Although prior literature
suggested various boundary conditions of emotional contagion related to the characteristics of the
expresser, the perceiver, and their relationship (Doherty 1997; van der Schalk et al. 2011), we confirm the
existence of emotional contagion even when the expresser is an AI agent. This finding also contributes to
the information systems literature on emotional contagion by supplementing prior findings on how
emotional contagion may occur through IT artifacts that lack human presence, such as on social media
and via instant messaging (Cheshin et al. 2011; Ferrara and Yang 2015; Goldenberg and Gross 2020).
Finally, this paper unravels the underlying mechanisms and a boundary condition for the unique
impact of AI-expressed positive emotion in customer service. Our findings of expectation-disconfirmation
as an underlying pathway contribute to the emotion literature by highlighting the role of expectations in
the social impact of emotions when the expresser is not a human. Prior literature has shown that various
norms or display rules exist regarding emotional expressions (Ekman et al. 1969; Heise and Calhan
1995). Such norms are also present when communicating with others, and others’ emotions are one of the

27
key expectations that significantly impact interpersonal outcomes (Burgoon 1993). Our work extends
these prior findings by providing empirical evidence for the mediating role of expectation-disconfirmation
in human-AI interactions and suggesting relationship norm orientation as a novel boundary condition.
Practical Implications
This work provides valuable guidance for practitioners who are interested in deploying emotional AIs
in customer service. The argument of an AI becoming sentient has evoked a contentious debate not only
about whether the argument is true, but also about the benefits and costs of deploying AIs (The Economist
2022). AI service agents can save costs—both economic costs and emotional labor of human
employees—and streamline firm-customer interactions. However, one of the primary goals of customer
service is to maximize customers’ service evaluations through their experience and interaction with a
service agent. Our findings suggest that the positive effect of expressing positive emotion on service
evaluations may not materialize when the source of the emotion is not a human. Practitioners should be
cautious about the unique impact of equipping AI agents with emotion-expressing capabilities.
In addition, our findings indicate that an AI agent expressing positive emotion is beneficial when
customers expect a communal relationship, but such a beneficial effect may not exist or even backfire
when they expect an exchange relationship from the interaction. Companies can design emotional AIs in
such a way that they are context-aware and express positive emotion only when the expression effectively
facilitates service outcomes. For example, they may benefit from switching on or off the emotionexpressing
capabilities of AI agents based on the type of customers that could be determined through past
communication histories. Alternatively, companies can selectively deploy emotion-expressing AIs based
on the nature of their tasks because different tasks may activate different relationship norms. For instance,
AIs dealing with personalized tasks (activating a communal-oriented relationship norm) might benefit by
expressing positive emotion, whereas AIs dealing with more standardized tasks (activating an exchangeoriented
norm) might not. Companies may also set up a more communal environment beforehand to
nudge customers’ expectations in such a way that can reduce their expectation disconfirmation when
encountering emotional expressions of an AI agent.

28
Limitations and Future Research
Several opportunities present themselves for future research. First, our findings for the moderating role
of relationship norm orientation can be extended to various avenues. For instance, researchers can
examine how customers’ norms toward their relationship with a brand (Aggarwal 2004) can influence the
impact of AI-expressed emotion. A brand that oversees close interactions with customers and holds a
communal relationship (e.g., in healthcare and education markets) may benefit from AI-expressed
emotion. However, a brand with a pure exchange relationship (e.g., in finance markets) may not witness
such a beneficial impact. In addition to relationship norm orientation, future research can also explore
other factors that may vary the impact of AI-expressed emotion on customers’ expectations and norms
during a service interaction, such as price, culture, etc.
Second, our manipulation of emotional intensity is restricted to emotional phrases that are expressed
normally or appropriately because companies are unlikely to configure AIs to express extremely intense
emotion. Still, varying emotional intensity at a more granular level may yield interesting findings not
uncovered in this research. Furthermore, emotional intensity can be manipulated through various vocal
qualities (Murray and Arnott 1993). As voice-based AIs are another emerging trend in both personal lives
(e.g., virtual assistants such as Apple’s “Siri” and Amazon’s “Alexa”) and customer service interactions
(during phone calls), future research can look into the impact of emotions expressed through the voice.
Third, our proposed theoretical model does not address the interdependencies of affective and
cognitive processes. Due to the complex relationship between affect and cognition (Izard 2011; Phelps
2006), it is likely for our two proposed mechanisms to influence each other. Although this work provides
suggestive evidence for our parallel model after accounting for possible interdependencies (see footnote
1), future research can attempt to disentangle affective and cognitive processing more clearly.
Fourth, in addition to relationship norm orientation, other boundary conditions for our proposed
mechanisms are worthy of further exploration. Because the likelihood and extent of the emotional
contagion process in human relationships depend on the expresser, the perceiver, and the relationship
between the two, it is also possible that boundary conditions exist for emotional contagion between an AI

29
and a human. For instance, emotional contagion may be stronger for those individuals who have more
experience with AI agents or feel more attached to AIs. Furthermore, the expectation-disconfirmation
process may depend on when and how expectations are formed. Whereas our studies disclosed the AI
agent’s identity before the interaction, a disclosure during or after the interaction may lead to different
expectations toward the agent, which can, in turn, influence the extent of expectation-disconfirmation and
customers’ reactions to the agent’s emotional expression.
Lastly, emotion is a complex concept that comprises various aspects, such as other dimensions (e.g.,
valence) and discrete emotions. The ability of an AI to express emotion has just started to emerge, and
further research into other aspects of emotional expressions can provide additional insights into the best
ways of deploying emotionally intelligent AIs. For example, AI agents may empathize with customers’
concerns by expressing sadness or responding to customers’ anger in an apologetic manner. Delving into
other emotions can help draw a comprehensive picture of the unique impact of AI-expressed emotions.
The emotion used in our work is also fixed to be appropriate because we primarily investigate the unique
impact of emotion expressed by an AI rather than a human. AIs may be prone to errors or express
irrelevant emotions, so exploring the consequences of inappropriate emotional expressions can have
significant implications. Our work opens up exciting opportunities for future research to look into the role
of emotion in this nascent but essential area.
Conclusion
Considering the recent trend in the rapid deployment of AIs across various industries and the growing
capabilities of emotional AIs, this research points to the importance of studying the unique impact of AIexpressed
emotion. Our paper provides experimental evidence that the emotional expressions of an AI
service agent have a distinct impact on customers’ evaluations of service outcomes compared to those of a
human agent. We also reveal a novel individual-difference variable, relationship norm orientation, further
enriching our theoretical framework. We believe this work represents an initial step into a nascent yet
critical area of human-AI interactions. We anticipate future research to further expand our understanding
of the role of an AI’s emotional expressions in diverse contexts.







----------------------------------------------------------------------------------






Luo, X., Tong, S., Fang, Z., & Qu, Z. (2019). Frontiers: Machines vs. humans: The impact of
artificial intelligence chatbot disclosure on customer purchases. Marketing Science, 38(6),
937-947.

Abstract
Empowered by artificial intelligence (AI), chatbots are surging as new technologies
with both business potential and customer pushback. This study exploits field experiment
data on more than 6,200 customers who are randomized to receive highly
structured outbound sales calls from chatbots or human workers. Results suggest that
undisclosed chatbots are as effective as proficient workers and four times more effective
than inexperienced workers in engendering customer purchases. However, a disclosure of
chatbot identity before the machine–customer conversation reduces purchase rates by
more than 79.7%. Additional analyses find that these results are robust to nonresponse bias
and hang-ups, and the chatbot disclosure substantially decreases call length. Exploration of
the mechanisms reveals that when customers know the conversational partner is not a
human, they are curt and purchase less because they perceive the disclosed bot as less
knowledgeable and less empathetic. The negative disclosure effect seems to be driven by a
subjective human perception against machines, despite the objective competence of AI
chatbots. Fortunately, such negative impact can be mitigated by a late disclosure timing
strategy and customer prior AI experience. These findings offer useful implications for
chatbot applications, customer targeting, and advertising in conversational commerce.

Keywords: artificial intelligence, chatbot, conversational commerce, new technology, disclosure

Introduction
Chatbots are a popular new technology with unprecedented
business potential, galvanized by artificial intelligence
(AI) and machine learning. Essentially, AI
chatbots are computer programs that simulate human
conversations through voice commands or text chats
and serve as virtual assistants to users. Google Duplex,
a groundbreaking application of AI chatbots, can make
restaurant and haircut reservations over the phone,
wherein people answering the call may not know they
are engaging conversations with bots (Leviathan and
Matias 2018).
The market size of chatbots is expanding quickly,
from $250 million in 2017 tomore than $1.34 billion in
2024 (Pise 2018). More than 21% of U.S. adults and
more than 80% ofGeneration Z use voice/text bots for
information search and shopping (Del Valle 2018).
Many brands, such as American Eagle Outfitters and
Domino’s Pizza, have rolled out chatbots to take orders
or recommend products, and major platforms,
such as Amazon, eBay, Facebook, and WeChat,
have adopted chatbots for conversational commerce
(Thompson 2018).
AI chatbots can provide several unique business
benefits. First, they automate customer services and
facilitate firm-initiated communications. Chatbots are
equipped with sophisticated speech recognition and
natural language-processing tools that enable them to
understand complex and subtle dialogs and address
consumer requests with depth, compassion, and even
humor (Wilson et al. 2017). Moreover, chatbots can
converse in a friendly way with customers because
they don’t have bad days and never get frustrated or
tired like humans. In addition, they can easily scale
up to handle a large volume of customer
Despite such potential benefits for the supply side,
a key challenge for AI chatbot applications is customer
pushback from the demand side (Froehlich
2018). Customers may feel uncomfortable in talking
with computer programs for personal needs or letting
chatbots assist in purchase decisions. That is, humans
may prejudice that chatbots lack personal feeling and
empathy, perceiving bots as less trustworthy with
payment information and product recommendations
(i.e., the uncanny valley feelings and algorithm aversion
inDietvorst et al. (2018) andKestenbaum(2018)).
Therefore, firms face a dilemma in disclosing the
usage of AI chatbot technology to customers. On the
one hand, if firms disclose the machine identity, they
might not gain the full business value of AI chatbots
because of customer pushback. On the other hand,
customers have the right to knowwhether it is a bot or
a human that handles their communications because
of business ethics (Wise 2018). Moreover, regulators
are increasingly concerned about customer privacy
protection and have encouraged companies to be
transparent on chatbot applications during customer
communications (Federal Trade Commission 2017).
Against this backdrop, we collaborate with a large
financial service company to conduct a randomized
field experiment on chatbot disclosure. The company
randomly assigned 6,255 customers to receive
highly structured outbound sales calls from chatbots
or human workers. A novel part of our experiment
design is to vary the disclosure of chatbots (no disclosure,
disclosure before conversation, disclosure
after conversation, or disclosure after decision) as
well as human expertise (proficient or inexperienced
workers).1 This allows us to test the causal impact of
chatbot disclosure on customer purchases and compare
the performance of chatbots and human workers
in the six-condition experiment.
Our data suggest that undisclosed chatbots are as
effective as proficient workers and four times more effective
than inexperienced workers in engendering
customer purchases. However, the disclosure of chatbot
machine identity before conversation reduces purchase
rates by more than 79.7%. Our results are robust to
various falsification checks and additional analyses
with nonresponse bias and hang-ups.Also, compared
with the condition of no disclosure, disclosure before
conversation substantially reduces the call length.
Next, we test the behavioral mechanisms by augmenting
the field experiment with survey data and
voice-mining of conversation records. The survey data
support that, when customers know the conversational
partner is not a human, they are brusque and purchase
less because they perceive the disclosed bot as less
knowledgeable and less empathetic. However,
voice-mining of the objective conversation records
suggests that the undisclosed chatbot is competent in
terms of knowledge and empathy. Thus, the negative
chatbot disclosure effect seems to be driven by a subjective
human perception against machines despite the
objective competence of AI chatbots.
Moreover, we explore various ways to mitigate the
negative effect of chatbot disclosure on customer
purchases. We find that such negative impact can be
allayed by a late disclosure timing strategy and customer
prior AI experience.
Our research makes several contributions. It provides
the first field experiment evidence for the business value
of emerging AI technology and challenges of chatbot
applications. Our field data and voice-mining approaches
not only reveal the negative impact of chatbot
disclosure on customer purchases, but they also shed
light on the underlying mechanism. Our findings of the
mitigated effects are nontrivial because they empower
marketers to target certain customer segments for more
optimal value of AI chatbot services. Also, brands can
advertise the role of experiential learning so as to cultivate
consumer trust in chatbots, that is, from aversion
to appreciation of bots.
More broadly speaking, we extend the discussion
about machines versus humans. Our data suggest
that undisclosed chatbots that incur almost zero
marginal costs can outperform the paid underdogs by
five times in purchase rates. These findings imply that
the potential replacement of underperforming human
workers by AI chatbots and other new automation
technologies is an inevitable trend. However, our results
of the negative disclosure effect also imply that
chatbots may not perfectly substitute human labors in
the near future because of a subjective human perception
again bots. These findings have useful implications
for chatbot applications in conversational
commerce. Indeed, motivated by our findings, the financial
service company has taken actions to implement
a human–AI assemblage strategy. AI chatbots
assist call center workers, especially the underdogs, by
analyzing customer queries and emotional stresswith
voice-mining and by displaying best answers from the
depository of company knowledge bank as possible
solutions to customer needs.
Related Literature on AI Applications and
Text-Based Bots
Prior research has recognized the benefits of AI
technologies across various fields. In finance, trading
bots and robo-advisors can facilitate investors for
stock analytics (Trippi and Turban 1993). AI applications
can improve banks’ operation efficiency, fraud
detection, and asset management (Fethi and Pasiouras
2010). Studies in healthcare have explored how AIpowered
algorithms can help doctors diagnose
cancers (Esteva et al. 2017, Leachman and Merlino
2017). AI applications can reduce medical errors
and improve hospital efficiency (Patel et al. 2009,
Bennett and Hauser 2013). In marketing, Huang and
Rust (2018) note that the future trend of AI applications
hinges upon empathetic tasks that require
computers to understand people’s emotional status
and respond appropriately with care and feeling.
Leung et al. (2018) find that AI automation may be
undesirable to consumers when the identity motives
are important drivers of consumption. However,
Logg et al. (2019) document that nonexperts appreciate
algorithmic advice based on laboratory experiments.
Prior research also discusses how AI
and robots will replace the labor and work force
(Brynjolfsson and Mitchell 2017, Lu et al. 2018). We
extend this literature by providing real-world field
experiment evidence for the negative impact of AI
chatbot disclosure on customer purchases. We demonstrate
the challenges of and harsh reactions to disclosed
chatbots in outbound sales calls although the
bots can simulate human conversations in an intelligent
and empathetic manner.
Our work on voice-based chatbots is related to and
extends the literature on text-based chatbots (e.g.,
Sivaramakrishnan et al. 2007, K¨ohler et al. 2011, Saad
and Abida 2016,Mimoun et al. 2017). Compared with
text-based bots, voice-based bots offer more anthropomorphism
in the humanized computer representations
and richer interaction data, such as voice pitch
and tone beyond the narratives. Importantly, narratives
only capture what is said but miss how it is
said (e.g., do the conversation participants raise their
voices suddenly, or is there a frustration tone?). Extending
prior literature on text-based chatbots, our
research involves voice-mining analytics that can
provide auditory cues of the sentiment and intent of
the conversation participants. Also, extending prior
research with surveys or laboratory studies measuring
soft outcomes, such as perceived fun and social
presence, we conduct a field experiment addressing
the hard metrics in terms of customer purchases.
We further leverage deep learning methods of voicemining
and survey data to identify behavioral mechanisms
that might account for the negative impact of
disclosed bots on customer purchases.
Company Background and
Experiment Settings
The randomized field experiment was conducted
by a major internet-based financial service company
in Asia (that wishes to be anonymous). In terms of
types of business, this company offers various financial
services, such as personal loans, refinance, and
equity investments, to individual customers through
its mobile app. Ranked among the top 20 in the Fintech
internet loan industry, this multibillion-dollar
company hasmore than 23 million registered customers.
Its customers are from all major provinces in the
country (see Online Appendix C). In our experiment,
the customers are borrowers who keep a high credit
score and have successfully repaid their loans to the
company in the past 11 months for the 12 monthly
installments. Because only one repayment is left,
there is a sales opportunity of loan renewal. Most loans
are in the amount between USD $800 to $2,500 for the
purposes of purchasing electronic products, such as
smartphones, computers, and TVs. The company can
assign chatbots or human workers in its call center to
make the outbound sales calls. In order to boost responses,
the service agents inform customers about a
special promotional offer for renewing the loan. The
promotion is a 24-hour limited-time offer to waive the
regular loan application processing fees if the customer
decides to renew the loan with the same terms
(loan amount, interest, and installments). All the
outbound sales calls occur on a Tuesday afternoon
from 2 p.m. to 4 p.m. during working hours of the day,
and most customers would be at their workplace rather
than home.
The company implements a sophisticated voice AI
chatbot in its call center to provide timely customer
services and improve the operational efficiency with
lower labor costs. Unlike traditional rule-based systems
that only handle simple inquiries with prerecorded
messages, the voice chatbot can conduct live
and natural conversations with customers. The AI
chatbot here is trained with the company’s call center
voice data to emulate the best-performing human
workers in terms of understanding financial loan
product features and deploying adaptive selling strategies
(Churchill et al. 1985) in serving customers over
the phone. The chatbot is applied to make highly
structured outbound sales calls because outbound
calls have relatively standard conversation content
for computers to handle. In the setting of structured
outbound calls, without disclosure, customers would
not realize the machine identity of the AI chatbot
over the phone.2 The chatbot in our experiment has
an optimized female voice, that is, with the most
appealing pitch, tone, speed, and intonation to capture
customer attention. The company uses a female
voice because there is no significant difference between
optimized female and male voices in call performance
during pilot tests. Indeed, most chatbots
(e.g., Alexa) in the industry adopt a female voice.
Next, we present the field experiment design.
Field Experiment Design
In the field experiment, the company randomly assigned
customers to receive a sales call from either
human agents or AI chatbots. Each customer receives
only one call and is randomized into one of the six
experimental conditions in a between-subject design.
Figure 1 (the top panel) presents the six conditions
and sample sizes.
The first condition is underdogs in the call center,
that is, unseasoned human workers whose past sixmonth
call report performance is among the bottom
20th percentile. The second condition is proficient
workers, that is, experienced human agents whose
past performance is among the top 20th percentile.
The third condition is AI chatbot without disclosure. In
this group, the chatbot initiates the sales call without
revealing its machine identity. For these three
conditions, the agent starts the call with a greeting
statement: “Dear customer, I am the service agent of
the company XYZ” prior to communicating the promotional
deal to the customers.
The fourth condition is AI chatbot with disclosure
before conversation. Here, the chatbot reveals its machine
identity at the beginning of the conversation
with the customer. The disclosure of chatbot identity
is a simple statement: “Dear customer, I am the AI
voice chatbot of the company XYZ” prior to communicating
the same promotional deal. The fifth condition
is AI chatbot with disclosure after conversation.
In this group, the chatbot does not reveal its machine
identity (with the same statement as in the fourth
condition) until after communicating the promotional
deal to customers but right before they decide
whether to purchase. The sixth and final condition
is AI chatbot with disclosure after decision,
wherein the chatbot reveals its machine identity (also
with the same statement as in the fourth condition)
right after customers decide whether to purchase.3
All service agents across the six conditions follow
the same sales call procedure as shown in Online
Appendix A. Service agents first greet customers and
appreciate their good repayment history before offering
the special promotion deal over the phone. If
customers are not interested, the agent tries to remedy
the sales call by elaborating that the deal is designed
for high-value customers and expires in 24 hours and
by encouraging customers to review the promotion
details on themobile app.4 However, if customers are
interested in the promotion, the agent asks follow-up
questions about their changes in job as well as credit
card balance. Customers are then asked to confirm
whether to renew the loan. If customers agree to
renew the loan, they need to log on to the mobile
app to sign the documents (99% of the people who
agreed to do so indeed followed through ultimately
according to the company records). Examples of the
call transcripts of the six experimental conditions
can be found in Online Appendix B, and audio examples
of the AI chatbot used in our experiment
are available online. In the data, making a purchase
means that customers agree to renew their loans during
the promotion period with the financial service
company.
Data and Randomization Check
Figure 1 shows that there are a total of 6,255 attempted
customers who are called by service agents.
Out of these, 255 are nonresponses (customers who
may be too busy or have changed their contact numbers),
and each condition has 1,000 responses to
achieve the promotion goal with an automated replacement
technique. Our proprietary data set includes
rich information about the customers. Table 1
summarizes the descriptive statistics. According to
average age of 30.86, and most of them have a high
school or higher degree. The statistics also indicate
that targeted customers tend to be young working
professionals who frequently use credit cards and
engage in online shopping. They have, on average,
1.26 credit cards, US$1,843 credit card spending,
and US$107 online spending in the past 30 days as
well as 10 online personal loan inquiries in the past
30 days. Their personal loan amount with the company
is around US$2,017. We conducted randomization
checks with these background variables. The
results in Table 2 suggest that there is no significant
difference among these variables across the six
experimental conditions according to F-test statistics.
Thus, the data passed the randomization check.
Effects of Chatbot Disclosure on
Customer Purchases
The model-free results based on the raw data across
the six treatments in Table 3 suggest that the condition
of disclosure before conversation tends to have
lower purchase rates, higher hang-up rates, and shorter
call length.
Next, we apply econometric models to test the effects.
Because we have randomized field experiment
data to identify causal effects, our modeling analyses
of purchase rates are straightforward. We develop
a logit model, in which the unobserved purchase
likelihood is a logit function of the randomized
conditions:
Purchase Likelihoodi  Exp(Ui)
Exp(Ui) + 1
Ui α + α1 *Underdogsi + α2 * WithoutDisclosurei
+ α3 * Before Conversationi + α4 * AfterConversationi
+ α5 * AfterDecisioni + ΓControlsi + εi, (1)
where Ui denotes the latent utility of making a purchase,
and the dependent variable of purchase is
whether the customer has decided to renew the loan.
The key independent variables are the six groups
in our experiment, that is, the five dummy variables
with the proficient human agent group as the
comparison baseline. Controlsi is a vector of control
variables with individual customer profiles, including
gender, age, education, and location dummies
(see Online Appendix C for a frequency distribution
of the 33 provinces); number of credit cards; online
loan inquiries; loan amount; credit card spending;
and online spending as well as customer voice pitch
(which are derived from speech-to-text, Word2Vec,
and Hierarchical Softmax Python tools; see Online
Appendix D for details). Note that, in the natural
holdout case, without any sales call, the organic purchase
rate is zero during the promotion period because
customers would not know the loan renewal opportunitywithout
the sales calls. Thus, all effects on purchases
here are incremental.
Table 4, columns (1)–(3), reports the results for all
attempted calls. Across three models (logit, probit,
and ordinary least squares (OLS)), the results consistently
suggest that, relative to proficient human
workers, disclosing the chatbot machine identity before
the conversation statistically significantly reduces
customer purchase rates (p < 0.01).
Besides the statistical significance, we present the
magnitude of the effects in Figure 2. Compared with
the without disclosure condition, disclosure before
conversation decreases customer purchase rates dramatically
by 79.7% (from 0.237 to 0.048).
Robustness Checks with
Falsification Tests
Our results are robust to various falsification checks.
First, because the AI chatbot is trained by the calling
records of the company’s proficient workers, performance
should be similar. Results in Table 4 indeed
support that the purchase rate of no disclosure is not
significantly different from that of proficient workers
(p > 0.10). This also rules out an alternative explanation
that it might be the bad service quality of the
chatbot itself rather than the act of disclosure that
drives the negative effects. Also, the underdogs generate
a significantly low purchase rate of 0.05 (p < 0.01).
This makes sense because they are inexperienced
rookies and unseasoned call center employees in the
company. Still, they get some purchase results because
of the exerted sales efforts. Moreover, we expect
that the condition of after decision will not differ
from the condition of proficient workers because it is
after the fact (customers have already made the decision
of purchasing or not). This is confirmed by
the insignificant coefficient of after decision in Table 4,
thus passing another sanity or falsification check.
More Robustness Checks with
Nonresponses and Hang-ups
First, we conducted additional analyses with possible
nonresponse bias. Customers are randomized to receive
the call but not answer it. Thus, one possible
concern is that customers may self-select to ignore the
call and not purchase. That is, not all attempted calls
are answered by customers because some customers
cannot answer the phone (as this study was done
during work hours), and others might have changed
their contact numbers. As presented in Figure 1, the
middle panel, our data have a total of 255 nonresponses
with a response rate of 96% from attempted
customers. This high response rate is not surprising
because the targeted loan borrowers may fear missing
out on important loan-update information from the
lending company.More importantly, our data suggest
that the nonresponse rates are almost evenly distributed
among the six experiment groups, ranging from
3.5% to 5% as shown in Figure 1 and Table 2, last
column. We also run the models after excluding the
nonresponses. Results in Table 4, columns (4)–(6),
confirm that all our main results are robust. Thus,
possible selection effects resulting from nonresponses
cannot explain our results.
Moreover, we check our data regarding hang-ups
(defined as the cases in which customers terminate
the call within five seconds right after knowing the
bot machine identity). If customers terminate the
call or hang up too early, they might not have indicated
theirpurchasedecisions.As reportedinFigure 1,
bottom panel, there are a total of 608 hang-ups. The
condition of disclosure before conversation had 563
cases (hang-ups without much interaction with the
AI chatbot), and the condition of disclosure after conversation
had 45 cases (hang-ups after the initial interaction
with the AI chatbot). The remaining four
groups had zero hang-up cases. We rerun the models
after further excluding the hang-ups so as to scale the
purchase rate {= number of “yes” purchase decisions/
(numberof“yes” purchase decisions + number of “no”
purchase decisions)}. Again, Table 4, columns (7)–(9),
confirm that all our main results are robust after accounting
for hang-ups. We also check the robustness
by measuring hang-ups within four, three, two, and
one seconds after the botmachine identity disclosure,
and again, all results are robust across these different
measures of hang-ups. These analyses of hang-ups
resulting from chatbot disclosure motivated us to
dive deeper by examining call length.
Additional Analyses with Call Length
One plausible explanation for our results is that, when
customers know the conversational partner is not a
human, they tend to be curt (i.e., hang up abruptly or
terminate early) and purchase less. If so, the call length
in the disclosed chatbot condition should be significantly
shorter than that of the undisclosed chatbot condition.
This is confirmed by the Online Appendix D
histograms of call length. Among the six experimental
conditions, the case of chatbot identity disclosure before
conversation has the shortest call length. We also run
the models with call length as the dependent variable.
Results in Table 5 with both OLS and tobit models
consistently support the negative and significant effect
of before conversation on call length for the samples
of attempted calls, excluding nonresponses and hangups.
However, these results cannot reveal the underlying
psychological mechanisms, which are explored next.
Behavioral Mechanisms for the Negative
Effects of Chatbot Disclosure
To understand the behavioral mechanism, we augment
the field experiment with subjective data from
postcall surveys as well as objective voice data from
audio analytics of the conversation records. The
surveys poll all customers who have completed or
hung up the calls and ask their satisfaction with the
service agent’s knowledge level and sentimental empathy
(see Online Appendix E). Figure 3 reports the
results of a formal mediation test with 5,000 replications
in bootstrapping (Preacher and Hayes 2004).
The results confirmthat, relative tonodisclosure, chatbot
disclosure before conversation significantly reduces the
perceived knowledge and empathy of chatbots and,
through these two mediational routes, decreases call
length and purchase rates (all path p < 0.01; see Online
Appendix E for more details). In other words, when
customers know the conversational partner is not a
human, they are brusque and purchase less because
they perceive the disclosed bot as less knowledgeable
and less empathetic. However, voice-mining of the objective
conversation records suggests that the undisclosed
chatbot is indeed as competent as proficient workers in
terms of knowledge and empathy (see Online Appendix
F). Thus, the negative impact of chatbot disclosure may be
driven by a subjective human perception against machines
despite the objective competence of AI chatbots.
Additional Checks on Deception Feeling
and Order Cancellation
Another alternative explanation is a customer feeling
of deception. However, in the condition of disclosure
before conversation, the customers are informed up
front about the chatbot machine identity; that is, the
disclosure is done immediately. Thus, it is more likely
that customers’ subjective perception against the chatbot
rather than their feeling of deception drives the negative
disclosure effect. Also, voice-mining of the conversation
records failed to find words with strongly
negative feelings across all experimental conditions,
more evidence ofnoseriousdeceptionfeeling.Moreover,
according to company records, there are no order
cancellation or overt consumer complaints against
the company in the conditions of chatbot identity
disclosure after the experiment.
Strategies to Mitigate the Negative Effects
of Chatbot Disclosure
Mitigation Strategy One
Results in Table 4 on the coefficient comparisons
indicate that customer purchase rates significantly
improve when the disclosure is delayed from before
to after the conversation and to after the decision (all
ps < 0.01). Thus, more interactions with and experiential
learning of chatbots may help allay the negative
chatbot disclosure effect. In other words, as long as
the chatbot identity is disclosed, regardless of before
or after the conversation, customer purchase rates
are negatively affected. However, disclosing the bot
identity after the conversation helps mitigate such
negative impact. This is reasonable because the customer
might form a good impression in the first oneminute
interaction with the AI chatbot, which can
help reduce the distrust of the chatbot.
Mitigation Strategy Two
We also explore how customers’ prior AI experience
can affect the negative effects of chatbot disclosure.
The data set provided by the company includes a
binary variable that indicates whether a customer
downloaded and used other AI apps on the smartphone
(1 = has at least one AI app with smart digital
agents similar to Google Allo, ELSA Speak, Cortana,
FaceApp, Edison Assistant and 0 = otherwise).
As shown in Table 6, prior experience with AI induces
more customer purchases. More importantly, the coefficient
of the interaction term PriorAI Experiencei *
Before Conversationi is positive and significant (p < 0.01),
suggesting that prior AI experience is helpful in reducing
the negative disclosure effect.
Conclusion and Future Research
This research examines AI chatbots, a timely and managerially
relevant topic. On the basis of a six-condition
field experiment, it finds that the disclosure of chatbot
machine identity reduces purchase rates substantially.
Further analyses reveal that customers tend to purchase
less and even terminate the calls early because
they perceive the disclosed chatbot as less knowledgeable
and empathetic.
Our setting of structured outbound calls is limited
because the chatbot only engages in a restricted two-way
information exchange rather than a highly interactive
two-way conversation. This restrictive nature is an
important limitation here, which may help open up
new research. For example, it would be fruitful for
future research to investigate dynamic differences of
the two-way conversation between chatbot–customer
dyads versus worker–customer dyads. Another direction
for future research is to test the generalizability
of our results in other settings, such as themore dynamic
inbound calls. Moreover, we address the first-order
disclosure effects (with or without disclosure). Future
research may test the second-order effects with different
framings in the introduction of disclosed bots. For
instance, the AI chatbots may self-introduce to customers
with the framing of enhanced technological
benefits (big data computing and fast quantitative
learning of AI chatbots), reduced customer hassle costs
(less waiting time to get answers from AI chatbots), or
even surprising consumer welfare (offering the product
at a lower price because bots help save labor costs).
Indeed, bots may help make life less prickly in certain
interactions that are inherently bleak (e.g., call customer
service support to fix computers or replenish a product).
Paradoxically, in these interactions, humans are trained
to behave like a bot. Also, customers have different
innate preferences of talking to bots as some can be cordial
and don’t feel judged, but others tend to be rude and
brusque (Thompson 2018). Thus, depending on the
degree of task complexity and customer preference
heterogeneity, future endeavors may let customers
self-selectwho (bots or humans) to serve themover the
phone in order to boost purchases in conversational
commerce. As millions tell Alexa, Siri, or Google Assistant
to play music, reorder products, and make
appointments, the impact of AI new frontiers on our
daily life will be ubiquitous in the long run.
In conclusion, more scholarly works are strongly encouraged
to address this pivotal area of AI chatbot applications
for marketing promotions and customer services.







----------------------------------------------------------------------------------





Boyacı, Tamer, et al. (2024). “Human and machine: The impact of machine input on decision
making under cognitive limitations.” Management Science,
https://doi.org/10.1287/mnsc.2023.4744.

Abstract
The rapidadoptionofAItechnologiesbymanyorganizationshasrecentlyraisedconcernsthatAImayeven-
tually replacehumansincertaintasks.Infact,whenusedincollaboration,machinescansignificantlyenhance
the complementarystrengthsofhumans.Indeed,becauseoftheirimmensecomputingpower,machinescan
performspecifictaskswithincredibleaccuracy.Incontrast,humandecision-makers(DM)areflexibleand
adaptivebutconstrainedbytheirlimitedcognitivecapacity.Thispaperinvestigateshowmachine-based
predictions mayaffectthedecisionprocessandoutcomesofahumanDM.Westudytheimpactofthese
predictions ondecisionaccuracy,thepropensityandnatureofdecisionerrorsaswellastheDM’scognitive
efforts. Toaccountforbothflexibilityandlimitedcognitivecapacity,wemodelthehumandecision-making
processinarationalinattentionframework.Inthissetup,themachineprovidestheDMwithaccuratebut
sometimes incompleteinformationatnocognitivecost.Wefullycharacterizetheimpactofmachineinput
on thehumandecisionprocessinthisframework.Weshowthatmachineinputalwaysimprovestheoverall
accuracy ofhumandecisions,butmaynonethelessincreasethepropensityofcertaintypesoferrors(suchas
false positives).Themachinecanalsoinducethehumantoexertmorecognitiveefforts,eventhoughitsinput
is highlyaccurate.Interestingly,thishappenswhentheDMismostcognitivelyconstrained,forinstance,
becauseoftimepressureormultitasking.Synthesizingtheseresults,wepinpointthedecisionenvironments
in whichhuman-machinecollaborationislikelytobemostbeneficial.Ourmaininsightsholdfordifferent
information andrewardstructures,andwhentheDMmistrustthemachine.

Key words : machine-learning,rationalinattention,human-machinecollaboration,cognitiveeffort

1. Introduction
The increasingadoptionofsmartmachinesanddata-basedtechnologieshavequestionedthefuture
role ofhuman-baseddecisionsinorganizations(Kleinbergetal.2017).Whilenewtechnologies
sometimes substituteforlabor,awealthofevidencesuggestthattheycanalsocomplementhuman
skills (seeFeltenetal.2019andreferencestherein).Indeed,thepurposeofmanyreal-worldapplica-
tions ofsupervisedmachinelearningisnottoproduceafinaldecisionbasedsolelyonanalgorithm’s
output, butrathertoprovideusefulinformationintheformofautomatedpredictionstoahuman
decision-maker(Lipton2016,Agrawaletal.2018).Varioussectorscurrentlyseektoharnesssuch

human-machinecomplementarity,includingthedefenseandhealthcareindustries(DARPA2018),
legal andtranslationservices(Katz2017),humanresourcesmanagement(Gee2017),supplier
management(Saenzetal.2020)orsupplychainoperations(IBM2017).
Humans andmachinescomplementeachotherbecausemachinesoftensubstituteforonlyasubset
of thedifferenttasksrequiredtoperformanactivity(Autor2015).Thisistypicallythecasefor
judgmentanddecisionproblems.Indeed,humandecision-makersrelyontheircognitiveflexibility
to integrateinformationfromvastlydiversesources,includingtheverycontextinwhichthese
decisions aremade(Diamond2013,Laureiro-Mart´ınezandBrusoni2018).Machines,bycontrast,
are muchmorerigidandcanonlyextractalimitedsubsetofthisinformation(Marcus2018).
Hence, humansmayhaveaccesstopredictivevariablesthat,forexample,amachine-learning(ML)
algorithm cannotsee(Cowgill2018).However,machine-extractedinformationcanhavehigher
accuracy becauseoftheenormousandreliablequantitativecapacityofmachines.Incontrast,
the cognitivecapacityofhumansislimited,andhencehumandecision-makersneedtoconstantly
balance thequalityoftheirdecisionswiththeircognitiveefforts(Payneetal.1993).
Forinstance,whendecidingonwhichstockstoinvestin,mutualfundmanagersestimateboth
idiosyncratic shocks(forstockpicking)andaggregateshocks(formarkettiming)(Kacperczyk
et al.2016).Becauseoftheirsuperiorcomputingcapability,MLalgorithmsidentifyidiosyncratic
shockswithgreatersuccess,butfailtodetectaggregateonescomparedtohumans(Fabozzietal.
2008, Abis2020).Inthemedicaldomain,MLalgorithmscaneasilyprocesslargeandrichmedical
histories, butmaynotobtainvaluableinformationfromthepersonalinteractionbetweenphysicians
and theirpatients.Similarly,manyHRmanagersbasetheirhiringdecisionsoninformationthat
ML algorithmscannotaccess(Hoffmanetal.2017).
Totheextentthatdata-basedtechnologiesimprovetheprovisionofcertaininformation,the
co-productionofdecisionsbyhumansandmachinestypicallybooststheoverallqualityofthese
decisions (Mims2017).Forinstance,thecollaborationbetweenhumanradiologistsandmachines
improvestheoverallaccuracyofdiagnosesforpneumoniaovertheperformancesofradiologists
alone, ormachinesalone(Pateletal.2019).Effectivehuman-machinecollaborationssuchasthese1
are sometimescoined“centaurs”(half-human,half-machine)intheliteratureandpopularpress
(Case 2018).Yet,theprovisionofmachine-basedpredictionsmaynotimproveallaspectsofhuman
decisions. Forinstance,Stoffeletal.(2018)findthatwhenradiologiststakeintoaccountthe
deep-learning analysisofultrasoundimages,thediagnosesofbreasttumorssignificantlyimprove.
This isconsistentwiththeclaimthathuman-machinecollaborationimprovesoverallperformance.
1 The ideaofhuman-machinecollaborations–orchesscentaurs–werepopularizedbyWorldChessChampionGary
KasparovfollowinghisnotoriousdefeatagainstIBMDeepBluein1997.Anonlinechesstournamentin2005confirmed
the superiorityofchesscentaursovermachines.

3
However,thisimprovementmainlystemmedfromaradicaldecreaseinfalsenegatives,whilethe
false positiveratedidnotsignificantlychange.
This impactofmachine-basedpredictionsondecisionerrors,andmoregenerallythetimeand
cognitiveeffortsthathumansputintotheirdecisions,remainslargelyunknown.Asaresult,the
participation ofmachinesinhumandecisionsmayhaveunintendedconsequences.Increasingthe
numberoffalsepositiverates,forinstance,mayexertunduepressureonahealthcaredelivery
system andputhealthypatientsatrisk.Andincreasingthecognitiveloadofadecision-makermay
slowdownthedecisionprocess,whichmayresultindelaysandcongestion.
In thispaper,weconsiderthedefiningcharacteristicsofhumanandmachineintelligenceto
address thefollowingfundamentalquestions:Whatistheimpactofhavingmachine-basedpre-
dictions onhumanjudgment?Inwhichwaysdothesepredictionsinfluencethedecision-making
processofhumans,theextentoftheircognitiveefforts,andthenatureoftheirdecisionerrors?In
whichdecisionenvironmentsarethecollaborationsbetweenhumansandmachinesmorefruitful?
Toanswerthesequestions,weconsideranelementarydecisionprobleminwhichanMLalgorithm
(the machine)assistsahumandecision-maker(theDM)byassessingpartof,butnotall,the
uncertaintythattheDMfaces.Wemodelthisproblemwithinthetheoryofrationalinattention
formalized bySims(2003,2006)tocapturethemostfundamentalsourcesofcomplementarity
betweenmachineandhumanintelligence.Namely,theDMleverageshercognitiveflexibilityto
integratevarioussourcesofinformation,includingherdomainknowledgeorspecificaspectsofthe
contextinwhichthedecisionismade.However,theDMisconstrainedbyherlimitedcognitive
capacity,sothatassessinginformationrequiresexertingcognitiveefforts.ThemoreefforttheDM
exerts, themoreaccurateherassessmentis.Incontrast,themachinedoesnotsufferfromthis
limitation andcanprovideanaccurateassessmentofsomeinformationatnocost.Yet,themachine
cannot assessallinformationsourcessuchastheDM’sdomainknowledgeandthedecisioncontext.
The rationalinattentionframework,withinwhichwedevelopourmodel,enablesustorepresent
the DM’scognitiveflexibilityandlimitedcapacityinacoherentmanner.Indeed,thistheory
assumes thatpeoplerationallydecideonwhatpieceofinformationtolookfor,inwhatdetail,
and theydosoinanadaptivemanner.Inparticular,theframeworkendogenouslyaccountsfor
people’sscarceresources,suchastime,attentionandcognitivecapacityaswellasthenatureof
the decisionenvironment.Peoplearefreetouseanyinformationsource,inanyorder,togenerate
knowledgeatanyprecisionlevel,butlimitedcognitiveresourcesleadtoinformationfrictionsand
hence, possiblemistakenjudgments.Inotherwords,theframeworkdoesnotimposeanyapriori
restrictions onpeople’ssearchstrategy(cognitiveflexibility)otherthanalimitontheamountof
processedinformation(limitedcognitivecapacity).Moregenerally,thistheorynaturallyconnects
the fundamentaldriversinhumandecision-making,suchaspayoffs,beliefs,andcognitivedifficulties

4
in arationallearningsetup,andisperceivedasabridgingtheorybetweenclassicalandbehavioral
economics. Thereisalsoagrowingbodyofempiricalresearchthatfindsevidenceofdecision-making
behaviorconsistentwiththetheory(Mackowiaketal.2021).
In thissetup,weanalyticallycomparetheDM’schoice,errorrates,expectedpayoff,cognitive
effort, andoverallexpectedutilitywhentheDMdecidesaloneandwhensheisassistedbya
machine.Ouranalysisfirstconfirmsthesuperiorityofthehuman-machinecollaboration,i.e.,we
showthattheaccuracyandtheDM’soverallexpectedutilityalways(weakly)improveinthe
presence ofamachine.Wefurtherfindthatthemachinealwaysreducesfalsenegativeerrors.
Yet,ourresultsalsoindicatethatmachine-basedpredictionscanimpairhumandecisions.Specif-
ically,wefindthatmachine-assisteddecisionssometimes increase the numberoffalsepositives
compared towhentheDMdecidesalone.(Incidentally,thisfinding,alongwithourresultthatthe
machinereducesthefalsenegativerates,offerssometheoreticalfoundationfortheempiricalresults
of Stoffeletal.2018.)Inaddition,themachinecaninducetheDMtoexert more cognitiveefforts
in expectation,andmakeherultimatechoice moreuncertain a priori.Inotherwords,themachine
can worsencertaintypesofdecisionerrors,andincreaseboththetimeandvarianceinvolvedina
decision process,whichisknowntocreatecostlydelaysandcongestion(Alizamiretal.2013).
Wefullycharacterizetheconditionsunderwhichtheseadverseeffectsoccurinoursetup.A
prominentcaseiswhentheDM’spriorbeliefisrelativelyweakandhercognitivecostofassessing
information isrelativelyhigh(i.e.,hercognitivecapacityisreducedduetoexogenoustimepressure,
or consumedbycompetitivetasksbecauseofmultitasking).Yet,thoseareconditionsunderwhich
using amachinetooffloadtheDMismostappealing.Inotherwords,improvingtheefficiency
of humandecisionsbyrelyingonmachine-basedpredictionsmayinfactbackfirepreciselywhen
these improvementsaremostneeded.Theseresultsholdatleastdirectionallyfordifferentpayoff
structures, whentheDMisbiasedagainst(ormistrusts)themachine,andwhenthemachineis
also imprecise.Weexplainindetailwhereandwhytheyoccur.
Our findingsaremostrelevantinsettingsinwhichahumandecisionmakerneedstoexertsome
cognitiveefforttomakerepetitivedecisionsthathingeonpredictions.Examplesincludediagnostic
tasks byradiologists(Liuetal.2019),predictivemaintenanceandqualitycontrolinmanufacturing
(Brosset etal.2019),theassessmentofwhetherapartcanberemanufacturedinaproduction
system (Nwankpaetal.2021),evaluatingapplicationsbyHRprofessionals(Gee2017)orassessing
a legalcaseinjudicialsystems(Cowgill2018).Ourframeworkislesssuited,however,fordecision
tasks wherethekeyunknownisacausalrelationship.
The restofthepaperisorganizedasfollows.In §2, werelateourworktotheexistingliterature.
In §3, weintroduceourbasicmodelofhumansandmachinesandfollowin §4 bycharacterizingthe
choicebehaviorandcognitiveeffortthathumansspend,aswellastheirimplieddecisionerrors.In

5
§5, weanalyzetheimpactofmachinesontheseandexplainourfindings.In §6, wediscussfurther
extensions tothedecisionandlearningenvironmentandinvestigatetheirimplicationsforhuman
and machineinteraction.Finally,in §7 wepresentourconcludingremarks.
2. RelatedLiterature
Overthepastdecade,researchersinMLhaverepeatedlydemonstratedthatalgorithmicpredictions
can match,andattimesevenoutperform,theeffectivenessofhumandecisionsinmanycontexts
(see, forinstance,Liuetal.2019forarecentandsystematicreviewinnhealthcare).Morerecently,
however,anemergingliteraturehasfocusedonimprovingthecollaborationbetweenmachinesand
humansasopposedtopitchingthemagainsteachother.Forinstance,averyrecentstreamof
researchincomputerscienceaimsatoptimizingalgorithmsbylettingthemautomaticallyseek
humanassistancewhenneeded(e.g.,Bansaletal.2019).Moregenerally,thefieldaimstoimprove
the interpretabilityofML-basedpredictionssoastofacilitatetheirintegrationintoahuman
decision-making process(e.g.,Doshi-VelezandKim2017).
Researchersinmanagementsciencehavealsostartedtostudytheintegrationofhumanjudg-
mentsintothedevelopmentofMLalgorithms.Ibrahimetal.(2021),forinstance,explorehowthe
elicitation processofhumanforecastsbooststheperformanceofanalgorithminanexperimental
setup. Petropoulosetal.(2018)similarlystudyhowhumanjudgmentcanbeusedtoimprove
the selectionofaforecastingmodel.Sunetal.(2021)alsoproposesanewbinpackingalgorithm
that accountsforthetendencyofhumanworkerstodeviatefrommachine’srecommendations.
Karlinsky-ShichorandNetzer(2019)findthatprovidinganalgorithm-basedpricerecommendation
to salespeopleimprovestheirpricingperformances,whichrelyontheirexpertise,relationshipsand
salesmanship skills.Conversely,KesavanandKushwaha(2020)findinaspare-partsretailersetting
that allowingmanagerstodeviatefromthesuggestionofanalgorithmincreasesprofitability.Oth-
ers havefurtherexploredtheconditionsunderwhichproductcategorymanagers(VanDonselaar
et al.2010)orradiologists(Lebovitzetal.2020)deviatefromanalgorithmrecommendation.
Overall,thesedifferentstreamsofresearchfocusonempircialyidentifyingwhenhumansdevi-
ate fromanalgorithm’srecommendation,andimprovingtheinteractionbetweenhumansand
machinesAfewauthorshavenonethelessanalyzedthishuman-machineinteractioninatheoretical
decision-making framework.Agrawaletal.(2018),inparticular,postulatethatAIandhumans
complementoneanotherinthatalgorithmsprovidecheapandaccuratepredictionswhilehumans
determine, atacost,thepotentialpayoffsassociatedwiththedecision,i.e.,theDMneedstoexert
effort tolearnherutilityfunction.Ourworkaddressesadifferentformofcomplementarity,in
whichhumancognitionisflexiblebutoflimitedcapacitywhilethemachineisrigidbuthasample
capacity.Morerecently,BordtandvonLuxburg(2020)proposerepresentingthehuman-machine

6
jointdecisionprocessinadynamicmulti-armbanditframework.Thegoalistostudyunderwhich
conditions humansandmachineslearntointeractovertimeanddynamicallyimprovetheirdeci-
sions. Incontrast,westudytheimpactofmachine-basedpredictionsonhumancognitionand
decisions. Oursetupisthereforestatic,butitendogenizesthehumancognitiveefforts.
The rationalinattentiontheoryonwhichourmodelisbasedwasfirstintroducedbySims(2003,
2006) andhassincebeenappliedinmanydifferentcontexts,suchasdiscretechoiceandpric-
ing (Matˇejka2015,BoyacıandAk¸cay2018),finance(Kacperczyketal.2016)orservicesystems
(CanyakmazandBoyaci2021)amongmanyothers.Severalempiricalstudieshavefurtheradded
supporttothetheory(see,forinstance,Mackowiaketal.2021forarecentsurvey).Notably,Abis
(2020) proposesanempiricaltestforasimplemodeloffinancialmarketsmadeofrationallyinat-
tentivehumansandmachineswithunconstrainedcapacity.Whilemachinesandhumansdecide
independentlyandmayevencompeteinthissetup,ourmodelconsiderstheircomplementarity.
Perhapsclosertoourpaper,JerathandRen(2021)showinastandardrationalinattention
set-up thatDMsalwaysprocessinformationthatconfirmstheirpriorbeliefswheninformationcost
is high.Themachine’spredictioninoursettinginteractswiththistendencytoconfirmaprior
belief,leadingsometimestheDMtoactuallyexertmoreeffortswhentheinformationcostishigh.
Besides rationalinattention,othermodelsofattentionhavebeenproposed.ForinstanceChe
and Mierendorff(2019)investigateasequentialattentionallocationproblembetweentwoPoisson
signals aboutatruestate.However,theDM’sinformationsourcesarerestrictedtothesetwo
signals intheirmodel,whiletheDMhasfullflexibilitytoelicitanysignalinoursetup.Inaddition,
the DM’sinformationacquisitionstrategyisonlydrivenbytheincentivestructureinCheand
Mierendorff (2019),whilethisstrategyisalsodeterminedbytheDM’spriorbeliefinoursetting.
Our workisalsorelatedtothehypothesistestingBayesianframework,inwhichtheDMruns
a seriesofimperfecttestsanddynamicallyupdatesherbeliefaccordinglyaboutwhichdecisionis
best(DeGroot1970).Thisapproachhasbeensuccessfullyappliedtoavarietyofproblems,suchas
the managementofresearchprojectsordiagnosticservices(McCardleetal.2018,Alizamiretal.
2013, 2019),butislesssuitedtorepresentthecognitiveprocessofadecision-maker.Indeed,this
Bayesianframeworktypicallyassumesthateachtest’sprecisionortheorderinwhichtheyare
run areexogenouslydetermined.Incontrast,ourset-upfullyendogenizesthelevelofprecisionas
wellastheassociatedcognitiveeffortinatractableway.Thisenablestoproperlyaccountforthe
flexibilityofhumancognition(Diamond2013,Laureiro-Mart´ınezandBrusoni2018).
Wefurthercontributetothenascentbehavioralresearchonmachine-humaninteractions,and
the issueoftrustinparticular.Forinstance,Dietvorstetal.(2016)findincontrolledexperiments
that DMsareadversetomachine-basedpredictions.deV´ericourtandGurkan(2022)alsoexplore
to whichextentaDMmaydoubtamachineassheobservesthecorrectnessofitsprescriptions

7
overtime.Moregenerally,Donohueetal.(2020)callformoreresearchinthisfieldtobetterunder-
stand whenhuman-machinecollaborationsprovidesuperiorpreformance.Wecontributetothisby
identifyingenvironments,inwhichusingamachinetoimproveefficiencyiscounter-productive.
Finally inoursetup,humansassessinformationfrommultiplesources,whichjointlydesignate
the truestateoftheworld.Inthisregard,ourpaperisrelatedtotherichliteratureonsearchwith
multipleattributes(see,forinstance,OlszewskiandWolinsky2016,Sanjurjo2017,andreferences
therein). Inparticular,Huettneretal.(2019)studyamulti-attributediscretechoiceproblem
whichgeneralizestherationalinattentionmodelinMatˇejka(2015)toaccountforheterogenous
information costs.Inourmodel,someattributesareeasiertoassesswhenthemachineispresent,
as inHuettneretal.(2019),butwespecificallyinvestigatetheimpactofthisonhumanchoice,the
extentofdecisionerrorsandcognitiveefforts.
3. AModelofHumanandMachine
In thissection,wefirstpresentadecisionmodelthatcapturestheflexibilityandlimitedcognitive
capacityofthehumaninarationalinattentionframework.Wethenconsiderthecasewherethe
DM isassistedbyamachine.
Consider ahumandecision-maker(whichwewillrefertoasDMhereon),whoneedstocorrectly
assess thetruestateoftheworld ω ∈ {g,b}, whichcanbegood(ω =g) orbad(ω =b). Wedenote
by μ the DM’spriorbeliefthatthestateisgood(μ = P{ω = g}). TheDMcanexertcognitive
efforts toevaluatetherelevantinformationandadjustherbeliefaccordingly.Themoreeffortshe
exerts, themoreaccurateherevaluationis.Whenavailable,amachine-learningalgorithm(which
wesimplyrefertoas“themachine”inthefollowing)assiststheDMbyaccuratelyevaluatingsome
of thisinformation,atnocognitivecost,toaccountforitsimmensecomputingcapabilities.Based
on herassessment,theDMthenannounceswhetherornotthestateisgood.Wedenotethischoice
by a ∈ {y,n} (yes/no),where a=y when theDMchoosesthegoodstateand a=n otherwise. The
choiceisaccurateifshechooses a = y and thetruestateis ω = g, orif a = n and ω = b. TheDM
enjoysa(normalized)unitofpayoffifherdecisionisaccurate,andnothingotherwise.Thus,her
expectedpayoffistheprobabilitythatshewillmakeanaccuratechoice,whichwedefineasthe
accuracy ofherdecision.TheDM’sobjectiveisthentomaximizetheexpectedaccuracyofher
decision,2 net ofanycognitivecosts.
2 In otherwords,DM’spayoffsarethesamewhethershecorrectlyidentifiesthegoodstate(a=y when ω =g) orthe
bad one(a = n when ω = b). Thisisforthesakeofclarityonly,though.Ouranalysisdirectlyextendstoageneral
payoffstructure,aswediscussin §6.1.

8
3.1. TheHumanDecision-Maker
The DMisconstrainedbyherlimitedcognitivecapacity,sothatassessingavailableinformation
requires exertingcognitiveefforts,aprocessweformalizewithinthetheoryofrationalinattention.
In thisframework,theDMisawareofhercognitivelimitationsandendogenouslyoptimizeshowto
allocatehereffortaccordingly.Todothis,theDMelicitsinformativesignalsaboutthetruestate
of theworldfromdifferentsourcesofinformationwhichreduceherprioruncertainty.
Specifically,theDMcanelicitanysignal s of anyprecisionlevelaboutstate ω ∈ Ω={g,b} from
anyinformationsource.Wedefineaninformationprocessingstrategyasajointdistribution f (s,ω)
betweensignalsandstates.TheDMisfreetochooseanyinformationprocessingstrategyaslong
as itisBayesianconsistentwithherpriorbelief(i.e.,
R
s f (s, g) ds = μ musthold).Thisimplies
that choosingastrategy f (s,ω) is equivalenttodetermining f(ω|s), theDM’sposteriorbeliefthat
the truestateis w givensignal s. Inotherwords,theDMisfreetochoosetheprecisionofher
posteriorbelief.Thus,theDMmayelicitdifferentsignalsfromdifferentinformationsourcesinany
particular sequence,andmakehersearchfornewsignalscontingentonpreviousonestodetermine
the precisionofherposteriorbelief.3 She mayalsodecidenottoprocessanyinformationatallso
that f(g|s)=μ or equivalently f(s, g)=μf(s).
CognitiveEffort. The DM’sbeliefaboutthestateoftheworldspecifiestheprevalentinitial
uncertainty.Bygeneratinganinformativesignal s, theDMupdatesprior μ to posterior f(g|s).
Wemeasureuncertaintyintermsofentropy,denotedas H(p) foraprobability p that theworldis
in thegoodstate,where H(p) = −p log p−(1−p) log (1−p). Entropyisameasureofuncertainty
whichcorrespondstotheexpectedlossfromnotknowingthestate(FrankelandKamenica2019).
In oursetup, H(μ) measuresthepriorlevelofuncertaintythattheDMneedstoresolve,and
thusfullycapturesthedifficultylevelofthedecisiontask.Thetaskpresentsnodifficultywhen
the DMisfullyinformedaboutthestate,thatis,when μ = 1or μ = 0forwhich H(μ) isnull.
The decisiontaskismostdifficultwhentheDMhasnopriorinformationaboutthestates,thatis,
when μ=1/2 whichmaximizes H(·). Wethusreferto H(μ) as the taskdifficulty in thefollowing.
Similarly,ex-postentropy H(f(g|s)) measuresthelevelofuncertaintyuponelicitingsignal s and
thus Es[H(f(g|s))] istheexpectedlevelofremaininguncertaintyunderstrategy f, beforetheDM
processesanyinformation.Wereferto Es[H(f(g|s))] as the residualuncertainty in thefollowing.
The expectedreductioninuncertaintyisthenequalto H(μ)−Es[H(f(g|s))], whichcorresponds
to themutualinformationbetweenpriorandposteriordistributionsininformationtheoryand
3 Eliciting informativesignalscanalsobeimaginedastheDMaskingaseriesofyes-or-noquestionsandobserving
the outcomes.Bychoosinganinformationprocessingstrategy,theDMiseffectivelychoosingwhatquestionstoask
and inwhichsequence.

9
specifiestheexpectedamountofelicitedinformation.Thisquantityisalwayspositive,thatis,
information alwaysdecreasesuncertainty,duetotheconcavityofentropy H(·).
Reducing uncertainty,however,comesatacognitivecost.Thelargerthereductioninuncertainty,
the moreinformationisprocessedandthusthemorecognitiveeffortisrequired.Followingthe
rational inattentionliterature,weassumethattheDM’scognitivecostislinearintheexpected
reduction inuncertainty.Formally,thecognitivecostassociatedwithaninformationprocessing
strategy f is equalto
C (f)=λ(H (μ)−Es [H (f(g|s))]) (1)
where λ>0 isthemarginalcognitivecostofinformationwhichwerefertoas the informationcost.
Overall,informationcost λ determines howconstrainedtheDMisintermsoftime,attention,
and cognitiveability.Itmayrepresenttheinherentdifficultyofassessingapieceofinformationor
the extenttowhichtheDM’scognitivecapacityisconsumedbycompetitivetasks,becauseoftime
pressure ormultitasking.Inthelattercase, λ is theshadowpriceoftheconstraintcorresponding
to thelimitedcognitivecapacity.Thus,thehigherthevalueof λ, themoreefforttheDMneeds
to exerttoelicitsignalsthatreduceuncertainty.Inthelimitwhere λ is infinite,theDMcannot
assess anyinformationandonlydecidesbasedonherpriorbelief μ. Incontrast,theDMdoesnot
haveanylimitonhercapacitywhen λ=0,andcanperfectlyassessthetruestateoftheworld.
The linearityofthecognitivecostintheexpectedreductioninentropyisastandardassumption
in therationalinattentionliterature,whichisjustifiedbythefundamentalcodingtheoremof
information theory(see,e.g.,MatˇejkaandMcKay2015formoredetails).Importantly,however,
the cognitivecostisconvexintheprecisionofthegeneratedsignals(i.e. C is convexin f(s|ω)).
That is,elicitingadditionalmoreinformativesignalsbecomesincreasinglycostly.
Decisions andAccuracy. The DMchoosesinformationprocessingstrategy f, atcost C(f),
to yieldupdatedbelief f(g|s). Giventhisupdatedbelief,theDMthenchoosesheraction a ∈ {y,n}
to maximizeaccuracy,suchthat a = y if f(g|s) > f(b|s) and a = n otherwise (recallthatinour
setup, theexpectedpayoffisequaltotheexpectedaccuracy).Thus,thepriorprobabilitythat
the DMwillchooseaction a = y beforeshestartsassessinganyinformation4 is equalto p(f) ≡
R
s I{f(g|s)≥f(b|s)}f (s) ds, where I denotes theindicatorfunction,whichyieldsexpectedaccuracy
A(f)≡
R
s maxa∈{y,n} {f(g|s)Ia=y +f(b|s)Ia=n} f (s) ds=
R
s max{f(g|s), f(b|s)} f (s) ds.
4 Note thattheDMcommitstoadecisionwithcertaintyex-post,i.e.,aftersheassessestheavailableinformation.But
becausethesignalsshewillobtainareunknownbeforeshestartstheprocess,herfinaldecisionisrandomex-ante.

10
The DecisionProblem. Anticipatingherexpectedposteriorpayoffuponreceivingsignals,the
DM firstdecidesonherinformationacquisitionstrategy,takingintoaccountthecognitivecost
associatedwithitsimplementation.TheDMthenchoosesheraction.Itfollowsthatgivenherchoice
of informationprocessingstrategy f, theDMenjoysanexpectedtotalvalueof V (f)≡A(f)−C(f).
She determinesherinformationprocessingstrategybysolvingthefollowingoptimizationproblem:
max
f
V (f) s.t.
Z
s
f (s, g) ds=μ (2)
where theconstraintguaranteesthattheDM’sinformationprocessingstrategyisBayesianconsis-
tentwithherpriorbelief.Givenprior μ, wedenoteby V ⋆(μ), theoptimalexpectedvaluesuchthat
V ⋆(μ) = V (f⋆), where f⋆ solves(2).Similarly,wedefineby A⋆(μ), C⋆(μ) and p⋆(μ) theoptimal
accuracy,cognitivecost,andchoiceprobability,respectively,givenprior μ.
Takentogether,oursetupcapturesboththecognitiveflexibilityandcognitivelimitationsof
humans.Inthisframework,theDMendogenouslydecideshowtoallocateherlimitedattention
and howmuchefforttoputintoresolvingtheprevalentuncertainty.Indoingso,theDMchooses
howmucherrorshewilltolerateandtheprecisionofherdecisions.Thisframeworkfurtherallows
us toaccountformachine-basedpredictionsintheDM’sdecisionprocess,asweshownext.
3.2. AccountingfortheMachine
Toassessthestateoftheworld,theDMleverageshercognitiveflexibility(Diamond2013,Laureiro-
Mart´ınezandBrusoni2018)tointegrateinformationfromdiversesources.Themachine,bycon-
trast, onlyextractsalimitedsubsetofthisinformation(Marcus2018).Thus,wepartitiontheset
of informationsourcesfromwhichsignals s are drawnintotwodistinctsubsets:afirstonethat
boththemachineandtheDMcanevaluate,andasecondonewhichisonlyavailabletotheDM.
Werepresenttheaggregateinformationcontainedinthesetwosubsetsasrandomvariables X1
and X2, respectively.Inparticular,r.v. X2 summarizes thepredictivevariablesthatareunobserv-
able totheMLalgorithm.ThesemayincludeinformationdrawnfromtheDM’sdomainknowledge
or specificaspectsofthecontextinwhichthedecisionismade.Toputthissetupintoperspective,
consider themedicaldomain.Randomvariable X1 maythenrepresentthestatisticalsummaryof
all thetangibleinformationthatisobservabletothealgorithm,suchasthepatient’sfullmedical
history.Randomvariable X2, ontheotherhand,mayrepresenttheinformationthatthephysician
obtains throughpersonalinteractionwiththepatient.IncontrasttotheMLalgorithm,theDM
can elicitsignalsfrombothsources.RecallthatwedonotimposeanyrestrictionontheDM’s
strategy,particularlytheorderinwhichshemayassessthesesources.
Realization xi ∈ {−,+} of Xi, i=1, 2, issuchthat xi =+(resp. −) isindicativeofagood(resp.
bad) state.Thetruestateoftheworldisgoodonlyifallavailableinformationispositive,5 i.e.,
5 When onepositiveinformationsufficestodeterminethegoodstate,theproblemcanbemadeequivalenttothe
currentsituationbyrelabelingthegoodstateandthepositiveinformationasthebadandnegativeones,respectively.

11
ω =g if andonlyif x1 =x2 =+.Wereferto π(x1,x2)>0 with(x1,x2) ∈ {−,+}2 as theDM’sprior
distribution of(X1,X2). Thus,theDM’spriorbeliefinthegoodstateis μ = π (+,+) . Without
machine,theDMneedstoallocatehercognitiveeffortbetweentheassessmentsof x1 and x2.
In contrasttothehuman,themachinedoesnotsufferfromanycognitivelimitationsduetoits
virtually unboundedcomputingcapacity.Weassumethatitcanextracttheexactvalueof x1 at no
cognitivecost,sothattheDMcandedicatehereffortsolelytotheassessmentof x2. Inthepresence
of themachine,therefore,theDMonlyassesses x2 so astoupdatehernewbelief,whichaccounts
for themachine’sevaluation x1. Specifically,define μx as theDM’snewbeliefthatthestateis
good,giventhemachine’sevaluation x ∈ {−,+}. Wehave,usingBayes’rulewith μ=π (+,+),
μ− =0and μ+ =
μ
μ + π (+,−)
> μ. (3)
That is,anegativeevaluationbythemachinerevealsthatthetruestateisbad,whiletheDM’s
beliefthatthestateisgoodincreaseswithapositiveevaluation.ItfollowsfromSection3.1that
when themachineoutputis x, theoptimalexpectedvalue,accuracy,cognitivecost,andchoice
probability,areequalto V ⋆(μx), A⋆(μx), C⋆(μx) and p⋆(μx), respectively.
Weconsideraperfectlyaccuratemachineforclarityonly.AswediscussattheendofSection6.2
and inAppendixD.1,ourapproachcanbeextendedtoaccountforinaccuratemachinepredictions.
(All proofsandappendicescanbefoundinthepaper’selectroniccompanion.)
4. OptimalDecisions,AccuracyandCognitiveCost
In thissection,wecharacterizeoptimalchoice p⋆(·) asafunctionofpriorbelief μ ∈ (0, 1), fromwhich
wededucetheoptimalexpectedvalue,accuracy,andcognitivecost(V ⋆, A⋆, and C⋆, respectively).
Tothatend,wefollowMatˇejkaandMcKay(2015)whoestablishthatproblemsofthetype(2)
where theDMchoosesstrategy f, areequivalenttoproblemsinwhichshedirectlyselectsthe
conditional probabilitiesofchoosingaction a givenstate w.6 The intuitionforthisequivalenceis
that aone-to-onecorrespondenceexistsbetweenactions a and signals s in theoptimalsolution.
Indeed, elicitingdistinctsignalsthatleadtothesameposteriorbelief(andhencedecision)incur
additional costswithoutchangingtheDM’sdecision,whichissuboptimal.Inadiscretechoice
setting, thisyieldsanoptimalsolutionofGMNL(generalizedmultinomiallogit)formwherepayoffs
include endogenouslydeterminedterms.ThenextLemmaformalizesthisresultinoursetup.
Lemma 1. Given prior 0 < μ< 1, theoptimalchoiceprobability p∗ (μ) is theuniquesolutionto
the followingequationsin p ∈ [0, 1],
p=(1−μ) pb +μpg, where pg =
pe1/λ
pe1/λ +1−p
, pb =
p
p+(1−p) e1/λ . (4)
6 Note thatthisisan“asif”resultsuchthattheDMisnotactuallyoptimizingoverchoiceprobabilitiesbutusingan
optimal informationprocessingstrategythatisbehaviorallyequivalenttotheinducedoptimalchoiceprobabilities.

12
Further,wehave
A⋆(μ)=(1−μ) (1−pb)+μpg and C⋆(μ)=λ[H (p)−(1−μ)H (pb)−μH (pg)] (5)
Probabilities pg and pb correspondtotheoptimalconditionalprobabilitiesthattheDMchooses y
giventhatthetruestateis g and b, respectively.Probability p is thenthe(unconditional)probability
of choosing y according toconsistencyequation(4).Probabilities pg and pb also determinethe
extentofthemistakestheDMtolerates.Specifically,theoptimalfalsepositiveandfalsenegative
rates, whichwedenoteas α⋆ and β⋆, respectivelysuchthat α⋆ +β⋆ =1−A⋆, areequalto
α⋆ =(1−μ)pb and β⋆ =μ(1−pg). (6)
4.1. OptimalDecisions
Lemma 1statesthattheoptimalchoiceprobability p⋆(μ) correspondingtoproblem(2)isthe
solution ofasystemofequations,whichalsodeterminesdecisionaccuracy A⋆(μ), cognitivecost
C⋆(μ), andhenceexpectedvalueobtained V ⋆(μ) = A⋆(μ) − C⋆(μ). Thenextresultprovidesthe
explicit solutiontotheseequations.
Theorem 1. The optimalchoiceprobability p⋆(μ) that solves (4) , is equalto
p⋆ (μ)=


0 if μ ≤μ
μ
1−e−1/λ − 1−μ
e1/λ−1
if μ<μ<μ
1 if μ ≥μ
(7)
where μ = (e1/λ +1)−1 < 1/2 < μ = e1/λ(e1/λ +1)−1. Furthermore, p⋆ (μ) is non-decreasingin μ, μ
is increasingin λ and ¯μ is decreasingin λ.
Overall,Theorem1characterizestheeffectoftheDM’spriorbelief μ on heroptimalchoice
probability p⋆(μ). IftheDM’spriorbeliefaboutthetruestateoftheworldissufficientlystrong
(i.e., μ ≥μ or μ ≤μ), exertinganyefforttolearnmoreaboutthisstateisnotworththecognitive
cost. TheDMthenmakesanimmediatedecisionwithoutassessinganyinformation,basedsolelyon
her prior(i.e., p⋆ (μ)=1or0).Otherwise,theDMexertsefforttoassesstheavailableinformation
untilherbeliefaboutthetruestateofthewordissufficientlystrong,atwhichpointshecommits
to achoice.But,becauseshedoesnotknowwhatthisassessmentwillrevealapriori,herfinal
decision isuncertainex-ante(i.e.,0 < p⋆(μ) < 1). Furthermore,thestrongertheDMbelievesa
priori thattheworldisinthegoodstate,themorelikelyshewilldecideaccordinglybychoosing
a=y (i.e., p⋆ (μ) is non-decreasingin μ).
Theorem 1alsoenablescharacterizingtheimpactofinformationcost λ on theoptimalchoice
probability,whichwedenoteby p⋆(λ) inthenextresultwithaslightabuseofnotation.

13
Corollary1. Given prior 0 < μ< 1, apositive(possiblyinfinite)threshold ¯λ exists suchthat
the optimalchoiceprobabilityisequalto
p⋆ (λ)=


μ
1−e−1/λ − 1−μ
e1/λ−1
if λ<¯λ
0 if λ≥¯λ and μ<0.5
1 if λ≥¯λ and μ>0.5,
(8)
where ¯ λ(μ) =

log 1−μ
μ

−1
if μ ̸= 0.5 and ¯λ = +∞ if μ = 0.5. Further, p⋆ (λ) is decreasing(resp.
increasing)in λ, and ¯λ increasing(resp.decreasing)in μ when μ<0.5 (resp. μ>0.5).
0 0.20.40.60.81
Priorbelief μ
0
2
4
6
8
Informationcost 6
6
ChoosenChoosey
Acquire
information
Figure 1Effectofpriorbelief μ on theDM’stolerancetoinformationcost ¯λ
Hence, theDMexertseffortonlyiftheinformationcostisnottoohigh,i.e.,lessthanathreshold.
In thiscase,herprobabilityofchoosingthegoodstateincreaseswith λ if shefavorsthisstateapriori
(μ > 1/2), anddecreasesotherwise.Indeed,thehighertheinformationcost,thelessinformation
the DMassessesandthusthelesslikelyherupdatedbeliefwillsignificantlychangefromher
prior. Otherwise,shedecidesapriorithatthestateisgood(resp.bad)ifherpriorislarger(resp.
smaller) than1/2.Inthiscase,theDMjumpstoconclusionsasshereliessolelyonherpriorbelief
without assessinganyinformation.Inthissense,threshold ¯λ determines theDM’stolerancetothe
information cost.Takentogether,Corollary1statesthatthesetofpriorbeliefsforwhichtheDM
processesinformationisanintervalcenteredat1/2,thatshrinkswithinformationcost λ.
Figure 1depictstheimpactofprior μ on threshold ¯λ. WhentheDMdoesnothavemuchprior
knowledgeaboutthetruestateoftheworld(thevalueof μ is closeto1/2),sheisreadytoexert
a lotofcognitiveefforttolearnmoreandhencetoleratehighinformationcosts(thevalueof ¯λ is
high). Inparticular,theDMalwaysassessesinformationandexertseffortwhenthetruestateis
perfectlyunknown(¯λ =+∞ for μ=1/2). AstheDMismorecertainaprioriaboutthetruestate
(μ approaches0or1),sheislesswillingtoexerteffortandjumpstoconclusionsforlowervalues
of informationcosts(¯λ decreases as μ approaches0or1).

14
4.2. DecisionAccuracyandCognitiveEffort
FromLemma1andTheorem1,weobtainthefollowingclosedformsfor A⋆(μ), C⋆(μ) and V ⋆(μ).
Corollary2. Given prior μ, wehave
 If μ ≤μ, then A⋆(μ)=1−μ and C⋆(μ)=0.
 If μ<μ<μ, then A⋆(μ)= e
1λ
e
1λ
+1
and C⋆(μ)=λ[H (μ)−φ(λ)].
 If μ ≥μ, then A⋆(μ)=μ and C⋆(μ)=0, where
φ(λ)≡log

e
1λ
+1

−
1
λ
e
1λ
e
1λ
+1
. (9)
Further, φ(·) is increasing,with φ(0)=0 and limλ→∞ φ(λ)=log2. Also, V ⋆(μ)=A⋆(μ)−C⋆(μ).
Function φ(λ) is theresidualuncertainty Es[H(f(g|s))] (seeSection3.1)atoptimality.The
higher theinformationcost,thelessprecisetheelicitedsignalsare,andthusthelessuncertainty
is reduced.PerCorollary2,residualuncertainty φ(λ) is fullydeterminedbytheinformationcost
and independentoftheprior.Infact,aslongastheDMchoosestoprocessinformation(i.e.,
μ<μ<μ), herdecision’sexpectedaccuracydependssolelyontheinformationcostandnotonher
prior belief.Figure2aillustratesthisforafixed λ. Here,thereddottedcurvegivenbymax(μ, 1−μ)
correspondstothedecisionaccuracyleveltheDMobtainswhenshebasesherdecisionsolelyonher
prior belief(i.e., λ→∞). Thesolidbluecurveistheaccuracyfunction A(μ) forafiniteinformation
cost value,whichisconstantwhentheDMchoosestoprocessinformation.Thedifferencebetween
these twocurvespreciselycorrespondstothegaininaccuracytheDMenjoysduetocognitive
effort. Whenthedecisiontaskismostdifficult(i.e.,whentheDMismostuncertainwith μ=0.5),
the DMobtainsthehighestaccuracygain,whilethemagnitudeofthisgaindependson λ.
In contrast,theDM’sprioraffectsexpectedvalue V ⋆ through taskdifficulty H(μ), ifshechooses
to exerteffort.Specifically,thetaskdifficultyincreasesthereductioninuncertainty H(μ)−φ(λ)
that theDM’seffortbringsabout.Thus,Corollary2impliesthattheexpecteduncertaintyreduc-
tion andhencetheoptimalexpectedcostincrease,whiletheexpectedvaluedecreaseswiththe
task difficulty(i.e.,as μ approaches1/2)whichisillustratedinFigure2b.SimilartoFigure2a,
the dottedcurvecorrespondstotheexpectedvaluetheDMobtainswhenthereisnocognitive
effort, inwhichcaseitisequaltotheexpectedaccuracy.Thedifferencebetweenthesetwocurves
correspondsthentotheexpectedgainthattheDMenjoyforexertingcognitiveeffort.
The structureofoptimalcost C⋆ in Corollary2shedsfurtherlightonthresholds μ and ¯μ.
Indeed, thesethresholdsdeterminewhenthetaskdifficultyisexactlyequaltotheoptimalreduced
uncertainty,thatis, H(μ) = H(¯μ) = φ(λ). If μ <μ or μ > ¯μ, theleveloftaskdifficultyisalready
lowerthanthereduceduncertaintythatanycognitiveeffortwouldachieveinoptimality,thatis,
H(μ)<φ(λ), andtheDMpreferstodecideapriori,withoutassessinganyinformation.

15
0 0.20.40.60.81
Priorbelief μ
0
0.2
0.4
0.6
0.8
1
Decisionaccuracy
accuracyfor 6 = 0.5
accuracyfor 6 !1
accuracy gain
due to effort
(a) Decisionaccuracy
0 0.20.40.60.81
Priorbelief μ
0
0.2
0.4
0.6
0.8
1
Expectedvalue
valuefor 6 = 0.5
valuefor 6 !1
value gain
due to effort
(b) Expectedvalue
Figure 2TheDM’saccuracyandvaluefunctions,andcorrespondinggainsduetocognitiveeffort.
That theoptimalaccuracyisindependentofthepriorstemsfromawell-knownpropertyof
rationally inattentivechoiceandthefactthattheDMmaximizesaccuracy(netofcognitivecosts).
Indeed, whensomeinformationisprocessedatoptimality,rationallyinattentiveagentsalwaysform
the sameposteriorbeliefregardlessoftheirprior(seeCaplinandDean2013).Theseoptimalpos-
teriors correspondexactlytothebeliefthresholdsthatdefinewhetheritiseconomicallyattractive
for theDMtoprocessinformation(μ and μ), whichdependonlyonthepayoffsandinformation
cost λ. Intuitively,thismeansthattheDMsharpensherbeliefbyprocessingcostlyinformation,
up untilthepointbeyondwhichitisnolongerjustified.Morespecifically,inourcontext,theDM’s
optimal posteriorbeliefthatthestateisgoodgiventheaggregatesignalsthatleadtotheaction
a = y (resp. a = n) isprecisely μ (resp. μ) whensheprocessesinformation.Additionally,sincethe
payoffstructureissymmetricinthestates,thesethresholds(hence,theoptimalposteriors)are
also symmetric.Thatis,theDM’sposteriorbeliefthatthestateisgoodgivenaction a=y (i.e., μ)
is equaltoherposteriorbeliefthatstateisbadgiven a=n (i.e., 1−μ). Inoursetup,thesearealso
equal totheaccuracy,asitisjusttheexpectationoftheseoverthechoice(action)probabilities.
4.3. DecisionErrors
Being constrainedoncognitivecapacity,theDMisboundtomakechoicesbasedonpartialinfor-
mation. Indeed,eliminatingalluncertaintyisneveroptimal(φ(λ)>0 for λ>0). Hence,accuracy
is strictlylessthanoneandtheDMmakesfalsepositiveandnegativeerrors,withrates α⋆ and β⋆,
respectively.FromTheorem1,weobtaintheseerrorratesinclosedforminthefollowingcorollary.

16
Corollary3. Given prior μ, errorrates α⋆(μ) and β⋆(μ) areequalto
α⋆ (μ)=


0 if μ ≤μ
1−μ if μ ≥μ
μ(e1/λ+1)−1
e2/λ−1
otherwise
and β⋆ (μ)=


μ if μ ≤μ
0 if μ ≥μ
e1/λ−μ(e1/λ+1)
e2/λ−1
otherwise.
If theDMisconfidentenoughthatthestateisbad(μ ≤ μ), shechooses a = n without any
cognitiveeffort,preventingherfrommakingafalsepositiveerror(α⋆ = 0)butmaximizingher
chanceofmakingafalsenegativeone(β⋆ = μ). Thereverseistrue(a = y, α⋆ = 1−μ and β⋆ = 0)
when theDMissufficientlyconfidentthatthestateisgood(μ ≥ ¯μ). Otherwise,theDMprocesses
some informationandtheerrorratesdependonboththepriorandtheinformationcost(with
0<α⋆ <1−μ and 0<β⋆ <μ).
Both α⋆ and β⋆ are piecewiselinearandunimodalfunctionsof μ. Inparticular,whentheDM
exerts effort(μ<μ< ¯μ), thefalsepositiverateincreases,whilethefalsenegativeonedecreasesas
the priorincreases.Infact,anincreaseinprior μ has twoconflictingeffectsonthefalsepositive
rate. Ononehand,thegoodstateismorelikely,whichdecreasesthechanceoffalsepositiveerrors.
On theotherhand,theDMismorelikelytochooseaction a=y for ahigherlevelof μ perTheorem
1, whichincreasesthechanceoffalsepositiveerrors.Inessence,Corollary3indicatesthatthe
second effectalwaysdominatesthefirstone.Asimilarresultholdsforthefalsenegativerate.
5. ImpactofMachineInputonHumanDecisions
Thusfar,wehaveconsideredarationallyinattentiveDMthatdecidesalone.Wenowinvestigate
howtheDM’sdecisionprocessanditsoutcomeschangewhensheisassistedbyamachine-based
assessment.Inparticular,wecomparetheDM’sdecisions,theextentoferrorsshemakes,andthe
amountofeffortsheexpendswithandwithoutthemachine.
5.1. Machine-AssistedDecision-Making
With themachine,theDMfirstobservesthemachine’soutput x1, whichdetermineshernewbelief
μx, x ∈ {+,−}, accordingto(3).TheDMthendedicatesallhercognitivecapacitytoevaluating
x2. Wedenoteby p⋆
m(μ) theresultingex-anteprobabilitythattheDMchooses a=y as afunction
of herinitialpriorbelief μ. Similarly, A⋆
m(μ), C⋆m(μ), V ⋆
m(μ), α⋆m (μ) and β⋆m (μ) denotedecision
accuracy,cognitivecost,expectedvalue,anderrorrates,respectively,thattheDMachievesinthe
presence ofthemachine.Thefollowing(immediate)lemmacharacterizesthesedifferentmetrics.
Lemma 2. Given prior μ, wehave
p⋆
m(μ)=
μ
μ+ p⋆ 􀀀
μ+
, α⋆
m(μ)=
μ
μ+α⋆ 􀀀
μ+
β⋆
m(μ)=
μ
μ+ β⋆ 􀀀
μ+
A⋆
m(μ)=1−
μ
μ+ +
μ
μ+A⋆ 􀀀
μ+
, C⋆
m(μ)=
μ
μ+C⋆ 􀀀
μ+
, V ⋆
m =1−
μ
μ+ +
μ
μ+ V ⋆ 􀀀
μ+
.

17
Thus,giveninformationcost λ, thedecision’soutcomesinthepresenceofthemachinecanbe
describedwithtwofreeparameters(μ,μ+) ∈ S≡{(x, y) ∈ [0, 1]2, s.t. x <y}; prior μ, andupdated
prior μ+ when themachinegivesapositivesignalon X1.
5.2. ImpactonDecisionAccuracyandValue
Since themachineprovidesaccurateinformationatnocognitivecost,themachinealwaysimproves
the expectedaccuracyandtotalvalueoftheDM,asstatedbythefollowingresult.
Proposition1. Foranygiven λ>0 and (μ,μ+) ∈ S, wehave A⋆
m ≥A⋆ and V ⋆
m ≥V ⋆.
Figure 2illustratesProposition1.Theaccuracylevelsthatcanbeachievedwithamachinefor
all combinationsof(μ,μ+) ∈ S correspondtotheconvexhulloftheaccuracycurveinFigure2a
(solid bluecurve)withoutthemachine.Allthesepointslieabovethecurveandhenceprovide
greater accuracy.Similarly,theconvexhullofthevaluecurveinFigure2bdepictsthesetofall
possibleexpectedvaluesthattheDMcanachievewithamachine,showingthatitalwaysincreases
the DM’sexpectedvalue.
This resultprovidestheoreticalsupportforthegrowingempiricalliteratureshowingthathuman-
machinecollaborationsboostoverallaccuracy.Interestingly,Proposition1ispartlydrivenbyour
premise thathumancognitionisflexible.Thisfeaturecorrespondsinoursetuptotheunrestricted
feasible setofinformationprocessingstrategies(otherthantheBayesianconsistencyrequirement).
Indeed, whenapriorirestrictionsareimposedonthisfeasibleset,andhencehumancognitionis
less flexible,accuracysometimesdecreasewiththemachine(seeAppendixF).
5.3. ImpactonDecisions
The machineimprovestheexpectedaccuracyandtotalvalueofthedecisionbyinfluencingthe
DM’s choice.Thenextresultdetermineshowthepresenceofthemachineaffectsthischoiceasa
function ofprior μ and posteriorbelief μ+.
Theorem 2. Given informationcost λ, wehave
i) If μ+ ≤μ, then p⋆
m =p⋆ =0.
ii) If μ ≤μ and μ+ ∈
􀀀
μ,μ

, then p⋆
m >p⋆ =0.
iii) If μ ≤μ and μ+ ≥μ, then p⋆
m >p⋆ =0.
iv) If μ<μ<μ+ <μ, then p⋆
m >p⋆.
v) If μ ∈
􀀀
μ,μ

and μ+ ≥μ, then ˆμc exists suchthat p⋆
m >p⋆ if μ< ˆμc and p⋆
m ≤p⋆ otherwise.
vi) If μ ≥μ, then 1=p⋆ >p⋆
m.
Further,threshold ˆμc is decreasingin μ+ and equalto ˆμc =

e1/λ +1− e1/λ−1
μ+
−1
≥ 1/2.

18
0 μ 0.5 μ 1
Priorbelief μ
0
μ
0.5
μ
1
Posteriorbelief μ+
ˆμc
p⋆
m
6 p⋆
p⋆
m
5 p⋆
0.5
i
ii
iii
iv
v vi
Figure 3ImpactofthemachineontheDM’sdecisioninparameterspaceS,for λ= 1.
Overall,Theorem2identifiesnecessaryandsufficientconditionsunderwhichthepresenceof
the machine decreases the DM’sprobabilityofchoosing a=y. ThishappenswhentheDM’sprior
beliefisstrongenough(ˆμc < μ), andapositiveassessmentbythemachinebooststhisbelieftoa
sufficientlyhighlevel(μ+ ≥ ¯μ). Thresholdˆμc is thenthevalueofprior μ, atwhichthedirectionof
the machine’simpactchanges.
Figure 3illustratesthisresultinparameterspace S, foragiven λ. Thepartitionofparameter
space S in sixdifferentsubsetscorrespondstocases i-vi in thetheorem.Cases i, ii and iii depict
situations inwhichtheDMdoesnotexertanyeffortintheabsenceofthemachineandchooses
a = n as aresult.Thishappenswhenherpriorissufficientlylow(i.e., μ ≤ μ) perTheorem1.
Similarly,case vi correspondstosituationsinwhichtheDMchooses a = y a prioribecauseher
prior issufficientlyhigh(i.e., μ ≥ μ). Incases iv and v, however,theDMalwaysexertseffortto
assess informationintheabsenceofthemachine.Thefiguredemonstratesthatthresholdˆμc divides
space S intotwo(top-rightandbottom-left)areas,suchthatthepresenceofthemachinedecreases
the DM’sprobabilityofchoosingthegoodstate(i.e., p⋆
m ≤ p⋆), when(μ,μ+) liesinthetop-right
area, andincreasesthechoiceprobabilityotherwise.
This resultstemsfromthefactthatthemachinesometimesdispensestheDMfromexertingany
effort aswellastheimpactoftheinformationcostontheDM’schoice.Toseewhy,considerthe
effect ofthemachineontheDM’schoiceprobabilityasafunctionoftheinformationcost,which
wecharacterizenext.
Corollary4. We havethefollowing:

19
 If μ ≤0.5, then p⋆
m ≥p⋆.
 If μ>0.5, then threshold λ∗ exists suchthat p⋆
m ≥p⋆ if λ<λ∗ and p⋆
m ≤p⋆ otherwise.
Further,threshold λ∗ is decreasinginpriorbelief μ with, λ∗ =log

μ+μ+μ− μ+
μ(1 −μ+)
−1
.
In otherwords,whentheDMbelievesapriorithatthegoodstateismorelikely(μ > 1/2),
the presenceofthemachinereducesherprobabilityofchoosing a = y if theinformationcostis
sufficientlyhigh(λ >λ∗) andincreasesthisprobabilityotherwise.Figure4illustratestheresult
and depictsthreshold λ∗ as afunctionofprior μ.
0 0.20.40.60.8
Priorbelief μ
0
1
2
3
4
5
6
Informationcost 6
6$
p⋆
m
6 p⋆
p⋆
m
5 p⋆
Figure 4ImpactofthemachineontheDM’sdecisionasafunctionofinformationcost λ and prior μ, for
μ+ = 0.8
Without themachine,probability p⋆ is increasingintheinformationcostwhentheDMfavors
the goodstateapriori,thatis, μ>1/2 (perCorollary1).Thisisbecausethehighertheinformation
cost, thelessinformationtheDMassessesandthusthelesslikelyitisthatshewilldeviatefrom
her priorchoice.Withthemachine,apositiveassessmentbythemachinebooststheDM’sbelief,
further amplifyingthiseffect.Infact,wheninformationcost λ is greaterthanthreshold λ(μ+)
defined inCorollary1,apositivemachine’sassessmentpromptstheDMtoimmediatelychoose
a = y without exertinganyadditionaleffort(since0.5 < μ<μ+). Thus,theex-anteprobabilityof
choosingthegoodstate, p⋆
m, correspondsexactlytothechanceofapositiveresultbythemachine.
And sincethemachinedoesnotexertanycognitiveeffort,thisprobabilityisindependentofthe
information cost.Hence,probability p⋆ increases, whileprobability p⋆
m remains constantandthe
former dominatesthelaterwhentheinformationcostissufficientlylarge.7
7 By thesametokenwhen μ < 1/2, thechoiceprobabilityisnon-increasingintheinformationcostswhichexplains
whywehave p⋆ < p⋆
m in thiscase.

20
In otherwords,aDMwithoutmachinestickstoherex-antechoicewithhighprobabilityunder
high informationcost.Incontrast,aDMassistedbyamachineexclusivelyreliesonthemachine’s
result underhighinformationcost.IfthemachineisnotsufficientlylikelytoconfirmtheDM’s
prior, thepresenceofthemachinereducestheDM’schanceofchoosingthegoodstate.Itincreases
this probabilityotherwise.Ineffect,themachinemayincreasethevariabilityoftheDM’sdecision.
5.4. ImpactonDecisionErrors
FromProposition1,weknowthatthemachinealwaysimprovesaccuracyandhencereducesthe
overallprobabilityofmakingamistake.ButTheorem2indicatesthatthemachinechangesthe
ex-anteprobabilityofchoosinganaction.This,inturn,shouldaffectthenatureoferrorsthatthe
DM islikelytomake.Thenextresultcharacterizesthiseffect.
Theorem 3. Given informationcost λ, β⋆m ≤β for all μ ∈ [0, 1]. Further,wehave
i) If μ+ ≤μ, then α⋆m =α⋆ =0.
ii) If μ ≤μ and μ+ ∈ (μ,μ), then α⋆m >α⋆ =0.
iii) If μ ≤μ and μ+ ≥μ, then α⋆m >α⋆ =0.
iv) If μ<μ<μ+ <μ, and μ+ ∈ (μ,μ), then α⋆m >α⋆.
v) If μ ∈
􀀀
μ,μ

and μ+ ≥ μ, thenthreshold ˆμfp < ˆμc exists suchthat α⋆m > α⋆ if μ < ˆμfp, and
α⋆m ≤α⋆ otherwise.
vi) If μ ≥μ, then α⋆m <α⋆.
Further,threshold ˆμfp is decreasingin μ+ and equalto ˆμfp =

e2/λ +e1/λ − e2/λ−1
μ+
−1
.
0 μ 0.5 μ 1
Priorbelief μ
0
μ
0.5
μ
1
Posteriorbelief μ+
ˆμfp
ˆμc
,⋆
m
6 ,⋆
,⋆
m
5 ,⋆
0.5
Figure 5ImpactofthemachineonDM’sfalsepositiveerrorrateinparameterspaceS,for λ = 1.

21
Theorem 3statesthatthemachinealwaysimprovesthefalsenegativerateandthusdecreases
the DM’spropensityofchoosing a = n when thestateisactuallygood.Thishappensevenwhen
the machineinducestheDMtochoose a=n more apriori(i.e., p⋆
m ≤p⋆ when μ ≥μ+c perTheorem
2). However,themachinesometimesbooststhefalsepositiverateandthusincreasesthechance
that theDMwillchoose a=y while thestateisactuallybad.ThishappensiftheDM’spriorbelief
is nottoostrong(μ < ˆμfp). Themachinedecreasesthefalsepositiverateotherwise.Infactthis
mayhappenevenwhenthemachineraisesthepossibilityofmakingthismistakebyincreasingthe
overallprobabilityofchoosingthegoodstate(i.e.,whenˆμfp <μ< ˆμc perTheorem2).
Figure 5illustratesthisresultinparameterspace S, foragiven λ. Itdemonstratesthatthreshold
ˆμfp divides space S intotwo(top-rightandbottom-left)areas,suchthatthepresenceofthe
machinedecreasestheDM’sprobabilityofmakingafalsepositivetypeerror(i.e., α⋆m ≤α⋆), when
(μ,μ+) liesinthetop-rightarea,andincreasesotherwise.Theeffectofinformationcost λ on DM’s
error rates,however,ismoresubtleasthenextcorollaryshows.
Corollary5. Given prior μ, we α⋆m ≥α⋆ for μ+ ≤0.5. For μ+ >0.5, we have
 If μ ≤μ∗ =4μ+ 1−μ+
(2−μ+)2 , then α⋆m ≥α⋆.
 If μ∗ <μ<0.5, λfp and λfp exist s.t. α⋆m ≥α⋆ if λ<λfp and λ>λfp. Otherwise α⋆m ≤α⋆.
 If μ ≥0.5, α⋆m ≥α⋆ if λ<λfp. Otherwise α⋆m ≤α⋆.
Corollary 5establishesthatregardlessofthecostofinformation,iftheDM’spriorissufficiently
low,themachinealwaysincreasestheDM’spropensityofmakingafalsepositiveerrorwhichis
consistentwithTheorem3.ThisisbecausewhentheDMsufficientlyfavorsthebadstate,she
chooses a = n more often,whichgreatlyreducesherchanceofmakingafalsepositiveerror.In
fact, when μ <μ, shenevermakesafalsepositiveerror.Ontheotherhand,apositivemachine
assessmentmayrendertheDMmoreuncertain(when μ+ is closeto0.5) ormaygreatlyfavorthe
goodstate,promptinghertomakemorefalsepositiveerrors.
When theDM’spriorisnottoolow,theinformationcostplaysacentralroleindetermining
the machine’simpactontheDM’sdecisionerrors.Tounderstandthiseffect,firstconsiderthecase
where theDMinitiallyfavorsthegoodstate(i.e., μ>0.5). Whentheinformationcostissufficiently
low,itiseasierfortheDMtodistinguishthestatesandlesslikelythatshewillmakeanerror.Yet,
the machinecanincreasetheDM’schancesofmakingafalsepositiveerrorbyincreasingherprior
to asufficientlyhighlevelwhereshechooses a=y directly withoutacquiringfurtherinformation.
On theotherhand,whentheinformationcostishigh,theDMwithoutthemachineislikelyto
makeafalsepositiveerrorassheisinclinedtochoose a=y based onherpriorbelief(seeCorollary
1). Themachine,however,candecreasethischancebycompletelyrevealingthebadstates.

22
A moresubtleeffectoftheinformationcostemergeswhentheDMissufficientlyuncertain,but
favorsthebadstateinitially(μ is closebutstrictlylessthan0.5). Again,whentheinformation
cost issufficientlylow,shemakesfewerfalsepositiveerrorswithoutthemachineasshecanstill
distinguish thestates,andthemachinemayinducehertochoose a=y directly withoutacquiring
further information.However,contrarytothepreviouscase,shealsomakesfewerfalsepositive
errors withoutthemachinewhentheinformationcostissufficientlyhigh,assheisinclinedtochoose
a = n based onherpriorbelief.Thus,themachineonlyhelpstheDMtoreduceherfalsepositive
errors formoderateinformationcostlevels.Figure6illustratesthis.Thefigureplotsinformation
cost thresholds λfp and λfp as functionsofpriorbelief μ for thecasewhere μ+ > 0.5. Theprior
belief μ at whichthetwocurvesmeetpreciselycorrespondsto μ∗. Weprovidetheclosed-form
characterizationsofthetwoinformationcostthresholdsinAppendixA(proofofCorollary5).
0 0.2 μ$ 0.50.70.9
Priorbelief μ
0
2
4
6
8
10
Informationcost 6
6fp
6fp
,⋆
m
6 ,⋆ ,⋆
m
5 ,⋆
Figure 6ImpactofthemachineonDM’sfalsepositiveerrorininformationcost λ and prior μ, for μ+ = 0.9
Takentogether,theresultsofthissectionhaveimportantimplicationsfortheoperationsof
organizations. Inparticular,theincreaseinfalsepositivesthatthemachinemayinducetranslates
intoanunnecessaryincreaseincapacityutilizationinthedownstreamstagesofaprocess.This
means, forinstance,thatcongestionlevelsandwaitingtimesfollowingthedecisiontaskcanincrease
exponentially,asperbasicqueueingtheory.Thisisindeedthecaseinmanufacturingoperations
involvingAI-assistedqualityinspectionandfaultdetectionwhereincreasingfalsepositivesmay
lead tounnecessarydowntime,productivitylossesandincreasedcosts.Consequencesareeven
more severewhenthetaskconsistsindetectinglow-frequencyandhigh-riskeventssuchasmoney
laundering andfraudinbanking,whereevenslightincreasesinfalse-positiveratesmaydramatically
increase subsequentworkload(KaminskyandSchonert2017).

23
5.5. ImpactonCognitiveEffort
The machineimprovestheexpectedvalueofhumandecisions, V ⋆ =A⋆−C⋆, byincreasingaccuracy
A⋆ (Proposition1)duetoadecreaseindecisionerrors,butalsoachangeoferrortypes(Theorem
3). Anadditionalandperhapsmoreintuitivechannelbywhichthemachinemightimprovethis
expectedvalueiscognitivecost C⋆. Indeed,themachineprovidesinformationatnocostandmay
partially relievetheDMofhercognitiveeffort.This,inturn,shouldimprovethedecision’sexpected
value.Yet,thefollowingresult,oneofourmainfindings,showsthatthisisnotalwaysthecase.In
fact, themachinesometimesincreasestheDM’scognitivecostwith C⋆m >C⋆.
Theorem 4. Given informationcost λ we have,
i) If μ+ ≤μ, then C⋆m =C⋆ =0.
ii) If μ ≤μ and μ+ ∈
􀀀
μ,μ

, then C⋆m >C⋆ =0.
iii) If μ ≤μ and μ+ ≥μ, then C⋆m =C⋆ =0.
iv) If μ< μ<μ+ <μ, then ˆμe ≤1/2 exists suchthat C⋆m >C⋆ if μ< ˆμe and C⋆m ≤C⋆ otherwise.
v) If μ ∈
􀀀
μ,μ

and μ+ ≥μ, then 0=C⋆m <C⋆.
vi) If μ ≥μ, then C⋆m =C⋆ =0.
Furthermore,threshold ˆμe is decreasingin μ+ and theuniquevalueof μ, for μ < μ<μ+ < μ, that
satisfies
H(μ)−
μ
μ+H(μ+)=(1−
μ
μ+ )φ(λ) (10)
0 μ 0.5 μ 1
Priorbelief μ
0
μ
0.5
μ
1
Posteriorbelief μ+
ˆμe
C⋆
m > C⋆
C⋆
m < C⋆
Figure 7ImpactofthemachineonDM’scognitiveeffortinparameterspace S, for λ = 1
Theorem 4identifiesthenecessaryandsufficientconditionsunderwhichthemachineinduces
the DMtoexert more effort. ThishappenswhentheDMsufficientlyfavorsthebadstateapriori

24
(μ < ˆμe ≤ 1/2), whichisillustratedinFigure7.Inthiscase,thetaskdifficultyincreaseswitha
positivemachineoutputandtheDMneedstoexertmoreeffort.
More generally,themachineaffectstheDM’scognitivecostviathetaskdifficultyandtheresidual
uncertainty(H(μ) and φ(λ), respectively,with C⋆ =H(μ)−φ(λ)) butinoppositedirections.On
one hand,themachinealwaysprovidesadditionalinformation,whichthusalwaysreducesthetask
difficultyinexpectation(H (μ) > EX1H(μX1 )). Thistasksimplificationcontributestoreducing
cognitiveeffort.Notethattheeffectisexante.TheDMexpectsthemachinetoreducethedifficulty
beforeobtainingthemachineassessment.Expost,apositiveresultofthemachinecanincrease
the taskdifficulty(i.e., H(μ) < H(μ+)). Ontheotherhand,themachineispreciseandhence
alwaysdecreasestheresidualuncertainty.Inparticular,thestateisknownwhenthemachine’s
result isnegativeand,thus,themachinealwaysreducestheresidualuncertaintyinexpectation
(φ(λ)>P (X1 =1)φ(λ)). ThisgaininprecisioncontributestoincreasingtheDM’scognitiveeffort.
Hence, themachineinducestheDMtoexertmoreeffortwhentheprecisiongaindominatesthe
task simplificationthatthemachinebringsabout.Thishappenswhenthepriorissufficientlysmall
and theinformationcostislargeenough,asstatedbythefollowingcorollary.
Corollary6. If μ+ ≥ 0.5 and μ > 1 − μ+, then C⋆m ≤ C⋆. Otherwise,auniquethreshold λ∗
e
exists suchthat C⋆m > C⋆ if λ >λ∗
e and C⋆m ≤ C⋆ otherwise. Further,threshold λ∗
e is increasingin
μ and satisfies
H (μ)− μ
μ+H (μ+)
1− μ
μ+
=φ(λ∗
e) (11)
0 0.10.20.30.40.50.60.7
Priorbelief μ
0
0.5
1
1.5
Informationcost 6
6$
e
C⋆
m
6 C⋆
C⋆
m
5 C⋆
Figure 8ImpactofthemachineonDM’scognitiveeffortininformationcost λ and prior μ, for μ+ = 0.7
In otherwords,iftheDMsufficientlybelievesthatthestateisgood(μ > 1−μ+), themachine
alwaysdecreaseshercognitivecostsinexpectation.Otherwise,themachineincreasesthecognitive

25
cost whentheinformationcostissufficientlylarge(λ>λ∗
e). Thismeans,perhapssurprisingly,that
a machineinducesmorecognitiveeffortswhentheDMisnotsureaboutthegoodstateandis
already experiencingahighlevelofcognitiveload(i.e.,forahigh λ), butreducestheseefforts
when sheisrelativelysureaboutthegoodstate or has alreadyamplecognitivecapacity(i.e.,for
a low λ). Figure8illustratesthis.Thefiguredepicts λ∗
e as afunctionofpriorbelief μ for thecase
where μ+ = 0.7. Notethat λ∗
e is definedonlyforbeliefvaluesthatarelessthan1−μ+ = 0.3 and
determines whetherthemachineincreasestheDM’scognitiveeffortornot.
6. Extensions
Weextendourbaselinemodelinvariousdirectionstogleanfurtherinsightsregardingtheimpact
of machineonhumanbehavior.Inparticular,wefirstgeneralizethepayoffstructureandassume
that DMaimstomaximizeherexpectedpayoff(netofcognitiveeffort)insteadofaccuracy.This
allowsustoincorporateasymmetriccostsforfalsenegativeandpositiveerrors.Wethenexplore
settings wheretheDMmistrustsandisbiasedagainstthemachine.Lastly,westudythecasewhere
the machinereducestheDM’suncertaintyinasymmetricmanner(i.e.,notalwaysrevealingthe
bad stateuponnegativeassessment),byconsideringthreepossiblestatesoftheworldinsteadof
two.MoredetailsandallformalresultscanbefoundintheAppendix.
6.1. GeneralizedPayoffs
Our basemodelassumesthattheDM’spayoffcorrespondstotheoverallaccuracyofherdecisions.
Accuracy isindeedthemainperformancemetricofinterestintheempiricalliteratureonmachine-
assisted decisions.However,ourframeworkcanalsoaccountforageneralpayoffstructureofthe
form u(a,ω), for(a,ω) ∈ {y,n} ×{g,b}. Thisgeneralpayoffstructuremaypossiblycreatean
asymmetry intheDM’sincentivesthatourpreviousanalysisdoesnotcapture.Specifically,aDM
who caresonlyaboutaccuracydoesnotpreferonestateovertheother.Bycontrast,anasymmetric
payoffstructuremayinducetheDMtoallocatemoreefforttowardaspecificstateattheexpense
of theother.Thishasimplicationsforherchoicesanddecisionerrors.Forinstance,ifidentifying
the badstateismoreimportant(asisperhapsthecaseinamedicalsettingwhereitcorrespondsto
a sickpatient),theDMmaytoleratefalsenegativesmoreandchoose a=n more often.Weassume
w.l.g that u(y,g) = 1and u(n, g) = 0(seeAppendixB),suchthatdifference δ = u(n, b) − u(y,b)
representsthenetvalueofcorrectlyidentifyingthebadstate.
Wefindthatthethresholdstructureofourresultscontinuestoholdinthismoregeneralset-up
(see AppendixB).Further,thesetofbeliefs μ and μ+ for which p⋆
m ≥ p⋆ widens as δ increases.
Similar resultsholdforthefalsepositiveerrorrateandexpectedcognitiveeffort.Thesetofprior
valuesforwhichthemachineinducesfewerfalsepositives(α⋆m ≤ α⋆) andreducescognitiveeffort
(C⋆m≤ C⋆) shrinksas δ increases. Andasinourbasecase,themachineconsistentlyreducesthe
false negativerateregardlessoftheincentivestructureacrossstates.

26
6.2. IncorporatingTrust(Bias)toMachineInput
In ourbasemodel,theDMfullytruststhemachineinput.Weextendourmodeltoaccount
for possiblebiasesthattheDMmayholdagainst(ortoward)themachine.Asaresultofthis
mistrust, theDMmaynotfullybelieve,forinstance,thatthestateisbadwhenthemachine’s
signal isnegative.Inthissense,themachineisnotseenasperfectlyaccurateanymore.Infact,
the frameworkweproposenextcanalsoaccountforthefalsepositiveornegativeerrorsthatan
inaccurate machinemaygenerate.
ToaccountfortheDM’strustandbiastowardthemachine,wefollowthebehavioraloperations
literature (see ¨ Ozer etal.2011)andassumethatgivenmachineinput x1 ∈ {+,−}, theDMupdates
her beliefaccordingto μ+γ = (1−γ)μ + γμ+ and μ−
γ = (1−γ)μ, wherehighervaluesoftrust
parameter γ ∈ [0, 1] indicatesmoretrustinthemachine.Thatis,theDMmixesherpriorbelief μ
with theposteriorbeliefshewouldhavewereshetofullytrustthemachine.Weretrieveourbase
modelwhen γ = 1,whiletheDMfullyignoresthemachineandalwaysdecidesalonewhen γ = 0.
For0 < γ< 1, theDM’sleveloftrustweakenstheeffectofthemachineinputontheDM’sbelief,
i.e. μ− = 0 < μ−
γ < μ<μ+γ < μ+. Inparticular,thenegativesignalofthemachinedoesnotfully
revealthebadstate,thatis μ−
γ >0 inthiscase.
In thissetup,weshowthatthemachinealwaysimprovestheDM’sexpectedaccuracyandvalue
for anytrustlevel(seeProposition2,AppendixD).Wealsofullycharacterizetheimpactofthe
machineontheDM’sbehavior,andfindthatthemachinemaycontinuetoincreasetheDM’s
propensityofmakingfalsepositiveerrors.Asinourbasemodel,thishappenswhentheDMdoes
not stronglyfavorthegoodstateapriori.Incontrasttoourbasemodel,however,themachine
mayalsoincreasetheDM’spropensitytomakefalsenegativeerrors.ThishappenswhentheDM
strongly favorsthegoodstateapriori,andisduetotheDM’smistrustinthemachine’snegative
signal, whichyields μ−
γ >0 (seeProposition3,AppendixD).
Similarly,weshowthatthemachinecanincreasetheDM’scognitiveeffortinthissetupaswell.
This happenswhentheDMsufficientlyfavorseitherthebadorthegoodstate(seeProposition
4 andFigure13inAppendixD).TheformercaseisconsistentwithTheorem4,andasimilar
rationale holds.Thesecondcase,however,doesnotoccurinourbasemodel.
6.3. ASymmetricSettingwithanAdditionalState
In ourbasemodel,themachinereducestheDM’suncertaintyinanasymmetricwayasitfully
resolvesthebadstatefortheDMwhenthefirstinformationsource X1 is negative.Wenow
extend ourmodeltoaccountforasymmetricsetting,andshowthatourkeyinsightscontinueto
hold. Inparticular,weconsiderathirdstate,whichwecall moderate (denoted by ω = m) anda
correspondingaccuratedecision,whichisdeclaringthetestas inconclusive (denoted by a=o). We

27
assume thatthetruestateis good (resp. bad) ifandonlyifboth X1 and X2 are positive(resp.
negative).Otherwise(i.e.,if(X1,X2) ∈ {(+,−), (−,+)}), thestateisassumedtobe moderate. In
this setup,themachineneverfullyresolvestheDM’suncertainty.Thatis,althoughanegative
signal rulesoutthegoodstate,theDMmaystillneedtoprocessinformationtodistinguishthebad
from themoderatestate.Notethatwithmorethantwostates,theDMnowforms consideration
sets (see Caplinetal.2019)andmayruleoutsomeofthemapriori,beforeelicitinganysignal.
WefullycharacterizetheDM’schoiceprobabilitiesasafunctionofherpriorbeliefsinthisset-
up (Proposition5inAppendixE).Asinourbasemodel,weshowthatwheninformationcost λ
increases, theDMbecomeslesswillingtoprocessinformationforweakerpriorbeliefs(seeFigure14
in AppendixE).ThemachinealsocontinuestoalwaysimprovestheDM’saccuracyandexpected
value(Proposition6,AppendixE).
Although themachinealwaysimprovesoverallaccuracy,themachinemaystillincreasecertain
error typesandinducetheDMtoexertmorecognitiveeffortasinourbasemodel.Toexplorethis,
wefocusonsituationsinwhichthemachinereducesuncertaintyinasymmetricmanner,i.e.where
the DM’spriorandposteriorbeliefsaresymmetric.Inthiscase,wefindthatthemachineincreases
the DM’sfalsepositiveandnegativeerrors,aswellashercognitiveeffortifshesufficientlyfavors
the moderatestateapriori(Propositions7and8,AppendixE).8
7. ConcludingRemarks
Humans havealwaysbeeninterestedinharnessingtechnologyandmachinecapabilitiesforcom-
petitiveadvantage.Withtheadventofdata-basedtechnologiesandAI,thecollaborationbetween
humansandmachinehasmovedevenmoretotheforefront.Thisstemsfromtheincreasingrecog-
nition thathumanandmachinescancomplementeachotherinperformingtasksandmaking
decisions. Inthispaper,wedevelopananalyticalmodeltostudytheimpactofsuchcollaborations
on humanjudgmentanddecision-making.Ourmodelincorporatesthequintessentialdistinguishing
features ofhumanandmachineintelligenceinaprimarydecision-makingsettingunderuncertainty:
the flexibilityofhumanstoattendtoinformationfromdiversesources(and,inparticular,the
humandomainknowledgeandthedecisioncontext),butunderlimitedcognitivecapacity,andin
contrast,therigidityofmachinesthatonlyprocessalimitedsubsetofthisinformation,butwith
great efficiencyandaccuracy.
Weintegratethesefeaturesendogenouslyutilizingtherationalinattentionframework,andana-
lytically characterizethedecisionsaswellasthecognitiveeffortspent.Comparingthecasewhen
the humandecidesalonetothecasewithmachineinput,weareabletodiscerntheimpactof
machine-basedpredictionsondecisionsandexpectedpayoff,accuracy,errorrates,andcognitive
8 Accuracy increasesbecausefalsepositiveandnegativeerrorsareoffsetbyadecreaseinfalsemoderateerrors.

28
effort. Toputtheseresultsinperspective,consideragenericmedicalassessmentsetup,inwhich
machine-basedpredictions(e.g.,MLalgorithmprocessingdigitalimages)providediagnosticinput
to thephysician.Thephysiciancanconductmoreassessmentsandtestswiththepatient.When
both assessmentsarepositive,thenthepatientis“sick”.Thepriorreflectsthetruenatureofthe
disease’sincidence within thepatientpopulation(probabilityofpatientbeingsick).
Our findingssuggestthatthemachineimprovesoveralldiagnosticaccuracy(Proposition1)by
decreasing thenumberofmisdiagnosedsickpatients(Theorem3).Themachinefurtherboosts
the physician’spropensitytodiagnosepatientsashealthywhenthedisease’sincidenceishigh
(Theorem 1),andtomisdiagnosehealthypatientsmoreoftenwhentheincidenceislow.The
physicianalsoexertslesscognitiveeffortswiththemachine,whenthedisease’sincidenceishigh
(Theorem 4).Incontrast,themachineinducesthephysiciantoexertmorecognitiveeffortwhen
the disease’sincidenceislowandthephysicianisundersignificanttimepressure(Corollary6).
In thisexample,thepatientissickwhenbothassessmentsarepositive,whichcorrespondstoour
basic setup.Otherinformationstructures,however,arepossible.Forinstance,considerageneric
judicial rulingtask,inwhichmachine-basedpredictions(e.g.,MLalgorithmcheckingevidence
authenticity,orlie-detectiontest)provideevidencetothejudge.Thejudgecananalyzeadditional
data relevanttothecase.When any assessmentispositive,thenthesuspectis“guilty.”Theprior
reflects thetruenatureofthe crime level within thesuspectpopulation(probabilityofsuspect
beingguilty).AswebrieflymentioninSection3.2,ourbasicsetupcanaccountforthissituationby
relabelingthegoodstateandthepositiveinformationinourmodelasthebadandnegativeones,
respectively.Thisalsoreversestheeffectinourresults,asTable1depicts.Thistableprovidesa
flavorofthedifferentimplicationsthatcouldarisefromourfindingsintwohypotheticalsettings
fitting toourcontext.
Medical assessment&diagnosticaccuracy Judicial ruling&convictionaccuracy
• Overalldiagnosticaccuracyisimproved • Overallconvictionaccuracyisimproved
• Fewermisdiagnosedsickpatients • Feweracquittedguiltysuspects
• More patientsdeclaredhealthywhenthe
disease incidenceishigh
• More suspectsdeclaredguiltywhencrime
levelislow
• More misdiagnosedhealthypatientswhen
the diseaseincidenceislow
• More convictednon-guiltysuspectswhen
crime levelishigh
• Physicianspendslesscognitiveeffortto
diagnose whentheincidenceishigh
• Judge spendslesscognitiveefforttoassess
evidence whencrimelevelislow
• Physicianspendsmorecognitiveeffortto
diagnose whentheincidenceislowandtime
is constrained
• Judge spendsmorecognitiveeffortto
assess evidencewhenthecrimelevelishigh
and timeisconstrained
Table1: Impact ofthemachineonhumandecisionsfortwogenericsettings

29
As theaboveexampleshighlight,theincorporationofmachine-basedpredictionsonhumandeci-
sions isnotalwaysbeneficial,neitherintermsofthereductionoferrorsnortheamountofcognitive
effort. Thetheoreticalresultswepresentunderscorethecriticalimpactmachine-basedpredic-
tions haveonhumanjudgmentanddecisions.Ouranalysisalsoprovidesprescriptiveguidanceon
when andhowmachineinputshouldbeconsidered,andhenceonthedesignofhuman-machine
collaboration.Weofferbothhopeandcaution.
On thepositiveside,weestablishthat,onaverage,accuracyimprovesduetothiscollaboration.
However,thiscomesatthecostofmakingcertaindecisionerrorsmoreandincreasedcognitive
effort, inparticularwhenthepriorbelief(onthe“good”state)isrelativelyweak.Consequently,
applications ofmachine-assisteddecision-makingiscertainlybeneficialwhenthereisapriorisuffi-
cientconfidenceinthegoodstatetobeidentified.Inthiscase,themachineinputhasatendency
toward“confirmingtheratherexpected,”andthisprovablydecreasesallerrorratesandimproves
the “efficiency”ofthehumanbyreducingcognitiveeffort.Insharpcontrast,cautionisadvised
for applicationsthatinvolvesearchingandidentifyingasomewhatunlikelygoodstate,especially
when thehumanissignificantlyconstrainedincognitivecapacityduetolimitedtimeormulti-
tasking. Inthiscase,apositiveindicationbythemachinehasastrongeffectof“falsifyingthe
expected.”Theresultingincreaseintaskdifficultynotonlydeterioratestheefficiencyofthehuman
byinducingmorecognitiveeffort,butalsoincreasesherpropensitytoincorrectlyconcludethat
the stateisgood.Hence,human-machinecollaborationmayfailtoprovidetheexpectedefficiency
gain (andtosomeextentaccuracy)preciselywhentheyarearguablymostdesirable.Ourresults
and insightsarequiterobust;theyremainvalidwhentheDMhasamistrustorbiasagainstthe
machineassessment,andingeneralizedsettingswhenthepayoffsormachineimpactonpotential
false positiveandnegativeerrorsarealtered.
Finally,weconsiderinthispaperthreedifferentextensionsofourbasemodel,butothersare
possible.AnoteworthyresearchdirectionistoexplorehowourfindingschangewhentheDMdoes
not fullyknowthemachine’saccuracy.deV´ericourtandGurkan(2022)haverecentlyproposeda
dynamic bayesianframeworktostudythisproblem.Afruitfulapproachconsiststheninconsidering
a settingsimilartotheirs,inwhichtheDMisrationallyinattentiveasinours.
Another interestingavenueoffutureresearchistheestimationandvalidationofourmodelusing
actual data.Thiscouldbeconductedinaspecificmedicalassessmentsetting,suchasradiologists
making diagnosticdecisionswithMLinputfromdigitalimages.Anothersuitablesettingisthe
sepsis alertsystemdiscussedinAyvacietal.(2021).Here,analgorithm(machine)studiesthe
health statusofapatienttogenerateanalert,whichthentriggersadditionaldiagnosticactions
bythecaregiverstoconfirmsepsisdetection.Differentpatientcharacteristics(e.g.,age,disease
history) naturallyleadtodifferentriskprofilesregardingsepsis.Thesepriorscanbeestimated

30
on thebasisofpastdata.Throughcontrolledexperimentswithandwithoutmachineinput,it
wouldbepossibletostudythechangesinoverallaccuracyindetection,aswellastheerrorrates.
Conducting theseexperimentsundervaryingtimeconstraints,theimpactofinformationcostscan
bedetermined.Combiningsuchempiricalresultswiththetheoreticalpredictionswouldfurther
advanceourunderstandingoftheconditionsthatmakemachine-basedinputsmostbeneficial.







----------------------------------------------------------------------------------




De Véricourt, Francis, and Huseyin Gurkan (2023). “Is your machine better than you? you
may never know.” Management Science.

Abstract
Artificial intelligencesystemsareincreasinglydemonstratingtheircapacitytomakebetterpredictionsthan
humanexperts.Yet,recentstudiessuggestthatprofessionalssometimesdoubtthequalityofthesesystems
and overrulemachine-basedprescriptions.Thispaperexplorestheextenttowhichadecisionmaker(DM)
supervisingamachinetomakehigh-stakedecisionscanproperlyassesswhetherthemachineproducesbetter
recommendations. Tothatend,westudyaset-upinwhichamachineperformsrepeateddecisiontasks
(e.g., whethertoperformabiopsy)undertheDM’ssupervision.Becausestakesarehigh,theDMprimarily
focusesonmakingthebestchoiceforthetaskathand.Nonetheless,astheDMobservesthecorrectness
of themachine’sprescriptionsacrosstasks,sheupdatesherbeliefaboutthemachine.However,theDM
is subjecttoaso-calledverificationbiassuchthattheDMverifiesthemachine’scorrectnessandupdates
her beliefaccordinglyonlyifsheultimatelydecidestoactonthetask.Inthisset-up,wecharacterizethe
evolutionoftheDM’sbeliefandoverrulingdecisionsovertime.Weidentifysituationsunderwhichthe
DM hesitatesforeverwhetherthemachineisbetter,i.e.,sheneverfullyignoresbutregularlyoverrulesit.
Moreover,theDMsometimeswronglybelieveswithpositiveprobabilitythatthemachineisbetter.We
fully characterizetheconditionsunderwhichtheselearningfailuresoccurandexplorehowmistrustingthe
machineaffectsthem.Thesefindingsprovideanovelexplanationforhuman-machinecomplementarityand
suggest guidelinesonthedecisiontofullyadoptorrejectamachine.

Key words : machineaccuracy,decisionmaking,human-in-the-loop,algorithmaversion,dynamiclearning

2 de V´ericourtandGurkan: betterorworse
reporthowateamofradiologistsinalargeUS-basedhospitalabandoneddifferentMLalgorithms
after usingthemforseveralmonths.
This tendencytooverridealgorithmsistypicallyattributedtoanintrinsicmistrustofmachine-
based predictions,oftenreferredtoasan algorithm aversion (Dietvorstetal. 2015, Gaubeetal.
2021). Thisbias,however,maynotbethesolereasonforinappropriatelyandsystematicallyover-
riding analgorithm.Indeed,theverycontextinwhichahumandecisionmaker(DM)workscan
also preventtheDMfromlearningwhetheramachineproducesbetterprescriptions.
In thispaper,weexploretheconditionsunderwhichmakinghigh-stakedecisionshampersthe
DM’s abilitytoproperlylearnwhetheramachineissuperiortohumanexpertise.Importantly,
wecharacterizethenatureoftheinappropriateoverridingdecisionsthattheselearningfailures
giveriseto,withoutrelyingonanymistrustbias.Tothatend,weanalyzeaset-up,inwhicha
DM performsrepeateddecisiontasksusingtheprescriptionsofamachine.Eachtaskconsistsin
deciding whetherornottotakeaspecificaction.Thiscorresponds,forinstance,todecidingona
biopsy inamedicalcontext.Tomakethischoice,themachineproducesarecommendationthat
the DMmayoverrulebasedonherownexpertise.Crucially,theDMisuncertainaboutwhether
the machinemakesbetterorworsedecisionsthanshedoes,butastheDMverifiesthecorrectness
of thedifferentmachine’sprescriptions,sheformsabeliefaboutthemachine’strueaccuracy.
Our focusisthusontheDM’slearningbehavioroncethealgorithmhasbeendeployed,thatis,
after ithasbeenproperlytrainedandevaluatedonrepresentativedatasets(see Kubat 2017) and
possiblyshownbetter-than-humanaccuracylevels.Thesedatasets,however,neverfullycapture
the groundtruth,andtheissueofempiricalgeneralizabilityremains(see,e.g., Lebovitzetal. 2021).
Hence, anexpertmaycontinuetoobserveandadjustherbeliefaboutthemachineafteradopting
it. Yet,becausethemachineisdeployedandmakesprescriptionswithrealconsequences,learning
can beimpairedinwaysthatdonotexistduringthetrainingphaseofthealgorithm.
In particular,weconsidersituationsinwhichtheDMobservesthecorrectnessofthemachine’s
prediction andupdatesherbeliefaccordinglyonlyiftheactionisactuallytaken(e.g.,whena
biopsy isperformed).Inotherwords,theDMissubjecttotheso-called verificationbias (see,
e.g., Pepe 2003, p.169),suchthattheaccuracyparametersofadiagnostictestarelearnedonly
when atestresultisverifiedbyfollow-upwork(e.g.,abiopsyrevealsthepresenceorabsenceof
the disease).Thislimitationcanalsostemfromaformofsaliencebiasorinattentionalblindness
(TaylorandThompson 1982, Bordalo etal. 2012 and Tiefenbecketal. 2018), whichareespecially
prevalentforhigh-stakesdecisions(see Lee etal. 2018). Inthiscontext,theDMfocusesherlimited
attentiononmakingthedecisionsathandbutistriggeredtoreassessthemachine’squalityby
the salientobservationofaverifiedsuccessorfailure.(See Camachoetal. 2011 for anexampleof
similar salienteffectsinthecontextofnewdrugprescriptions.)
de V´ericourtandGurkan: betterorworse 3
Further,theDMonlydecideswhatisbestforthetaskathandandthusneveractsforthe
purposeofverifyingthemachine’saccuracy.Inthissense,theDM’sdecisionsare exploration-free.
This restrictionmaybeforlegalorethicalconcerns,whichareoftenwarrantedwhenthestakes
are highasinthemedicalandjudiciarysectors(Bastani, BayatiandKhosravi 2021).
In thispaper,wemainlyexaminethecasewherethemachineandtheDMare substitutes in that
the DM’saccuracyiseitherbetterorworsethanthemachine’s.Wefocusonsubstitutionfortwo
reasons. First,ourgoalistostudyinappropriateoverridingdecisionswithoutrelyingonalgorithm
aversion,thestudiesofwhichassumesubstitution(Dietvorstetal. 2015, Sun etal. 2021). Second,
and moreimportantly,weseektodetermineifacomplementaritybetweentheDMandthemachine
mightemergefromtheDM’sinabilitytolearnthenatureofthemachine.Assumingsubstitution
enables ustodisentanglethislearningeffectfromanintrinsiccomplementaritybetweentheDM
and themachine.Nonetheless,wealsoexploresituationswherethemachineandDMcomplement
one another(seeSection 8).
Our approachthusconsistsinanalyticallystudyingtheevolutionoftheDM’sbeliefandoverrul-
ing decisionsovertime.ThisenablesidentificationofsituationsinwhichtheDMproperlylearns
whether themachinemakesbetterpredictionsthanshedoes.TheasymptoticbehavioroftheDM’s
belieffurthercharacterizesthedifferentwaysinwhichtheDMfailstolearnthetruenatureofthe
machine.
Followingthisapproach,wefindthattheDMalwaysproperlylearnswhetherthemachineis
betterorworseintheabsenceofhuman-machineinteractions,i.e.,whenthemachine’sprescriptions
neverinfluencetheDM’sdecisions.Indeed,inthiscase,theDMverifiesthemachineindependently
of itsprediction.1 Hence, inappropriateoverridingdecisionsmayoccuronlyifthemachinehas
some influenceontheDM’schoices.
When thisinfluenceismaximal,i.e.,themachine’sprescriptionsfullydeterminetheDM’s
choices,wefindthattheDM’sabilitytolearndependsonherprioraboutthetask.Specifically,
the DMproperlylearnsthatthemachineisbetter(resp.,worse),ifthepriorprobabilitythatan
action isrequiredforthetaskathandisabove(resp.,below)acertainthreshold.Hence,theDM
can endupbelievingthatthemachinemakesworsepredictionsthanshedoes,eventhoughthe
machineisactuallybetter.Thisoccurswhentheactionisnottoofrequentlyrequiredforthetasks.
Conversely,theDMlearnsthatthemachineisbettereventhoughitisactuallyworsewhenthe
action isfrequentlyrequired.
In thesetwobenchmarks,theDM’sbeliefaboutthemachinehasnoeffectonherchoices.This
contrastswithourmainset-up,inwhichtheDM’sdecisiontoact,andhenceherabilitytolearn,
1 Tobemoreprecise,theverificationeventintheno-interactionbenchmarkisduetotheDM’sownjudgment,and
the eventthatthemachineprescribestoactisindependentoftheDM’sjudgmentconditionalonthetask’stype.
4 de V´ericourtandGurkan: betterorworse
are endogenouslydeterminedbyhercurrentbeliefaboutthemachine’squality.Specifically,in
this setting,theDMoverrulesthemachinewhentheDM’sjudgmentcontradictsthemachine’s
prescription andtheDMsufficientlybelievesthatthemachineisworse.
When thisisthecase,weagainfindthattheprioraboutthetaskdetermineswhentheDMfails
to learn.However,theDM’soverridingdecisionsfundamentallychangethenatureofmislearning.
Indeed, whenthemachineisactuallybetterthantheDMandtheprioraboutthetaskislow,the
DM’s beliefalwaysoscillatesovertime.Inotherwords,theDMpermanentlyremainsunsureabout
whether themachineisbetterornotandconstantlyalternatesbetweenfollowingandoverriding
its prescriptions.Further,andperhapsmoreinterestingly,theDMsometimestreatsthemachine
as ifitspredictioncomplementsherownjudgment,whileinfact,thetwoarefullsubstitutes.In
contrast,whenthemachineisworseandtheprioraboutthetaskishigh,thebeliefconverges
to aBernoullirandomvariable:theDMproperlylearnsthatthemachineisworsewithagiven
probabilitybutwronglylearnsthatitisbetterwiththeremainingprobability.Inotherwords,the
DM randomlyendsupincorrectlybelievingthatthemachineisbetter.
Takentogether,theseresultsidentifytwodifferentformsofmislearning—persistenthesitation
and randominference—thatcanoccurwhenaDMworkswithamachinetomakehigh-stakes
decisions. ThesefindingsalsohighlightthekeyrolethattheDM’sprioraboutthetaskplaysin
her abilitytolearnthetruenatureofthemachine.Additionallytheyuncoveranovelrationale—
the uncertaintyaboutthemachine’strueperformance—forwhyhumanexpertsmayco-produce
their decisionswithamachine.ThesemislearningbehaviorsdonotdependontheDM’sinitial
beliefaboutthemachineandthusholdevenwhentheDMsufficientlybelievedinthemachine’s
performancetodeployitinthefirstplace.
These resultsfurthersuggestguidelinesonthedecisiontofullyadoptorrejectamachineafterit
has beendeployed.Thequestioniswhetherthemachineshouldmakeallthedecisionshenceforth
or beabandonedforgoodatsomepointafterworkingwithit.Ourresultsindicatethatthelonger
the DMbelievesthemachineisworse,themorelikelysheiscorrectinherassessmentandhence
should abandonthemachine.Thesameisnottrue,however,iftheDMincreasinglybelievesthat
the machineisbetter.Inthiscase,ourfindingssuggesttorelyonmultipleDMs.Ifaconsensus
exists amongtheteamthatthemachineisbetter,thenthelargertheteamis,themorelikelyitis
that themachineshouldbeadopted.(SeeSection 6.)
Importantly,themislearningbehaviorswecharacterizeinthispaperdonotstemfromanintrinsic
algorithm aversionbutratherfromcertaincontextsinwhichDMsmakehigh-stakesdecisions,
as capturedwiththeverificationbiasandtheexploration-freecondition.Yet,aDMwhofaces
situations suchasthesemayalsobesubjecttomistrustbiasesagainstthemachine,whichcan
interactwithourfindings.
de V´ericourtandGurkan: betterorworse 5
Indeed, mistrustingthemachineaffectstheDM’sabilitytolearninatleasttwoways.First,the
DM maydownplaythemachine’sprescriptionwhendecidingtoact(consistentlywiththedecision-
making literature,see,e.g., Soll andMannes 2011), whichalterstheDM’sabilitytoobservethe
correctness ofthemachine’spredictions.Second,andinlinewiththealgorithmaversionreported
by Dietvorstetal. (2015), theDM’sbeliefinthemachinemaydisproportionatelydropupon
observing amachine’spredictionerror.Weexplorehowtheseeffectsinteractwithourresults(see
Section 7) andfindthatourresultsholdintheformercasebutnotinthelater.Whenmistrust
introducesanegativitybiasintheDM’slearningprocess,theDMdoesnotalwaysproperlylearn
that themachineisbetterifthepriorissufficientlyhigh,asinthemainset-up.Instead,theDM
can wronglylearnwithapositiveprobabilitythatthemachineisworse.Inthissense,algorithm
aversionsometimesinteractswithoursettingtorandomizetheDM’sabilitytolearn.
Finally,ourresultsarerobusttoapartialrelaxationoftheverificationbias,whichislegitimate,
for instance,whenthebiasstemsfromtheDM’slimitedattention.Inthiscontext,theDMalso
learns fromunverifiedcases.Ourresultscontinuetoholdaslongasunverifiedcasesaresufficiently
less salientthanverifiedones,forwhichthetruestateoftheworldisrevealed.
After reviewingtheliteratureinSection 2, wepresentthemodelinSection 3. InSection 4,
weanalyzetheno-interactionandno-overridingbenchmarksandthenfocusonthemainset-up
in Section 5. WehighlighttheimplicationsofourfindingsinSection 6 and studytheeffectsof
mistrust biasesonourfindingsinSection 7. Further,weexplorethesettingswherethemachine
and theDMcomplementoneanotherinSection 8, andtheverificationbiasispartiallyrelaxedin
Section 9. Finally,wediscussfutureresearchdirectionsintheconclusion.
2. LiteratureReview
Our studyisrelatedtotherecentandgrowingliteratureontheinteractionbetweenhumandecision-
makersanddata-drivenalgorithms.Thisresearchexplorestheextenttowhichco-productionof
decisions byamachineandaDMmayimproveperformance.Forinstance, Boyacietal. (2020)
demonstrate inarationalinattentionframeworkthathuman-machineinteractionimprovesthe
overallaccuracyofdecisions,butsometimesatthecostofhighercognitiveeffort(see Boyacietal.
2020 for additionalreferencesonformalmodelsofmachine-humaninteractions).Machinelearning
algorithms havealsobeenproposedtoprovideinterpretablecuestohelpdecisionmakersimprove
their decisions(see Bastani, BastaniandSinchaisri 2021, forinstance).Thisstreamofresearch
further exploreshowtousehumanjudgmenttotrainorimproveanalgorithm(VanDonselaar
et al. 2010, Ibrahim etal. 2021, Cowgill 2019).
WecontributetothisliteraturebyprovidinganovelrationaleforwhyaDMmaytreatthe
machine’sprescriptionsasacomplementtoherjudgment.Infact,thisstreamofresearchtypically
6 de V´ericourtandGurkan: betterorworse
assumes thatthemachine’saccuracyisknownandcomplementstheDM’sjudgement.Incontrast,
the DMandthemachinearesubstitutes,andthemachine’saccuracyisunknowninoursetting.
In thissense,ourstudyiscloselyrelatedtotheliteratureonoverridingdecisionsand,more
generally,trustinalgorithmicprescriptions.Inparticular, Lebovitzetal. (2021) documentover
severalmonths,howateamofradiologistslosttrustinthequalityofamachinelearningalgorithm
that helpedanalyzemedicalimages. Dietvorstetal. (2015) alsofoundinanexperimentalset-up
that theirparticipantsoverrodeamachine’sprescriptions,evenafterseeingthatthemachine’s
algorithm performedbetterthanthehumandidonaverage.Thistendencytowronglyoverride
machine-basedprescriptionsisfurthersupportedbyempiricalevidenceinthefield.Forinstance,
Sun etal. (2021) observedthatpackingworkersatthewarehousesoftheAlibabaGroupregularly
deviated fromalgorithmicprescriptions,whichreducedoperationalefficiency.Severalapproaches
havebeenexploredtoreducedeviationssuchasthese,eitherwithfieldexperiments(Sun etal.
2021) orinthelab(Dietvorstetal. 2018).
In contrasttothisstreamofpapers,ourstudyproposesanalternativeexplanationforinap-
propriately overridingdecisionssuchasthese,whichmostlystemsfromthecontextinwhichthe
decisions aremade.Specifically,wetracetheseerrorstofourfundamentals(exploration-free,verifi-
cation bias,informativenessandsubstitution),whichcapturesomeessentialfeaturesofhigh-stakes
decision makingusingmachine-basedpredictions.
RecentstudiesalsosuggestthathumansfollowtheprinciplesofBayesianinferencewhenobserv-
ing thecorrectnessofmachine-baseddecisions.Forinstance, Wangetal. (2018) and Guo etal.
(2020) analyzeinanexperimentalset-uphowobserversdynamicallyupdatetheirtrustinthe
machineastheyobservethefailuresandsuccessesofitspredictions(withoutoverridingthe
machine,asinthebenchmarkofSection 4.2). ThesestudiesfindthatassumingBayesianobservers
can explaintheempiricallevelofhumantrustinthemachineovertime.Thekeydifferencewith
our set-up,however,isthattheDMisnotsubjecttoverificationbiasandthusalwaysobserves
the correctnessofthemachine’spredictionintheirsettings.
Verificationbiasisaformofselectionbiasthatwasfirstintroducedby Ransohoff andFeinstein
(1978) todescribesituationswheretheaccuracyofadiagnostictestislearnedonlywiththeverified
cases, i.e.,whenfollow-upactionsaretakentoconfirmatestresult.Thisbiastowardsverified
cases permeatesthemedicalfield(e.g., Hujoeletal. 2021, Whiting etal. 2013, Petscavageetal.
2011, Bates etal. 1993, and Greenes andBegg 1985) andhasbeenfoundinstudiesevaluating
ML algorithmsformedicalapplications(see,e.g., Tschandletal. 2019). Mostofthisresearchhas
focusedondevelopingestimatorsofaccuracybasedonthemaximumlikelihoodtocorrectthebias.
de V´ericourtandGurkan: betterorworse 7
The conditionsrequiredtoavoidthisbiasinthefrequentistliterature,however,donotholdinour
set-up.2
Learning withselectiveobservationsasinverificationbiascanalsostemfromaformofsalience
bias orinattentionalblindness(TaylorandThompson 1982, Bordalo etal. 2012 and Tiefenbeck
et al. 2018). Thebehavioralscienceliteraturehasstudiedbiasessuchasthese,(see,e.g., Kahneman
1973, Chapter7,fortheeffectsoffocusedattentiononinformationfiltering),whicharedueto
the DM’slimitedcognitivecapacity(Simon 1955). Inthissense,ourstudyalsocontributestothe
growingstreamofoperationsandeconomicsliterature,whichdeviatesfromstandardBayesian
learning toaccountforlimitedcognitivecapacity(see,e.g., Allon etal. 2021, Boyacietal. 2020 and
the referencestherein).Inparticular,ourset-upisconsistentwiththenotionofselectiveBayesian
updating(seetheseminalworkof Schwartzstein 2014), inthattheDMonlyselectstheactual
success orfailureofthemachinetoupdateherbeliefaboutthemachine’stype.
Finally,ourworkisrelatedtothevastliteratureonlearningproblems,whichhavebeenexten-
sivelystudiedinmanagementscienceandoperationsmanagement.Forinstance,studieshave
considered priceexperimentationtolearndemandcurvesbyfocusingonthetradeoffsbetween
learning andearning,anddesignheuristicpoliciesachievinggoodregretperformance(Besbesand
Zeevi 2009, Boyacıand ¨ Ozer 2010, Cheung etal. 2017, Keskin andBirge 2019). Inthisstreamof
papers,theDMexperiments(explores)withdifferentpricesinthebeginningofthetimehorizon
to earn(exploit)moreintheremainingperiods.Becauseofthisabilitytoexplore,theDMcan,
in principle,properlyuncoverthetruedemandcurveinthelimit.Theobjectiveofthesepapersis
then tolearnsufficientlyfastsoastomaximizeprofit.Incontrast,weconsidersituationswhere
exploring isnotpossible.Thus,theDMoptimizeswithineachperiodandmislearningmayemerge
in ourset-up.
In thissense,ourapproachresembles Harrison etal. (2012) whichanalyzesmyopicpricing
policies(seeSection4inparticular).Intheirset-up,demandfunctionsarethefocusoflearning,
whereas weconsiderunknownaccuracyparameters.Therefore,thetypeofincompletelearning
that mayoccurdiffersradicallyineachsetting.Inparticular,incompletelearningtakestheformof
confounding beliefsin Harrison etal. (2012), suchthatthemyopicpolicychargesanuninformative
price, whichpreventsBayesianupdatingfromproducingadifferentposterior.Asaresult,theDM
becomesstuckinthesamebeliefovertime.Incontrast,mislearningcantaketheformofbelief
oscillation inourset-up,whichcannotoccurin Harrison etal. (2012) perProposition2.
2 Forinstance,ourmainset-updoesnotsatisfythemissing-at-randomassumptionusedby Begg andGreenes (1983)
or therestrictionsimposedonthedatageneratingprocessproposedby Zhou (1993). Inaddition,ourno-overriding
benchmarkcorrespondstoso-calledextremeverificationbias(Pepe 2003, p.180),forwhichtheestimationofaccuracy
parameters isimpossible(Broemeling 2011).
8 de V´ericourtandGurkan: betterorworse
Learning problemssuchasthesearealsoextensivelystudiedineconomics(seeforinstance
Smith andSørensen 2000, Acemoglu etal. 2011, andreferencestherein),withaparticularfocus
on equilibriumlearningdynamicsshapedbymultiplestrategicagents.Inthisstreamofresearch,
Herrera andH¨orner (2013) analyzesaset-upwithshort-livedmyopicinvestors,inwhichonly
investingdecisionsareobservable.Althoughthismayresembleourset-up,theirpayoff,signaland
learning structuresdiffer,whichyieldsadifferenttypeofmislearning.Inparticular,thebelief
convergestoaninteriorpointintheirset-up(seePropositions1and4in Herrera andH¨orner
2013), whileitmaynotconvergeinours.
3. ModelDescription
Weconsideradecisionmaker(DM)whofacesaseriesofindependentdecisiontasksoveradiscrete
time infinitehorizon.AmachinefurtherassiststheDMbyproducingarecommendationabout
whichdecisiontotakeforeachtask.TheDM,however,doesnotknowifthemachine’saccuracyis
superiortoherownjudgment.Astheaccuracyofthemachine’spredictionsisrevealedovertime,
the DMformsabeliefaboutwhetherornotsheshouldoverridethemachine’sprediction.Next,we
introducethesingledecisiontaskproblemthattheDMperformsineachperiod.Wethenconsider
the wholetimehorizon.
3.1. SingleDecisionTask
A taskconsistsindecidingwhetherornotaspecificaction(e.g.,abiopsy)isrequired.Wedenote
as Θ ∈ {A,NA} the typeoftasksuchthattheactionisrequiredifΘ= A and isnotrequiredif
Θ=NA. TheDMdoesnotknowthetask’stypebuthasapriorbelief p≜P(Θ=A) thatsheshould
act.
Toperformthistask,theDMappliesherexpertiseandelicitsimperfectsignal SH ∈ {+,−}, such
that SH = +(resp., SH = −) indicatesthatΘ= A (resp., Θ= NA). Wedenotethesensitivity(true
positiverate)andspecificity(truenegativerate)ofthesignalby αH and βH, respectively.The
DM isfurtherassistedbyamachinelearningalgorithm,whichmakesanindependentprediction
abouttypeΘ.Thispredictioncorrespondstoasecondsignal, SM ∈ {+,−}, withsensitivityand
specificityequalto(αM,βM).
Importantly,theDMisuncertainaboutwhetherthemachine’saccuracyisbetterthanherown.
Specifically,wedenotethemachine’stypeasΓ ∈ {B,W}. WhenΓ= B (resp., Γ=W), signal SM
is better (resp., worse) thansignal SH, andthesensitivityandspecificityofthesignalareequal
to (αB,βB) (resp.,(αW,βW)). Themachineisbetter(resp.,worse)inthesensethattheDMnever
(resp., always)overrulesthemachinewhenitstypeisperfectlyknown.Thiscorrespondstothe
notion ofsubstitution,whichweintroduceandformalizelaterinthissection(seeequations 4 and
de V´ericourtandGurkan: betterorworse 9
5). Toexcludedegeneratedcases,wefurtherfocusouranalysisonsituationswhere αB > αW and
βB >βW.3 This isonlyforthesakeofclarity,asallofourresultsextendtothemoregeneralcase.
Probability b≜P
􀀀
Γ=B

denotes thentheDM’sbeliefthatthemachineoutperformsherability
to decide.Ineffect,thesetwotypesofmachineinducetwodifferentprobabilitymeasures PB{·}
and PW{·} on thesamplespaceofthemachine’ssignals,suchthat PΓ(SM = +,Θ = A) = αΓp and
PΓ(SM =−,Θ=NA)=βΓ ¯p for Γ ∈ {B,W} (with ¯x=1−x for x ∈ [0, 1]).
Based onrealizations sH and sM of signals SH and SM, respectively,andherbelief b aboutthe
machine,theDMupdatesherprior p that anactionisrequiredusingBayes’rule.Thecorresponding
posteriorprobabilityisthus P(Θ=A|SH =sH, SM =sM, b) (withaslightabuseofnotation).4
The DMthendecidestoactifandonlyiftheposteriorisaboveapositivethreshold r, i.e.,
P(Θ=A|SH =sH, SM =sM, b)≥r; theDMdoesnotactotherwise.Thisdecisionruleisoptimal,for
instance, whentheDMseekstomaximizetheexpectedvalueassociatedwithcorrectlyidentifying
the task’stype.Inthiscase,threshold r accountsforthefalsepositiveandfalsenegativecosts
associatedwiththedecision.5
Informativeness: In thefollowing,weassumethatthesignalsproducedbyboththeDMand
the machineareinformative,inthesensethateachsignalprovidessufficientinformationforthe
DM todecide.Formally,thiscorrespondsto:
P(Θ=A|SH =+) ≥r and P(Θ=A|SH =−)<r, (1)
PB(Θ=A|SM =+) ≥r and PB(Θ=A|SM =−)<r, (2)
PW(Θ=A|SM =+) ≥r and PW(Θ=A|SM =−)<r. (3)
In otherwords,thesolerealizationofeither SH or SM, whetherthemachineisoftype B or W, fully
determines whetherornottheposteriorislargerthanthreshold r, i.e.,theDMtakestheaction.
These conditionsfurtherimplythatconsideringbothsignals SH and SM together isredundant
when theirrealizationsarealigned,i.e.,when sH = sM. OnesignalisthensufficientfortheDMto
decide sincetheDMactsif SH = SM = +anddoesnotactif SH = SM = −. Iftherealizationsare
misaligned with sH ̸=sM, however,theDMandthemachinemayoverrideoneanother.Inthiscase,
weconsidersituationswherethemachineandtheDMarefullsubstitutesinthefollowingsense.
3 This assumptionguaranteesthattheDM’sbeliefinabettermachinedecreases(resp.,increases)uponobservingan
incorrect (resp.,correct)machineprediction.Incontrast,assuming αW > αB (resp., βW > βB) impliesthattheDM’s
beliefthatthemachineisbetteractuallyincreasesafterobservingafalsenegative(resp.,falsepositive)error.
4 In particular,wehave P(Θ = A|SH = sH, SM = sM, 1) = PB(Θ = A|SH = sH, SM = sM) and P(Θ = A|SH = sH, SM =
sM, 0) =PW(Θ =A|SH = sH, SM = sM).
5 See, Alizamiretal. (2013) forinstance,foramicrofoundationofthreshold r.
10 de V´ericourtandGurkan: betterorworse
Substitution: Weassumethatatype B machinealwaysoverridestheDM’sjudgment,while
the DMalwaysoverridestheprescriptionofatype W machine.Formally,thiscorrespondsto:
PB(Θ=A|SH =+,SM =−)<r and PB(Θ=A|SH =−,SM =+)≥r (4)
PW(Θ=A|SH =+,SM =−)≥r and PW(Θ=A|SH =−,SM =+)<r (5)
Thus,ifthesignalsoftheDMandatype B machinecontradictoneanother,signal SM alone
determines whetherornottheposteriorprobabilityislargerthanthethreshold(perequation(4)).
Along withtheInformativenessproperty,thismeansthatatype B machinealwaysdetermines
whether theDMshouldact,independentlyoftheDM’sjudgment.Incontrast,theDMdecidesalone
and canignoretheprescriptionofatype W machine(perequation(5)). Hence,ifthemachine’s
typeisfullyknown,theDMandthemachinenevercollaboratetomakeadecision.Inthissense,
the DMandthemachinearesubstitutesforthetask.
In essence,InformativenessandSubstitutionareconditionsontheDM’sposteriorprobability
aboutthetask’stype,whichinturndependsonthesignals’sensitivitiesandspecificities,aswell
as prior p and threshold r.
3.2. RepeatedTasksandLearning
WenowconsiderthesituationwheretheDMfacesaseriesofdecisiontasksoveradiscretetime
infinite horizon.TasktypesΘt, t ∈ N, areindependentandidenticallydistributedwithprobability
p. (Inthefollowing,weusesubscript t to denotetheparametersassociatedwiththetaskofperiod
t.) Atthebeginningofperiod t > 0, theDM’sbeliefaboutthemachine’stypeisgivenby bt−1,
where b0 is thepriorbeliefatthebeginningofthetimehorizon.TheDMthenobtainssignals
SH
t , SM
t and decideswhethertoact.
Exploration-Free: In makingthischoice,theDMconsidersonlythetaskathand.More
formally,theDMactsif P(Θt =A|SH
t , SM
t , bt−1)≥r and doesnothingotherwise.Inparticular,the
DM doesnotactforthesolepurposeofuncoveringthetruetask’stypeandthuslearningthe
machine’s.Instead,theDMdecideswhatshethinksisbestforthecurrenttaskandisthusmyopic
with respecttolearningthemachine’stype.
Attheendoftheperiod,theDMupdatesherbelief bt−1 to posterior bt according toBayes’rule,
if theDMobservestypeΘt.
VerificationBias: The DM,however,observesthetask’stypeandupdatesherbeliefaccord-
ingly ifandonlyifanactionistaken.Becausedecisionsareexploration-free,theverificationbias
de V´ericourtandGurkan: betterorworse 11
implies thattheDMupdatesherbeliefifandonlyif P(Θt =A|SH
t , SM
t , bt−1)≥r, inwhichcasewe
assume thattheDMfollowsBayes’rule.Thus,wehave
bt =


bt−1 if P(Θt =A|SH
t =sH, SM
t =sM, bt−1)<r 
1+
¯b
t−1
bt−1
PW(SM
t =sM |Θt =θ)
PB(SM
t =sM |Θt =θ)
−1
if P(Θt =A|SH
t =sH, SM
t =sM, bt−1)≥r ,
(6)
where θ ∈ {A,NA} is theobservedvalueofΘt.
Equation (6) highlightstwomechanismsbywhichtheDM’sbeliefaboutthemachine’stypeis
endogenously determinedovertime.ThefirstcorrespondstotheBayesianupdatingofprior bt−1
when theDMobservestypeΘt. ThesecondcorrespondstotheDM’sabilitytoverifytypeΘt in
the firstplace,thatis,whetherposteriorbelief P(Θt = A|SH
t , SM
t , bt−1) issufficientlylarge.This,
in turn,dependsonbelief bt−1. Equation(6) furtherimpliesthatwhentheDMacts,sheincreases
(resp., decreases)herbeliefthatthemachineisbetterifthemachine’sprescriptionturnsoutto
becorrect(resp.,wrong).
This learningmechanismalsocorrespondstoaselectiveBayesianupdatingset-up(Schwartzstein
2014) inwhichaDMfocusesherlimitedattentiononmakingdiagnosticdecisions,insteadof
evaluatingthemachinethatassistsher.Thesalientobservationofanactualsuccessorfailureof
the machine,however,redirectstheDM’sattentiontoreassessherbeliefaboutthemachine’stype.
This mechanismresemblesthetwo-stagelearningprocessof Allon etal. (2021), inwhichagents
allocatetheirattentiontodifferenttasks(screeningandbeliefupdatingintheirsetting)ineach
stage. (WerelaxthislimitationontheDM’sattentioninSection 9.)
When theDMacts,theDMupdatesbelief bt in partbasedonsignal SM
t . Themachine’stype,
however,determinestheprobabilitydistribution, PB{·} or PW{·}, ofthissignal.Hence,belief(bt)t∈N
can followtwodifferentstochasticprocessesdependingonmachinetypeΓ.Theasymptoticbehavior
of belief bt thuscapturestheDM’sabilitytolearnwhetherthemachinemakesbetterpredictions.
Indeed, theDMproperlylearnsthemachine’stypeifherbeliefconvergesovertimeto1(bt
a.s. −−→1)
when themachineisbetter(Γ=B) andconvergesto0(bt
a.s. −−→0) whenthemachineisworse(Γ=W).
(Notation a.s. −−→ indicates almost-sureconvergence.)Incontrast,theDMmislearnsthemachine’s
typewhen bt
a.s. −−→0 (resp.,1)andΓ= B (resp., Γ=W). Learningmayevenbeinconclusivewhen
belief bt convergestoaninteriorpointin(0, 1) oroscillates.Moreformally,astochasticprocess
Yt is saidtobeoscillatingandrecurrentifrecurrentinterval I exists suchthatforany τ > 0,
P(Yt ∈ I for some t>τ |Yτ ∈ I)=1(seeDefinition8.1in Gut 2009 for instance).
Our objective,therefore,istostudytheasymptoticbehaviorof bt and characterizetheresulting
learning behavioroftheDM.
12 de V´ericourtandGurkan: betterorworse
4. Benchmarks
Wefirststudytwosettings,inwhichtheDMdoesnotaccountforherbeliefaboutthemachine
when decidingtoact.Inthefirst no-interaction setting, theDMalwaysignoresthemachine’s
prescription andbasesherchoicesolelyonherownjudgment SH
t . Inthissense,theDMandthe
machinedonotinteractwhendecidingontasks.Inthesecond no-overriding setting, themachine
fully determinestheDM’schoicesothattheDMneveroverridesitsprediction SM
t . Importantly,
belief bt−1 doesnotdeterminewhetheranactionistakeninbothbenchmarksandthuswhether
a machine’spredictionisverifiedexpost.Asaresult,thesecondmechanismbywhichbelief bt−1
influences posterior bt in equation(6) ismute.Thisbeliefaffectslearningthroughonlythefirst
mechanism,i.e.,theapplicationofBayes’rulewhentheDMacts.
More specifically,theDMactsifandonlyif SH
t = +,regardlessofthemachine’ssignal SM
t in
the no-interactionbenchmark,andifandonlyif SM
t = +,regardlessofherownjudgment SH
t in
the no-overridingbenchmark.Theconditionforacting, P(Θt =A|SH
t ,SM
t , bt−1)≥r, thusreducesto
P(Θt = A|SH
t ) ≥ r in thefirstbenchmarkandto P(Θt = A|SM
t , bt−1) ≥ r in thesecondone,which
are, respectively,equivalentto SH
t = +andto SM
t = +forany bt−1 due toInformativeness(1)-(3).
In bothbenchmarks,equation(6) thenbecomes
bt =


bt−1 if Sσ
t =− 
1+
¯b
t−1
bt−1
PW(SM
t |Θt =θ)
PB(SM
t |Θt =θ)
−1
if Sσ
t =+.
(7)
where σ = H and σ = M in theno-interactionandno-overridingbenchmark,respectively.Inpar-
ticular, therealizationof Sσ
t is independentofbelief bt−1, whichisincontrasttotheconditionin
equation (6).
Tostudytheasymptoticbehaviorof bt, weconsiderinsteadthelog-likelihoodratio Lt of the
probabilitythatΓ= B in period t. Formally, Lt is amonotonecontinuoustransformationof bt
givenby Lt ≜log

bt
1−bt

, suchthat
Lt =Lt−1 +Rt,
where (Rt)t∈N are i.i.d.randomjumps.Inparticular,thelog-likelihoodratioisincreasinginthe
DM’s belief,andtheasymptoticbehaviorof Lt fully determinestheasymptoticbehaviorof bt.
Indeed, wehave bt
a.s. −−→1 (and bt
a.s. −−→0) ifandonlyif Lt
a.s. −−→+∞ (and resp. Lt
a.s. −−→−∞) perthe
continuousmappingtheorem.
Log-likelihoodratio Lt is arandomwalkgovernedbyjumps(Rt)t∈N, whichcapturethemagnitude
and directionofthebelief’supdates.Theserandomjumpstakethreepossiblevalues:apositive
(resp., negative)valuewhentheDMobservesthatthemachine’spredictioniscorrect(resp.,
wrong), i.e., SM
t =+andΘt =A (resp., Θt =NA), orzerowhenthetask’stypeisnotobserved,i.e.,
de V´ericourtandGurkan: betterorworse 13
when SH
t = − in theno-interactionbenchmarkand SM
t = − in theno-overridingbenchmark.The
asymptotic behaviorof Lt is thenfullydeterminedbythesignofthemean EΓ[Rt]. If EΓ[Rt] > 0
(resp., < 0), thenlog-likelihoodratio Lt increases inexpectationandconvergesalmostsurelyto
+∞ (resp., −∞),6 while Lt doesnotconvergewhen EΓ[Rt]=0.
4.1. No-InteractionBenchmark
First, wecharacterizetheDM’sabilitytolearnthemachine’stypeintheabsenceofDM-machine
interactions,i.e.,whentheDM’sdecisionsalwaysignorethemachine’sprescriptions.Specifically,
when equation(7) holdswith σ =H, wehave
Theorem 1 (Learning withNo-Interaction). When themachineisbetter Γ = B (is worse
Γ=W), bt
a.s. −−→1 (and resp., bt
a.s. −−→0).
Toprovethisresult,wefirstestablishthat EΓ[Rt]>0 (resp., <0) ifΓ=B (resp., Γ=W) andthen
apply thecontinuousmappingtheorem.(Allproofsareintheappendix.)
Theorem 1 states thattheDMcorrectlylearnsthemachine’stypewhenshedecidestoactsolely
based onherownjudgment.Inparticular,verificationbiasdoesnotpreventlearninginthelimit.
This isbecausetheDM’ssamplingofthemachine’scorrectandwrongpredictionsisnotbiased
in thiscase.Indeed,theDMactsandthusverifiesthemachinewhen {SH
t = +}, regardlessofthe
realization of SM
t and hence,theprobabilitytoverifyisindependentofthemachine’sprescription
(conditional onthetask’stype).This,ineffect,relaxestheexploration-freeconditionbyrandomly
enabling learning(withprobability P(SH
t = +)),evenwhenthemachine’sprescriptionwouldhave
induced theDMnottoact(i.e.,when P(Θt =A|SH
t =+, SM
t =−, bt−1)<r) inequation(6).
Overall,thetheoremrevealsthatinappropriateoverridingdecisionsmayoccuronlywhenthe
machinebiasestheDM’sdecisionssincefulllearningoccursintheabsenceofDM-machineinter-
actions. Next,weexplorethecasewherethemachinefullybiasesthesedecisionsandhencethe
sampling ofobservations.
4.2. No-OverridingBenchmark
In ournextresult,wecharacterizetheDM’sabilitytolearnthemachine’stypewhentheDM’s
choicesarefullydeterminedbythemachine’sprescriptions.Inthiscase,thebiasofthemachine
on theDM’sdecisionisextreme.Specifically,weconsidertheDM’sasymptoticlearningbehavior
when equation(7) holdswith σ =M. Wethenhave
Theorem 2 (Learning withNo-Overriding). Unique thresholds pB and pW exist suchthat
pB <pW and,
6 The divergenceof Lt is duetothestronglawlargenumbers;see Gut (Theorem 8.3inp.68 2009) formoredetails.
14 de V´ericourtandGurkan: betterorworse
• when themachineisbetter(Γ = B), bt
a.s. −−→0 if p <pB, bt
a.s. −−→1 if p >pB; and bt is recurrent
and oscillatesif p=pB,
• when themachineisworse(Γ =W), bt
a.s. −−→0 if p <pW, bt
a.s. −−→1 if p >pW; and bt is recurrent
and oscillatesif p=pW.
Further,wehave pB ≜
¯ βB log

¯βW
¯βB

¯ βB log

¯βW
¯βB

+αB log

αB
αW
 and pW ≜
¯ βW log

¯βW
¯βB

¯ βW log

¯βW
¯βB

+αW log

αB
αW
.
In essence,Theorem 2 states thattheDM’sabilitytolearndependsonherprioraboutthetask
as wellasthemachine’stype.TheDMlearnsthatthemachineisworse(resp.,better)whenher
prior isbelow(resp.,above) pΓ for typeΓ ∈ {B,W}. Importantly,thismeansthattheDMmay
not properlylearnwhetherthemachineisbetterthanher.Indeed,whenprior p is low(p <pB),
the DMlearnsthatthemachineisworse(bt
a.s. −−→0), whilethemachineisactuallybetter(Γ= B).
Similarly,whenprior p is high(p>pW), theDMlearnsthatthemachineisbetter(bt
a.s. −−→1), while
the machineisactuallyworse(Γ=W).
Theorem 2 stems fromthefactthat,inthisbenchmark,theDMactsandobservesthemachine’s
correctness onlyifthemachine’ssignalispositive.Inotherwords,theDM’sobservationsare
sampled solelyfromtrueandfalsepositivepredictionsandneverfromtrueorfalsenegativeones.
Indeed, recallfirstthattheDM’sbeliefincreases(resp.,decreases)whentheDMobservesacorrect
(resp., incorrect)machinerecommendation.BecausetheDMisabletoobservethisonlywhen
the machine’ssignalispositive,theDMincreasesherbeliefifandonlyifthemachinecorrectly
prescribestoact(SM
t =+andΘt =A) anddecreasesherbeliefifandonlyifthemachinewrongly
prescribestoact(SM
t =+andΘt =NA). Butwhetherthetasktrulyrequiresanaction(i.e.,Θt =A)
is determinedbyprior p. Hence,theDMincreasesherbeliefmorefrequentlywhenthetaskis
more likelytorequireanaction,i.e.,prior p takeshighervalues.TheDM’sbeliefwillconvergeto
one (resp.,tozero)whenprior p is sufficientlyhigh(resp.low)suchthatthenumberofobserved
correct predictionsisrelativelyhigherthanthenumberofincorrectones.
This effectofprior p is absentfromtheno-interactionbenchmarkbecausetheDMalsoobserves
the correctnessofthemachine’spredictionswhenthemachinerecommendsnottoact(i.e.,when
SM
t =− and Θt =A, or SM
t =− and Θt =NA ).
Note finallythatmagnitudesofthesechangesinbeliefsdonotdependonprior p but are
determined bytheaccuracyparametersofthemachine.Threshold pΓ thuscorrespondstothe
break-evenvalueofprior p suchthattheexpectedincreaseinbeliefcompensatesfortheexpected
decrease whenthemachineisoftypeΓ.When p>pΓ, theexpectedincreasedominatestheexpected
decrease andthebeliefconvergestoone.When p<pΓ, theoppositeistrue,andthebeliefconverges
to zero.
de V´ericourtandGurkan: betterorworse 15
5. MainSet-up:AccountingfortheDM’sBeliefabouttheMachine
In ourmainset-up,andincontrasttothepreviousbenchmarks,theDM’sbeliefaboutthemachine
influences theDM’sdecisiontoact,andhenceherabilitytoverifythemachine’spredictions.As
a result,learningisendogenouslydeterminedbytheDM’scurrentbeliefaboutthemachine.This
fundamentallychangesthenatureofmislearning.
More specifically,recallthatduetoInformativeness,theDMalwaysdecidesaccordingtoher
signal whenitisconsistentwiththemachine’ssignalwith SH
t = SM
t . Whenthesesignalsdiffer
with SH
t ̸= SM
t , theDMmayoverridethemachinewhenhercurrentbelief bt−1 that themachine
is betterissufficientlylow.Hence,belief bt−1 influences posterior bt via thetwomechanismscap-
tured byequation(6). Thefollowingresultdetermineswhensuchoverridingdecisionsoccur.(The
result followsfromSubstitution(4)-(5) andthecontinuityoftheposteriorprobabilitiesin bt−1; see
Appendix B.)
Lemma 1. Unique thresholds b− ∈ (0, 1) and b+ ∈ (0, 1) exist suchthat
P(Θt =A|SH
t =+,SM
t =−, bt−1)≥r ⇔ bt−1 ≤b− , (8)
P(Θt =A|SH
t =−,SM
t =+, bt−1)≤r ⇔ bt−1 ≤b+ . (9)
Lemma 1 states thatwhentheDM’sjudgmentcontradictsthemachine’sprescription,i.e., SH
t ̸=
SM
t , theDMoverridesthemachineifandonlyifherbeliefinabettermachineissufficientlylow,
i.e., belowathreshold.However,theDMcanoverridethemachineintwodifferentways,depending
on whetherthemachineprescribestoactornot.Thisyieldstwodifferentthresholds b− and b+,
whichcorrespondtoanoverridingdecisionforanegativeandpositivemachinesignal,respectively.
These thresholdsactuallycorrespondtothevalueofbelief b that makestheDMindifferent
betweenactingandnotactingwhen SH
t = −,SM
t = +and SH
t = +,SM
t = −, respectively.Note
also thattherankingbetween b− and b+ dependsontheproblem’sparameters,andwedefine
the minimumandmaximumofthesetwothresholdsas bH ≜ min(b−, b+) and bM ≜ max(b−, b+),
respectively(where bH and bM can beequal).
Thus,whenbelief bt−1 is sufficientlylargewith bt−1 > bM, theDMhassufficientconfidencein
the machinetoalwaysfollowitsprescriptions;thiscorrespondstotheno-overridingbenchmark.
However,whenthebeliefissufficientlylowwith bt−1 < bH, theDMalwaysoverridesthemachine
and decidessolelybasedonherjudgment,whichcorrespondstotheno-interactionbenchmark.
Overall,thesetwocasesareconsistentwithSubstitution,whichstipulatesthatthemachineis
either betterorworsethantheDM.
Interestingly,Lemma 1 further revealsthattheDMmaytreatthemachine’sprescriptionas
complementing—insteadofsubstituting—herexpertise.ThisoccurswhentheDMissufficiently
16 de V´ericourtandGurkan: betterorworse
unsure aboutthemachine’stypewith bt−1 ∈ (bH, bM). Inthiscase,theDMandthemachinecomple-
mentoneanotherintwopossibleways,dependingonwhetherthreshold b− is largerorsmallerthan
threshold b+. If b+ <b−: theDMoverrulesthemachinewhenitssignalisnegativebutfollowsthe
machine’sprescriptionwhenitispositive.Inotherwords,theDMassumesthatshemakesbetter
positivebutworsenegativedecisionsthanthemachine.Inthissense,theDMandthemachine
collaborateonthetask.Asaresult,theDMactsifandonlyifeithertheDMorthemachinefind
evidence todoso(SH
t =+or SM
t =+).If b− <b+, however,theDMoverrulesapositivemachine’s
signal butfollowsanegativemachine’ssignalandthusactsifandonlyiftheDMandthemachine
agree thatanactionisrequired(SH
t =+and SM
t =+).
Overall,Lemma 1 indicates thattheDM’sabilitytolearnthetruetypeoftaskdependson
her currentbeliefaboutthemachine.Thismeans,inparticular,thattherandomjumpsofthe
correspondinglog-likelihoodratioalsodependonthecurrentratio.Formally,wehave
Lt =Lt−1 +RHM
t (Lt−1)
when theDMcanoverridethemachine.Incontrasttotheno-overridingbenchmark,therandom
jumps RHM
t are nolongeri.i.d.,astheirdistributionsnowdependonthemagnitudeof Lt−1. Thus,
the signoftheexpectedjump,whichdeterminestheasymptoticbehaviorofbelief bt, ispath-
dependent.Next,weexplorehowthisdependencyaffectstheabilityoftheDMtolearnthetrue
machinetype.
5.1. LearningWhentheMachineisBetter
WefirststudytheDM’sabilitytoproperlylearnthemachine’stypewhenthemachineisinfact
better.Ournextresultcharacterizesthesituationsinwhichmislearningoccursinthiscase.
Theorem 3. When themachineisbetter,i.e. Γ=B, if p ≤pB, then bt oscillatesandisrecurrent;
otherwise bt
a.s. −−→1.
ThustheDM’sabilitytolearnthemachine’stypecontinuestodependonwhetherherprior
aboutthetaskissufficientlyhigh.Infact,thethresholdcharacterizingwhenproperlearningoccurs
is theexactsameastheonewithoutoverriding(definedinTheorem 2). Specifically,theDMlearns
that themachineisindeedbetter(bt
a.s. −−→1) ifandonlyifprior p is sufficientlyhighwith p >pB.
Figure 1b illustratesthiscaseandexhibitsasymptoticbehavior.
The DM’sabilitytooverridethemachine,however,fundamentallychangesthenatureofmis-
learning. Indeed,whenprior p is suchthat p ≤pB, theDMwronglylearnsthatthemachineisworse
in theno-overridingbenchmark.Inthemainset-up,however,thebeliefoscillatesasillustratedin
Figure 1a. Additionally,becausethebeliefisalsorecurrent,theDMconstantlyswitchesamong
de V´ericourtandGurkan: betterorworse 17
overrulingthemachine(bt < bH), collaboratingwithit(bt ∈ (bH, bM)) orlettingthemachinedecide
(bt >bM), asstatedbythefollowingcorollary.
Corollary1. When themachineisbetter,i.e., Γ = B and p ≤ pB, intervals (0, bH], (bH, bM) and
[bM, 1) arerecurrentforbelief bt.
Hence, whentheDMsufficientlybelievesthatthemachineisbetter,sheneveroverrulesitand
weretrievethedynamicsoftheno-overridingbenchmark.Thatis,when bt >bM, learningisentirely
drivenbywhetherornotamachine’sprescriptiontoactiscorrect.Additionally,becauseprior p is
low,thefrequencyofthesecorrectpredictionsisalsolow,sothebeliefisdecreasinginexpectation.
In contrast,whentheDMsufficientlybelievesthatthemachineisworsewith bt <bH, shealways
overridesthemachine.Inthiscase,theDMsometimesobservesthemachine’saccuracyevenwhen
it prescribesnottoact.ThisoccurswhentheDM’ssignalispositiveandoverrulesamachine’s
negativesignal.Inthiscase,learningisdrivenbythetruemachinetype,andbecausethemachine
is actuallybetter,thebeliefincreasesinexpectation.
Overall,theresultholdsbecausetheDM’sbeliefinthemachine’stypenegativelyreinforces
her samplingbias:Whenthebeliefbiasesthesampleofobservations,theresultingupdatedbelief
tends todebiasthesampling—andviceversa.Asaresult,belief bt is pushedbackdownwardwhen
it reacheshighvalues(bt >bM) andispushedupwardwhenittakeslowvalues(bt <bH). Hence,the
DM neverfullylearnsthatthemachineisbetter,butduetooverriding,nevermislearnsthatitis
worseeither.Inthissense,theDMalwaysremainsinperpetualuncertaintyaboutwhetherornot
to disregardthemachine.
Interestingly,thislong-rununcertaintyinducestheDMtosometimestreatthemachine’spre-
scription asacomplementtoherjudgment.Thishappenswhenthebeliefreaches bt ∈ (bH, bM),
whichisarecurrentevent.Inthiscase,theDMandthemachineco-producethedecisionper
Lemma 1 (and theexplanationsthatfollow).BecausethemachineandDMareactuallysubsti-
tutes, theemergenceofthiscomplementarityisdrivenonlybytheDM’sinabilitytolearnthetrue
machinetype.
5.2. LearningWhentheMachineisWorse
PerTheorems 2 and 3, theDMproperlylearnsthatthemachineisgoodwhenprior p takeshigh
values(i.e., p >pB), whethertheDMcanoverridethemachineornot.Inthiscase,overriding
essentiallypreventstheDMfromwronglylearningthatthemachineisworse,whichcreatesa
perpetualstateofuncertainty.Incontrast,whenthemachineisindeedworseandtheDMcan
overruleit,theDMmaylearnitstruetypeforanyprior p. This,however,occursonlyrandomly
when prior p takeslowvalues,asstatedbythefollowingresult.
18 de V´ericourtandGurkan: betterorworse
Figure 1TheDM’sbelief bt when themachineisbetter, Γ =B
500 100015002000250030003500
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
H
(a) p ≤pB
500 100015002000250030003500
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
H
(b) p>pB
Note. αH = βH = 0.95, αB = βB = 0.99, αW = βW = 0.85, pB = 0.15 and r = 0.07, (a) p = 0.05, bH = 0.57, bM = 0.81, (b)
p = 0.2, bH = 0.01, bM = 0.96.
Theorem 4. When themachineisworse,i.e., Γ=W, if p ≤pW, then bt
a.s. −−→0; otherwise, bt
a.s. −−→X
where X is aBernoullirandomvariable.
Theorem 4 indicates thattheDM’sabilitytolearnhingesagainonprior p. Asintheno-overriding
benchmark,theDMcanproperlylearnthatthemachineisworse(b a.s. −−→0) ifprior p takeslow
values(p <pW, wherethreshold pW is, again,thesameasthatintheno-overridingbenchmark).
Figure 2a illustratesthispoint,anddepictsasamplepathof bt.
When thepriorishigh(p >pW), however,thebeliefconvergestoaBernoullirandomvariable.
That is,thesamplepathsofbelief bt convergetozerowithacertainprobabilityandtoonewith
the complementprobability.Inparticular,thebeliefneveroscillatesnorconvergestoaninterior
pointinthelimit.Thus,theDM’sabilitytoproperlylearnthemachine’stypeisrandominthis
case. Inparticular,theDMmaysometimeswronglylearnthatthemachineisbetter,whileitis
actually worse.Figure 2b illustrates thispointanddepictsexamplesofthetwopossiblesample
paths for bt, one(dashedline)convergingtooneandtheother(solidline)tozero.
Similar tothebettermachinecase,learningisdrivenbyprior p as intheno-overridingbenchmark
when thebeliefishigh(bt >bM), andbythetruetypeofthemachinewhenthebeliefislow(bt <bH).
In thelattercase,thebeliefdecreasesinexpectationsincethemachineisworse.
Thus,forlowprior p <pW, thebeliefmovesdownwardinexpectationwhenittakessufficiently
high orlowvaluesandhenceconvergestozerointhelongrun.TheDMthenproperlylearnsthat
the machineisworse.
Forhighprior p >pW, however,thebeliefincreasesinexpectationwhenthebeliefisalready
high anddecreaseswhenitisalreadylow.Inthelong-run,thebeliefisthuspushedclosetoeither
de V´ericourtandGurkan: betterorworse 19
Figure 2TheDM’sbelief bt when themachineisworse, Γ =W
100 2003004005006007008009001000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
M
H
(a) p ≤pW
100 2003004005006007008009001000
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
H
M
(b) p>pW
Note. αH = βH = 0.95, αB = βB = 0.99, αW = βW = 0.9, pW = 0.72 and r = 0.8, (a) p = 0.71, bH = 0.23, bM = 0.76, (b)
p = 0.75, bH = 0.38, bM = 0.68.
one orzero.Thisisbecause,incontrasttothecasewherethemachineisbetter,theDM’sbelief
positivelyreinforcesthesamplingbias:Whenthebeliefbiases(resp.,doesnotbias)thesample,
the resultingupdatedbelieftendstoperpetuatethebias(resp.,remainunbiased).Whetherthe
beliefreacheshighorlowvaluesisthendeterminedbyrealizationsofthedifferentsignalsandthe
task typesandisthusrandom.Notethatwhenthebelieftakesintermediaryvalues(bt ∈ (bH, bM))
it caneitherdecreaseorincreaseinexpectationdependingontheproblemparameters.However,
this regionistransientsincethebeliefispushedawayfromtheregionwhenthebeliefismore
extreme (bt /∈ (bH, bM)).
6. Implications
6.1. LearningandMislearning
Takentogether,theseresultsprovidetheoreticallimitsonourabilitytolearnwhetheramachine
makesbetterdecisionsthananexpert.Interestingly,thisinabilitytolearnsometimesinducesthe
DM totreatthemachine’sprescriptionasacomplementtoherownjudgment,eventhoughthe
twoareactuallysubstitutes.Forinstance,theDMmaybelievethatherpredictionshavebetter
sensitivitybutworsespecificitythanthoseofthemachine,whileinfact,themachineisbetterin
terms ofbothaccuracymetrics.Inthissense,theDM’suncertaintyaboutthemachineprovidesa
novelrationaleforwhyexpertsandmachinesmaycollaborateinpractice.
Our resultsfurtheridentifytheuncertaintysurroundingthedecisiontaskasthekeyfactorfor
mislearning. Infact,theDMfailstolearnwhensheismostcertainaprioriaboutwhetheran
action isrequiredforatask(i.e.,whenprior p takesmoreextremevalueswith p ≤ pB or p >pW).
20 de V´ericourtandGurkan: betterorworse
Conversely,theDMalwaysproperlylearnsthemachine’stypewhensheismostuncertainabout
whether ornottoact(i.e.,prior p takesmoderatevalues),asstatedbythenextcorollaryof
Theorems 3 and 4.
Corollary2. The DMalwayscorrectlylearnsthetypeofthemachineifandonlyif p ∈ (pB, pW].
6.2. LearningwithAnticipation
In ourset-up,asintheliterature,theDMupdatesherbeliefusingthepasthistoryoftheobserved
accuracy ofthemachine’spredictions.Nonetheless,ourresultscharacterizetheasymptoticbehavior
of thislearningprocessand,assuch,provideguidelinesforDMswhoanticipatethefuturebehavior
of theirownbelief.Inparticular,thenatureofalearningfailureisindicativeofthemachine’stype
in ourresults.TheDMmaythusleveragethisinformationtodeterminewhetherthemachineis
better.
Indeed, theDM’sbeliefmayoscillateonlywhenthemachineisbetter(seeFigure 1a), and
alwaysconvergeswhenitisworse(seeFigure 2). Thus,thelongertheDMremainsuncertain(in
the senseofTheorem 3), themorelikelythemachineisactuallybetter.Similarly,theDM’sbelief
can convergetozeroonlyifthemachineisworse.Indeed,thebeliefeitheroscillatesorconverges
to onewhenthemachineisbetter.Hence,thelongertheDMbelievesthatthemachineisworse,
the morelikelysheiscorrectinherassessment.
Assessing iftheDMiscorrectwhensheincreasinglybelievesthatthemachineisbetterappears
to bemorechallenging.Indeed,theDM’sbeliefcanconvergetoonewhetherthemachineisbetter
(see Figure 1b) orworse(seeFigure 2b). Tocircumventthisissue,oneapproachconsistsinrelying
on morethanonedecisionmaker.Toseehow,considerseveralidenticaldecisionmakerswho
independentlyhandleaseriesoftasksthatarerandomlydrawnfromthesamesampleanduse
the samemachine.Ifthismachineisbetter,allDMsshouldhavethesamelimitingbehavior:they
either allremainuncertainoralllearnthatthemachineisindeedbetter(perTheorem 3). However,
if themachineisworse,theconvergencetoeitherzerooroneisrandom(perTheorem 4). Thus,if
a singleDMintheteambelievesovertimethatthemachineisworse,thenthemachineisindeed
likelytobeworse—eveniftherestoftheteambelievesittobebetter.Incontrast,ifthereis
consensus intheteamthatthemachineisbetter,thenthelargertheteamis,themorelikelyitis
that themachineisbetter.
In short,long-termuncertaintyoraunanimousbeliefamonglargeteamsthatthemachineis
betterisindicativeofabettermachine.Incontrast,persistentlyoverrulingthemachineisindicative
of aworseone.
de V´ericourtandGurkan: betterorworse 21
6.3. AdoptionorRejection
Our studyalsoshedslightsonthedecisiontofullyadoptorrejectthemachine.Indeed,after
observing andattimesoverridingthemachine’sprescriptions,theDM’sbeliefmayreachextreme
levels.Inthesecases,theDMdecideseithertoletthemachinemakeallthedecisions(asin
Section 4.2), ortoabandonthemachinealtogether,dependingonwhetherthebeliefissufficiently
high orlow,respectively.Onceamachineisabandoned,however,theDMcannotlearnaboutit
anymore.
If theDMdecidestofullyadoptthemachine—butcontinuestoobserveitsperformance—our
results indicatethattheDMwillbecomeincreasinglyconfidentaboutheradoptiondecisionover
time whenprior p aboutthetaskishigh(p>pB for abettermachine,and p>pW, foraworseone
perTheorems 3-4). Thisoccursevenwhenthemachineisactuallyworseandshouldbeabandoned.
In contrast,whentheprioraboutthetaskislow(p <pB or p <pW, dependingonthetrue
machinetype),theDMincreasinglydoubtsheradoptiondecisionovertime.Thisisbecausethe
DM’s beliefinabettermachinedecreasesinexpectationovertimeandalwaysapproaches0inthe
limit inthiscase(perTheorems 3-4). Thishappensevenwhenthemachineisactuallybetterand
should beadopted.
Recall, finally,thatwhenthemachineisbetterandtheprioraboutthetaskislow,theDM’s
beliefinthemainset-uposcillatesandisrecurrentin(0,1)(perTheorem 3 and Corollary 1).
Therefore, theDM’sbeliefeventuallyreachesanylowlevelwithprobabilityone.Inotherwords,
the DMalwaysendsupabandoningthemachineinthelongrun,eventhoughthemachineis
actually better.
7. MistrustBiasesagainsttheMachine
A keyfeatureofthemislearningbehaviorinTheorems 3-4 is thattheydonotstemforman
inherentmistrustagainstthemachine.Instead,theystemfromfourfundamentals(verification
bias, exploration-freedecisions,informativenessandsubstitution),whichcharacterizetheset-up
in whichtheDMworkswiththemachine.Nonetheless,theDMmayalsobesubjecttomistrust
biases againstthemachineinsituationswherethesefundamentalsareatplay.Inthissection,we
explore towhichextentbiasessuchastheseinteractwithourfourfundamentalstoaffecttheDM’s
learning behavior.
In ourmainset-up,mistrustingthemachineaffectstheDM’sabilitytolearninatleasttwo
ways.First,theDMmaydownplaythemachine’sprescriptionwhendecidingtoact,whichalters
the DM’sabilitytoobservethecorrectnessofthemachine’spredictions.Second,andinlinewith
the algorithmaversionreportedby Dietvorstetal. (2015), theDM’sbeliefinthemachinemay
disproportionatelydropuponobservingthefailureofamachine’sprediction.Inthefollowing,we
inspectthesedifferentbiasesinturn.
22 de V´ericourtandGurkan: betterorworse
7.1. MistrustingtheMachineWhenDeciding
The DM’smistrustinthemachinemayaffectthewaysheweightsthemachine’sprescription
when decidingtoact.Thisisconsistentwiththedecision-makingliterature,whichfindsthat
individuals tendtodiscountinformationcomingfromexternalsourcesandoverweighttheirown
opinions (seeforinstance, Soll andMannes 2011). Toaccountforthispossibility,wefollow Stone
(1961), whoproposesanon-Bayesianapproachtorepresenttheaggregationofdifferentopinions.
This approachiscommonlyusedtomodelmistrustbias( ¨ Ozer andZheng 2018), inparticularwhen
a humanmakesdecisionsbasedontheinputofadata-drivenalgorithm(Ahsen etal. 2019, Boyaci
et al. 2020). Specifically,theDM’supdatedbeliefthatanactionisrequiredgiventheDM’sand
machine’ssignalsisalinearcombinationbetweentheupdatedbeliefofthehumanandthatofthe
machine.Moreformally,theDM’sposteriorbeliefaboutthetaskisdefinedas
˜Pλ(sH, sM, bt−1)≜λP(Θt =A|SH =sH)+(1−λ)P(Θt =A|SM =sM, bt−1) (10)
where λ ∈ (0, 1) representstheDM’smistrustbiasagainstthemachine’ssignal.Thehigherthe
valueof λ is, themoretheDMmistruststhemachine.Belief ˜Pλ(SH,SM, bt−1) correspondsthen
to theposteriorprobability P(Θ = A|SH,SM, bt−1) derivedfromBayes’ruleinthemainset-up.In
particular, theDMdecidestoactifandonlyif ˜Pλ(sH, sM, bt−1)≥r.
In thisset-up,theDMalwaysoverrulesabettermachine(neveroverrulesaworsemachine)if
the biasistoohigh(resp.,toolow).Intheseextremesituations,theDMandthemachinenolonger
substitute oneanother.Wethusrestrictouranalysistomoderatevaluesofmistrustparameter λ,
as formalizedbythefollowinglemma,where ˜PB
λ(sH, sM)≜ ˜Pλ(sH, sM, 1) and ˜PWλ
(sH, sM)≜ ˜Pλ(sH, sM, 0)
representtheDM’sbeliefsaboutthetask’stypewhenthemachine’stypeisknown.
Lemma 2. Two thresholds λmin and λmax exist suchthatif λ ∈ (λmin,λmax), then
˜PB
λ(SH
t =+,SM
t =−)<r and ˜PB
λ(SH
t =−,SM
t =+)≥r , (11)
˜PW
λ (SH
t =+,SM
t =−)≥r and ˜PW
λ (SH
t =−,SM
t =+)<r. (12)
In thefollowing,wefocuson λ ∈ (λmin,λmax) toensurethatSubstitutionpersistsinthepresence
of mistrustbias.Thenexttheoremshowsthatundertheseconditions,thestructureofourmain
results continuetohold.
Theorem 5. Assume λ ∈ (λmin,λmax).
• When Γ=B, if p ≤pB, then bt oscillatesandisrecurrent;otherwise, bt
a.s. −−→1.
• When Γ = W, if p ≤ pW, then bt
a.s. −−→0; otherwise, bt
a.s. −−→X where X is aBernoullirandom
variable.
de V´ericourtandGurkan: betterorworse 23
Thus,theDM’slearningbehaviorcharacterizedinTheorems 3-4 doesnotchangeoverallifthe
DM isbiasedagainstthemachine’sprescriptionwhenmakingadecision.NotealsothatCorollary 1
continuestoholdinthiscasebutwiththresholds b− and b+ dependingon λ (see Lemma 5 in the
appendix).
7.2. MistrustingtheMachineWhenUpdatingBelief
The DM’smistrustagainstthemachinecanalsoaffectthewaytheDMupdatesherbeliefaboutthe
machine. Dietvorstetal. (2015), forinstance,experimentallyshowthatindividualsaremorelikely
to ignorealgorithm-basedpredictionsafterobservingthesealgorithmserr.Moregenerally,the
observationofnegativeoutcomes,suchasapredictionfailure,morestronglyimpacttheformation
of anindividualimpressionthandopositiveones—aphenomenonreferredtoasnegativitybiasin
the literature(Baumeister etal. 2001).
Toaccountforthisbias,wefollowtheliterature(seeforinstance Coutts 2019, M¨obiusetal. 2022)
and allowupdatedbelief bt to dropsignificantlyuponobservinganincorrectmachineprediction.
Specifically,theDMupdatesherbelieffollowingBayes’rulewhenthemachineiscorrectbut
magnifies thedecreaseinbeliefwhenthemachineiswrong.Moreformally,wehave
bt =


bt−1 if P(Θt =A|SH
t =sH, SM
t =sM, bt−1)<r 
1+
¯b
t−1
bt−1

PW(SM
t =sM |Θt =θ)
PB(SM
t =sM |Θt =θ)
ϕ(sM, θ)


−1
if P(Θt =A|SH
t =sH, SM
t =sM, bt−1)≥r ,
where function ϕ(sM, θ)=μ>1 ifthemachineisincorrect(i.e.,for sM =+and θ =NA or sM =− and
θ = A) and ϕ(sM, θ) =1otherwise.Becauseratio PW(SM
t |Θt)/PB(SM
t |Θt) > 1 whenthemachineis
incorrect, thehigherthevalueofmistrustparameter μ is, thelowerbelief bt becomes.Inparticular,
the mainset-upcorrespondsto μ=1,whichcoincideswithBayes’rule.
The nextresultcharacterizestheasymptoticbehavioroftheDM’sbeliefinthepresenceofthis
negativitybias.
Theorem 6 (Learning withNegativityBias). Unique thresholds μB, μW, μH exist suchthat
• when themachineisbetter(Γ=B),
—if μ ≥μB and μ>μH, then bt
a.s. −−→0.
—if μB >μ>μH, then bt
a.s. −−→X where X is aBernoullirandomvariable.
—if μH ≥μ ≥μB, then bt is recurrentandoscillates.
—if μB >μ and μH ≥μ, then bt
a.s. −−→1.
• when themachineisworse(Γ=W),
—if μ ≥μW, then bt
a.s. −−→0.
—if μW >μ, then bt
a.s. −−→X where X is aBernoullirandomvariable.
24 de V´ericourtandGurkan: betterorworse
Further,wehave
μB ≜
pαB log

αB
αW

¯p ¯ βB log

¯βW
¯βB
, μW ≜
pαW log

αB
αW

¯p ¯ βW log

¯βW
¯βB
 and μH ≜
pαHαB log

αB
αW

+ ¯p ¯ βHβB log

βB
βW

pαH ¯αB log

¯αW
¯αB

+ ¯p ¯ βH ¯ βB log

¯βW
¯βB
 >1 .
Note thatthresholds μB and μW actually playthesameroleas pB and pW in Theorems 3 and 4,
respectively.Indeed,thresholds pB
μ and pWμ
exist suchthat μ >μΓ ⇔ p <pΓμ
, forΓ= {B,W}. In
particular, thestructureoftheresultswhenthemachineisworse(seeTheorem 4) doesnotchange
in thepresenceofnegativitybias.Inthiscase,mistrustparameter μ affects thelearningonly
through thevalueofthreshold μW and, hence, pWμ
.
When themachineisbetter,however,thepresenceofmistrustchangesthestructureofthe
results. Inparticular,undermoderatemistrustsuchthat μB > μ>μH, theDMlearnsthatthe
machineisbetteronlywithsomeprobability(secondpointinTheorem 4). Thisisincontrasttothe
main set-upwithoutmistrust,inwhichtheDMalwayslearnsthatthemachineisbetterif p>pB.
In fact,theDM’sbeliefcanconvergetoaBernoullirandomvariableinourmainset-uponlywhen
the machineisworse.Ifthemistrustinthemachineistoostrongwith μ>max(μH,μB) (firstpoint
of thetheorem),however,theDMalwayswronglylearnsthatthemachineisworse.Otherwise,the
bias doesnotalterthelearningbehavior.Infact,Theorem 6 reduces toTheorems 3-4 when μ=1.
In thiscase,wehave μH > μ = 1andthebeliefeitherconvergestooneoroscillatesdependingon
whether μB ≤μ=1ornot,whichisequivalentto pB ≥p.
Overall,mistrustintheformofanegativitybiasinteractswithourfundamentalsinameaningful
wayonlywhenthelevelofmistrustismoderateandthemachineisactuallybetter.Inthiscase,
whether theDMlearnsthetruenatureofthemachinebecomesrandom—whiletheDMalways
properlylearnsthatthemachineisbetterwhensheisnotbiasedagainstthemachine.
8. Complementarity
Thusfar,wehavefocusedonsettingsinwhichthemachineandtheDMaresubstitutes.Nonethe-
less, ourframeworkalsoappliestothecaseofcomplementarity,whichcantakedifferentforms.
In thissection,weexplorethelearningbehaviorofaDMwhouncovershowamachinemay
complementherownjudgment.
In ourset-up,onlytwopossiblewaysactuallyexistbywhichthemachineandtheDMcomple-
mentoneanother.Indeed,amachinethatcomplementstheDMissuperiorinonlyoneofthetwo
dimensions ofajudgment—positiveornegativesignals—andinferiorintheother.Thus,thefirst
form ofcomplementaritycorrespondstoaDMwhoalwaysoverridesthemachinewhenherjudg-
mentindicatesthatanactionisrequiredbutalwaysfollowsthemachine’sprescriptionifshefinds
de V´ericourtandGurkan: betterorworse 25
that sheshouldnotact.Theconverseholdsforthesecondform:theDMoverridesthemachine
when herjudgmentindicatesnottoact,butalwaysfollowsthemachineotherwise.
Wedenotethesetwomachinetypesas C+ and C−, respectively,andtheirsensitivityandspecificity
are αΓ and βΓ for Γ ∈ {C+,C−}. Inthissection,westudytheDM’slearningbehaviorinthesame
settings asourbasemodel,exceptforthesubstitution(4)-(5), whichwereplacebythefollowing
complementarityconditions.
Complementarity:
PC+(Θ=A|SH =+,SM =−)<r and PC+(Θ=A|SH =−,SM =+)<r (13)
PC−(Θ=A|SH =+,SM =−)≥r and PC−(Θ=A|SH =−,SM =+)≥r (14)
where PC+{·} and PC−{·} denote theprobabilitymeasuresinducedbythetwotypesofthemachine.
The DMdoesnotknowthemachine’stypeapriori.However,sheformsabeliefovertimeabout
whichtypeofcomplementaritysheisfacing.Withaslightabuseofnotation,wereferto bt as the
DM’s priorbeliefthatΓ= C+. ThenextresultthencharacterizestheDM’sabilitytolearnhow
the machinecomplementsherjudgment.
Theorem 7. We have,
• when Γ=C+, then bt
a.s. −−→1,
• when Γ = C−, auniquethreshold pC exists suchthat bt
a.s. −−→ 1 if p ≤ pC; otherwise bt
a.s. −−→ X
where X is aBernoullirandomvariable.(Threshold pC is definedin (95) in Appendix D.)
Thus,theDMalwaysproperlylearnsthemachine’stypewhentheactualformofcomplementarity
is C+. Incontrast,theDMcanmislearnhowthemachinecomplementsherjudgmentwhenthe
true typeis C− and theprioraboutthetaskishigh(i.e., p >pC). Inthiscase,learningisrandom
and theDMwronglylearnswithpositiveprobabilitythatthemachineisoftype C+.
This resultisakintoTheorem 4 when themachineandDMaresubstitutes.IncontrasttoThe-
orem 3, however,theDM’sbeliefneveroscillatesandalwaysconvergestoeitherzeroorone.Thus,
in thelimit,theDMisalwayscertainoftheformofcomplementaritythatthemachineprovides.
In particular,theDMneverbehavesasifthemachineandDMweresubstitutes.Again,thisis
in contrasttoourmainset-up,inwhichtheDM’sdecisionssometimesexhibitcomplementarity,
while infact,theDMandthemachinearesubstitutes(seeCorollary 1).
Note finallythatconditions(13)-(14) correspondtoacomplementaritybetweenthemachine’s
and theDM’ssignals.Otherformsofcomplementarity,however,exist.Inparticular,theDMmay
seek touncoverforwhichdecisiontasksthemachineisbetterandforwhichonestheDMis.Inthe
contextofbiopsies,forinstance,thiscorrespondstounderstandingforwhatkindsofpatientsthe
26 de V´ericourtandGurkan: betterorworse
machinedoesbetterandforwhatkindsofpatientsitdoesworse.Apossibleapproachtostudythis
case istoconsiderourmainset-upbutwithmorethanonetypeofdecisiontask.Denotethistype
as T, withthreshold rT and prior pT for T ∈ {T1,T2, ... }. Thesedifferenttypesmaycorrespondto
differentkindsofpatients,forinstance.TheDMthenformsdifferentbeliefs bTt
overtimesothat
the findingsofSection 5 independentlyapplytoeachtask’stype T ∈ {T1,T2, ... }. Withmultiple
task types,thesefindingscharacterizewhentheDMwronglylearnsthedecisiontasksforwhich
the machine’spredictionsaresuperior,andtheonesforwhichherownjudgmentisbetter.
9. PartialRelaxationoftheVerificationBias
In thissection,weexploretherobustnessofourresultswhentheverificationbiasispartially
relaxed, whichislegitimatewhenthebiasstemsfromtheDM’slimitedattention.Inthiscontext,
the DMalsolearnsfromunverifiedcasesandupdatesherbeliefbasedonthemachine’s(andher
own)signalwhenshedoesnotact.Becauseofsalienceeffectsandinattentionalblindness,however,
the DMassignsrelativelylessweighttothisunverifiedinformationcomparedtoinformationbased
on averifiedcase,forwhichthetruestateisrevealed.
Formally,weconsiderinattentionalblindnessparameter ε ∈ [0, 1], suchthattheDM’sbelief bt−1
is updatedto bt, asfollows
bt =



1+
¯b
t−1
bt−1

PW(SM
t =sM,SH
t =sH)
PB(SM
t =sM,SH
t =sH)
ε −1
if P(Θt =A|SH
t =sH, SM
t =sM, bt−1)<r

1+
¯b
t−1
bt−1
PW(SM
t =sM |Θt =θ)
PB(SM
t =sM |Θt =θ)
−1
if P(Θt =A|SH
t =sH, SM
t =sM, bt−1)≥r.
(15)
Here, ε representshowlesssalientunverifiedinformationiscomparedtoverifiedinformation.7
The higherthevalueof ε is, themoresensitivetheDMistotheinformativenessofthemachine’s
signal foranunverifiedcasecomparedtoaverifiedone.Theverificationbiasisfullyrelaxedand
properlearningoccurswhen ε=1perProposition 1 in Appendix E.8 By contrast,ourmainset-up
correspondsto ε=0.Thenexttheoremshowsthatourresultscontinuetoholdwhen ε is positive
but sufficientlylow.
Theorem 8. Unique thresholds εB and εW exist suchthat
• when themachineisbetter (Γ = B), if ε ≤ εB and p <pB, then bt oscillatesandisrecurrent;
otherwise, bt
a.s. −−→1.
• when themachineisworse (Γ=W), if ε<εW and p>pW, then bt
a.s. −−→X where X is aBernoulli
randomvariable;otherwise, bt
a.s. −−→0.
7 Toseethis,consideraset-upwithtwodifferentabsoluteweightsfortheverifiedandunverifiedcases,say ωv and
ωu, respectively.Thisset-upisequivalenttotheoneinSection 9 bytaking ε = ωu/ωv.
8 This propositionisconsistentwiththefrequentistconsistencyofBayesianupdating(see,e.g., Diaconis andFreedman
1986), whichimpliesperfectlearningwhentheverificationbiasisfullyrelaxedwith ε = 1.
de V´ericourtandGurkan: betterorworse 27
Thresholds εB and εW are,respectively,definedin (98) and (99) in Appendix E, and pB and pW are
in Theorem 2.
Theorem 8 correspondstoTheorems 3-4 with theadditionalconditionthat ε is lessthan εΓ for
Γ ∈ {B,W}, respectively.Inparticular,whentheunverifiedcasesaresufficientlylesssalientthan
the verifiedoneswith ε<min(εB, εW), ourmainresultsalwayshold.
10. Conclusion
This paperproposesaframeworkinwhichamachineperformsrepeateddecisiontasksunderthe
supervisionofaDM.Inthisset-up,wefullycharacterizetheevolutionoftheDM’sbeliefabout
the machineandoverrulingdecisionsovertime.Wefindthatmislearningcantaketworadically
differentforms:aconstantchangeofmind(oscillationoftheDM’sbeliefperTheorem 3) anda
chanceofbeingpersuadedthatthemachinehasthewrongaccuracylevels(convergenceofthe
belieftoaBernoullivariableperTheorem 4). ThiscontrastswiththeconvergenceoftheDM’s
belieftoaninteriorpointin(0, 1), whichisoftenfoundinthedynamiclearningliterature(seee.g.,
confounding beliefsin Harrison etal. 2012). Thisanalysisalsoprovidesanovelexplanationforthe
jointproductionofdecisionsbymachinesandexpertsandsuggestsseveralguidelinesforadopting
or abandoningamachine.
The differentformsofmislearningweuncoverinthispaperstemfromtheinteractionbetween
the DM’sbeliefinthemachineandherdecisiontoact,whichinturndetermineshersampling
of correctandincorrectmachinepredictions.Thebeliefandresultingsamplingbiasinteractina
negativefeed-backloopwhenthemachineisbetter,whilethefeed-backloopispositivewhenthe
machineisworse.
These learningfailuresdonotarisefromanintrinsicmistrustbiasagainstmachine-basedpre-
dictions, suchasalgorithmicaversion.Rather,theystemfromtheproblemoflearningabouta
machinewhileactuallyusingitspredictionstomakehigh-stakedecisions.Wecapturethekeyfea-
tures ofthisproblemwithfourfundamentals:informativeness,substitution,verificationbiasand
exploration-free decisions.
Of thesefour,thelasttwoconditionsarecrucialforourfindings.Indeed,theDMalwaysprop-
erly learnsthetruenatureofthemachinewhentheverificationbiasissufficientlyrelaxed(per
Theorem 8). Similarly,ourno-interactionbenchmarkcorrespondstoapartialrelaxationofthe
exploration-free condition,whichalsoinducesproperlearning(seeTheorem 1). Incontrast,we
find thattheDMsometimesrandomlyfailstolearnthemachine’saccuracywhenitspredictions
complementtheDM’sjudgment(seeTheorem 7). Wefurtherexpectmislearningtooccureven
when someofthesignalsarenotinformative,althoughtheproblemcanbecomedegenerateinthis
case (whennoneofthesignalsareinformative,forinstance).
28 de V´ericourtandGurkan: betterorworse
Wealsorestrictouranalysistotwopossiblemachinetypes,mostlyforsimplicitybutourframe-
workcanbeextendedtoaccountformore,possiblycontinuoustypes.Ourresultsshouldnotchange
overallaslongasthepreviousfundamentalshold.Indeed,theDM’sbeliefthatthemachineout-
performsherexpertiseiswhatfundamentallymatterswhendecidingtooverridethemachine.This,
in essence,dividesthedifferentpossiblemachinetypesintotwodistinctpartitionsdependingon
whether ornotthetypeisbetterthantheDM.Inthissense,weretrieveasetupwithtwo—albeit
more convoluted—machinetypes.
Eventhoughweassumethemaway,aDMmaynonethelessbesubjecttomistrustbiasesagainst
the machineinourset-up.Ourresultsindicatethatthesebiasescaninteractwithourresultsina
significantway.Inparticular,thepresenceofmistrustbiasakintoalgorithmaversionsometimes
randomizes theDM’sabilitytoproperlylearnthetruenatureofthemachine.Theseresultsalso
providenovelhypothesesthatfutureexperimentalresearchcantest.
Wefocusonmistrustbiasesinthispaper,butourframeworkcanpotentiallyaccommodateother
typesofbiasessuchasoverconfidenceandlossaversion(Benjamin 2019). Further,ourframework
can potentiallyaccountforsituationsinwhichtheDMdoesnotperfectlyknowherownaccuracy,
or hasamisspecifiedrepresentationofthemachine(Fudenbergetal. 2017).Alternatively,the
machinemayprovidepartialexplanationsforthemachine’sprescription,whichmayhelptheDM
to learnthetruemachineaccuracy(see,e.g., Puranam andTsetlin 2021, forawaytomodel
explainability).
Note finallythatourframeworkmayalsobeappliedtosituationswhereanexpertsupervises
another expertinsteadofamachine.Doingso,however,requiresassumingthatexpertslearnthe
levelofexpertisesolelybyobservingtheexpostaccuracyofsomeone’sjudgments.Whilethis
precise settingmayexist,expertssuchasradiologiststypicallyprovidearationaleorcausalexpla-
nation tojustifytheirprescriptions.Theseexplanationsarealsoindicativeofsomeone’sknowledge
and expertise.Inotherwords,ahumanexpertcanmoredirectly,andapriori,assessthequalityof
someone’s judgmentinawaythatisdifficultwithanMLalgorithm(see,e.g., Cukier etal. 2021
for moreonthedifferencebetweenmachine-basedpredictionsandhumancognition).Inthissense,
our frameworkisbettersuitedforandoffersafruitfulapproachtoexploringtheissueoflearning
whether humanexpertiseshouldoverrulemachine-basedprescriptions.







----------------------------------------------------------------------------------






Grand-Clément, J., & Pauphilet, J. (2024). The best decisions are not the best advice:
Making adherence-aware recommendations. Management Science.

Abstract
Many high-stake decisions follow an expert-in-loop structure in that a human operator receives recommendations
from an algorithm but is the ultimate decision maker. Hence, the algorithm’s recommendation may
differ from the actual decision implemented in practice. However, most algorithmic recommendations are
obtained by solving an optimization problem that assumes recommendations will be perfectly implemented.
We propose an adherence-aware optimization framework to capture the dichotomy between the recommended
and the implemented policy and analyze the impact of partial adherence on the optimal recommendation.
Our framework provides useful tools to analyze the structure and to compute optimal recommendation policies
that are naturally immune against such human deviations, and are guaranteed to improve upon the
baseline policy.

Key words : Expert-in-the-loop systems; Prescriptive analytics; Recommender systems; Discretion; Markov
Decision Processes

1. Introduction
While some decisions can be automated and made directly by algorithms based on artificial intelligence
(AI), many high-stake decisions follow an expert-in-loop structure in that an expert decision
maker (e.g., a doctor) receives information, predictions, or even recommendations, and decides
which course of action to follow. Consequently, the human decision maker (DM) does not systematically
implement what the algorithm recommended. In other words, they may have a discretionary
power to override/reject the recommendations from the algorithm, hence impacting the potential
benefits from the AI tool. For instance, in a field experiment, Kesavan and Kushwaha (2020)
1

2
observed that merchants overrode the recommendations from a data-driven decision tool 71.24%
of the time, resulting in a 5.77% reduction in profitability.
To understand this phenomenon and its ultimate impact on the quality of the decision being
made, a growing body of literature has investigated the mechanisms driving non- or partial adherence
of humans to algorithmic recommendations. In this work, we ask a complementary question:
Given the fact that the decision maker will partially implement recommendations made by an
algorithm, should we adjust these recommendations in the first place and how? In other words,
we investigate the impact of partial adherence on algorithm design and decision recommendation.
Our main contributions are as follows.
A new model of partial adherence. We consider a model of sequential decision-making based
on Markov decision processes (MDPs) and assume that the decision maker currently follows a
baseline policy πbase (or state of practice) and is provided with a recommendation policy πalg by an
algorithm. We propose a framework, namely adherence-aware MDP, to compute recommendations
that are immune against human deviations. Our framework is behavioral in that it models the
human switching behavior between their baseline policy and the algorithmic recommendations,
but without specifying why these deviations are undertaken by the DM. Despite its simplicity,
we show that our model is consistent with five different models for the DM’s adherence decision,
including random or adversarial adherence decisions. Furthermore, we provide examples where the
co-existence of the human DM and the algorithmic recommendations performs either strictly worse
or strictly better than any of the two policies alone, hence illustrating the ability of our model to
capture the rich range of situations observed in practice. In particular, we show that (even rare)
human deviations from algorithmic recommendations can lead to arbitrarily poor performance
compared with both the expected performance of the algorithm and that of the current state of
practice. In other words, we show that deploying a recommendation engine that was designed
assuming its recommendations will be final decisions can have a dramatic impact on the effective
performance. This set of negative results underscores the importance of accounting for the current
baseline and the partial adherence phenomenon when building recommendation systems.
3
A tractable, structured, and flexible model. We study the appealing structural and computational
properties of our adherence-aware MDP framework. In particular, we show that an optimal
recommendation policy may be chosen stationary and deterministic, which is important from an
implementation standpoint, and that it may be computed efficiently by a reduction to a classical
MDP problem. We also show several structural properties, such as piecewise constant optimal
recommendation policy and monotonicity of the optimal return (both as regards the adherence
level). We identify classes of MDPs for which the decision maker may overlook the issue of partial
adherence at some states (i.e., where the partial adherence phenomenon has no impact on the algorithmic
recommendation to be made). We finally present extensions of our framework, including
models where the adherence levels are state-dependent, action-dependent, uncertain, or where the
baseline policy is not entirely known.
Numerical study. We evaluate the practical impact of our model on a series of numerical experiments.
Our simulations highlight the importance of accounting for the potential non-adherence of
the decision maker, showing empirically that severe performance deteriorations can happen when
partial adherence is overlooked in the search for an optimal policy. The magnitude of this performance
deterioration depends both on the current baseline policy and on the level of adherence of
the decision maker. Consequently, in addition to classical sensitivity and robustness analyses used
in the literature, we encourage practitioners to conduct a systematic adherence-robustness analysis
of their algorithms to assess their effective performance prior to deployment.
The rest of the paper is organized as follows: We present related work from the operations
literature in Section 2. Section 3 introduces our framework for sequential decision-making under
partial adherence, discusses its connection with various models for the DM’s adherence decision,
and provides examples of situations where the co-existence of human and algorithmic decisions leads
to improved or, on the contrary, impaired system performance. In Section 4, we present algorithms
to compute optimal recommendation policies, and we analyze their structural properties and their
sensitivity to the adherence level. We illustrate the practical impact of imperfect adherence and
the value of our framework on numerical experiments in Section 5. Finally, we discuss extensions
of our framework in Section 6.
4
2. Literature review
Our paper contributes to the rich literature of behavioral operations that studies the partial adherence
of decision makers to machine recommendations. This phenomenon is also referred to in the
literature as discretion, overriding, or deviation.
Many field studies have documented this phenomenon in a wide range of tasks and industries
such as demand forecasting (Fildes et al. 2009, Kremer et al. 2011, Kesavan and Kushwaha 2020),
warehouse operations (Sun et al. 2022), medical treatment adherence (Lin et al. 2021), or task
sequencing (Ibanez et al. 2018). Actually, partial adherence also occurs when the recommendation
does not come from a machine. In the context of chronic diseases, for instance, the World Health
Organization (WHO) defines adherence as “the extent to which a person’s behavior-taking medication,
following a diet, and/or executing lifestyle changes corresponds with agreed recommendations
from a health-care provider” (Sabat´e 2003). The WHO notes that adherence of the patients to
therapy for chronic illnesses is as low as 50 % in the long-term, and that this partial adherence leads
to suboptimal clinical outcomes. To anticipate its potential impact on operational performance,
it is important to understand the drivers of partial adherence, such as information asymmetry or
algorithmic aversion.
In the context of operations, assuming that humans have more and better information than
the machine, deviations due to information asymmetry can be beneficial to effective performance.
In an inventory management setting, Van Donselaar et al. (2010) conclude that providing store
manager discretion may result in higher profits due to their superior information. In a field experiment
with an automotive replacement parts retailer, Kesavan and Kushwaha (2020) evaluate that
merchants overriding demand forecasts increases (resp. decreases) profitability for growth- (resp.
decline-) stage products, suggesting that the information advantage of merchants increases when
the machine has limited access to historical data on the product. However, on average, they observe
a negative effect of human overriding power. Similarly, Fildes et al. (2009) document the heterogeneous
impact of human adjustment on prediction accuracy, depending on the company but also
5
the magnitude and direction of the adjustment. In another context, Sun et al. (2022) study the
box size recommendation algorithm of Alibaba. Since the algorithm ignores the foldability and
compressibility of the items, they observe that warehouse workers are able to pack some orders in
smaller boxes than the ones recommended.
Partial adherence can also result from multiple conflicting objectives that are weighted differently
by the human and the algorithm. In Alibaba’s warehouses for instance, Sun et al. (2022) hypothesize
that workers switching to larger boxes might do so to save packing effort at the expense of time and
cost. In a healthcare setting, Ibanez et al. (2018) observe that doctors tend to re-prioritize tasks so
as to group similar tasks together and reduce mental switching costs, but that such prioritization
may reduce long-term productivity.
Another reason that could explain why humans fail to follow machine recommendation is algorithm
aversion, as first documented by Dietvorst et al. (2015). Algorithm aversion refers to a general
preference to rely on humans instead of algorithms. This general preference could be due to an
inflated confidence in human performance. In a lab experiment, for instance, Logg et al. (2019)
observed that subjects (and in particular experts) were more prone to follow their own judgment
over an algorithm’s advice, or advice provided by another human. Alternatively, Dietvorst et al.
(2018) hypothesize that decision makers seek control over the output. In an empirical study, they
successfully reduced algorithm aversion by offering decision makers some control over the machine’s
output. Lin et al. (2021) propose and empirically evaluate algorithm use determinants in algorithm
aversion.
In an effort to propose alternative explanations to algorithm aversion, de V´ericourt and Gurkan
(2023) develop a theoretical framework to study the evolution of the decision maker’s belief about
the performance of a machine and her overruling decisions over time. In their setting, decisions
and recommendations are binary (to act or not to act, e.g., collect a biopsy or not) and the
decision maker only collects performance data when choosing to act. Because of this verification
bias, de V´ericourt and Gurkan (2023) identify situations under which a (rational) decision maker
6
fails to learn the true performance of the machine, and indefinitely overrules its recommendation
with some non-zero probability.
Understanding the drivers of partial adherence is useful to propose solutions and incorporate
behavioral aspects into the algorithmic recommendations. In a pricing setting, for example, Caro
and de Tejada Cuenca (2023) observe adherence patterns that are consistent with the fact that
inventory and sales are more salient to managers and conduct two interventions aimed at increasing
the salience of revenues. A growing literature has studied features of the recommendation system
or the recommended policy that could increase adoption, such as partial control over the output
(see discussion above and Dietvorst et al. 2018), simplicity (Bastani et al. 2021), or interpretability
(see, e.g., Kallus 2017, Bravo and Shaposhnik 2020, Ciocan and Miˇsi´c 2022, Jacq et al. 2022).
The underlying intuition is that policies that have simple structural forms are more likely to be
adopted because of legal requirements for a ‘right to explanation’ (Goodman and Flaxman 2017)
and because decision makers and stake-holders value policy they can understand and audit (Bertsimas
et al. 2013, 2022). Assuming that humans are more likely to adhere to recommendations
that constitute small changes to their current practice, Bastani et al. (2021) propose a reinforcement
learning approach to compute optimal ‘tips’, i.e., small changes in the current practice, and
validate their approach in a controlled experiment. In an attempt to increase interpretability of
reinforcement learning policies, Jacq et al. (2022) propose the lazy-MDP framework to learn and
recommend when to act (i.e., in what states of the system), on the top of the decisions. Meresht
et al. (2020) propose to learn when to switch control between machines and human decision makers.
Nonetheless, these works assume that the simplicity or interpretability of the recommendation
will not only increase adherence, but will lead to perfect adherence. In this paper, we complement
this literature by challenging this assumption and investigating the impact of partial adherence
directly on the actions to be recommended. We develop a framework to incorporate the potential
departure of the human decision maker within the search for a good recommendation policy. Our
goal resembles that of robust optimization under implementation errors where there is a similar
7
discrepancy between the computed solution and the implemented one as in Bertsimas et al. (2010),
Men et al. (2014), except that their error model is purely adversarial and their decision problem
static, and that our model accounts for the current baseline practices.
In a similar vein, Sun et al. (2022) reduce non-adherence in Alibaba’s warehouses by 19.3%
and packing time by 4.5%, by modifying the box size recommendations for the “at-risk” orders
(defined as having >50% chance of being overruled). In this paper, we have a similar objective of
adjusting the recommendation of the algorithm to the expected adherence level. However, instead
of an ad-hoc adjustment, we propose to account for the adherence level directly in the optimization
problem which the recommendation is a solution of. Furthermore, our objective is not to increase
adherence per se but to adjust the algorithm’s recommendation to the adherence level, so as to
increase the performance of the human-in-the-loop system.
3. Modeling partial adherence in a decision framework
In this section we formally introduce our model of decision under partial adherence.
We consider a human decision maker (DM) which repeatedly interacts with an environment. The
goal of the DM is to maximize a cumulative expected return, which captures both the instantaneous
reward and the long-run objective. A policy of the DM is a map from the set of possible states of
the environment to the set of actions. We assume that we have access to a baseline policy, called
πbase, which models the historical decisions of the DM. In a healthcare setting, for example, the
DM is a medical practitioner, observes the health condition of a patient at each time period, and
chooses a treatment to maximize the chances of survival, e.g., intravenous fluids and vasopressors
for hospital patients with sepsis (Komorowski et al. 2018), proactive transfers to the intensive care
units for patients in the emergency room (?), or drug treatment decisions for heart disease in
patients with type 2 diabetes (Steimle and Denton 2017). The baseline policy πbase captures the
current standard of care.
Classical methods from the operations management literature design models and algorithms to
compute an alternative recommendation policy πalg that leads to improved performance compared
8
with the baseline. The underlying assumption is that the DM, convinced by the value of the algorithmic
approach, will systematically follow πalg and not revert to πbase. However, in many practical
problems, πalg is only a recommendation. The practitioner does not commit to implementing it.
She has some discretionary power and the resulting policy is likely to be neither πbase nor πalg,
but a mixture of the two. The main objective and contribution of our paper is to incorporate this
partial adherence phenomenon within the optimization problem that defines πalg, i.e., adjust the
recommended policy to the adherence level.
3.1. Preliminaries on Markov decision process
Formally, we adopt the framework of Markov Decision Processes (MDPs; Puterman 2014). The
system or environment is described via a set of possible states S. At every decision period, the
DM is at a given state s ∈ S, chooses an action a ∈ A, transitions to the next state s′ ∈ S with
a probability Psas′ ∈ [0, 1] and obtains a reward rsas′ ∈ R. The future rewards are discounted by a
factor λ ∈ (0, 1) and we assume that S and A are finite sets. An MDP instance M consists of a
tuple M= (S,A,P, r,p0,λ), with r = (rsas′)s,a,s′ ∈ RS×A×S and P = (Psas′)sas′ ∈ (Δ(S))S×A, and
p0 ∈ Δ(S) is an initial probability distribution over the set of states S. Here, we denote Δ(S) the
simplex over S, defined as
Δ(S)=
(
p ∈ RS | ps ≥0, ∀ s ∈ S,
X
s∈S
ps =1
)
.
A policy π maps, for each period t ∈ N, the state-action history (s0, a0, s1, a1, ..., st) to a probability
distribution over the set of actions A. A policy π is Markovian if it only depends of the current
state st, and stationary if it is Markovian and it does not depend on time. Therefore, a stationary
policy is simply a map π : S →Δ(A). We call Π = (Δ(A))S the set of stationary policies, ΠM the
set of Markovian policies, and ΠH the set of all policies (possibly history-dependent). In an MDP,
the goal of the DM is to compute a policy π to maximize the return R(π), defined as
R(π)=Eπ
"
X+∞
t=0
λtrstatst+1
#
, (3.1)
9
with st the state visited at time period t, at the action chosen with probability πsa, and the
expectation is as regards with the distribution defined by the policy π on the set of infinite-horizon
trajectories. The return R(·) is sometimes called expected reward, and we use the term return to
distinguish it from the instantaneous reward rsa. The value function vπ ∈ RS of a policy π ∈ ΠH
represents the return obtained starting from any state: vπ
s = Eπ
hP+∞
t=0 λtrstatst+1 | s0 =s
i
, ∀ s ∈ S.
Note that in all generality, the return function π 7→ R(π) is neither convex nor concave on Π. An
optimal policy can be chosen stationary and deterministic and can be computed efficiently (see
Puterman 2014, chapter 6). We will say that a policy π′ is an ϵ-optimal policy if its return is within
ϵ>0 of the optimal return: R(π′)+ϵ≥max{R(π) | π ∈ Π}.
Remark 3.1 (Finite-horizon setting). In this paper, we only consider MDPs with infinite
horizon. It is straightforward to extend our framework and results to the case of finite-horizon
MDPs by adding an absorbing state with instantaneous reward 0 after the last period.
3.2. Adherence-aware MDP
We now incorporate the phenomenon of partial adherence into an MDP framework. Let M be
an MDP instance, πbase a baseline policy, and πalg a recommendation policy. We assume that πbase
belongs to the set Π of stationary policies. To capture the fact that the DM does not systematically
implement πalg, let us introduce a parameter θ ∈ [0, 1], which we call the adherence level. Intuitively,
the adherence-level θ quantifies the compliance of the decision maker to follow the recommendation
policy πalg instead of the baseline policy πbase. Therefore, the policy effectively implemented by the
DM depends on πalg, πbase, and θ. In particular, we consider an effective policy of the form:
πeff(πalg, θ)=θπalg +(1−θ)πbase. (3.2)
According to this model, when θ = 0, the DM always follows the baseline policy πbase, and when
θ = 1, the DM always follows the recommendation policy πalg. When θ ∈ (0, 1), the DM follows an
effective policy πeff(πalg, θ), which is a mixture of πalg and πbase. Consequently, the effective return
for the DM is R(πeff(πalg, θ)), with πeff(πalg, θ) = θπalg + (1 − θ)πbase. For a fixed adherence level
10
θ, our objective is to compute an optimal recommendation policy such that the effective return
πalg 7→ R(πeff(πalg, θ)) is maximized, i.e., our goal is to solve the following decision problem, called
Adherence-aware MDP (AdaMDP):
sup
πalg∈ΠH
R(πeff(πalg, θ)). (AdaMDP)
When the supremum in the above optimization program is attained, we write π⋆
alg(θ) for an optimal
recommendation policy and we write π⋆
eff(θ) for the resulting optimal effective policy, i.e., π⋆
eff(θ)=
πeff(π⋆
alg(θ), θ). For simplicity, we assume for now that θ is the same for all states s ∈ S, an assumption
we will challenge in Section 6. We first note that an optimal policy π⋆
alg(θ) for AdaMDP can be
chosen stationary and deterministic, two properties that are appealing from an implementation
standpoint.
Proposition 3.1. The supremum in AdaMDP is attained at an optimal recommendation policy
π⋆
alg(θ) that can be chosen stationary and deterministic:
sup
πalg∈ΠH
R(πeff(πalg, θ))= max
πalg∈Π
R(πeff(πalg, θ)).
The proof of Proposition 3.1 uses some more advanced results that we will introduce in Section
4.2. We present the detailed proof in Appendix F.
Remark 3.2. Interestingly, a similar type of mixture policies have been studied in the online
learning literature, yet with a different motivation. To address the exploration-exploitation tradeoff,
many policies obtained via reinforcement learning are implemented together with an ad-hoc
exploration mechanism. Instead, Shani et al. (2019) propose to compute “exploration-conscious”
policies that are designed for a particular exploration policy (e.g., choosing actions uniformly at
random) and exploration rate, which play a similar role as πbase and 1 − θ in our framework.
However, they view the exploration policy and exploration rate as additional parameters one can
tune to mitigate the exploration-exploitation tradeoff, while we consider πbase and θ as uncontrolled
inputs (arising from potential human deviations) and study their impact on actual performance.
11
3.3. Discussion: Mechanisms for partial adherence and effective policy
Our adherence-aware MDP framework posits that the effective policy can be simply expressed as
a convex combination of the algorithmic and the baseline policies, as presented in (3.2). In this
section, we further justify the practical relevance of our framework by discussing how different
models for the DM’s adherence decision connects with our framework.
To model the DM’s decision to adhere, we introduce a variable us,t ∈ [0, 1] indicating, in state
s, at time t, whether she follows the recommended policy πalg (the case us,t = 1) or whether she
follows πbase (the case us,t =0). We call us,t the adherence decision at state s and period t, and we
write u := (us,t)s∈S,t∈N. With this notation, the effective policy at state s at time t is given as
πeff(πalg,u)s,t =us,tπalgs,t +(1−us,t)πbases,t, (3.3)
and specifying an adherence mechanism is equivalent to specifying how the DM chooses u.
Random model. For example, the DM could sample us,t following any distribution with support
included in [0, 1] and with a mean θ. For instance, in the case of a Bernoulli distribution with
parameter θ, at each time period, the decision maker follows πalg with probability θ and πbase with
probability 1 − θ. In practice, this random model of adherence decisions can be interpreted as
being agnostic to the reasons for partial adherence. Whatever the cause (e.g., algorithm aversion,
information asymmetry), they are inaccessible to the algorithm, hence are perceived by the algorithm
as random deviations from the recommended policy. In other words, this model mimics the
observed behavior of DM but does not capture from first principles why she sometimes decides to
deviate from the recommendations. For example, in a stylized setting with a rational DM trying
to learn whether a machine is more accurate than her, de V´ericourt and Gurkan (2023) identify
regimes where the DM’s belief oscillates permanently, hence justifying models like this one, where
the DM’s adherence decisions us,t and us,t′ may be different for t ̸=t′, even though the state is the
same. In the next theorem, we show that this model with random adherence decision u is exactly
equivalent to AdaMDP.
12
Theorem 3.1. Consider the following model of random adherence decisions, where each
(us,t)s∈S is sampled from a distribution with mean (θ, ..., θ) ∈ [0, 1]S, independently across t ∈ N.
Then
sup
πalg∈ΠH
Eu [R(πeff(πalg,u))]= max
πalg∈Π
R(πeff(πalg, θ))
and an optimal recommendation may be chosen stationary and deterministic in the left-hand side
of the above equation.
We present a detailed proof in Appendix A. Note that under the assumption of Theorem 3.1,
the random variables us,t and us′,t may be dependent for s ̸=s′. In fact, the proof relies on showing
that Eu [R(πeff(πalg,u))]=R(Eu [πeff(πalg,u)]), despite the return R(·) being non-linear. This follows
from the properties that us,t and us′,t′ are independent across pairs (s, t), (s′, t′) such that t ̸= t′.
Noting that Eu [πeff(πalg,u)]=πeff(πalg, θ) concludes the proof.
Adversarial model. Alternatively, as discussed in the literature review in Section 2, partial adherence
can be driven by information asymmetry or conflicting objectives between the algorithm and
the DM. In other words, the decision maker could choose to follow the recommendation policy πalg
or the baseline policy πbase according to a different MDP instance M′ than the MDP instance M
that parametrized the algorithm. Adopting a conservative view, one can assume the DM picks each
us,t ∈ [θ, 1] adversarially in a set B ⊆[θ, 1]S×N:
sup
πalg∈ΠH
min
u∈B
R(πeff(πalg,u)) . (3.4)
Without any restrictions, i.e., in the case B =[θ, 1]S×N, the DM could decide to follow the algorithm
in state s at time t and, when visiting the same state s at a later stage, decide to override it. Hence,
we can enrich the set B with several consistency constraints to model more realistic situations. In
some settings, for instance, it might be more realistic to assume a time-invariant adversarial model,
i.e., to assume that the DM’s adherence behavior depends on the state but is consistent over time.
For example, one could assume that she chooses an adherence decision us ∈ [θ, 1] adversarially for
each state s and adopts this policy throughout, i.e., us,t =us,∀ t ∈ N. Note that the time-invariant
13
adversarial model assumes that the decision maker has some discretionary power at the beginning
but commits to one policy for the rest of the trajectory, which can be seen as contradictory. Another
realistic model consists of state-invariant adherence decisions, i.e., us,t = ut ∈ [0, 1] across all pairs
(s, t) ∈ S × N. A fourth model could assume that the adherence decisions are time- and stateinvariant,
i.e., that us,t =u ∈ [0, 1] across all pairs (s, t) ∈ S ×N. Fortunately, as stated (informally)
in Theorem 3.2, studying our effective policy (3.2) is equivalent to studying any of these three
adherence mechanisms:
Theorem 3.2. (Informal statement) An optimal algorithmic recommendation π⋆
alg(θ), solution
to AdaMDP, is an optimal solution of the decision problem (3.4), whenever the adherence decision
u is chosen according to one of the following adversarial models: for all (s, t) ∈ S ×N,
• (Unconstrained Adversarial) us,t chosen independently and adversarially in [θ, 1].
• (Time-invariant Adversarial) us,t = us with us chosen independently and adversarially in
[θ, 1].
• (State-invariant Adversarial) us,t = ut with ut chosen independently and adversarially in
[θ, 1].
• (Time- and State-invariant Adversarial) us,t =u with u chosen adversarially in [θ, 1].
Additionally, strong duality holds for these models of adversarial adherence decisions.
We defer a formal statement and proof of Theorem 3.2 to Appendix B. Theorem 3.2 shows that
AdaMDP can be interpreted as the robust counterpart of the aforementioned adversarial models,
and perhaps surprisingly, that these robust models yield the same worst-case return, and from the
proof of Theorem 3.2, the same optimal policy as well. The strong duality results show that the
case where πalg is chosen before the adherence decisions u and the case where πalg is chosen after
the adherence decisions u are equivalent. We should emphasize, however, that Theorem 3.2 only
claims an equivalence in terms of optimal effective return. For a given (sub-optimal) policy, its
effective return under each model (AdaMDP or one of the adversarial models) can differ.
14
Remark 3.3. The proof of Theorem 3.2 shows that for these adversarial models, a worst-case
us,t can be chosen as us,t = θ,∀(s, t) ∈ S ×N. Therefore, when θ = 0, we recover the fact that the
agent never follows the algorithmic recommendation πalg.
Overall, Theorems 3.1 and 3.2 show that our simple proposal for adherence-aware MDPs subsumes
a collection of DM-level models of partial adherence, hence justifying our subsequent analysis
of the effective policy (3.2) and the optimal recommendation problem (AdaMDP).
We summarize the equivalences obtained in this section in Table 1. For the adversarial model,
time-invariance and state-invariance are described in Theorem 3.2. For the random model of adherence
decisions, time-invariance corresponds to a model where there exist two periods t ̸=t′ for which
the random variables us,t and us′,t′ are dependent for some states s, s′ ∈ S, and state-invariance
corresponds to the case where there exist s ̸= s′ and t ∈ N for which us,t and us′,t are dependent
random variables. The assumption in Theorem 3.1 corresponds to random models that are not
time-invariant. We provide more discussion on these time-invariant and state-invariant random
models at the end of Appendix A.
Constraints Model of adherence decisions
Time-invariance State-invariance Random Adversarial
× × AdaMDP AdaMDP
× ✓ AdaMDP AdaMDP
✓ × unknown AdaMDP
✓ ✓ unknown AdaMDP
Table 1 Summary of the adherence decision models considered in this paper and their relations with AdaMDP.
Cardinality-constrained model. Under an adversarial lens, one could model the DM’s unwillingness
to implement a large number of changes to her current practice by, e.g., imposing a limit
on the number of states where she adheres. For example, let us assume that adherence decisions
15
are time-invariant and let us model the DM’s adherence problem as that of finding up to k states
where she follows the algorithmic recommendation, with k ∈ N:
min
u∈{0,1}S,
P
s∈S us≤k
R(πeff(πalg,u)). (Constrained-AdaMDP)
The evaluation problem above (let alone the problem of then optimizing for πalg) is hard, as we
characterize in the following result:
Theorem 3.3. Constrained-AdaMDP is APX-hard, i.e., there exists a constant α >0, for which
it is NP-hard to approximate Constrained-AdaMDP within a factor smaller than 1+α.
Our proof of Theorem 3.3 is based on a reduction from the constrained assortment optimization
under the Markov Chain-based choice model (D´esir et al. 2020) and we provide the details in
Appendix C. This shows that adding a simple cardinality constraint to AdaMDP makes the decision
problem intractable. For the sake of completeness, and since Constrained-AdaMDP may be of
independent interest, we provide a mixed-integer optimization formulation for solving Constrained-
AdaMDP in Appendix D.
3.4. Examples of competition/complementarity between the human and the
algorithm
Before turning to a more formal analysis of our framework, we demonstrate the implications of the
effective policy (3.2) on a simple MDP instance, to provide some intuition on the interactions at
play between πalg and πbase as well as illustrate the rich range of situations that can arise in our
framework. Indeed, we provide an example where the co-existence of the algorithmic and baseline
policies can lead to arbitrarily bad performance and another example where, on the contrary, they
complement each other.
We consider the MDP instance from Figure 1. There are 5 states, the rewards are independent
from the chosen action and only depend on the current state. We assume that the transitions are
deterministic and are represented with dashed arcs in Figure 1a, along with the rewards above
the states. The actions consist in choosing the possible next states. The MDP starts in State 1,
16
and State 4 and State 5 are absorbing. The MDP instance is parametrized by ϵ ∈ {−1, 1}, which
impacts the reward of State 5.
The current policy πbase is represented in Figure 1b. Observe that πbase prescribes to transition
from State 2 to State 5 but that, according to πbase, State 2 should not be visited in the first place.
For example, in a healthcare setting, State 2 could correspond to a newly introduced treatment,
which the practitioner is not used to prescribing. The expected return of πbase is
R(πbase)=
λ2
1−λ
,
where λ ∈ (0, 1) is the discount factor. Note that, by definition of the effective policy πeff , for any
θ ∈ [0, 1], πbase = πeff(πbase, θ). In other words, for any adherence level θ ∈ [0, 1], recommending πbase
leads exactly to the implementation of πbase. We further consider that the algorithm prescribes the
policy πalg represented in Figure 1c, whose expected return is
R(πalg)=0.1λ+
λ2
1−λ
>R(πbase) .
Detailed computations of policy returns reported in this section are presented in Appendix E.
1
2
3
r4 = 1
r5 = 1 + ϵ
r2 = 0.1
r3 = 0
r1 = 0 4
5
(a) MDP instance.
1
2
3
r4 = 1
r5 = 1 + ϵ
r2 = 0.1
r3 = 0
r1 = 0 4
5
(b) Baseline policy πbase.
1
2
3
r4 = 1
r5 = 1 + ϵ
r2 = 0.1
r3 = 0
r1 = 0 4
5
(c) Representation of πalg.
Figure 1 Details on the transitions and rewards of our MDP instance.
Case 1: partial adherence hurts. We first assume that ϵ=−1. In this case, it is easy to verify that
πalg is optimal under perfect adherence (θ =1). If adherence is not perfect, however, continuing to
recommend πalg can lead to sub-optimal performance. Indeed, πbase chooses suboptimal actions in
State 2, which πalg recommends to visit (unlike πbase). So, the mixture policy πeff(πalg, θ) can lead to
17
worse performance than either πalg or πbase. Formally, the return of the effective policy πeff(πalg, θ)
is equal to
R(πeff(πalg, θ))=R(πbase)+2θ
λ2
1−λ

θ − ˜θ

with ˜θ := 1 − 0.1
1−λ
2λ
≤ 1. If ˜θ ≤ 0, the behavior of the effective return function is intuitive: In
this case, we observe that θ 7→ R(πeff(πalg, θ)) is increasing. In particular, R(πalg)=R(πeff(πalg, 1))≥
R(πeff(πalg, θ)), i.e., partial adherence degrades the effective return obtained by recommending πalg
compared with the perfect adherence case. Furthermore, R(πeff(πalg, θ))≥R(πbase), i.e., recommending
πalg improves over the current standard of practice, πbase.
However, the analytic expression above reveals surprising behaviors when ˜θ >0. In this case, the
function θ 7→ R(πeff(πalg, θ)) is non-monotone (see Figure 2a, obtained with λ=0.5, hence ˜θ =0.95):
It decreases on [0, ˜θ/2] and increases on [˜θ/2, 1]. Since the effective policy is a convex combination
of πalg and πbase, it is intuitive to believe that its performance will be bounded above and below
by R(πalg) and R(πbase) respectively. This example disproves this intuition. In particular, we have
R

πeff(πalg, ˜θ)

<R(πbase). In other words, overlooking the adherence level θ and recommending the
same policy πalg may lead to lower return than the baseline policy itself! Actually, as we formally
prove in the next section, this sub-optimality gap can be made arbitrarily large.
Finally, via backward induction, we can find an optimal recommendation policy π⋆
alg(θ) for any
value of θ ∈ [0, 1]. In particular, we find an optimal recommendation policy of the following form
(see derivations in Appendix E): π⋆
alg(θ)=π⋆ if θ >max(0, ¯θ) for π⋆ that chooses 1→2, 2→4, 3→4
and ¯θ =1−0.1(1−λ)/λ; and π⋆
alg(θ)=πbase if θ ≤max(0, ¯θ). Note that by varying λ, the breakpoint
max(0, ¯θ) can be made arbitrarily close to 1. In the following section, we show that, for any MDP
instance, the optimal recommendation policy π⋆
alg(θ) enjoys such piecewise constant structure.
Case 2: partial adherence helps (complementarity). We now consider the case where ϵ = 1 so
that neither πalg nor πbase are optimal and there is room for improvement. Actually, we show in this
example that partial adherence improves upon both policies, illustrating complementarity benefits
18
between the human DM and the algorithm. We now compute the expected return of the effective
policy πeff(πalg, θ)=θπalg +(1−θ)πbase. In particular, we obtain that
R(πeff(πalg, θ))=R(πalg)+2R(πbase)(1−θ)(θ −(1− ˜θ)),
with ˜θ previously defined. Thus, if 1−˜θ <1, we observe that R(πeff(πalg, θ))>max{R(πalg),R(πbase)}
for any θ ∈ (1− ˜θ, 1). In other words, there exists a regime where the partial implementation of πalg
leads to greater performance than πalg or πbase alone.
These examples show that, despite its simple form, the class of effective policies defined in (3.2)
can capture many realistic situations where the co-existence of the algorithm and the DM hurts or
benefits the overall system performance. Because our objective is prescriptive and we are interested
in informing the design of the algorithmic recommendations πalg, we assume in the rest of the paper
that recommendations are optimal for the true MDP parameter r,P,λ and the adherence level θ,
i.e., where πalg = π⋆
alg(θ) with π⋆
alg(θ) an optimal solution to the optimization problem (AdaMDP).
This corresponds to the case where there is no model misspecification, and where θ is known. In
particular, under this assumption, algorithmic recommendations that ignore the issue of partial
adherence correspond to πalg = π⋆
alg(1), and Case 1 in this section shows that R(π⋆
eff(θ)) may be
much greater than R(πeff(πalg, θ)). Given an estimate of the adherence level θ, our objective is thus
to compute an optimal recommendation π⋆
alg(θ) as a solution of an optimization problem, enabling
us to prove important structural properties and tractability results in the next sections. We should
emphasize that diverting from the assumption that the algorithmic recommendation is the solution
of an optimization model leaves open the question of how to define (and compute) the algorithmic
recommendation in practice.
Remark 3.4. In our MDP instance for the second case (complementarity), neither πalg nor πbase
are optimal. Indeed, by definition, if πalg or πbase is an optimal policy for the nominal MDP, then it is
impossible that R(πeff(πalg, θ)) > max{R(πalg),R(πbase)}, i.e., complementarity cannot occur. More
complex models of partial adherence could lead to interesting human-machine complementarity,
for instance in the case where both the algorithm and the human only have access to partial
19
information on the state or action sets or have different objectives. Our agnostic model may
adequately complement these cases where more is known (or assumed) about the rational behind
partial adherence. Because decision models are necessarily a simplification of real-life decisions,
integrating more complex behavioural models behind partial adherence is an important direction
for future work.
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
0.30
0.35
0.40
0.45
0.50
0.55
Return
R( eff( alg, ))
R( alg)
R( base)
R( eff( ))
(a) Co-existence hurts (ϵ=−1)
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
0.5
0.6
0.7
0.8
0.9
1.0
Return
R( eff( alg, ))
R( alg)
R( base)
R( eff( ))
(b) Co-existence helps (ϵ=1)
Figure 2 Illustrating the impact of the partial adherence phenomenon (hence the coexistence of a baseline and
algorithmic policy) in the MDP instance from Figure 1a. We choose λ = 0.5 in our simulations.
4. Analyzing adherence-aware MDPs
We now theoretically analyze the class of adherence-aware MDPs we introduced in the previous
section. As a motivation, we first provide negative results showing the worst-case performance
deterioration that can be experienced by overlooking the partial adherence phenomenon, i.e., by
recommending π⋆
alg(1) instead of π⋆
alg(θ). We then show how to compute optimal adherence-aware
recommendations efficiently and investigate how they depend structurally on θ.
4.1. Worst-case analysis of the performance of π⋆
alg(1)
As the example in Section 3.4 shows, an optimal recommendation policy π⋆
alg(θ) may be different
from an optimal nominal policy π⋆
alg(1), which itself can lead to worse performance than the baseline
policy πbase alone. We now formalize these observations.
20
First, we analyze the performance of πeff(π⋆
alg(1), θ) for π⋆
alg(1) an optimal nominal policy and
show that recommending π⋆
alg(1) (i.e., ignoring the partial adherence effect) can lead to arbitrarily
worse returns than the baseline policy.
Proposition 4.1. For any scalar M ≥0, for any adherence level θ ∈ (0, 1), there exists an MDP
instance M such that R(πbase) ≥M +R
􀀀
πeff(π⋆
alg(1), θ)

, where π⋆
alg(1) is an optimal policy for the
nominal MDP instance M.
Proof of Proposition 4.1 Fix M ≥ 0 and θ ∈ (0, 1) and consider the MDP instance of Section
3.4 with ϵ=−1, with π⋆
alg(1) as in Figure 1c. In the limit where λ→1, we have R
􀀀
πeff(π⋆
alg(1), θ)

−
R(πbase)∼2θ λ2
1−λ (θ− ˜θ)→−∞ since θ <1. Hence, we can have R
􀀀
πeff(π⋆
alg(1), θ)

−R(πbase)≤−M
for λ close to 1. □
Proposition 4.1 generalizes the observation that πeff(π⋆
alg(1), θ) can lead to arbitrarily worse performance
than the current baseline policy itself (e.g., the current state of practice). As elicited in
the example from Section 3.4, this phenomenon happens when the baseline policy πbase chooses
sub-optimal actions in some states. As a result, the effective policy πeff(π⋆
alg(1), θ) can also end
up in these bad states that are overlooked by π⋆
alg(1), which assumes that the actions are always
chosen from π⋆
alg(1). Consequently, for any value of θ ∈ (0, 1), the policy π⋆
alg(1) can be arbitrarily
sub-optimal.
Corollary 4.1. For any scalar M ≥0, for any adherence level θ ∈ (0, 1), there exists an MDP
instance M such that R(π⋆
eff(θ))≥M +R
􀀀
πeff(π⋆
alg(1), θ)

.
Proof of Corollary 4.1 The result follows from Proposition 4.1 since R(πbase) =
R(πeff(πbase, θ))≤R(π⋆
eff(θ)). □
While Proposition 4.1 and Corollary 4.1 show that ignoring the adherence level θ can lead to
arbitrarily large losses in performance, there are worst-case statements where, for each value of
θ ∈ [0, 1), a particular MDP instance M is constructed. In practice, one might be interested in a
single MDP instance and the impact of varying θ ∈ [0, 1] on this instance in particular, which is
the focus of the rest of this section.
21
4.2. Solving adherence-aware MDPs
We now show how to efficiently compute an optimal policy π⋆
eff(θ) for adherence-aware MDPs.
Note that when θ = 1, the DM is simply solving a classical MDP problem, which can be done
efficiently with various algorithms such as value iteration, policy iteration, and linear programming
(see chapter 6 in Puterman 2014). Additionally, for the classical MDP problem, it is well-known
that an optimal policy can be chosen stationary and deterministic without loss of optimality, which
greatly simplifies implementation and interpretation of such policies in practice. We show that the
same holds for the adherence-aware MDP problem in the next proposition.
Proposition 4.2. There exists a unique vector v∞ ∈ RS defined as
v∞
s = max
πs∈Δ(A)
θ ·
X
a∈A
πsaP⊤
sa (rsa +λv∞)+(1−θ) ·
X
a∈A
πbase,saP⊤
sa (rsa +λv∞) ,∀ s ∈ S, (4.1)
and an optimal recommendation policy π⋆
alg(θ) can be computed as a stationary deterministic policy
attaining the argmax of Equation (4.1) for each s ∈ S.
The proof of Proposition 4.2 is akin to our proof of Proposition 3.1, presented in Appendix F, and
we omit it for conciseness. We note that we can rewrite Equation (4.1) as
v∞
s = max
πs∈Δ(A)
X
a∈A
πsa
􀀀
r′
sa +λP′⊤
sa v∞
,∀ s ∈ S, (4.2)
with P′ ∈ (Δ(S))S×A , r′ ∈ RS×A defined as
P′
sa := θ ·Psa +(1−θ) ·
X
a′∈A
πbase,sa′Psa′ ,
r′
sa := θ ·P⊤
sarsa +(1−θ) ·
X
a′∈A
πbase,sa′P⊤
sa′rsa′ ,
(4.3)
for all (s, a) ∈ S ×A. This shows that for any θ ∈ [0, 1], an optimal recommendation π⋆
alg(θ) can be
viewed as the optimal policy for another MDP instance M′ = (S,A,P′, r′,p0,λ), where the new
transition probabilities P′ and the new rewards r′ are defined as (4.3), and, interestingly, where the
instantaneous rewards only depend on the current state-action pair (s, a) but not on the subsequent
state s′. In the context of “exploration-conscious” reinforcement learning and in the simpler case
22
where rsas′ = rsa, ∀ (s, a, s′) ∈ S ×A×S in the MDP instance M, Shani et al. (2019) refer to the
MDP instance M′ as the surrogate MDP. This shows that we can efficiently compute an optimal
recommendation policy by computing an optimal policy of the surrogate MDP. Note that even
though π⋆
alg(θ) can be chosen deterministic since it is an optimal policy to the surrogate MDPs, the
effective policy π⋆
eff(θ) may be randomized, since by definition π⋆
eff(θ)=θπ⋆
alg(θ)+(1−θ)πbase.
For the sake of completeness, we now describe two efficient methods to compute v∞.
Iterative method: value iteration. Let us define the operator f : RS →RS as
fs(v)= max
πs∈Δ(A)
θ ·
X
a∈A
πsaP⊤
sa (rsa +λv)+(1−θ)
X
a∈A
πbase,saP⊤
sa (rsa +λv) ,∀ s ∈ S. (4.4)
Note that when θ =1, this is the classical Bellman operator. The operator f is a contraction for ℓ∞:
for any v,w ∈ RS, we have ∥f(v) − f(w)∥∞ ≤ λ∥v −w∥∞. Therefore, as for classical MDPs, the
fixed-point v∞ can be computed efficiently via value iteration (VI): v0 =0,vt+1 =f(vt),∀ t ∈ N. To
obtain an ϵ-optimal recommendation policy, we can stop as soon as ∥vt−f (vt) ∥∞ ≤ϵ(1−λ)(2λ)−1,
which is satisfied after O(log (ϵ−1)) iterations (Puterman 2014, theorem 6.3.3).
Linear programming formulation. The optimal value function v∞ ∈ RS can also be computed
with linear programming (Puterman 2014, section 6.9). In particular, v∞ is the unique solution
to the optimization problem min
P
s∈S vs | vs ≥fs(v),∀ s ∈ S
	
, which can reformulated in the following
linear program with |S| decision variables and |S|×|A| linear constraints:
min {p⊤
0 v | vs ≥θP⊤
sa (rsa +λv)+(1−θ)
X
a′∈A
πbase,sa′P⊤
sa′ (rsa′ +λv) , ∀ (s, a) ∈ S ×A}.
4.3. Structure and sensitivity of π⋆
alg(θ) with respect to the adherence level
We now investigate how the optimal recommendation π⋆
alg(θ) and its performance R(πeff(π⋆
alg(θ), θ))
depend on the adherence level θ.
First, the example from Section 3.4 illustrates that the mapping θ 7→ R(πeff(π, θ)), for a fixed
policy π, is not necessarily monotone. Still, we can recover monotonicity when considering π⋆
alg(θ)
instead, as shown in the next proposition.
23
Proposition 4.3. For any MDP instanceM, the map θ 7→ R(π⋆
eff(θ)) is non-decreasing on [0, 1].
Proof. This is straightforward from the equivalence of AdaMDP and the models of adversarial
adherence decisions from Theorem 3.2. We provide a simple, more direct proof below. Let θ1, θ2 ∈
[0, 1] with θ1 ≤ θ2. We will show that R(π⋆
eff(θ1)) ≤ R(π⋆
eff(θ2)). Following the definition of π⋆
eff(θ1),
we have π⋆
eff(θ1)=θ1π⋆
alg(θ1)+(1−θ1)πbase. We can rewrite this as
π⋆
eff(θ1)=θ2

θ1
θ2
π⋆
alg(θ1)+
θ2 −θ1
θ2
πbase

+(1−θ2)πbase,
and ˆπ := θ1
θ2
π⋆
alg(θ1) + θ2−θ1
θ2
πbase is a policy since 0 ≤ θ1 ≤ θ2 ≤ 1. Overall, we conclude that
R(π⋆
eff(θ1))=R(πeff(ˆπ, θ2))≤R(π⋆
eff(θ2)), by optimality of π⋆
alg(θ2). □
Proposition 4.3 shows that as the DM deviates more and more from the recommendation policy
(i.e., as θ decreases), the optimal effective return decreases. Note that this result holds because we
consider π⋆
alg(θ), in other words because we adjust our recommended policy as the adherence level
varies. Since π⋆
eff(0) = πbase, Proposition 4.3 also implies that R(π⋆
eff(θ)) ≥ R(πbase): recommending
π⋆
eff(θ) can only improve performance compared with the current baseline, which may not be the
case when recommending π⋆
alg(1), as highlighted in Proposition 4.1. Overall, Proposition 4.3 also
suggests that it is always beneficial to try to increase the compliance of the decision maker (i.e.,
increase the value of θ), as this leads to more returns for the optimal effective policy π⋆
eff(θ).
Actually, we now show that the optimal recommendation π⋆
alg(θ) does not vary continuously in
θ but rather enjoys a piecewise constant structure:
Proposition 4.4. For any MDP instance M:
1. There exists ¯θ ∈ [0, 1), such that π⋆
alg(θ)=π⋆
alg(1) for any θ ∈ [¯θ, 1].
2. There exists n ∈ N and 0 = θ1 < θ2 < · · · < θn = 1 such that, for any i ∈ {1, ...,n − 1}, π⋆
alg(θ)
can be chosen constant over the interval [θi, θi+1].
3. If πbase =π⋆
alg(θ) for some θ ∈ [0, 1], then π⋆
alg(θ)=πbase for any θ ∈ [0, θ].
Combined with the fact that π⋆
alg(1) is an optimal recommendation for θ = 1, Statement 1 shows
that, when the adherence level is sufficiently close to 1, we can overlook the issue of partial adherence
24
and output the same recommendation as when θ = 1, which reduces to the classical MDP model.
More generally, Statement 2 in Proposition 4.4 shows that, in general, π⋆
alg(θ) has a piecewise
constant structure. The piecewise constant structure of π⋆
alg(θ) combined with the fact that πbase is
an optimal recommendation for θ = 0 also ensures that πbase is an optimal recommendation in a
neighborhood of 0. Statement 3 generalizes this observation and states that if the baseline policy
is an optimal recommendation policy for an adherence level θ, then it is optimal for any lower
adherence level. A trivial example is the case where θ =1, i.e., when πbase is optimal in the classical
MDP model, then we should systematically recommend the baseline. To motivate our study, we
implicitly assumed that R(πbase)<R(π⋆
alg(1)), i.e., that the baseline policy could be improved.
Lastly, we uncover two conditions on the MDP instance under which the partial adherence phenomenon
can be ignored by the decision-maker. We start with a simple example where the optimal
recommendation π⋆
alg(θ) does not depend on θ and πbase. We observe that when the transitions
Psa ∈ Δ(S) do not depend on the action but only on the current state: Psa =Ps ∈ Δ(S) and when
rsas′ =rsa for all (s, a, s′) ∈ S ×A×S, then the optimality equation (4.1) becomes
v∞
s =θ · max
πs∈Δ(A)

π⊤
s rs
	
+θ · λP⊤
s v∞ +(1−θ) ·π⊤
base,srs +(1−θ) · λP⊤
s v∞, ∀ s ∈ S,
and we can choose an optimal recommendation policy π⋆
alg(θ) that is independent from θ and πbase.
In other words, partial adherence only impacts the effective return but it does not change the
optimal recommendation. This special case occurs, for example, when the DM faces a sequence
of independent single-stage decision problems (e.g., patients arriving independently to be treated)
where each decision provides an immediate reward but does not impact the next decision problem,
see de V´ericourt and Gurkan (2023) for a detailed study of this case in a learning setting.
We now describe a condition under which the decision-maker may ignore partial adherence at a
given state. Inspecting the surrogate MDP defined in Equation (4.3), we note that the new pair of
rewards and transitions (r′,P′) is a convex combination of the nominal parameters (r,P) and the
rewards and transitions induced by πbase. Therefore, if πbase chooses an optimal action at a state
¯s ∈ S, we may expect that the algorithmic recommendation coincides with πbase at ¯s. We show that
this intuition is true in the next proposition.
25
Proposition 4.5. Let ¯s ∈ S such that v
π⋆
alg(1)
¯s = vπbase
¯s . Then for any θ ∈ [0, 1], we have vπ⋆
eff (θ)
¯s =
vπbase
¯s and we can choose π⋆
alg(θ)¯s =πbase,¯s.
We provide the proof of Proposition 4.5 in Appendix H. Proposition 4.5 shows that if the baseline
policy obtains the optimal nominal value at a given state ¯s ∈ S, then the decision-maker can
guarantee this same value at ¯s for any value of the adherence level θ ∈ [0, 1] by recommending the
same action as the baseline policy. We conclude this section by noting that obtaining a meaningful
bound on the suboptimality of a policy πalg against π⋆
alg(θ) for a given value θ ∈ [0, 1] of the adherence
level is an interesting direction for future work. We derive a bound in Appendix I, noting that it
may be hard to interpret, due to the piece-wise constant structure of the optimal recommendation
policies (Proposition 4.4).
5. Numerical experiments
In this section, we numerically study the impact of the adherence level and of the baseline policy on
two decision-making examples, in machine replacement and healthcare respectively, that have been
studied in the MDP literature.We solve all the decision problems using the value iteration algorithm
presented in Section 4.2. Among others, these numerical results illustrate the importance of taking
into account the current state of practice and the adherence level when designing algorithmic
recommendations. In particular, the adherence-aware optimization framework we develop in this
paper provides simple tools to evaluate the robustness of a policy with respect to the adherence
level and to obtain improved solutions in situations where the performance is the most impacted.
5.1. Machine replacement problem
We start with the a machine replacement problem introduced in Delage and Mannor (2010) and
studied in Wiesemann et al. (2013), ?.
MDP instance. We represent the machine replacement MDP in Figure 3. The set of states is
{1, 2, 3, 4, 5, 6, 7, 8,R1,R2} and the set of actions is {repair, wait}. Each state models the condition
of the same machine. In State 8 the machine is broken, while State R1 and State R2 model some
ongoing reparations. State R1 is a normal repair while State R2 is a long repair. We use the same
26
rewards and transitions as in Delage and Mannor (2010). In particular, there is a reward of 0 in
State 8, a reward of 18 in State R1, a reward of 10 in State R2, and a reward of 20 in the remaining
states. We set a discount factor of λ=0.99 and the DM starts in State 1.
R1
1 8
0.1
R2
2
0.3
0.3 0.3
0.1 0.1
0.6
0.6
0.6
1
0.4
(a) Transition probabilities for
action repair.
R1
1 8
R2
2
1
0.8 0.8
0.2
1
0.8
0.2 0.2
(b) Transition probabilities for
action wait.
Figure 3 Transition probabilities for the machine replacement MDP. There is a reward of 18 in state R1, of 10
in state R2 and of 0 in state 8. All others states have a reward of 20.
Numerical results. Assuming θ = 1, an optimal policy π⋆
alg(1) is to choose action wait in States
1, 2, 3, 4,R2 and action repair in States 5, 6, 7, 8,R1. We now compare the effective return of π⋆
alg(1)
with that of the best recommendation π⋆
alg(θ), for varying values of the adherence level θ. We first
consider the case where πbase chooses to always wait instead of repairing the machine. We present
the results of our empirical study in Figure 4. In Figure 4a, we report the effective return of
both policies, namely R(π⋆
eff(θ)) and R(πeff(π⋆
alg(1), θ)), for varying θ ∈ [0, 1]. We also compute the
proportional deterioration in performance,
􀀀
R(π⋆
eff(θ))−R(πeff(π⋆
alg(1), θ))

/R(π⋆
eff(θ)) in Figure 4b.
As expected from Proposition 4.4, when θ is sufficiently close to 1 (here, for θ ≥ 0.88), we have
π⋆
eff(θ) = π⋆
eff(1) and there is no deterioration in performance. However, as the value of θ decreases
towards 0, overlooking the adherence level and recommending π⋆
alg(1) can lead to as much as 13.34%
proportional deterioration compared with the optimal return R(π⋆
eff(θ)). We also note in Figure 4b
that small changes in θ can lead to very severe deterioration, for instance in the region θ ∈ [0, 0.20],
i.e., for very low adherence from the human decision maker. The different regions over which the
27
optimal decision θ 7→ π⋆
alg(θ) is constant are shown in Figure 4c, which highlights that the optimal
recommendation policy may change many times as the adherence level decreases.
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
250
500
750
1000
1250
1500
1750
2000
Return
Return for eff( ) Return for eff( alg(1), )
(a) Returns for recommending
π⋆
alg(θ) and π⋆
alg(1).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
14
12
10
8
6
4
2
0
Deterioration (%)
Proportional deterioration for alg(1) against alg( )
(b) Proportional deterioration
for π⋆
alg(1) against π⋆
alg(θ).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1
2
3
4
5
6
7
Different policies
Regions with different alg( )
(c) Subregions with constant
recommendation policies.
Figure 4 Numerical results for the machine replacement MDP with πbase always choosing action wait.
We also study the impact of the adherence level when πbase is the policy that avoids being
trapped in the “bad” states (States 8,R1,R2). In particular, let us consider a policy πbase that
always waits when the machine is not broken (State 1 to State 7) or in the normal repair state
(State R2), but chooses to repair in State 8 and in the long repair state (State R1). The numerical
results are presented in Figure 5. In this case, we see that the performance of π⋆
alg(1) are robust
for θ ≥0.35, with a proportional deterioration of only 0.5% compared to the return of the optimal
recommendation policy π⋆
alg(θ) (Figure 5b). However, for θ ≤ 0.35, there is a significant drop in
performance, leading to a 4.01% reduction in effective return.
5.2. Stylized healthcare decision problem
We consider an MDP instance inspired from sequential decision-making in healthcare. In particular,
we approximate the evolution of the patient’s health dynamics using a Markov chain, using a
simplification of the models in Goh et al. (2018) and ?.
MDP instance. The dynamics of the MDP is represented in Figure 6. There are 5 states representing
the severity of the health condition of the patient, and an absorbing mortality state m. State
1 represents a healthy condition for the patient while State 5 is more likely to lead to mortality.
28
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1700
1750
1800
1850
1900
Return
Return for eff( ) Return for eff( alg(1), )
(a) Returns for recommending
π⋆
alg(θ) and π⋆
alg(1).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
4.0
3.5
3.0
2.5
2.0
1.5
1.0
0.5
0.0
Deterioration (%)
Proportional deterioration for alg(1) against alg( )
(b) Proportional deterioration
for π⋆
alg(1) against π⋆
alg(θ).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1
2
3
4
5
6
7
Different policies
Regions with different alg( )
(c) Subregions with constant
recommendation policies.
Figure 5 Numerical results for the machine replacement MDP with πbase repairing in the absorbing states 8,R1
and waiting in the other states.
There are three actions {low, medium, high}, corresponding to prescription of a given drug dosage
at every state. In any given state (except mortality), there is a reward of 20 for choosing action
low, a reward of 15 for choosing action medium, and a reward of 10 for choosing action high. There
is a reward of 0 in the mortality state m. The goal of the decision maker is to choose a policy to
keep the patient alive (by avoiding the mortality state m) while minimizing the invasiveness of the
treatment. We choose a discount factor of λ=0.99 and the patient starts in State 1.
1 2 5
0.4
0.3
0.7 0.4
0.3
0.3 0.3
0.3
m
1
0.3
(a) Transition probabilities for
action low.
1 2 5
0.4
0.2
0.8 0.4
0.4
0.2 0.2
0.4
m
1
0.2
(b) Transition probabilities for
action medium.
1 2 5
0.4
0.1
0.9 0.4
0.5
0.1 0.1
0.5
m
1
0.1
(c) Transition probabilities for
action high.
Figure 6 Transition probabilities for the healthcare MDP instance.
Numerical results. An optimal policy π⋆
alg(1) is to choose action low in States 1, 2, and to choose
action high in States 3, 4, 5. We now test the robustness of π⋆
alg(1) to partial adherence of the
patient. In particular, we consider three different baseline policies πbase. In Figure 7, Figure 8 and
29
Figure 9, we consider baseline policies πbase that always chooses action low, medium or high in every
health states, respectively. Our simulations highlights the sensitivity of the effective performance of
π⋆
alg(1), with respect to both the baseline policy and the adherence level. In particular, while π⋆
alg(1)
may loose up to 6.52% of the optimal effective return when the baseline policy always chooses low
dosage (Figure 7b), it only loses a maximum of 0.97% of the optimal effective return when the
baseline policy always chooses medium dosage (Figure 8b), and loses close to 0% of the optimal
effective return when the baseline policy always chooses high dosage (Figure 9b). In addition, we
observe that the range of the θ-values for which π⋆
alg(1) is optimal differs greatly from one baseline
policy to another (Figures 7c-8c-9c): when πbase always chooses low dosage, π⋆
alg(1) is optimal for
θ ≥ 0.82, whereas when πbase always chooses medium dosage, π⋆
alg(1) is optimal for θ ≥ 0.51, and
when πbase always chooses high dosage, π⋆
alg(1) is optimal for θ ≥0.20.
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
800
1000
1200
1400
1600
Return
Return for eff( ) Return for eff( alg(1), )
(a) Returns for recommending
π⋆
alg(θ) and π⋆
alg(1).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
6
5
4
3
2
1
0
Decrease (%)
Proportional deterioration for alg(1) against alg( )
(b) Proportional deterioration
for π⋆
alg(1) against π⋆
alg(θ).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00
Different policies
Regions with different alg( )
(c) Subregions with constant
recommendation policies.
Figure 7 Numerical results for the healthcare MDP with πbase choosing action low in all states.
30
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1200
1300
1400
1500
1600
Return
Return for eff( ) Return for eff( alg(1), )
(a) Returns for recommending
π⋆
alg(θ) and π⋆
alg(1).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1.0
0.8
0.6
0.4
0.2
0.0
Decrease (%)
Proportional deterioration for alg(1) against alg( )
(b) Proportional deterioration
for π⋆
alg(1) against π⋆
alg(θ).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00
Different policies
Regions with different alg( )
(c) Subregions with constant
recommendation policies.
Figure 8 Numerical results for the healthcare MDP with πbase choosing action medium in all states.
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1000
1100
1200
1300
1400
1500
1600
Return
Return for eff( ) Return for eff( alg(1), )
(a) Returns for recommending
π⋆
alg(θ) and π⋆
alg(1).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
0.06
0.05
0.04
0.03
0.02
0.01
0.00
Decrease (%)
Proportional deterioration for alg(1) against alg( )
(b) Proportional deterioration
for π⋆
alg(1) against π⋆
alg(θ).
0.0 0.2 0.4 0.6 0.8 1.0
Adherence level
1.00
1.25
1.50
1.75
2.00
2.25
2.50
2.75
3.00
Different policies
Regions with different alg( )
(c) Subregions with constant
recommendation policies.
Figure 9 Numerical results for the healthcare MDP with πbase choosing action high in all states.
6. Extensions and discussion
Finally, we discuss additional properties and potential extensions of our adherence-aware decision
framework.
6.1. Heterogeneous adherence levels across states
We have restricted our previous analysis to the case of a homogeneous adherence level θ ∈ [0, 1],
common to all states s ∈ S. However, in practice, it is possible that the adherence level differs
across states. For instance, in a healthcare setting, practitioners may be more prone to overlook
the algorithms’ recommendations when the patient is in a critical health condition because any
error may have life-threatening consequences. To model this practical consideration, we can extend
our model to heterogeneous adherence levels, θs ∈ [0, 1] for each state s ∈ S. In this model, at every
31
decision period t ∈ N and visited state st, the decision maker decides to follow the recommendation
policy πalg (with probability θs) or the baseline policy πbase (with probability 1−θs). The effective
policy πeff(πalg,θ) is now defined as
πeff(πalg,θ)s =θsπalg,s +(1−θs)πbase,s,∀ s ∈ S. (6.1)
All the structural results from Section 4.1 would generalize to this simple extension. In particular,
Proposition 4.3 still holds provided the non-decreasing property of θ 7→ R(π⋆
eff(θ)) is replaced with
an order-preserving property:
θs ≤θ′
s, ∀ s ∈ S ⇒R(π⋆
eff(θ))≤R(π⋆
eff(θ′)).
Importantly, we can still efficiently find an optimal recommendation policy π⋆
alg(θ) for any adherence
level θ ∈ [0, 1]S, by adapting the value iteration and the linear programming formulation to the
map fθ : RS →RS, defined as
fθ,s(v)= max
πs∈Δ(A)
θs ·
X
a∈A
πsaP⊤
sa (rsa +λv)+(1−θs) ·
X
a∈A
πbase,saP⊤
sa (rsa +λv) ,∀ s ∈ S.
6.2. Heterogeneous adherence levels across states and actions
Furthermore, it is plausible in practice that recommendations that are close to the baseline actions
are more likely to be followed than drastically different ones, e.g., in a healthcare setting where the
actions correspond to drug dosages. To model this situation, we can extend our framework further
to involve an adherence level that depends on each state s ∈ S and each action in a ∈A. Formally,
we could study policies of the form
πeff(πalg,θ)sa =θsaπalg,sa +(1−θsa)πbase,sa, ∀ (s, a) ∈ S ×A.
However, for every state s ∈ S, we need πeff(πalg,θ)s ∈ Δ(A), which imposes some non-trivial restrictions
on the values of θsa (which would depend on the probability of playing each action according
to πalg and πbase).
32
To circumvent this issue, we propose an alternative model where πeff(πalg,θ)s ∈ Δ(A) by design.
For the sake of simplicity, in this section, we assume that πbase is a deterministic stationary policy:
for each state s ∈ S we write πbase(s) ∈A for the action chosen by the policy πbase. At a state s ∈ S,
a recommended action a is sampled from the probability distribution πalg,s ∈ Δ(A). Then with
probability θsa ∈ [0, 1] the DM follows the recommendation (action a), otherwise the action selected
by the DM is πbase(s). With this model, the effective policy πeff(πalg,θ) for some (θsa)(s,a)∈S×A ∈
[0, 1]S×A is such that
πeff(πalg,θ)sa =


πalg,saθsa if a ̸=πbase(s),
1−
P
a′∈A\{πbase(s)} πalg,sa′θsa′ if a=πbase(s).
Note that the expression for the case a=πbase(s) simply follows from
1−
X
a′∈A\{πbase(s)}
πalg,sa′θsa′ =πalg,sπbase(s) +
X
a′∈A\{πbase(s)}
πalg,sa′(1−θsa′), (6.2)
i.e., action πbase(s) is chosen either because it has been sampled following πalg,s or because another
action a′ was sampled but the decision maker chose to follow πbase, which happens with probability
1 − θsa′ . We can now write the value function of a policy πeff(πalg,θ). For any s ∈ S, we obtain,
using (6.2):
v
πeff (πalg,θ)
s =
X
a∈A
πsa
􀀀
θsaP⊤
sa
􀀀
rsa +λvπeff (πalg,θ)
+(1−θsa)P⊤
sπbase(s)
􀀀
rsπbase(s) +λvπeff (πalg,θ)
.
Overall, we have obtained that the value function vπeff (πalg,θ) satisfies
v
πeff (πalg,θ)
s =
X
a∈A
πsa
􀀀
r′
sa +λP′⊤
sa vπeff (πalg,θ)
,∀ s ∈ S
with P′ ∈ (Δ(S))S×A , r′ ∈ RS×A the transition probabilities and the instantaneous rewards of
another surrogate MDP M′ with transitions and rewards defined as P′
sa := θsa · Psa + (1 − θsa) ·
Psπbase(s), r′
sa := θsa ·P⊤
sarsa+(1−θsa) ·P⊤
sπbase(a)rsπbase(a), for all (s, a) ∈ S×A. This shows that for this
model of state-action-dependent adherence level, we can efficiently find an optimal recommendation
policy by computing an optimal (nominal) policy for the surrogate MDP M′.
33
6.3. Uncertain adherence level
In our framework, we have assumed that the adherence level θ ∈ [0, 1] was known and used as
an input to design the recommendation policy πalg. This assumption is likely violated in practice,
where θ is not perfectly known. Instead, we can assume that the true adherence level θ is
uncertain but belongs to an interval [θ, ¯θ]. Under this assumption, we take a robust optimization
approach (Bertsimas and Sim 2004, Ben-Tal et al. 2009) and model the uncertainty in the value of
θ as an adversarial choice from the set [θ, ¯θ] of all possible realizations. The goal is to compute an
optimal robust recommendation policy, that optimizes the worst-case objective over all plausible
values of the adherence levels:
sup
πalg∈ΠH
min
θ∈[θ,¯θ]
R(πeff(πalg, θ)). (6.3)
The optimization problem (6.3) is reminiscent to robust MDPs, which consider the case where the
rewards and/or the transition probabilities are unknown (Iyengar 2005, Wiesemann et al. 2013),
but in our setting the same adherence level θ has an impact on the transition probabilities out of
every states s ∈ S in the surrogate MDP, which contradicts the classical rectangularity assumption
for robust MDPs. However, thanks to the structural properties highlighted in Section 4.1, the
optimization problem (6.3) can be solved as efficiently as AdaMDP, the adherence-aware decisionmaking
problem with known adherence level θ. Crucially, an optimal recommendation policy can
still be chosen stationary (i.e., in the set Π) instead of history-dependent (i.e., in the set ΠH), and
deterministic. Formally, we have the following theorem (proof detailed in Appendix J):
Theorem 6.1. An optimal robust recommendation policy in (6.3) may be chosen stationary:
sup
πalg∈ΠH
min
θ∈[θ,¯θ]
R(πeff(πalg, θ))= max
πalg∈Π
min
θ∈[θ,¯θ]
R(πeff(πalg, θ)).
Additionally, the pair
􀀀
π⋆
alg(θ), θ

with π⋆
alg(θ) a deterministic policy is an optimal solution to (6.3).
Theorem 6.1 is remarkable in that it shows that the same value of θ (in particular, the most
pessimistic value θ) is attaining the worst-case return for all policies. In practice, it reduces the
problem of estimating the true adherence level to the (admittedly easier) task of obtaining a valid
34
lower bound only. Furthermore, Theorem 6.1 also has significant computational impact since it
shows that solving (6.3) can be done by applying the same algorithms as the one described in
Section 4.2 with θ = θ. The resulting recommendation will also be a deterministic policy, which is
desirable in practice. The proof is very similar to the case of time- and state-invariant adversarial
adherence decision in Theorem 3.2 and we present it in Appendix J.
6.4. Uncertain baseline policy
Similarly, the baseline policy πbase is currently a known input to our adherence-aware MDP framework.
However, in practice it is possible that the algorithm only has access to an estimation ˆπbase of
the baseline policy, learned from a finite dataset, and that the true baseline policy differs from ˆπbase.
We consider a robust approach where the recommendation policy optimizes over the worst-case
baseline policy πbase ∈ Γ, where the set Γ ⊆ (Δ(A))S represents feasible baseline policies that are
close to the estimation ˆπbase, i.e., we consider
sup
πalg∈ΠH
min
πbase∈Γ
R(θπalg +(1−θ)πbase). (6.4)
The following theorem shows that (6.4) is still a tractable optimization problem under some mild
assumption on Γ. We provide the detailed proof in Appendix K.
Theorem 6.2. Assume that the set of feasible baseline policies Γ satisfies the following rectangularity
assumption: Γ=×s∈AΓs where Γs ⊆Δ(A) is a convex, compact set for each s ∈ S. Then an
optimal solution to (6.4) exists and can be chosen stationary. Additionally, if the set Γ is a polytope
or defined with conic constraints, then an optimal solution to (6.4) can be computed efficiently.
Our proof is based on showing that the optimization problem (6.4) can be reformulated as an
s-rectangular robust MDP (Wiesemann et al. 2013) with uncertain pair (r,P) of instantaneous
rewards and transition probabilities. This follows from the interpretation of AdaMDP as solving a
surrogate MDP, where the rewards and transitions, defined in (4.3), are dependent on πbase.
35
6.5. Varying adherence level
The adherence level θ may also vary over time. As the DM observes the recommendation made by
the algorithm over time, her trust in the recommendation, hence her adherence, may increase (or
decrease).
One could endogeneize these dynamics by making θ explicitly dependent on the recommended
policy πalg. However, the works of Boyacı et al. (2023), de V´ericourt and Gurkan (2023) highlight
how complex these dynamics can be, even for highly stylized decision problems, because of cognitive
limitations and asymmetric performance evaluation. Therefore, we conjecture that such gametheoretic
approaches (where πalg and θ are updated at each step) would be intractable for the type
of complex multi-stage decision problems we consider in this paper. Furthermore, as discussed
in Section 2, many mechanisms could explain partial adherence. Consequently, any method that
restricts the reasons for non-adherence (e.g., information asymmetry, algorithm aversion, cognitive
limitations) and derives update rules for the adherence level θ based on these mechanisms could
suffer from model misspecification.
Alternatively, one could capture the dynamic nature of θ by estimating it from past observations
in an online fashion. At a high-level, the optimization problem to which π⋆
alg(θ) is a solution
resembles that of an MDP whose transition probabilities depend on θ (and πbase). Hence, a varying
adherence level would lead to non-stationary transition probabilities. In the multi-armed bandit
literature, two types of assumptions are used to address non-stationarity. Garivier and Moulines
(2011) introduced a piecewise stationary assumption, where the parameters are constant over certain
time periods and change at unknown time steps. Alternatively, Besbes et al. (2014, 2015)
considered a slowly varying setting where the absolute difference between parameters at two consecutive
time-steps are bounded (by a so-called variation budget). Although originally derived for
multi-armed bandit problems, both these frameworks have been extended and used to solve nonstationary
MDPs (or non-stationary reinforcement learning problems) as well. We refer to Auer
et al. (2008) and Cheung et al. (2023) for an analysis of non-stationary MDPs under the piecewise
36
stationary and slowly varying assumptions respectively. Beyond the technical difficulties addressed
by the aforementioned works, learning θ from past historical data also suffers from a censorship
issue: if both πalg and πbase recommend the same action at a given state st, then it is impossible to
distinguish adherence from non-adherence.
We see our model based on partial adherence in offline sequential decision-making as a first
step towards a better understanding of the phenomena arising in expert-in-loop systems and a
better design of algorithmic recommendations. The online extension of our framework, where the
adherence level (and potentially the baseline policy πbase) needs to be continuously learned from
past observations constitutes an interesting future direction, as well as the case where the real MDP
parameters (r,P) themselves are only partially known to the human agent and the algorithm and
must be learned over time.










