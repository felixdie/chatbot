Wang, L., Huang, N., Hong, Y., Liu, L., Guo, X., & Chen, G. (2023). Voice‐based AI in call
center customer service: A natural field experiment. Production and Operations
Management, 32(4), 1002-1018.

Voice-based AI in Call Center Customer Service:A Natural Field Experiment

Abstract
Voice-based artificial intelligence (AI) systems have been recently deployed to replace traditional
interactive voice response (IVR) systems in call center customer service. However, there is little
evidence that sheds light on how the implementation of AI systems impacts customer behavior, as well
as AI systems’ effects on call center customer service performance. By leveraging the proprietary data
obtained from a natural field experiment in a large telecommunication company, we examine how the
introduction of a voice-based AI system affects call length, customers’ demand for human service, and
customer complaints in call center customer service. We find that the implementation of the AI system
temporarily increases the duration of machine service and customers’ demand for human service;
however, it persistently reduces customer complaints. Furthermore, our results reveal interesting
heterogeneity in the effectiveness of the voice-based AI system. For relatively simple service requests,
the AI system reduces customer complaints for both experienced and inexperienced customers.
However, for complex requests, customers appear to learn from the prior experience of interacting with
the AI system, which leads to fewer complaints. Moreover, the AI-based system has a significantly
larger effect on reducing customer complaints for older and female customers as well as for customers
who have had extensive experience using the IVR system. Finally, we find that the speech-recognition
failures in customer-AI interactions lead to increases in customers’ demand for human service and
customer complaints. The results from this study provide implications for the implementation of an AI
system in call center operations.

Keywords: Artificial intelligence; customer service; service flexibility; natural field experiment;
difference-in-differences

Advances in machine learning (ML) technology have accelerated the application of voice-based
artificial intelligence (AI) systems in various business functions, performing tasks such as speech
recognition and natural language processing.1 With the intention of improving customer experience as
well as reducing service costs, an increasing number of companies are deploying voice-based AI to
complement or replace current systems and services provided by human agents (Xiao and Kumar
2021). According to Markets and Markets (2021), the global conversational AI market size is predicted
to grow from $6.8 billion in 2021 to $18.4 billion by 2026, and AI-supported customer service is a
major factor driving the growth. Moreover, the value of global call center AI market reached 959.80
million in 2020 and is predicted to reach $ 9,949.61 Million by 2030 (Valuates Reports 2022).
Our study examines the implementation of a voice-based AI system that replaces the traditional
interactive voice response (IVR) system in a customer service call center. In the absence of the AI
system, customer calls are first connected to the IVR system and customers communicate with the IVR
system through phone keypads to obtain specific services. The service requests that the IVR system
cannot handle are then transferred to human agents. Upon the rollout of a voice-based AI system,
customers communicate with the AI in natural dialogues, and the AI system performs tasks, such as
processing natural language in a manner that resembles human intelligence. Note that the voice-based
AI system is different from the traditional IVR system in several ways, as summarized in Table 1. To
begin with, the AI system continuously evolves with the accumulation of a large amount of service data,
enhancement in computing power, and improvement in learning algorithms (LeCun et al. 2015). In
contrast, the IVR system was designed by industry experts based on their service experiences, does not
change with service data, and requires customers to strictly follow pre-set rules when interacting with
the system (Resnick and Virzi 1995). Through speech recognition in the AI system, customers can tell
the system which kinds of services they require; on this basis, they are directly routed to certain services
(Tang et al. 2003). In contrast, the IVR system typically relies on a hierarchical structure that directs
customers in a step-by-step manner to locate specific services (Suhm et al. 2002). To switch services,
customers are required to return to the main service menu and repeat the above-mentioned actions to
select another service.
Table 1. Voice-based AI System vs. IVR System
Voice-Based AI System IVR System
Inputs for Building the System Large amounts of service data Expert knowledge
Technological Characteristics
Natural language processing
Speech recognition
Improves with service data
Pre-designed services transferring rules
Remains the same as originally designed
Customer-System Interaction Interacting in natural dialogues Inputting specific information
Service Organization Direct routing Hierarchical structure
Considering the technological advantages of the voice-based AI system, the implementation of
such a system as a replacement for the traditional IVR system might significantly influence customer
service experiences. First, the AI system improves the flexibility of service flows and enables
personalized customer service. Instead of strictly following pre-defined service flows like in the IVR
system, customers can actively control the pace of service when they interact with an AI-based system.
For example, because of the AI system’s flexible navigation structure, customers can skip over the
layers of IVR structures and directly access the desired services. Second, the voice-based AI system can
adapt to customers’ interaction preferences to improve their service experiences. While the IVR system
provides highly structured and limited choices for customers, the AI system can interact with customers
in natural dialogues, allowing customers to express their needs adequately (Fountain et al. 2019).
Moreover, the voice-based AI system has the ability to learn from prior interactions with the input data
from customers and iteratively improve its performance. In scenarios wherein the AI system gets stuck,
it can tag the problems with the help of human agents and learn from the scenarios to resolve similar
problems in the future (Wilson and Daugherty 2018).
According to Forbes Insights, call centers are predicted to be the new sandbox for AI-powered
customer experience, as they are expected to deploy AI-based tools to boost retention, loyalty, and
profit.2 AI technology has gained widespread adoption in call center services over the past few years.3
Nonetheless, despite the growing interests in AI, its implementation has a
proof-of-concept-to-production gap (Perry 2021). In other words, AI can work well theoretically or on
test data, but it may fail to reach expectations in practical settings. In our study context, an IVR system
operates with pre-designed fixed logic, while an AI-based system relies on a complex algorithmic
structure. In turn, given the high variability of customer interactions in call center service, the service
efficacy of an AI system is likely to be subject to variation (Brynjolfsson and Mcafee 2017). For
example, customers may speak with accents or dialects while communicating with AI, thereby resulting
in speech-recognition failures that influence the effectiveness of the systems. However, without the
rigid logic of an IVR system, an AI-based system might learn to adapt to the customer interactions and
even be more effective than an IVR system. The high variability of tasks within customer service
requires more flexible, human-like responses, and the AI system may work better than the strictly
programmed IVR systems that are inflexible. Therefore, we believe it is important for researchers to
empirically test the effectiveness of AI in real-life settings.
Most prior studies on AI in operation management (OM) have mainly focused on the effects of
AI-supported automation and smartness, and examine how related technologies are deployed to
facilitate operation decisions or redesign operation process in product pricing (Karlinsky-Shichor and
Netzer 2019), order decision-making (Li and Li 2022), and quality management (Senoner et al. 2021).
Limited work explores the role of AI in interactions between customers and service systems,
particularly in service contact design scenarios (Roth and Mentor 2003). With few exceptions, for
example, Cui et al. (2021) examined the role of AI in buyer price requests and its influence on seller
price quotations in business-to-business (B2B) wholesaling. Contributing to this knowledge gap in
prior literature, our study focuses on the effects of implementing a voice-based AI system in the
business-to-consumer (B2C) customer service setting. Specifically, analyzing data from a natural field
experiment in a large telecommunication company’s customer service call center, we seek to answer the
following research questions:
How does the introduction of voice-based AI systems impact call length, customer’s
demand for human service, and customer complaints in call center customer services? How do the
effects of AI implementation in customer service vary for different customers?
To this end, we examine a natural field experiment4 with a voice-based AI system implemented
in a telecommunication company’s call center customer service operation. In the experiment, the
company’s customer service operation rolls out the AI system to replace the IVR system in different
phases, serving a portion of its customers based on the last digit of the customer’s phone number,
4 A natural field experiment (NFE) is the type of experiment ―where the environment is one where the subjects naturally
undertake these tasks and where the subjects do not know that they are participants in an experiment. Such an exercise
represents an approach that combines the most attractive elements of the lab and naturally-occurring data: randomization and
realism. (List 2007)
thereby allowing them to engage in customer service calls through natural dialogues with AI. We then
use difference-in-differences (DID) estimations to identify the effects of the AI-based system on key
outcomes. Our results reveal that the duration of machine service and customers’ demand for human
service increases temporarily after the introduction of the voice-based AI system, suggesting a possible
novelty effect. Meanwhile, the AI system significantly and persistently reduces customer complaints.
Moreover, we find interesting heterogeneity in the main effects of the AI system. To begin
with, the effects of the AI system on customer complaints appear to depend on the complexity of the
service requests. Compared with the customers who continue to use the traditional IVR system, the
customers assigned to use the AI-based system tend to make fewer complaints when they have
relatively simple service requests. In contrast, with relatively complex service requests (i.e., service
calls transferred to human agents), customers learn from their prior interactions with AI; this learning
effect leads to fewer complaints. Lastly, we find that the AI-based system exerts a significantly greater
effect on reducing customer complaints for older customers, female customers, and for customers with
longer user tenure.
Our study makes several important contributions to the related literature on AI applications and
call center operations. First, our study adds to research on AI applications by extending the scope to the
call center customer service setting and offers useful insights into how AI-powered service flexibility
impacts different outcomes in human-AI interactions (Luo et al. 2019, Sun et al. 2019, Cui et al. 2021).
Second, we contribute to the literature on call center customer service operations by empirically
examining how the implementation of the AI system affects customer behavior and the performance of
customer service, responding to calls for research on using disruptive technologies like AI to address
OM problems in general (Kumar et al. 2018, Karlinsky-Shichor and Netzer 2019) and to explore the
direct effects of technology-mediated customer-involved service contact designs in particular (Roth and
Menor 2003). In addition, building on related OM literature on call center operations, which views
customers as mostly homogenous and uses a single metric to represent the performance of operation
systems for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), our study further explores
customer heterogeneity in responding to the operations of the voice-based AI system.
Furthermore, our findings also offer useful implications for practice. We demonstrate that using
the voice-based AI system to replace the IVR system does not result in customer aversion to the AI
system, thereby validating the effectiveness of using voice-based AI systems in call center customer
service. We also demonstrate the novelty effect of implementing an AI system and find significant
heterogeneity in the effectiveness of voice-based AI systems on reducing customer complaints based on
the complexity of customer requests as well as customers’ age, gender, and tenure with the company
service, respectively. These results provide actionable insights into the implementation and further
development of voice-based AI systems. For example, companies must consider the possible short-term
increases in the duration of machine service and customers’ demand for human service while
scheduling for a service system that applies voice-based AI to replace the IVR system. Instead of
relying on customers’ self-learning, companies could educate their customers on using an AI system
with relatively complex requests. Moreover, the details obtained from customer-AI conversations
reveal that speech-recognition failures may lead to negative consequences. Therefore, it is necessary for
companies to continuously improve the capability of their AI systems to cater to a diverse customer
base.
2. Related Literature
2.1. Application of Artificial Intelligence (AI) Systems
Following prior literature, we define AI systems as algorithms that perform perceptual, cognitive, and
conversational functions typical of the human mind (Longoni et al. 2019). In recent years, the
significant development of AI systems has led to wide adoptions and applications in various domains.
Specifically, in the OM literature (see the summary of related literature in E-Companion A), from the
technical perspective, some prior work attempted to design AI-based algorithms to solve operational
problems such as demand or sales forecasting (Cui et al. 2018), product pricing (Yang et al. 2022), and
quality inferring (Senoner et al. 2021). Meanwhile, scholars have also explored how AI-enabled
automation and smartness features facilitate or support operational decisions in contexts such as price
request (Cui et al. 2021), order decision-making (Li and Li 2022), and automated pricing
(Karlinsky-Shichor and Netzer 2019).
Recently, a few studies on the application of AI systems have begun to understand the use of
such systems in commerce operations, where the AI system directly interact with individuals. For
example, Cui et al. (2021) examined how AI chatbots affect suppliers’ price quoting strategies. They
found that automation of chatbots alone leads to discrimination against chatbot buyers, but signaling the
use of a smart recommendation enabled by an AI system effectively reduces suppliers’ price quote for
chatbot buyers. In addition, Sun et al. (2019) suggested that the use of voice-based AI in online
shopping significantly affects consumers’ search behavior and purchase decisions. These prior studies
primarily focused on the role of AI systems in facilitating sales, but not much is known regarding the
application of voice-based AI systems in post-sales—that is, the scenario of customer service. In this
regard, our study aims at addressing this research void in the stream of work on AI system applications
by empirically examining the impact of implementing a voice-based AI system as a replacement for an
IVR system for customer service on the key outcomes related to customer experience and call center
operations.
2.2. Information Technology and Service Operation
Information technology plays an important role in improving service operations (Roth and Menor
2003). Companies increasingly rely on technology-based services to reduce service costs (Krishnan et
al. 1999) and increase service efficiency (Beckman and Sinha 2005). In recent years, with the
accumulation of large amounts of data on customers and transactions, companies have gradually
applied data-driven algorithms to automatically process service-related tasks—such as customer
segmentation, pattern identification, service instruction, and real-time personalization—which, in turn,
help companies to improve service quality (Sodhi et al. 2022, Sun et al. 2022).
Customers play an essential role in the delivery of services (Roth and Menor 2003). The design
of customer contact—the interaction between a customer and a service provider—is important for
shaping customers’ service experiences (Kellogg and Chase 1995). Prior research has empirically
examined the contact between customers and employees (Kellogg and Chase 1995, Soteriou and Chase
1998) and demonstrated that the physical service environment significantly influences customers’
perceptions and behavior (Bitner 1992). However, the advancement of information technology is
changing the ways in which customers interface with service providers. For example, companies
commonly establish self-service systems to cater to customers’ real-time service needs (Tezcan and
Behzad 2012). More recently, AI is being implemented to replace or complement conventional service
providers (Xiao and Kumar 2021). Froehle and Roth (2004) extended the customer contact perspective
to technology-mediated services and called for research on exploring the effects of virtual service
contact designs. Therefore, this paper focuses on the effects of different virtual service contact designs
(i.e., IVR and AI systems) in the context of call center customer service and specifically investigates
how replacing IVR systems with voice-based AI directly influences customers’ interaction outcomes.
2.3. Information Technology and Call Center Customer Service
Call center customer service has been an essential channel through which customers interact with firms
(Aksin et al. 2007, Tezcan and Behzad 2012). New developments in information technology provide an
opportunity to redesign and improve service-delivery operations in call centers. For example,
information technology supports a call center to expand to a larger scale (Adria and Chowdhury 2004).
In such contexts, researchers have examined the effects of call center centralization (Adria and
Chowdhury 2004) and discussed the risks caused by large-scale service systems (Pang and Whitt 2009).
Meanwhile, capacity management translates into a complex process in modern call centers. Researchers
have thus investigated the impacts of flexible labor resources (Kesavan et al. 2014) and attempted to
develop real-time schedule adjustment frameworks (Mehrotra et al. 2010). Another prevalent
technology-enabled change in call center operation is outsourcing; a wealth of research has explored
issues related to outsourcing (Kocaga et al. 2015) and call-routing (Gans and Zhou 2007) strategies in
such contexts. In the above studies, researchers have mainly focused on optimizing system designs in
contexts where technologies have been deployed to facilitate service operations (e.g., Aksin et al.
2007). However, little research has explored the effects of different technology-mediated contact
designs with customers directly involved in service delivery (Roth and Mentor 2003).
Specifically, in call center customer service, one typical technology-mediated service contact
design is the IVR system, which enables self-service at the front end of phone calls (Tezcan and Behzad
2012). Well-implemented IVR systems have the potential to automate a significant portion of services
and lead to improved customer service experiences (Tezcan and Behzad 2012). Thus, ample prior work
has examined the design of IVR-equipped service systems (Khudyakov et al. 2010, Suhm and Peterson
2002). Meanwhile, related studies from the user’s perspective reveal that customers often feel
when they interact with an IVR system because they perceive the services provided by IVR systems
be less customized and report that such systems occasionally do not understand their needs (Dean
2008). Consequently, customers often attempt to avoid IVR systems due to the lack of personalized
services or social interactions; instead, they seek direct interaction with human agents (Tezcan and
Behzad 2012).
Recent developments in AI technologies have enabled its applications in various contexts
(Brynjolfsson et al. 2019, Cui et al. 2021, Sodhi et al. 2022). For example, in 2017, Google’s machine
learning algorithms achieved a 95% accuracy rate for speech recognition in the English language, a
level that is close to actual human dialogue.5 In the customer service setting, voice-based AI systems
can understand customer needs through their voice inputs and can interact with customers in a
human-like manner (Van Doorn et al. 2017, Wilson and Daugherty 2018, Xiao and Kumar 2021).
However, considering the complexity of AI technology, it is challenging to determine the
effectiveness of AI systems in a real-world setting that goes beyond training data (Brynjolfsson and
Mcafee 2017). Once an AI system is deployed, it is expected to handle a large variety of situations
that may be unforeseen in training data. For example, customers may speak with accents or dialects
while communicating with AI, thereby resulting in speech-recognition failures that influence the
effectiveness of systems. Therefore, how AI implementation affects customer behaviors and the
performance of customer services remains an important empirical question that warrants further
investigation.
3. The Effects of the Implementation of AI on Call Center Customer Service
Based on the above discussions, in this section, we seek to discuss a few predictions on how the
implementation of a voice-based AI system in call center customer service will affect three key
metrics that are of interest to OM researchers: call length, demand for human service, and customer
complaints. While we do not provide any directional hypotheses in this section, the discussion serves
as a theoretical basis that guides our empirical analyses, which we report in subsequent sections.
5 Google’s ability to understand natural language is almost equivalent to that of humans.
Call length. Call length represents the duration of a customer’s service call (Gans et al. 2003),
which is important in the management of call center customer service operations because it directly
impacts scheduling and routing designs (Gans et al. 2003). In a traditional IVR system, the services
are organized in a tree-like hierarchical structure, whereby the leaves represent different services, the
nodes indicate customer states in the system, and the connections among different nodes indicate the
paths to specific services. All paths are pre-designed and customers can move only from one node to
another by inputting information in accordance with the guidance of the system. Typically, customers
must pass through several nodes before reaching certain services. Meanwhile, they must pay attention
to obtaining information on how to move from one node to another. An IVR system design typically
entails a time-consuming service experience. In contrast, with an AI-based service system, customers
can skip all the layers of IVR structures and directly access intended services by briefly summarizing
their needs to the system, which is likely to result in shorter call lengths, compared to a traditional
IVR system.
Conversely, it is also possible that an AI system leads to an increase in call lengths, as
compared to IVR systems, due to the characteristics of the speech-based interaction mode. To begin
with, when using the AI system, customers need to take time to summarize their needs in the form of
dialogues for the AI system to predict the intended services. Second, according to prior research on
communication modes, individuals interacting with text-based service systems (e.g., by inputting
numbers in the IVR systems) follow the cognitive economy principle, such that they are more likely
to focus on service requests and use keyword commands to improve communication efficiency (Le
Bigot et al. 2007). In contrast, the speech-based interaction mode enhances users’ involvement; users
tend to use quest-irrelevant expressions, such as politeness expressions in their interactions (Chafe
1982, Le Bigot et al. 2007), which could make information exchanges less effective (Le Bigot et al.
2007). In addition, when in conversational mode with an AI, users are expected to adapt their
behavior to the interaction system (Le Bigot et al. 2007, Cowan et al. 2015). Consequently, users
might devote more time and cognitive effort to formulate their speech and repeat information heard
during the interactions in order to share a common lexicon and syntactic structure with the interaction
system (Le Bigot et al. 2007). Therefore, speech-based AI service interactions may have longer
service durations than services handled by an IVR system. Based on the above discussions, it is
challenging to clearly predict the direction as well as the magnitude of changes in call length after the
introduction of the voice-based AI system; thus, the effect of the voice-based AI system (vs. IVR
system) remains an open question that warrants further empirical investigation.
Demand for human service. Customers’ demand for human service has direct implications
for staffing problems and operational costs within call center customer service (Tezcan and Behzad
2012). The introduction of AI may have mixed effects on customers’ demand for human service. On
the one hand, prior studies on AI applications demonstrate that, in certain contexts, individuals have a
subjective perception against AI and, thus, might be reluctant to interact with it even though AI now
offers high-level performance (Dietvorst et al. 2015; Longoni et al. 2019; Luo et al. 2019). When the
AI system offers the flexibility of transferring to human agents, customers may skip interacting with
AI and turn directly to human agents. Therefore, the AI-based system may increase customers’
demand for human service.
On the other hand, previous research also suggests that providing individuals with even a
slight amount of control over the AI’s behavior has the potential to mitigate their aversion to it
(Dietvorst et al. 2018), and this could be the case in our study. For example, the AI system enables
customers to control the pace of service and customers have the freedom to decide when to transfer to
human agents. Such a user-friendly design may mitigate customers’ aversion to interacting with AI
systems as well as mitigate any potential increase in customers’ demand for human service.
Considering both sides of the arguments, it is unclear whether and to what extent the implementation
of a voiced-based AI system would influence customers’ demand for human services; thus, we seek to
test this relationship empirically.
Customer complaints. Customer complaints are manifestations of customers’ negative
service experience (Singh 1988). Firms expend significant effort to improve customer service
experience and reduce customer complaints. According to the service operations literature, firms
create standardized service routines to control service delivery and ensure a uniform service level
(Leidner 1993). Standardized service routines reflect the preference of service providers with regard
to the manner in which customer needs must be met, with the process steps being organized in a
particular order. The service processes are largely determined by average customer demands and
preferences (Victorino et al. 2013). Since they follow service routines designed for an average
customer, service systems lack flexibility, cannot spontaneously react to unforeseen situations (Groth
et al. 2009), and are likely to overlook customer heterogeneities (Ashforth and Fried 1988). Dealing
with customer heterogeneities (e.g., request and preference variations) was a major challenge for
service operations (Frei 2006), and flexibility is one of the important capabilities in operational design
(De Groote 1994, Aksin et al. 2007).
A diverse environment and heterogenous needs are best fitted with flexible technology (de
Groote 1994). Specifically, improving the degree of flexibility in how service systems react to
customer requests enables the delivery of customized services (Tansik and Smith 1991), thereby
enabling an enhancement of customers’ service experiences (Roth et al. 2006). For example,
Victorino et al. (2013) reported that customers’ perceived lower service quality from dinner
recommendation services provided by an employee who rigidly follows the service script. In contrast,
customers gave high ratings to service interactions in which the employee offers the flexibility of
reacting to customer varieties (Victorino et al. 2013). In addition, Heim and Sinha (2002) showed that
the flexibility of the service process in electronic retailing is positively associated with customer
satisfaction. However, increased flexibility in a rule-based service system could potentially be
accompanied by an increased complexity of the system, which makes it more likely to result in
subjective service failures.
In the context of our study, the voice-based AI system accommodates customers’
communication preference heterogeneities by enabling customers to express their service needs in
ways that are most suitable for them. Meanwhile, compared with the IVR system, the AI system
enhances the flexibility of service flows so that customers can directly locate their intended services,
switch among different services, and transfer to human agents whenever they want. Therefore, we
expect the implementation of the voice-based AI system to enhance customers’ experience and reduce
customer complaints; moreover, we also seek to empirically evaluate this effect.
4. Background and Data
4.1. Natural Field Experiment
Our study considers a natural field experiment conducted by a large telecommunication company’s
call center customer service, which serves as an important channel for customer-firm interaction
(Aksin et al. 2007). The company was established in 1995 and now has 14 branches and over 8,000
employees, providing services to over three million customers in a major city (covering an area of
approximately 53,100 km2) in northeast China, with a market share of 33%. The company rolled out
its voice-based AI system in its call center customer service system based on the last digit of customer
phone numbers. Figure 1 summarizes the timeline of the natural field experiment. Before Dec. 19,
2018, all service calls were connected to the IVR system. From Dec. 19, 2018, the AI system was
implemented to replace the IVR system for a certain portion of the company’s customers, which was
chosen on the basis of a set of randomly selected last digits of the customer’s phone number.
Specifically, service calls from phone numbers with the last digit 1 or 7 were connected to AI, while
calls from other phone numbers remained connected to the IVR system. Between Dec. 19 and Dec.
31, 2018, the updated service system was in the beta-testing phase and was not connected to the
internal databases. Therefore, no service records were stored. Beginning on Jan. 1, 2019, the AI
system was connected to internal databases with phone records stored. Thereafter, beginning Jan. 10,
2019, in addition to service calls from phone numbers ending in 1 or 7, service calls from phone
numbers ending in 3, 5, or 9 were also connected to the AI system. After Jan. 15, 2019, the AI system
completely replaced the IVR system.
Figure 1. Timeline of the Natural Field Experiment
While interacting with the AI-based system, customers verbally state their requests briefly
and the AI system provides instant responses based on the analysis of information input by the
customers. If customers do not describe their requests clearly, the AI system asks specific questions to
guide customers to providing more information so that the AI can route them to the specific services
in order to meet their needs (e.g., payments, check balance, temporarily stop service). If the IVR
system or AI system is unable to provide specific services that customers are looking for, the
customers have the option to be transferred to human agents. When interacting with the IVR system,
customers need to strictly follow pre-designed service flows. After navigating the entire voice
guidance on possible services, the system tells customers to press a specific number to be transferred
to human agents. In contrast, the AI-based system sets no restrictions on when and how customers can
transfer to human agents. At the beginning of the service, the AI system tells customers ―If you need
help from human agents, please say ‘Transfer to human agents’.‖
4.2. Data & Measures
The implementation of the voice-based AI system in call center customer service provides exogenous
variations on the type of service system (voice-based AI vs. IVR system) that a customer experiences.
The observation duration in our study was 30 days, including a 21-day pre-treatment period (Nov. 28
to Dec. 18, 2018) and a 9-day treatment period (Jan. 1–9, 2019).6 We exclude the beta-testing phase
between Dec. 19 and Dec. 31, 2018, because we were unable to observe the outcome variables during
this phase.
Our data set contains timestamps of customers’ phone call records, such as the start and end
times of service calls, customers’ profile information, such as age and gender, when a customer began
using the telecommunication service, as well as the transcripts of customer-AI conversations from all
the customers served by the company. We constructed the variable Call Length to measure a service
call’s duration using the difference between a call’s start and end times. Considering the skewed
distribution of Call Length (Gans et al. 2003), we log-transformed this variable in our estimations. In
addition, we constructed the variable Human Service to capture whether a customer transferred to
human agents in the service call. Further, extending the related research on customer service
6 In this natural field experiment, we have a nine-day treatment period (between Jan. 1 and Jan. 9, 2019, when the customers
with the last digit of phone number 7 connected to AI system vs. those with the last digit of phone number 9 connected to
IVR system). Also, a three-week pre-treatment period allows us to check the parallel trend of our key outcome variables
before the experiment (from Nov. 28 to Dec. 18, 2018, when all service calls were connected to the IVR system). In
addition, the cooperating telecommunication company designs its services on a monthly basis (e.g., customers choose a
monthly service package, charge bills for the next month). Taken together, we chose a 30-day observation window.
experiences in OM (Aksin et al. 2007), we considered customer complaints as one typical
consequence of negative service experiences. Here, we constructed the variable Customer Complaint
to capture whether the customer complained about the service within 30 minutes after a service call.
Following prior research on technology acceptance (Venkatesh et al. 2012), we also included
a few additional variables on the observable individual characteristics to understand whether
individual differences (e.g., age, gender, experience) moderate users’ acceptance and use of the
voice-based AI system. Specifically, we observed user age and gender. In addition, we constructed the
variable Service Tenure to measure how many years a customer has been using his or her phone
number and consider it a proxy for the customer’s experience using the IVR system. Table 2 presents
a summary of the operationalization of the main variables.
Table 2. Variables and Definitions
Variables Definitions
Call Length The duration of a service call. It is measured by the difference between a call’s
end time and start time
Human Service Whether a customer chose to transfer to human agents, with Yes = 1 and No =
0
Customer
Complaint
Whether a customer complained about the service within 30 minutes, with Yes
= 1 and No = 0
Age The actual age of a customer calculated based on his/her birthday information
Gender Female = 1 and Male = 0
Service Tenure The number of years a customer has been using his/her phone number. It is
regarded as a proxy for the customer’s experience using the IVR system
5. Methodology & Results
5.1. Econometric Identification
First, we processed the data to ensure comparability between our treatment and control groups in the
company’s natural field experiment. In the experiment, the group assignment hinges on the last digit
of the customers’ phone number, instead of randomly assigning customers to either the treatment or
control group. To ensure comparability of observations, given the lack of individual random
assignment, we tried to balance our samples before data analyses. If certain individuals prefer an even
number as the last digit, it may result in significant differences between the two groups (i.e., even
numbers vs. odd numbers); this is also observed in our data (see E-Companion B). To address this
issue, we first excluded data from customers with phone numbers that end in even numbers (i.e., 0, 2,
4, 6, and 8). Further, we conducted a series of pairwise comparisons of the observable covariates (i.e.,
Age, Gender, Service Tenure) and the pre-treatment values of outcome variables by the last digit of
the customers’ phone numbers (see Table B1 in E-Companion B). We found no significant
differences in the observable covariates across the groups of customers whose phone numbers end
with 7 and 9, as indicated in Table 3. Table 4 reports the descriptive statistics of the main variables.
Table 3. Comparisons of Observable Covariates and Pre-Treatment Values of Outcome
Variables
Last Digit Age Gender
Service
Tenure
Log (Call
Length)
Human
Service
Customer
Complaint
7 43.579
(11.516)
0.639
(0.480)
8.507
(3.740)
4.453
(0.953)
0.320
(0.466)
0.014
(0.116)
9
43.954
(11.590)
0.648
(0.478)
8.650
(3.728)
4.470
(0.922)
0.316
(0.465)
0.012
(0.109)
p-value 0.466 0.993 0.207 0.345 0.658 0.455
Notes: Standard errors are given in parentheses. We observed insignificant differences on the observable
covariates and the pre-treatment values of the outcome variables, thereby suggesting comparability of the groups
with customers whose phone numbers end with 7 vs. 9.
Table 4. Descriptive Statistics
Variables Observations Mean SD Min Max Median
Log (Call Length) 18,580 4.446 0.869 2.996 7.365 4.407
Human Service 18,580 0.288 0.453 0 1 0
Customer Complaint 18,580 0.013 0.114 0 1 0
Age 18,580 43.765 11.554 15 70 43
Gender 18,580 0.643 0.479 0 1 1
Service Tenure 18,580 8.578 3.734 2 23 8
To provide additional model-free evidence for the treatment effect and comparability of the
two groups, we also show the pre-treatment parallel trends of the outcome variables in Figures C1–C3
in E-Companion C. Overall, the evidence indicates that customer groups with the last digit as 7 vs. 9
are comparable. Thus, we opt to use the customer groups whose phone numbers ended with 7
(treated) and 9 (control) to estimate the effects of the AI-based system on the outcome variables in our
main analyses.
Thereafter, we performed difference-in-differences (DID) analyses to estimate the effects of
the voice-based AI system on service call length, customers’ need for human service, and customer
complaints. We specify the DID estimations with Equations (1), (2), and (3). In addition, we also
incorporate customer-level random effects in these estimations.7
( )
. (1)
( )
( )
. (2)
7 In this paper, we selected random-effects models for two reasons. First, in our field experiment, randomization occurred
based on the last digit of the phone number. Thus, for the same customer, all his/her service records were either in the
treatment group or the control group. When conducting regressions with customer fixed effects, a few variables would have
been subsumed, like the variable AI_agent that indexes whether a record was in the treatment or control group, and dummies
that capture the features of specific customers (e.g., Age, Gender, Service Tenure) in the regression models. Second, for
dummy variables Human Service and Customer Complaint, the records of these variables with a value of 1 are relatively
sparse. If we chose fixed-effects models, many observation groups would have been omitted because of all negative
outcomes (i.e., Human Service = 0 or Customer Complaint = 0). In order to check the robustness of our results, we also
present the estimation results of fixed-effects models.
( )
( )
.
(3)
In the equations above, i denotes customers, and t denotes observation time; AI_agent is a
dummy variable, with 1 representing service calls from customers in the treatment group (i.e.,
customers whose phone numbers end with 7) and 0 representing service calls from customers in the
control group (i.e., customers whose phone numbers end with 9). After_AI is a dummy variable that
equals 1 for observations that took place after the introduction of the AI system and 0 for observations
on or prior to Dec. 18, 2018; Day Dummyt is a vector of time dummies representing each day during
our observational period; ui represents customer-specific random effects; and is the error term. In
Equation (1), the dependent variable is Log (Call Length). In Equations (2) and (3), we observed the
binary indicators of whether a call is transferred to human service and whether a call service
eventually received a customer complaint; we estimated these outcomes with logistic regressions. We
are interested in the coefficients of the interaction term, AI_agent * After_AI, as they capture the
effects of the AI system (compared with the IVR system) on the outcomes. For example, if the
coefficient in Equation (1) (i.e., ) is positive and statistically significant, it suggests that compared
with control customers—who used the IVR system and did not access the AI system—the treated
customers, who did use the AI system experienced longer average call length after the implementation
of the AI system.8
5.2. Main Findings
The regression estimations are presented in Tables 5 and 6, demonstrating the effects of the AI system
on Log (Call Length), Human Service, and Customer Complaint, respectively. To explore how the
durations of machine and human calls change after the implementation of the AI system, we separated
8 We also conducted a series of placebo tests to check whether the identified effects existed before the introduction of the AI
system or between customer groups who did not get access to the AI system. The results of the placebo tests are reported in
E-Companion D.
the duration of machine service (Machine_Call Length) from human service (Human_Call Length)9
and then conducted regressions. The results presented in Columns 1–4 in Table 5 suggest that the
implementation of voice-based AI significantly increases the duration of machine service by 5.65%
(i.e., 100 × ( −1) %) but exerts no effect on the duration of human service. These results indicate
that customers tend to spend more time interacting with the AI system as compared to the IVR
system. The results echo previous literature stating that, compared to mechanical self-service systems,
customers tend to engage more with natural language-based service robots (Huang and Rust 2021).
Specifically, diving into the records of customer-AI conversations, we found evidence for the wide
use of quest-irrelevant characteristics (e.g., the use of first- or second-person pronouns, politeness
expressions, and hesitation expressions), suggesting enhanced involvement during speech-based
interactions, which can lead to an increase in the service duration (Hauptmann and Rudnicky 1988, Le
Bigot et al. 2007).10 Meanwhile, per the results in Columns 5 and 6 in Table 5, we found inconclusive
evidence for the effects of the voice-based AI system on the total call length (i.e., Log (Call Length)).
Table 5. Effects of AI Implementation on Call Length
Variables (1) Log
(Machine_
Call
Length)
(2) Log
(Machine_
Call
Length)
(3) Log
(Human_C
all Length)
(4) Log
(Human_C
all Length
(5) Log
(Call
Length)
(6) Log
(Call
Length)
AI_agent 0.003
(0.018)
0.021
(0.071)
0.005
(0.022)
AI_agent * After_AI 0.055**
(0.021)
0.041*
(0.022)
0.082
(0.082)
0.012
(0.083)
0.050**
(0.023)
0.032
(0.023)
Age -0.003***
(0.001)
-0.027**
(0.003)
-0.006***
(0.001)
9 Machine_Call Length measures the duration of a machine (i.e., AI or IVR) service, while Human_Call Length captures the
duration of a service delivered by human agents.
10 We examined the records of customer-AI conversations to provide possible explanations for the increase in call length
(particularly in machine call length) and found that, in 28.8% of the conversations, customers use first- or second-person
pronouns (e.g., ―I‖, ―you‖), suggesting enhanced involvement during conversations. In addition, approximately 21.3% of the
conversations include at least one utterance to express politeness (e.g., ―Thank you‖) and 16.0% of the conversations contain
hesitation expressions (e.g., ―Uh‖) when customers form utterances during conversations. Consistent with research on
interaction mode, all these quest-irrelevant characteristics in speech-based interactions—although making the interaction
more natural—can also lead to an increase in the service duration (Hauptmann and Rudnicky 1988, Le Bigot et al. 2007).
Gender -0.024
(0.017)
-0.022
(0.065)
-0.027
(0.021)
Service Tenure -0.004*
(0.002)
-0.022**
(0.009)
-0.006**
(0.003)
Observations 18,580 18,580 18,580 18,580 18,580 18,580
Between R-square 0.080 0.061 0.045 0.006 0.043 0.013
Number of Customers 3,625 3,625 1,818 5,359 3,625 3,625
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 3 and 4, the
values of Human_Call Length are 0 for services successfully handled by the AI system or IVR system and we
calculated Log (Human_Call Length) = log (Human_Call Length + 1).
Table 6 reports the results from estimating Equations (2) and (3). Specifically, the estimates
presented in Columns 1 and 2 in Table 6 suggest that the introduction of a voice-based AI system
does not appear to exert a significant effect on customers’ demand for human service, even though it
is easier for customers to transfer to human agents when interacting with the voice-based AI system.
The above results provide null evidence on the possible negative consequence of implementing
voice-based AI in supporting customer service. When interacting with the AI system, customers can
choose to transfer to human agents at the beginning of the services, which may increase the workload
of human agents. However, we did not observe such an effect in our results. One possible explanation
for the results is that the AI-based service system in our research context enables customers to control
the service pace by transferring to human agents anytime they want. Giving customers the freedom to
control AI can reduce their aversion against AI (Dietvorst et al. 2018).
Table 6. Effects of AI Implementation on Human Service and Customer Complaint
Variables (1) Human
Service
(2) Human
Service
(3) Customer
Complaint
(4) Customer
Complaint
AI_agent 0.007 (0.083) 0.129 (0.369)
AI_agent * After_AI 0.103 (0.089) 0.036 (0.092) -1.037** (0.406) -1.101** (0.438)
Age -0.035*** (0.004) -0.069*** (0.017)
Gender -0.030 (0.079) 0.249 (0.358)
Service Tenure -0.025** (0.011) 0.117** (0.048)
Observations 18,580 10,621 18,169 959
Number of Customers 3,625 1,658 3,625 107
Day Dummies Y Y Y Y
Customer Random Effects Y - Y -
Customer Fixed Effects - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 2 and 4, some
observations were excluded when we conducted logistic regressions considering customer fixed effects.
We also considered the effects of AI implementation on Customer Complaint. In Columns 3
and 4 in Table 6, we observed that the AI system significantly reduces customers’ likelihood of filing
complaint reports. We also quantified the magnitude of this effect in accordance with Hosmer et al.
(2013). Specifically, compared with the average Customer Complaint before the implementation of AI
in the sample (M = 0.013), we estimated that the implementation of AI reduces the probability of
customer complaints to 0.005 (i.e., 0.013 * / (1 + 0.013 * )), with a decrease of 61.54%
in customer complaints. As an extension to the literature that examines the impacts of AI-enabled
features in operation management, such as automation (Cui et al. 2021, Li and Li 2022) and smartness
(Cui et al. 2021), our results reveal that the service flexibility (a reflection of AI smartness) enhanced
by the AI systems does, indeed, improve the overall service performance. In E-Companion E, we
replicated our main analysis and found similar results from the data from customers with phone
numbers ending in odd numbers.
5.3. Additional Analyses
In the additional analyses, we first explored the heterogeneity in our main results (Section 5.3.1). Prior
work has demonstrated that user characteristics play critical roles in affecting the performance of
technology designs (Venkatesh and Morris 2000, Venkatesh et al. 2012), and we thus tested the
moderating effects of customer characteristics (e.g., age, gender, and service tenure). Next, we dived
into the customer-AI conversations and considered the consequences of possible AI service failure
(Section 5.3.2). Since AI cannot work perfectly to handle all service tasks, we tried to understand how
customer-AI interaction and AI’s speech-recognition failures influence customer service outcomes.
In addition, prior studies have found that individuals are more likely to accept and use new
technologies when they get used to them (Taylor and Todd 1995). In our research context, a customer
can use the call service system several times during our observation period. Therefore, in
E-Companion F, we further analyzed whether the effects of AI implementation change as customers
accumulate experience in interacting with the AI system. That is, the learning effects in customer-AI
interactions. Our results suggest that, for relatively simple requests, the implementation of
voice-based AI directly increases machine service duration and reduces customer complaints. When
dealing with complex requests (service calls handled by human agents), the AI system only reduces
customer complaints for customers who are experienced in using the AI system.
Finally, one potential explanation for the observed effects of the voice-based AI system on the
outcomes of interests is the novelty effect. For example, customers may be unfamiliar with the AI
system when the system is first introduced in the call center. In such a scenario, they are more likely
to spend a longer amount of time interacting with the AI system or they may be more tolerant of the
services provided by the AI system. In order to understand the possible novelty effect, in
E-Companion G, we re-estimated our regression equations in the main analysis by considering or
eliminating records on the voice-based AI system’s first- and second-time services for each customer.
The results suggest that the implementation of the AI system persistently reduces customer
complaints. However, possible novel effects of the AI system during the period of its early
introduction indicate that the duration of machine service and customer demand for human service
increases only temporarily after the introduction of the AI system and these effects are not significant
in the long term.
5.3.1. Heterogeneity by Customer Characteristics
After estimating the main effects of the AI system, we further examined how customer-level
covariates—including age, gender, and experience of using the IVR system—moderate the effects of
the voice-based AI system. The variable Service Tenure measures the number of years that a customer
has been using his/her phone number and we used it as a proxy for customers’ experience using the
IVR system. In terms of continuous variables, including Age and Service Tenure, we first
mean-centered these variables before constructing the interaction terms. Table 7 presents the results of
the moderation analyses.
As indicated in Panel A of Table 7, we found that Age moderates the effects of the AI system.
Specifically, older (vs. younger) customers may benefit more from the dialogue-based services
supported by AI, such that after the implementation of voice-based AI system, they spend less time on
call services, have less demand for human service, and register fewer complaints. In line with Meuter
et al. (2005), who found that older customers are not proficient at using traditional IVR systems and
thus are more reluctant to interact with these systems. Consequently, the convenience and flexibility
enabled by the voice-based AI system are more helpful for improving the service experience of older
customers. With regard to the moderating role of Gender in Panel B, the AI-based system is more
effective in reducing complaints from female customers (i.e., Gender = 0). Studies on the use of
self-service technology indicated that females are strongly influenced by their perceptions of ease of
use of technologies (Venkatesh and Morris 2000). Therefore, they experience a significant
improvement in AI-supported flexible services. Furthermore, we find that customers’ experience of
using the IVR system, Service Tenure, moderates the effect of the AI system on Customer Complaint
(Panel C). For customers who have more (vs. less) experience using the IVR system, the
implementation of the AI system has a greater effect in reducing their complaints. One possible
explanation for this is that experienced users are more familiar with the drawbacks of the IVR system
and more likely to appreciate the benefits of the AI system, thereby tending to have fewer complaints.
As an extension of prior literature that assumes service systems have the same service performance
for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), our results indicate that the
effects of AI-based systems vary in terms of customer gender, age, and service tenure.
Table 7. Heterogeneity by Customer Characteristics
Panel A.
Moderation by Age
(1)
Log (Call
Length)
(2) Log
(Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.004
(0.022)
0.009
(0.084)
0.332
(0.398)
AI_agent * After_AI 0.053**
(0.023)
0.035
(0.023)
0.084
(0.090)
0.003
(0.093)
-1.441***
(0.443)
-1.613***
(0.486)
AI_agent * After_AI * Age -0.004*
(0.002)
-0.004**
(0.002)
-0.014*
(0.008)
-0.022**
(0.009)
-0.109***
(0.041)
-0.146***
(0.047)
Age -0.009***
(0.001)
-0.036***
(0.005)
-0.104***
(0.027)
AI_agent * Age 0.001
(0.002)
0.000
(0.007)
0.058
(0.036)
After_AI * Age 0.008***
(0.001)
0.008***
(0.001)
0.008
(0.006)
0.010
(0.006)
0.055**
(0.028)
0.060**
(0.029)
Gender -0.027
(0.021)
-0.031
(0.079)
0.250
(0.364)
Service Tenure -0.006**
(0.003)
-0.025
(0.011)
0.119**
(0.049)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Panel B.
Moderation by Gender
(1)
Log (Call
Length)
(2) Log
(Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.005
(0.037)
-0.028
(0.140)
0.657
(0.673)
AI_agent * After_AI 0.038
(0.039)
0.015
(0.039)
0.053
(0.152)
-0.057
(0.156)
-2.669***
(0.904)
-2.873***
(0.990)
AI_agent * After_AI *
Gender
0.017
(0.478)
0.025
(0.049)
0.075
(0.188)
0.143
(0.193)
2.118**
(1.015)
2.306**
(1.116)
Age -0.006***
(0.001)
-0.035***
(0.004)
-0.069***
(0.017)
Gender -0.043
(0.033)
-0.054
(0.124)
0.599
(0.587)
AI_agent * Gender 0.001
(0.046)
0.056
(0.174)
-0.738
(0.809)
After_AI * Gender 0.031
(0.034)
0.025
(0.034)
-0.048
(0.133)
-0.070
(0.138)
-0.634
(0.584)
-0.700
(0.629)
Service Tenure -0.006**
(0.003)
-0.025**
(0.011)
0.118**
(0.049)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Panel C. Moderation by
Service Tenure
(1)
Log (Call
Length)
(2)
Log (Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.005
(0.022)
0.002
(0.083)
0.117
(0.402)
AI_agent * After_AI 0.050**
(0.023)
0.032
(0.023)
0.111
(0.090)
-0.049
(0.092)
-0.991**
(0.423)
0.998**
(0.445)
AI_agent * After_AI *
Tenure
-0.007
(0.006)
-0.006
(0.006)
0.033
(0.025)
0.036
(0.025)
-0.257**
(0.110)
-0.217*
(0.116)
Age -0.006***
(0.001)
-0.035***
(0.004)
-0.073***
(0.019)
Gender -0.027
(0.021)
-0.030
(0.079)
0.246
(0.390)
Service Tenure -0.015***
(0.004)
-0.037**
(0.017)
0.023
(0.082)
AI_agent * Service Tenure 0.002 -0.010
(0.023)
0.124
(0.108)
(0.006)
After_AI * Service Tenure 0.021***
(0.004)
0.020***
(0.004)
0.026
(0.018)
0.025
(0.018)
0.198***
(0.075)
0.170**
(0.078)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 4 and 6, some
observations were excluded when we conducted Logistic regressions considering customer fixed effects.
5.3.2. Speech-Recognition Failures in Customer-AI Interactions
We analyzed the transcripts of customer-AI conversations to examine how AI’s speech recognition
failures in customer-AI interactions may affect customers’ demand for human service and customer
complaints. To this end, we calculated the variable Failure_Count to measure the number of times
that the AI system failed to recognize a customer’s intention during a service call by counting the
number of times the AI system repeated the same question. On average, during our observational
window, approximately 28.5% of the customer-AI system service sessions involved
speech-recognition failures. We also measured Conversation_Count to quantify the rounds of
interaction between the AI system and a customer during a service call. Considering the skewed
distributions of the variables Failure_Count and Conversation_Count, we used the log-transformed
values of these variables in our regression estimations.11 Table 8 presents the results pertaining to
speech recognition failures of AI. The results suggest that Log(Conversation_Count) is negatively
related to Human Service and Customer Complaint. One possible explanation is that, for service
requests that can be handled by the AI system, customers tend to have more interaction rounds with
the AI system and are less likely to turn to human agents and complain about the service. Meanwhile,
Log(Failure_Count) is positively related to Human Service and Customer Complaint, thereby
indicating that speech-recognition failures in customer-AI conversations can lead to significant and
11 In cases when variables include 0, we added 1 before the logarithm transformation. For example, we calculated Log
(Failure_Count) = log (Failure_Count+1).
negative effects on call center service performance by increasing customers’ demand for human
service, thus leading to more customer complaints.
Table 8. Effects of Details in Customer-AI Interactions
Variables (1) Human
Service
(2) Human
Service
(3) Customer
Complaint
(4) Customer
Complaint
Log (Conversation_Count) -3.858*** (0.096) -1.971*** (0.104) -3.347*** (0.608) -1.122** (0.461)
Log (Failure_Count) 0.966*** (0.090) 0.242* (0.128) 1.610*** (0.594) 0.213 (0.585)
Age -0.033*** (0.003) -0.027 (0.017)
Gender -0.122* (0.067) 0.136 (0.388)
Service Tenure -0.042*** (0.009) -0.057 (0.052)
Observations 17,274 6,950 17,274 366
Number of Customers 9,042 1,880 9,042 78
Day Dummies Y Y Y Y
Customer Random Effects Y - Y -
Customer Fixed Effects - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 2 and 4,
most observations were excluded when we conducted logistic regressions considering customer fixed effects. In
Column 4, the coefficient of Log (Failure_Count) is not significant and one possible reason for the result may
be that some observations were excluded when we conducted Logistic regressions considering customer fixed
effects.
6. Discussion
6.1. Key Findings
This study investigates how the implementation of a voice-based AI system in call center customer
services affects customer behavior and call center performance. The results reveal several interesting
findings. First, we find that the voice-based AI system temporarily increases the duration of machine
service and customers’ demand for human service when the system is first introduced to customers,
but these effects were not significant after the customers gained experience with the AI system.
Second, the effects of the AI system on customer complaints vary in accordance with the complexity
of the customers’ service requests and the customers’ experience of using the AI system. Specifically,
the AI-system effectively reduces customer complaints for both experienced and inexperienced
customers when customers have relatively simple requests. In contrast, with regard to complex
requests, the AI system improves customers’ service experience only after they accrue sufficient
experience interacting with the AI system. Third, we explore how customer characteristics moderate
the effects of the AI system. The results reveal that the AI system is more helpful in reducing
complaints for older customers, female customers, and customers who are experienced in using the
IVR system.
6.2. Theoretical and Practical Implications
Our study contributes to the related literature on the application of AI systems and call center
customer service operations. To begin with, this work extends the literature on AI applications by
improving the understanding of the effects of AI systems in the customer service setting. Previous
studies have either focused on deploying AI-based algorithms to support or optimize operational
processes from the technical perspective (Senoner et al. 2021, Sun et al. 2022, Yang et al. 2022) or
examined the effects of different AI-enabled features in contexts such as price request (Cui et al.
2021), order decision-making (Li and Li 2022), and automated pricing (Karlinsky-Shichor and Netzer
2019). These studies mainly investigated certain advantages and drawbacks of AI-enabled automation
(Karlinsky-Shichor and Netzer 2019, Li and Li 2022, Cui et al. 2021) or smartness (Cui et al. 2021).
As an extension, our study explores the effectiveness of AI-enabled service flexibility—a specific
reflection of AI smartness—in call center customer service. Technology-based self-service systems
(e.g., ATMs) have already been demonstrated to work well in dealing with highly structured service
tasks (Barua et al. 1991), while our study suggests that AI-enabled service flexibility is more likely to
improve the service effectiveness when dealing with tasks with high variability (e.g., call center
services). Thus, IT investment decisions in service operations should match the features of both
service tasks and technologies.
In addition, prior studies have indicated that the realization of AI’s value in the context of
human-AI interaction crucially depends on institutional settings and the role that customers play in
using AI applications (Dietvorst et al. 2018, Luo et al. 2019). Often, customers are reluctant to interact
with AI when they passively receive AI-supported marketing information (Luo et al. 2019),
forecasting results (Dietvorst et al. 2015), or medical care (Longoni et al. 2019). However, our study
suggests that when customers have the freedom to control the flow and direction of the service, the
AI-based service system does not result in a significant increase in demand for human service, even
though the AI system allows customers to transfer to human agents at any time during the interaction.
Furthermore, our results offer preliminary insight into the negative effects of speech-recognition
failures on customer-AI system interactions, thereby enriching research on imperfect AI (Dietvorst et
al. 2015, 2018).
Our study also contributes to the literature on call center customer service operations by
investigating how AI technologies impact customer behavior and the performance of call center
customer service. Previous research has examined changes in call center customer service operations
elicited by technological advances, such as call center centralization (Adria and Chowdhury 2004),
flexible resource management (Kesavan et al. 2014), and outsourcing (Kocaga et al. 2015), but
limited attention has been paid to exploring the effects of the contact designs of different
technology-mediated services with direct customer involvement (Roth and Mentor 2003, Froehle and
Roth 2004). Moreover, the OM literature mainly focuses on measuring the performance of call center
customer services from the firm’s perspective, using easily trackable metrics, such as operational
costs (Tezcan and Behzad 2012) and wait time (Khudyakov et al. 2010, Singhal et al. 2019). There is
limited research on customers’ service experience (Aksin et al. 2007). Through the customers’
perspective, our study examines the effects of an AI-based service system on customer complaints,
which is a key consequence of customers’ negative service experiences. We find that customers tend
to make fewer complaints when served by the voice-based AI system. In addition, enriching prior
studies that treat customers as homogeneous and use a single metric to represent the performance of
service systems for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), this study
further examines how the effects of the AI-based system vary in accordance with customers
characteristics, such as age, gender, and experience in using the traditional IVR system.
Furthermore, our findings also have important practical implications. First, we find that the
implementation of the voice-based AI system in call center customer services helps improve customer
service experiences (i.e., reduces customer complaints) and that the flexibility of transferring to
human agents, enabled by the AI system, does not lead to a significant increase in customers’ demand
for human service in the long term. These findings showcase the value of voice-based AI systems in
the provision of customer service. Thus, companies can continue implementing AI systems to support
customer services. Second, our findings also shed light on bridging the
proof-of-concept-to-production gap (Perry 2021). We find that the effectiveness of the AI system is
closely dependent on the service tasks (e.g., the complexity of customers’ service requests) and
customers’ experience of using AI systems. As predicted, the implementation of the AI system
directly improves customers’ service experience in relatively simple service tasks. In terms of
handling complex requests, the voice-based AI system only operates effectively to reduce complaints
from customers who have gained enough knowledge about the interacted AI system. Thus, users may
suffer the proof-of-concept-to-production gap in the early stages of adoption, particularly when
dealing with complex tasks (Sodhi et al. 2022). Correspondingly, customer service operations that are
equipped with AI systems can initially distinguish simple customer requests from complex ones based
on historical service records and then encourage customers with simple requests to use AI-assisted
services. Platforms can consider guiding customers to establish appropriate expectations of the AI
system and transfer customers with complex requests to human agents as quickly as possible. Third,
our results indicate the possible novelty effect of a newly implemented AI system. We find that the
duration of machine service and customers’ need for human service temporally increases after the
introduction of the AI system. Companies can take these effects into account when scheduling
resources for their newly implemented AI-based service systems. Moreover, in our study, suggestive
evidence from customer-AI conversations reveals that customers are more likely to turn to human
agents and complain about services after experiencing speech-recognition failures. This is likely not a
huge concern in the longer run, as AI-based services are likely to improve in terms of speech
recognition, given their learning capabilities.
6.3. Limitations and Future Research
Our study has several limitations, which also indicate ample opportunities for future research. First,
our experimental randomization is based on the last digit of customers’ phone numbers, rather than
being performed at the individual level, and thus, we balanced our samples before data analysis.
Second, due to data limitations, we could not observe detailed records of specific service requests
handled by the IVR system; therefore, we were unable to categorize service requests based on
objective service types. It will be interesting for future researchers to extend our findings based on the
objective complexity of customer service requests. Third, the current study focuses on the effects of a
voice-based AI system on call length, customers’ demand for human service, and customer
complaints. Future research could explore other outcome variables, such as service satisfaction,
customer retention, and future customer engagement, which reflect the value of AI implementation for
businesses. Moreover, leveraging the transcripts of customer-AI conversations, we conducted a few
preliminary analyses on customer-AI interactions by examining the negative effects of
speech-recognition failures. It would be interesting for future research to explore other factors in
human-AI interactions, such as emotions expressed by AI and service tones used by AI in
conversations, which inform the design of voice-based AI systems. Lastly, we analyze the
effectiveness of deploying a voice-based AI system to replace the traditional IVR system in the
telecommunication customer service setting in China. The generalizability of our findings might be
subject to the technical designs of the AI and IVR systems, cultural variation, as well as differences in
levels of technology development among countries, all of which may affect users’ attitude to and
adoption of AI. Thus, we encourage future research to further explore the implications of voice-based
AI systems among different user populations or in other service settings.






----------------------------------------------------------------------------------







Lu, T., & Zhang, Y. (2024). 1+ 1> 2? information, humans, and machines. Information
Systems Research.

Abstract
With the explosive growth of data and the rapid rise of artificial intelligence (AI) and automated working processes, humans
inevitably fall into increasingly close collaboration with machines, either asemployees or consumers. Problems in human-machine
interaction arise as a consequence, not to mention the dilemmas posed by the need to manage information on ever-expanding
scales. Considering the general superiority of machines in this latter respect, compared to human performance, it is essential to
explore whether human–machine collaboration is valuable, and if so, why. Recent studies have proposed diverse explanation
methods to uncover machine learning algorithms’ “black boxes,” aiming to reduce human resistance and enhance efficiency.However,
the findings of this literature stream have been inconclusive. Little is known about the influential factors involved or the
rationale behind their impacts on human decision processes.We aimed to tackle the above issues in the present study by specifically
examining the joint impact of information complexity and machine explanations. Specifically, we cooperated with a large Asian
microloan company to conduct a two-stage field experiment. Drawing upon studies in dual-process theories of reasoning that have
proposed different conditions necessary to arouse humans’ active information processing and systematic thinking,we tailored the
treatments to vary the level of information complexity, the presence of collaboration, and the availability of machine explanations.
We observed that with large volumes of information and with machine explanations alone, human evaluators could not add extra
value to the final collaborative outcomes. However, when extensive information was coupled with machine explanations, human
involvement significantly reduced the default rate compared with machine-only decisions.We disentangled the underlying mechanisms
with three-step empirical analyses.We revealed that the co-existence of large-scale information and machine explanations
can invoke humans’ active rethinking, which in turn, shrinks gender gaps and increases prediction accuracy. In particular, we
demonstrated that humans could spontaneously associate newly emerging features with others that had been overlooked but had
the potential to correct the machine’s mistakes. This capacity not only underscores the necessity of human-machine collaboration
but also offers insights into system designs. Our experiments and empirical findings provide non-trivial implications that are
both theoretical and practical.

Key words: Decision-making, Gender Biases, Human-Machine Collaboration, Information Processing,Machine Explanations,
Micro-finance, Rethinking

1. Introduction
Given the fast rate of artificial intelligence (AI) commercialization and its penetration into daily life, humans have
started to closely collaborate with machines, both as employees and consumers (Alibaba 2018,Wang et al. 2023a).
For example, many companies have introduced AI-based coaching systems to assist humans and improve their
decision-making effectiveness and efficiency (Loutfi 2019). In reality, humans and machines can complement each
other. Previous research has found that the decision-making accuracy of machine-learning algorithms is generally
higher than that of humans under normal circumstances (Grove et al. 2000). However, humans are more likely to
use experience to identify and process low-frequency cases that are difficult to include in machine-learning algorithms;
humans also have more advantages than machines in terms of flexibility (Sawyer 1966).More importantly,
humans’ deep thinking is awell-established andwell-understood tool for augmenting performance on independent
or team tasks (Amit and Sagiv 2013).
Unfortunately, there are various constraints, such as information opacity, machine-learning algorithms’ complexity,
and personnel’s lack of experience with or understanding of advanced technologies. Accordingly, the realized
performance of human-machine collaborations falls short of the expectation due to distrust of machines
(Jacovi et al. 2021) or over-reliance on them (F¨ ugener et al. 2021). Even worse, without properly designed collaboration
systems, humans’ involvement could reduce the collaborative performance for various reasons, such as their
being over-cautious (Lu et al. 2023b) or hyper-focused on details (Wang et al. 2023c).
To address the urgent, essential question regarding how to efficiently change humans’ responses to machines
from either aversion or over-reliance to active contribution, researchers have recently begun to turn to machinelearning
model explanations (Schmidt et al. 2020, Bauer et al. 2023). However, previous investigations in this vein
have predominantly concentrated on technical solutions and lacked a comprehensive examination of the conditions
and underlying mechanisms that influence the solutions’ impact on human decision processes. This omission
introduces certain limitations, as not all model explanations prove effective in every scenario (Chen et al. 2023).
In this study,emphasis is placed on task complexity, particularly information complexity, a contingent factor that
plays a pivotal role in shaping the effectiveness of machine explanation implementations.We posit that task complexity
and machine explanations shouldwork concurrently to foster deep thinking in humans, thereby contributing
to the efficacy of human-machine collaborations. Specifically, task complexity and information richness engage
humans in deliberate information processing by capturing their attention and interest in complex decision tasks
(Levin et al. 2000). The presentation of machine explanations that serve as valuable cues and decision-making references
prompt humans to carefully reassess decisions, address conflicts, and actively process information through
cognitive reasoning (Mantel and Kardes 1999). Through the alignment of these conditions, humans are more likely
to employ enhanced decision-making strategies, ultimately improving the performance of human-machine collaborations.
Notably, prior studies exploring the value of machine explanations have typically conducted lab experiments
or simulations alone. This approach proves challenging, as participants tend to present differently and participate
more actively in a controlled lab environment (Keil et al. 2000). Consequently, there is a compelling need to adopt
a more pragmatic approach––a realization that led us to design and implement field experiments. These experiments
serve as a crucial means of observing and analyzing human behavior in more authentic, real-world scenarios,
particularly with regard to their ability to navigate and respond to varying levels of information complexity and
cues.
Therefore, in this paper,we apply field experiments to determine whether and howhumans’ potential to achieve
“1 + 1 > 2” can be realized, particularly in the context of increasing technological development and humanmachine
collaboration. Our three research questions are: (1) What is the realized performance when humans and
machines collaborate under different levels of information complexity and different system designs? (2) What are
the underlying mechanisms? (3) How do human characteristics affect collaborative performance?
We focused on the microloan industry and partnered with a large Asian microloan company to conduct a twostage
field experiment. We dove into the dual-process theories of reasoning (Evans 2003), suggesting two prerequisites
for invoking humans’ deep thinking: information complexity initially draws humans’ attention and engages
them in the tasks, and useful cues drive humans to actively consider the task. Accordingly,we experimentally manipulated
how much information about borrowers was provided to evaluators, whether evaluators got to see the
machine’s recommendation, and whether the machine’s recommendation was explained to the evaluators.
Our empirical analyses yielded several interesting findings. First, with small information volumes, human evaluators
could not add extra value to the final outcome (i.e., the default rate prediction accuracy). Second, the human
evaluators outperformed the machines when the human evaluators were allowed to observe the machine’s suggestions
before making their final decisions and when the machine explanations were offered and the information
volume was large. In these cases, human evaluation resulted in a 2.02% reduction in the default rate (from 5.15 to
3.13%). However, this improvement disappeared if either machine explanations or information complexity were
not given. Third, we observed that when humans and machines made decisions independently, a certain amount
of disagreement was inevitable. In the human-machine collaboration modes, a disagreement of 62.82% resulted
from a small information volume without machine explanations, compared with 85.67% disagreement resulting
from large amounts of information and disclosure of machine explanations.
To disentangle the potential mechanisms and explain the above findings, we employed a three-step analytical
framework. Our findings suggested several important insights. First, human evaluators tended to stick with traditionally
important features such as income or education level, while machines explored more possibilities using
other sources of information, including shopping and offline trajectory behavior. This explains why machines, in
general, performed better than humans, especially when large amounts of information were offered. Second, with
the availability of machine explanations and large information volumes, evaluators performed active rethinking
when inconsistent decisionswere made. This improved their final decision accuracy by, for example, correcting the
risk evaluation of female borrowers. However, such a rethinking process did not occur if either condition was not
satisfied. Third, we disentangled the “rethinking” procedure in which humans associate the machine explanations
with other features if they considered the displayed features to be “non-informative”.
Furthermore, when considering individual heterogeneity among human evaluators,we found that though more
experienced evaluatorswere less likely to followthe machines’ suggestions, theywere stimulated in their rethinking
by the machines’ suggestions and explanations, and this, in turn, improved company performance. In addition,
we compared repayment behavior to examine the existence of potential gender-based decision biases. Our findings
suggest that with more data and machine explanations, human-machine collaboration could potentially shrink the
inter-gender default rate gap, which was initially and unintentionally produced by machine-learning algorithms.
This further highlights the value and necessity of collaboration between humans and machines.
The contributions of our study are multi-fold. First, it adds to the emerging literature on human–machine collaboration.
Whereas a few of the most recent studies have investigated whether humans and machines complement
each other in decision-making in different contexts (e.g., Cao et al. 2021, Luo et al. 2019, Zhang et al. 2023), the
majority have suggested outcomes only implicitly or ostensibly. Through in-depth mechanism detection analyses,
our study unravels how and why properly designed collaboration can invoke humans to contribute. Thus, we
advance this stream of literature by revealing the existence and value of humans’ rethinking processes, both theoretically
and empirically. Second, we contribute to the recent literature on the value of offering machine explanations
within the context of human-machine collaboration. The existing literature has not reached a consensus on how
humans respond to machines’ advice in the case of machine explanations (Krishna et al. 2022). Our study proposes
and verifies one reason of inconclusive findings in prior literature: the outcome of providing machine explanations
is related to other conditions such as humans’ perception of the environmental or task complexity. Whereas previous
studies have largely suggested that displaying (feature-based) machine explanations would invoke humans’
System 1 thinking (i.e., heuristics or rules-of-thumb for making quick judgments) rather than System 2 (active
reasoning and rethinking) (Chen et al. 2023), we demonstrate that with a proper collaboration design, machine
explanations can prompt humans’ rethinking and improve human–machine collaboration. Third, we add to the
recent stream of literature regarding machine biases. Recent studies have proposed the utilization of multi-source
data to alleviate algorithmic discrimination and sample biases. In fact, there is evidence that alternative data sources
would eliminate biases related to race and socioeconomic factors (Lu et al. 2023a). However, machine failure has
already been proven (Fuster et al. 2022,Huet al. 2022), so this paper not only identifies the sources of gender biases
but also uncovers the value and necessity of human involvement to make up for machine failure.
2. Related Studies
This section first summarizes three related streams of literature, then offers an introduction to the theoretical framework
underpinning experimental treatment design.
2.1. Human CollaborationWith and Aversion to Machines
AI applications require human intervention and assistance. Previous studies have explored the pros and cons of
human–machine collaboration in decision-making. For example, studies have shown that most statistical models
exceed or approach the judgment accuracy of the average clinician (Camerer et al. 2019).Machine algorithms have
been extensively shown to manage substantial amounts of data more proficiently than humans (Peukert et al. 2023,
Wang et al. 2023c). However, despite the fact that machines can make highly accurate predictions, it is difficult for
them to handle random or uncertain cases and boundary cases whose features show contradictory patterns on the
prediction objectives (labels) (Guo andWang 2015). By contrast, humans are found to be better at identifying rare
cases (Sawyer 1966) and to perform more effectively in innovative areas such as new product development (Lou and
Wu 2021). Recent studies have shown the superiority of human–machine collaborations over both full machine
automation and human-only operations (F¨ ugener et al. 2022), and have shed light on the merits of “the humanin-
the-loop” (F¨ ugener et al. 2021). On the one hand, machines can augment the capabilities of humans, such as
managers (Davenport et al. 2020); and on the other hand, humans can complement machines by contributing their
general intelligence (Te’eni et al. 2023) and diverse ideas (Wang et al. 2023d, Zhang et al. 2023) and incorporating private
information (i.e., data that only humans can use such as in-house data) (Choudhury et al. 2020, Ibrahim et al.
2021, Sun et al. 2022). Cao et al. (2021) showed that when analysts are given access to a small amount of alternative
data and in-house machine resources, combining machines’ computational power and humans’ understanding of
soft information produces the best performance in generating accurate forecasts.
However, recent research has also revealed that humans might resist the adoption or usage of machines, resulting
in low efficiency of human–machine collaboration (Allen and Choudhury 2022, de V´ericourt and Gurkan
2023,Wang et al. 2023b). This resistance exists not only among those who accept machines’ advice (e.g., Commerford
et al. 2022, Liu et al. 2023), but also among machine-based service targets, namely ordinary consumers. For
example, the adoption of chatbots has had negative effects on user acceptance and efficiency due to consumers’
insufficient knowledge and relative lack of empathy from chatbots (Luo et al. 2019). However, this negative impact
may be mitigated by users’ experience levels (Luo et al. 2021, Tong et al. 2021), flexibility, and willingness to make
adjustments based on machines’ predictions (Dietvorst et al. 2018). Human aversion to machines could also be
due to the potential of machines to threaten human jobs. AI robots have replaced and will replace human labor
in different ways in various fields (Brynjolfsson and Mitchell 2017, Lu et al. 2018). Machines have outperformed
humans in many jobs, especially low-skilled, repetitive, and dangerous ones (Autor and Dorn 2013). Conversely,
F¨ ugener et al. (2021) warned that we must also attend to humans’ over-reliance on machines, which would render
human–machine collaboration useless.
2.2. Machine Explanations
The lack of model explanations could result in human aversion to machines, stemming from a sense of distrust
(Siau andWang 2018). To avoid such negative outcomes, the existing literature has examined multiple approaches.
Acommonlyadopted approach improves trust in human–machine collaboration settings by offering more detailed
information of machine-learning decisions (Lu et al. 2019, Rai 2020). Through various post-hoc explanation methods,
human participants can be assisted in constructing suitable mental models under diverse conditions, thereby
enhancing their trust and the model efficiency (Mohseni et al. 2020). However, this approach should be employed
with caution. Schmidt et al. (2020) indicated that offering unintuitive explanations (i.e., those dealing with features
humans are unfamiliar with) may fail to boost humans’ trust in machines. Rudin (2019) also cautioned that
post-hoc explanations tend to offer incomplete and biased information regarding the mechanisms underlying algorithms.
This may lead participants to overestimate their ability to explain decisions declaratively, resulting in misinformation.
Our research aligns with this common practice. However, while some previous studies have explored the impact
of machine explanations on human–machine collaboration, few have delved into the specific mechanisms of how
and why such an approachworks in influencing human decision processes. The most similar study to ours is Bauer
et al. (2023), which revealed that humans can dynamically adjust the importance they attribute to available information
and adapt their mental models based on machine explanations. Additionally, their findings highlighted
that the provision of machine explanations might reinforce confirmation bias, potentially resulting in suboptimal
or biased decisions.However, our study differs from Bauer et al. (2023) in at least two key aspects. First, while Bauer
et al. (2023) only attended to a limited number of borrower features,we additionally consider information complexity.
As outlined in Section 2.4, we contend that the effectiveness of machine explanations in shaping individuals’
information processing depends on the complexity of the information presented to them.Machine explanations
stimulate active cognitive information processing only under specific conditions of information complexity. Furthermore,
under certain conditions, the overall performance of human–machine collaboration may see improvement
rather than deterioration. Second, the findings of the study by Bauer et al. (2023) could have been influenced
by their use of online lab experiments. By their nature, lab experiments present challenges related to sample representativeness
(Compeau et al. 2012). Of greater significance is the potential for participants to react differently
within the confines of a lab setting, which is characterized by specific monitoring and anchoring conditions. Participants
might naturally respond more actively and attentively to the experimental manipulations, potentially
leading to an overestimation of their behavioral outcomes (Keil et al. 2000). In contrast, our study adopts a field
experiment approach within a real-world micro-finance context to examine individuals’ decision-making in a more
natural setting.
2.3. Investors’ Decision-Making in Micro-finance
Many scholars have focused on individual investors’ decision-making in micro-finance businesses, including P2P
lending, crowdfunding, and microloans.Asubset of the literature has revealed the important factors that investors
consider in their decision-making (e.g., Gonzalez and Loureiro 2014, Tao et al. 2017, Wang et al. 2019). Studies
have also identified biases in micro-finance investors’ decisions, including preferences regarding gender (Chen et al.
2017) or location (Lin and Viswanathan 2016). Recent research has paid attention to the value of machine-assisted
tools in financial decision-making. For example, Ge et al. (2021) found that P2P lending investors experiencing
more defaulted loans are more likely to perceive the market to be risky and thus tend to rely more on their own
judgment rather than a robot advisor. Additionally, some investors attempt to intervene in machine usage. They
may be more concerned about returns and less likely to lose confidence in machines immediately after observing
a machine failure (Germann andMerkle 2019), or they may tend to adjust their machine usage based on the latest
performance (Ge et al. 2021). In our study, we also delve into both decision-making accuracy and potential biases
within the micro-finance context. However, unlike existing studies, our emphasis lies in examining how machine
decisions function as recommendations to influence users’ decision-making.
2.4. Theoretical Underpinning: The Dual-Process Theories of Reasoning
Humans’ and machines’ respective advantages in decision-making and their collaborative value lie in their complementarity
(Feuerriegel et al. 2022). However, humans fall easily into aversion toward or over-reliance on machines;
neither situation yields better decision outcomes than either human-only or machine-only decision-making.
Therefore, one key to promoting the value of collaboration between humans and machines is to invoke humans’
deep thinking in their co-working with machines. The literature on dual-process theories of reasoning (Evans
2003), our theoretical underpinning, raises the question of howhumans’ deep thinking can be aroused in machineassisted
tasks. The dual-process theories of reasoning propose the existence of two cognitive systems, “System 1”
and “System 2”, that underlie thinking and reasoning. System 1 processes information and reasoning fast, automatically,
and with minimal effort, leading to quick and instinctive decision-making as a rapid response to familiar
situations and stimuli. In contrast, System 2 operates at a slower pace, involves deliberate thought, and requires
conscious effort. It incorporates logical reasoning and analysis and involves the application of cognitive resources
(Kahneman 2011).
Several factors can determine whether individuals opt for System 1 or System 2 information processing and reasoning.
To encourage individuals to embrace System 2 processing, certain conditions must be met. Specifically,
since System 2 is typically involved in complex tasks, problem-solving, critical thinking, and decision-making in
novel or challenging situations, task complexity is a primary condition.Task complexity, often represented by information
complexity (Amit and Sagiv 2013), stimulates deep thinking in individuals by capturing their attention
and interest in decision tasks (Levin et al. 2000). As proposed by Endsley (1995), being well-informed about the
situation at hand is a prerequisite for subsequent deep reasoning and action selection. Information complexity,
manifested as multiple alternatives and/or numerous attributes, influences users’ situational processing of observed
information (Bauer et al. 2023, Sun and Taylor 2020). Specifically, new attributes provide novel pieces of information
that enhance one’s recognition of the decision tasks and domain (He et al. 2020). Faced with greater volumes
of more diverse, unfamiliar information, individuals are inclined to invest more effort in reasoning through more
ambiguous task situations (Van der Schalk et al. 2010). In other words, although more complex information may
not necessarily result in increased decision-making accuracy, it does enhance individual’s willingness to actively participate
in decisions (Oskamp 1965). With complex information, people are more willing to perceive the increase
in information as useful and desirable, even if it comes with a certain level of burden (Amit and Sagiv 2013). In
contrast, with simple information, people tend to make rapid decisions via System 1 processing (Speier 2006).
The second condition for motivating people to engage in high-quality System 2 processing (i.e., active consideration
and systematic deep thinking) is the presence of useful cues for reference. A well-designed reference cue has
the potential to prompt individuals to meticulously reassess their decisions and compare them with the provided
references (Weiss 1982). Consequently, individuals can rectify their initial decisions, address conflicts, and even generate
novel ideas through cognitive reasoning, association, and imagination (Hollnagel 1987). Several approaches
can be effective in fostering such deep thinking. First, high information quality leads to elevated epistemic motivation
(Cacioppo et al. 1996). For instance, structured and concrete information can encourage individuals to
engage more deeply in a task and, therefore, process information more actively and positively (Mantel and Kardes
1999). Additionally, when individuals are provided with explicit reference points (Chernev 2003), they maintain
high motivation to engage in cognitive reasoning and adopt superior information-processing strategies to navigate
complex decision-making. Moreover, the decision to employ System 2 processing can be influenced by individuals’
experience and expertise. When faced with novel and unfamiliar situations, individuals are more inclined to
activate System 2 processing to tackle challenges and gain new knowledge (Smerek 2014).
Applying the lens of this theoretical literature stream to human–machine collaboration,we propose two designs,
each of which corresponds to one of the two conditions mentioned above: (1) offering humans and machines rich
information for decision-making, and (2) exposing humans to structured machine explanations for final decisions.
Specifically, decision-making with rich information requires strong cognitive abilities for information processing
(Icard 2018); this arouses humans’ perception of the task complexity (Sun and Taylor 2020). We thus posit that,
compared with limited information, offering rich information could enhance and maintain humans’ awareness
of decision-making tasks and their willingness to participate in the tasks, regardless of their capability for handling
large information volumes. Furthermore, presenting machines’ decisions as recommendations along with
proper machine explanations showing how the prediction outcomes were obtained by machines in a faithful and
human-interpretable manner (Krishna et al. 2022) can trigger individuals’ active cognitive reasoning. For example,
if machine explanations are provided, humans can learn from machines’ decision-making rationale, trace back their
own decision rules, and double-check whether the new knowledge from machines fits and actually improves decision
accuracy (Mohseni et al. 2020).We call this the rethinking process. In this paper, rethinking or reconsideration
refers to the process of carefully reviewing a decision or conclusion that has previously been made to determine
whether the initial decision should be changed. It is usually an inquiry into, or reflection on, the most basic given
information, or the asking of fundamental questions such aswhy and howbreakthrough improvementswere made
after observing new signals or outcomes (Jain and Pagrut 2001). Such a self- and system-monitoring process aligns
with the concept of active consideration (i.e., Pattern 5) developed by Jussupow et al. (2021), which was concluded
to be the best practice for achievement of satisfactory outcomes from human–machine collaboration.
Broadly speaking, notwithstanding the many and broad investigations into human–machine collaboration,
there is a dearth of literature unraveling the decision-making process during human interactions with machine
assistants under diverse conditions. This paper aims to bridge that void. Particularly, we focus on the role of information
complexity and machine explanations in prompting humans to actively rethink and improve the consequent
decision outcomes. Given the complex environments covering interactions among information volumes,
machine explanations, human experience, and behavioral biases, this question might not have a fixed and intuitive
answer.We also reveal the scenarios that can leverage humans’ and machines’ respective advantages to realize 1 + 1
> 2.
3. Experimentation
3.1. Experimental Background
We partnered with a large Asian microloan company to conduct a field experiment. The microloan company was
founded in 2011 and served over 250,000 borrowers by 2018, with unsecured microloans of approximately US$465.
The company uses only the owner’s money for lending, and their loans are mostly used for temporary financial
needs such as supplementary cash flow for small businesses and irregular shopping needs. The loans have a term of
1–7 months and are repaid in monthly installments starting one month after their issuance. The company sets its
annual interest rate from 12 to 16%.1
To apply for a loan, a borrower is required to provide their basic personal information such as name, phone
number, gender, age, educational level, and income level. Subsequently, borrowers are required to choose the loan
amount (US$46.5—US$1,240, US$465 by default) and loan term as well as check the annual interest rate. In this
study, we focused only on loans with a term of 1, 2, or 3 months.2 In addition, borrowers are required to clearly
state the purpose of the loan. They can then submit their application. Every new application is randomly assigned
1 The annual interest rate is set on a daily basis, rather than assigning different rates to borrowers with different assessed credit risks. This
daily interest rate is generally determined at a mediate level in the online lending market. The company does not announce the actual rates to
the market in advance, and borrowers are therefore less likely to decide strategically when to apply to get a lower rate. Such a design allowed
us to tease out the potential endogeneity issues brought by interest rates.
2 This was to facilitate our experimental observation, because it takes a long time to observe and confirm the repayment or default behavior
when the loan term is long. We compared the repayment performance among loans of different terms with the historical data and found
that the loan term was not highly related to repayment performance.
to a human evaluator who assesses the borrower’s credit risk (i.e., default probability) based on the collected information,
and makes the final loan-approval decision accordingly. The focal company’s loan-approval rate is approximately
47%, similar to the competitors in the market. The main goal of loan screening is to minimize the number
of defaulted cases while maintaining the approval rate specified above.
3.2. Experimental Setup
3.2.1 Implementation of Treatment I: Information Complexity
Inspired by the dual-process theories of reasoning, we introduced two factors that could influence human evaluators’
decision-making in collaboration with machines in Section 2.4. As the first step,we utilized the focal empirical
setup to incorporate variations in information complexity. Before our experiment, the focal platform granted loans
based entirely on human evaluators’ decisions. Evaluators only accessed borrowers’ basic information, loan history,
and current loan attributes (12 variables [features] in total) to make their credit risk evaluation. Thus, this information
comprises the first level of information complexity: small information volumes. To construct an alternative
information scenario (i.e., with large information volumes),we asked the focal company to collect additional information
from the borrowers starting June 1, 2017. The additional information included recent (past six months)
online shopping activities on the largest e-commerce platform in the focal country and cellphone usage information
collected from the pertinent communication carriers.3 Previous studies have suggested that shopping and cellphone
usage may be correlated with borrowers’ socioeconomic status and credit behaviors (e.g., Blumenstock et al.
2015). Therefore, based on the relevant literature and canonical behavioral theories (Lu et al. 2023a), we extracted
32 features for each source in order to comprehensively describe borrowers’ online shopping and cellphone usage
and mobility trace characteristics. Table A1 in Appendix A1 describes these features.
3.2.2 Machine Preparations and Implementation of Treatment II: Machine Explanations
Since the focal company had not sought any machine assistance before our collaboration, it was necessary for us
to design and train prediction models for each of the two information scenarios. Our training samples comprised
borrowers who submitted loan applications June 1–30, 2017. For these sampled borrowers, the human evaluators
3 The groups with large information volumes had access to multi-sourced information, emphasizing information diversity (i.e., new
attributes). Labeling one treatment as “large information volumes” is intentionally contrasting it with the small-sized demographic feature
set used in the other groups. Therefore, our manipulation is intricately aligned with the concept of information complexity, as elucidated
in Section 2.4.
assessed their credit risks and made loan-approval decisions using small-scale information, as usual. At this stage,
the human evaluators did not have access to the additional information collected. We then gathered repayment
information for the approved borrowers from more than 9,000 training sample loans made between July 1 and
November 30, 2017. Since the loan term was no longer than 3 months, a 5-month observation period was sufficient
for us to confirm borrowers’ repayment and default behaviors. Default is defined as the failure to fully repay the loan
at least 60 days after the loan due date. At the end of November, we obtained the borrowers’ basic and additional
information, as well as their repayment behaviors.
Based on the above information, we then trained machine-learning algorithms. For both information scenarios,
we implemented standard operationalizations (e.g., 10-fold cross-validation, out-of-sample prediction, and hyperparameter
tuning) and replicated the training procedures multiple times until they achieved stable loan default
prediction performance.We tried diverse, widely accepted machine-learning models, including logistic regression,
support vector machine, k-nearest neighbor, multi-level perceptron, random forest, and extreme gradient boosting
(XGBoost).XGBoost achieved the best performance, sowe employed it in our experiment.To maintain a relatively
comparable performance across experimental groups,we did not updateXGBoost during the experimental period.
Meanwhile,we leveraged the same training samples to train the human evaluators. Specifically,we randomly separated
the human evaluators into two groups: one group maintained the previous loan evaluation process with the
small information volume, and the other group evaluated credit risks and made loan-approval decisions with the
large information volume. After a 7-day training period, all human evaluators reached a stable evaluation performance.
Please refer to Appendix A2 for detailed information on the human evaluators and the training procedure.
With the pre-trained prediction models, we were able to design the second treatment. Specifically, to prepare
the machine explanation information based on the above machine-learning algorithms, we implemented a SHAP
analysis method, which yields Shapley values representing the average expected marginal contribution to predicting
the default probability of one feature after all possible combinations have been considered (Roth 1988). In Figure 1,
we present the most important features under the two information volume scenarios.
3.3. Experimental Design
To identify the loan approval decision performance under human-only, machine-only, and human–machine collaboration
decision-making scenarios, we designed and implemented a two-stage experiment, as illustrated in Figure
2.
(a) Decisions with Small Information Volume (b) Decisions with Large Information Volume
These features rank in the top 5 or 7 in respective analyses. The other features play only limited roles in machine-learning-based predictions
(i.e., they have very small absolute scores). Positive (negative) values mean that the features are positively (negatively) related to default
behavior.
Figure 1 Important Features in Machines’ Decision-making Processes
Figure 2 Experimental Process
Experimental Stage 1. The first stage began on December 8, 2017, and lasted for oneweek. The relatively short term
of the treatments helped tease out the potential confounders stemming from the substantial evolution (learning
or change) of the human evaluators, machine-learning algorithms, and borrower-characteristic distributions with
long-term experience. At this stage, the company collected basic and additional information from every new borrower,
and we randomly assigned the borrowers to one of the four groups. In Groups 1 (H & S) and 2 (H & L), a
credit risk assessment was completed by human evaluators. They had access to the small (Group 1) or large (Group
2) information volumes to inform their approval or rejection of each loan application. The two human evaluator
groups were consistent with those in the training process described earlier. In Groups 3 (M & S) and 4 (M &
L), we employed the corresponding pre-trained XGBoost to predict each application’s default probability based
on a small (Group 3) or large (Group 4) number of features and to make loan-approval decisions by ranking the
predicted default probability from lowest to highest. Following the company’s usual practice, we maintained the
loan-approval rate at 47% in all four experimental groups. For all granted loans,we continued tracing and collecting
their repayment behavior from January 8 toMay 14, 2018.
Experimental Stage 2. We spent another two weeks (from December 15 to 28, 2017) conducting the second stage
of our experiment. The two-week period ensured that the evaluation workload was similar to that in the first stage.
Again, we randomly assigned each new loan application to one of the four groups. In all groups, the human evaluators
were instructed to collaborate with the machine. Specifically, human evaluators in Group 1 were randomly
assigned to Groups 5 and 6 and those in Group 2 were assigned to Groups 7 and 8, with an equal number of evaluators
in each group to manage the same amount of information. As illustrated in Figure A2 in Appendix A3, the
loan-approval decision process had two steps. In the first step, human evaluators made credit risk evaluation and
loan-approval decisions independently with small (Groups 5 and 6) or large (Groups 7 and 8) information volumes;
this is identical to the situation in Stage 1. In the second, decision-making step, the machine-learning algorithm’s
loan-approval decision for the same loan was presented to the human evaluators. In Groups 5 and 6, the machinelearning
algorithm used the trained model with a small number of features (corresponding to Group 3), and in
Groups 7 and 8, it used a large number of features (corresponding to Group 4). The human evaluators did not
have much knowledge of the applied machine-learning algorithm; theywere simply notified that machine-learning
algorithms usually have strong decision-making abilities.
Next, we incorporated the second treatment, the existence of machine explanations. Specifically, in Groups 5
((H + M)&S&w/o Expl) and 7 ((H + M)&L&w/o Expl),we gave only the machine’s loan-approval decisions
to the human evaluators, without explanations regarding how the decision had been reached (see Figure A2a). In
Groups 6 ((H + M)&S&w/ Expl) and 8 ((H + M)&L&w/ Expl), the human evaluators could see not only the
machine’s loan-approval decisions but also the post-hoc explanations (i.e., the most important features presented
in Figure 1). For these features, the human evaluators could find and compare the values of the fixed features of
the focal borrower and the average values of non-defaulters (see Figure A2b). The human evaluators in Groups
6 and 8 were provided this information at the beginning of experimental stage 2. We conjecture, based on our
theoretical framework, that this information (strengthened by the value comparison) served as an ideal reference
due to machines’ superior capability (Chernev 2003). Then, human evaluators were required to make their final
loan-approval decisions. When their initial decisions were incongruent with the machine’s, they could either insist
on their own decisions or adjust them to followthe machine’s recommendations. As mentioned before, the human
evaluators were told to maintain a consistent approval rate before and during the experiment, and so the approval
rates in all of our experimental groups were maintained at approximately 47%. Similarly, we continued to collect
the Stage 2 borrowers’ repayment performance data over the subsequent 5 months.
3.4. Experimental Data
We obtained our experimental data after completing repayment information collection. The dataset contained
the borrowers’ basic and additional information, the human evaluators’ and machines’ initial approval decisions
(Groups 1 to 8), the human evaluators’ final approval decisions (Groups 5 to 8), and the repayment performance
(default or not) of the approved loans. Additionally, we collected background information on the human evaluators,
including their gender, education level, number of months’ experience (discretized by six month period), and
historical decision accuracy (i.e., the ratio of defaulted loans to all approved loans in the three months before our
experiment).
Table 1 Randomization Check
Loan characteristics Borrower characteristics
Group #Obs. Loan amount (US$) Interest rate (%) Loan purpose Gender Age Living city DPI (US$) Monthly income level Education level
1.H& S 2,924 472.8 13.888 0.446 0.235 25.17 6,528.9 4.886 4.252
2.H& L 2,930 473.7 13.911 0.437 0.241 25.18 6,505.0 4.886 4.256
3.M& S 3,001 472.9 13.930 0.431 0.249 25.12 6,524.7 4.902 4.201
4.M& L 3,020 472.8 13.913 0.430 0.237 25.19 6,565.2 4.942 4.9206
5. (H + M) & S & w/o Expl 2,885 474.7 13.920 0.437 0.245 25.07 6,545.5 4.960 4.223
6. (H + M) & S & w/ Expl 2,918 470.7 13.902 0.437 0.241 25.15 6,588.1 4.884 4.218
7. (H + M) & L & w/o Expl 2,978 475.2 13.924 0.428 0.233 25.09 6,563.7 4.874 4.216
8. (H + M) & L & w/ Expl 2,946 475.4 13.904 0.434 0.240 25.11 6,571.6 4.943 4.257
Group #Unique evaluators Evaluator gender Evaluator education level Evaluator months working Evaluator historical (decision) accuracy
1.H& S 31 0.774 4.452 2.516 2.000
2.H& L 31 0.774 4.452 2.516 2.065
a H= human decision,M= machine decision,H+M= human + machine decision, w/o Expl = without AI explanations, w/ Expl = with AI explanations.
b Loan purpose: 1 = consumption, 0 = others (e.g., for emergency). Gender: 1 = female, 0 = male.
c Monthly income level: 1 = US$150 or below, 2 = US$150–US$300, 3 = US$300–US$450, ..., 8 = US$1,050–US$1,200, 9 = US$1,200 or above.
d Education level: 1 = middle school or below, 2 = vocational school, 3 = high school, 4 = technical school, 5 = undergraduate, 6 = graduate or above.
e Evaluator months working: 1 = not longer than 6 months, 2 = 6–12 months, 3 = 13–18 months, 4 = longer than 18 months.
f Evaluator historical (decision) accuracy: 1 = low (default rate>15%), 2 = medium (10%<default rate<15% ), 3 = high (default rate<10%). Refer to Table A2 for descriptive statistics on evaluator
historical accuracy.
g Groups 3 and 4 did not involve human evaluators. In experimental stage 2, the human evaluators in Group 1 (or 2) were randomly and equally assigned to Groups 5 and 6 (or Groups 7 and 8).
h For every feature, the values show no significant differences across the groups based on the F-test.
Figure 3 Default Rates of Experimental Groups

There were a total of 23,805 loans in the 8 groups involved in our experiment.We removed 203 repeat borrowers
from the company to avoid interference from the previous experience. The final experimental sample size was
23,602. Table 1 reports the sample size and the major characteristics of borrowers, loans, and evaluators across the
experimental groups.Most of the borrowerswere men (>75%); 28.43% of the borrowers had received an undergraduate
education, and the average (self-reported) monthly income ranged between US$450 and US$600. Approximately
44% of the loans were for personal consumption purposes. Regarding the human evaluators, most were
female (77%) with a technical school or undergraduate-level education background. On average, the human evaluators
had been working for the company for approximately 1 year, and those with high, medium, and low levels of
historical decision performances were evenly distributed between the groups (i.e., around 1/3 each).We detected
no statistically significant differences between the groups, which suggested that the randomization had been successful.
4. Empirical Findings
Our key variable of interest was borrowers’ default rates. This is a common metric in the microloan industry (Fu
et al. 2021) and within the focal company. We defined it as the ratio of defaulted loans to the total number of
approved loans. Figure 3 plots the default rates across all groups, and Table 2 calculates the inter-group differences
with between-group t tests. The default rate in Group 1 was 12.83%, echoing the average performance of the focal
company before our experiment. The comparison yielded several interesting patterns. First, as expected, when making
decisions separately, the human evaluators performedworse than the machines, and the large-scale information
volumes increased the performance gap (i.e., Comparisons B and D). Second, the human evaluators did not add
additional value when jointly deciding based on a small information volume, regardless of whether the machine
explanationswere offered (i.e., Comparisons E, G, andHwith insignificant differences in the mean value of default
rates). Third, we observed different outcomes in the scenarios with large information volumes. In particular, when
the human evaluatorswere presented with the machines’ suggestions and the machine explanations before making
their final decisions, they performed better than the machines’ independent decisions, showing a 2.02% reduction
in default rates, from 5.15 to 3.13% (i.e., Comparison J). This suggests that the human evaluators contributed additional
value to the evaluation process that only they, as humans, could provide. However, this improvement disappeared
if no machine explanation was provided (i.e., Comparison I). In sum, the collaborative values were only

Table 2 Comparison of Default Rates among Different Experimental Groups
Comparison Experimental groups Difference in means p-values
A 1.H& S vs. 2.H& L 0.0228 0.0650*
B 1.H& S vs. 3.M& S 0.0268 0.0275**
C 3.M& S vs. 4.M& L 0.0500 0.0000***
D 2.H& L vs. 4.M& L 0.0539 0.0000***
E 5. (H+M) & S & w/o Expl vs. 6. (H+M) & S & w/ Expl 0.0032 0.7833
F 7. (H+M) & L & w/o Expl vs. 8. (H+M) & L & w/ Expl 0.0287 0.0003***
G 3.M& S vs. 5. (H+M) & S & w/o Expl -0.0048 0.6779
H 3.M& S vs. 6. (H+M) & S & w/ Expl -0.0016 0.8892
I 4.M& L vs. 7. (H+M) & L & w/o Expl -0.0085 0.3215
J 4.M& L vs. 8. (H+M) & L & w/ Expl 0.0202 0.0071***
a As our experiment comprised multiple treatments, we followed multiple hypothesis testing in experimental economics (List et al.
2019) to address the potential bias. Thus, p-values are multiplicity-adjusted values based on between-group t tests. *p < 0.10, **p <
0.05, ***p <0.01.
realized if the two conditions, information complexity and useful cues, were satisfied. We also considered profit
gains and evaluated the dollar values of the different factors. The results in Figure B1 in Appendix B1 confirmed the
consistency.
Noticing the above diverse patterns, we then further decomposed the decision-making behavior of human evaluators
after they had observed machines’ suggestions. Specifically, in Figure 4, we compared the decision consistency
between humans (initial decision) and machines (in Figure 4a), and calculated the adjustment ratios when
inconsistency arose (in Figure 4b). Our results indicate that when the human evaluators were making decisions
independently (i.e., before observing the machines’ suggestions), there were a certain number of cases in which
the humans disagreed with the machine’s decisions. As shown in Figure 4a, the agreement proportion was smaller
with the large information volume (83.78% consistency in Group 5 vs. 78.58% consistency in Group 7). This pattern
was similar regardless of whether machine explanations were available. In the human–machine collaboration
scenario, the human evaluators adjusted their decisions by following the machines’ recommendations. The proportion
of adjustment, however, varied across the experimental groups. In particular,we observed that only 62.82%
disagreement was eliminated with a small information volume and no machine explanation. The adjustment rates
significantly increased when the information volume was large or machine explanations were offered. For example,
compared with limited information, the availability of large amounts of information could mitigate the human
evaluators’ unwillingness to follow machines, decreasing it by 18.21% (Group 5 vs. Group 7, p-value<0.001).Meanwhile,
machine explanations also encouraged human evaluators to accept the machines’ decisions by improving
the ratio of following from 81.03 to 85.67% (Group 7 vs. Group 8, p-value = 0.026).

(a) Ratio of Decision Consistency between Humans andMachines (b) Ratio of FollowingMachines’ Decisions
Notes: p-values are multiplicity-adjusted values (List et al. 2019) based on between-group t tests.
Figure 4 Consistency and Following between Humans and Machines
5. Mechanism Examinations
This section aims to disentangle the potential mechanisms driving the differences in performance between humans
and machines and the contributions made by humans when collaborating with machines. This part consists of
three steps.We first examined empirically why humans and machines decided differently when making decisions
separately and how decision inconsistency explained the performance differences. Second, we isolated the underlying
behavioral mechanisms explaining why humans disagreed with the machines’ recommendations when collaboration
was allowed. Third, we discussed how disagreement affects decision quality, and decomposed the human
evaluators’ “rethinking” procedure in the collaborative mode.
5.1. Why Do Humans and Machines Behave Differently?
To answer this question, we explored decision-making processes by identifying the important features involved.
First, to determine the information that had played a part in either the human evaluators’ or the machines’ decisionmaking
processes, we considered a (loan-)application-level Probit model with all available information as independent
variables and defined the dependent variable (DV) using a dummy variable, IfApprove, which equaled one
if the loan was approved. We derived two sets of Probit models using all loan applications with either small or
large information volumes. The estimated coefficient of each information variable suggested the predictive power,
which served as a proxy for feature importance in the humans’ or the machines’ decision-making process. Features
with significant coefficients in the regressions were important features.

Table 3 Regressions on Humans’ and Machines’ Approval Decision (Groups 1, 3, 5, and 6; Probit Model)
Groups 3, 5, 6 (machines’ decision) Groups 1, 3, 5, 6 (humans vs. machines)
DV: IfApprove Model 1 Model 2
Loan purpose -0.161*** (0.034) 0.042 (0.048)
Gender 0.030 (0.029) 0.062 (0.058)
Age 0.087*** (0.005) 0.068 (0.062)
Living city DPI 0.212*** (0.007) 0.139*** (0.010)
Monthly income level 0.126*** (0.008) 0.082*** (0.008)
Education level 0.163*** (0.021) 0.055*** (0.028)
MInd -0.086 (0.203)
Loan purpose × MInd -0.208*** (0.040)
Gender × MInd -0.030 (0.045)
Age × MInd 0.024*** (0.006)
Living city DPI × MInd 0.066 (0.060)
Monthly income level × MInd 0.043 (0.036)
Education level × MInd 0.103 (0.087)
Other borrower-related variables Included Included
Other loan-related variables Included Included
Evaluator-related variables Included Included
Log likelihood -4,951.40 -10,363.18
#obs. 8,804 17,531
a Model 2 considers human evaluators’ initial decisions before displaying machines’ recommendations to them when using Groups 5 and
6.We duplicated the sample for Groups 5 and 6 to consider the humans’ initial decisions and machines’ decisions, respectively.
b The variables concretely reported in the table are those that might be useful in this paper’s analyses (although they may be insignificant
here).Most of the other variables were insignificant, and we do not report their details. Living city DPI was divided by 1,000.
c Standard errors are in parentheses. Significant results are in bold. *p <0.10, **p <0.05, ***p <0.01.
Furthermore, to compare the decision-making processes between humans and machines in a more explicit way,
we ran two additional Probit models, in which we included all related loan-level features as well as their interaction
terms and a new binary indicator, MInd, denoting whether the approval decision was made by a machine learning
model (=1 if yes,=0 otherwise).We reported the estimation results in Tables 3 and 4 for the small and large information
volumes scenarios. Model 1 in both tables reports the estimates of machine-only decisions. We estimated
the coefficients using samples from Groups 3, 5, and 6 for the small information volume scenario and from Groups
4, 7, and 8 for the large information volume scenario.Model 2 in both tables reports the models with interaction
terms. We included all human-only decisions (i.e., humans’ initial decisions without machine interventions) and
machine-only decisions in Groups 1, 3, 5, and 6 (in Table 3) and Groups 2, 4, 7, and 8 (in Table 4). The coefficients
of the interaction terms in Model 3 elaborate on whether and to what extent the corresponding features explain
the divergence between humans’ and machines’ decision-making processes.4
4 In Appendix C1, we conducted multiple robustness checks. First, we reran our regressions within each experimental group using different
samples. The results indicated that thehumanevaluators’ initial decisions did not involve any learning from the machines’ recommendations.
This also confirmed that the comparisons between the two stages in our experiment were reasonable. As another robustness check, we
employed decision tree approaches to infer the decision rules implemented by human evaluators. The results in Figure C1 confirm the
consistency. Additionally, we incorporated an alternative DV to offer more insights into how humans and machines reached the same or
different initial decisions.

Table 4 Regressions on Humans’ and Machines’ Approval Decision (Groups 2, 4, 7, and 8; Probit Model)
Groups 4, 7, 8 (machines’ decision) Groups 2, 4, 7, 8 (humans vs. machines)
DV: IfApprove Model 1 Model 2
Loan purpose -0.028*** (0.004) 0.021 (0.049)
Gender 0.045 (0.032) 0.070 (0.053)
Age 0.088*** (0.005) 0.060 (0.074)
Living city DPI 0.154*** (0.007) 0.091*** (0.011)
Monthly income level 0.121*** (0.009) 0.065*** (0.014)
Education level 0.075*** (0.023) 0.072*** (0.036)
Avg amount of game card -0.018*** (0.001) -0.009 (0.016)
ATV shopping durable 0.001 (0.003) 0.005 (0.005)
ATV shopping virtual -0.001 (0.001) -0.002 (0.002)
#Outgoing contacts -0.052*** (0.010) -0.050*** (0.018)
#Office by week 0.077*** (0.003) 0.004 (0.005)
#Recreational place by week -0.026 (0.028) -0.026 (0.042)
#Commercial place by week -0.097*** (0.008) -0.034 (0.069)
#Public service place by week 0.014 (0.014) 0.042 (0.043)
MInd -0.083 (0.225)
Loan purpose × MInd -0.064** (0.030)
Gender × MInd -0.022 (0.048)
Age × MInd 0.028*** (0.006)
Living city DPI × MInd 0.056 (0.049)
Monthly income level × MInd 0.006 (0.011)
Education level × MInd 0.006 (0.008)
Avg amount of game card × MInd -0.008*** (0.002)
ATV shopping durable × MInd 0.001 (0.001)
ATV shopping virtual × MInd -0.001 (0.001)
#Outgoing contacts × MInd -0.002* (0.001)
#Office by week × MInd 0.063*** (0.004)
#Recreational place by week × MInd 0.001 (0.002)
#Commercial place by week × MInd -0.063*** (0.011)
#Public service place by week × MInd -0.027 (0.029)
Other borrower-related variables Included Included
Other loan-related variables Included Included
Evaluator-related variables Included Included
Log likelihood -4,155.33 -9,642.50
#obs. 8,944 17,798
a Model 2 considers the human evaluators’ initial decisions before the machines’ recommendations were displayed to them when using
Groups 7 and 8.We duplicated the sample when using Groups 7 and 8 to consider the humans’ initial decisions and the machines’ decisions,
respectively. Other table notes are the same as [b] and [c] in Table 3.
Several interesting patterns explain the differences in performance between the human evaluators’ and machines’
individual decisions. When decisionswere made with the small information volume, the human and machine evaluators
considered similar features (i.e., living city DPI, monthly income level, and education level). The machines
additionally captured the applicants’ age and the loan purpose, which is known to have a relatively high correlation
with default behavior (refer toTable 5). This explains why the machines performed slightly better than the humans
with the small information volume. When a large information volume was available, the human and machine evaluators
deviated. Interestingly, we found that the human evaluators generally tended to stick with traditionally
important features (e.g., living city DPI, monthly income level, education level); the only new feature that human
evaluators adopted was the frequency of outgoing contacts. In contrast, the machines explored additional sources

Table 5 Correlations of Major Variables
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12)
(1) IfDefault 1
(2) Gender 0.025 1
(3) Living city DPI -0.195 -0.010 1
(4)Monthly income level -0.164 -0.041 0.022 1
(5) Age -0.120 -0.054 0.047 0.104 1
(6) Education level -0.103 -0.036 0.002 0.028 0.091 1
(7) Loan purpose 0.093 0.178 -0.036 -0.033 -0.065 -0.026 1
(8) Avg amount of game card 0.169 -0.247 0.001 -0.004 0.014 -0.017 -0.002 1
(9) #Outgoing contacts 0.104 -0.054 -0.023 0.036 0.012 0.033 0.001 -0.027 1
(10) #Office by week -0.115 -0.013 -0.034 -0.010 -0.021 0.017 0.009 0.046 0.049 1
(11) #Commercial place by week 0.090 0.096 -0.030 -0.016 -0.026 0.032 0.014 -0.089 0.019 -0.055 1
(12) ATV shopping virtual 0.094 0.098 -0.005 -0.070 -0.078 0.035 0.045 -0.038 0.010 -0.028 0.013 1
a Correlations are based on all loan samples. Relatively large values are in bold.
of information, with a particular focus on factors potentially linked to default behavior (refer to Table 5). These
factors included shopping behavior (e.g., average amounts spent on game cards), cellphone call behavior (e.g., the
frequency of outgoing contacts), and offline trajectory behavior (e.g., frequency of visiting the office or commercial
places per week). This is reasonable because humans might resist or be incapable of handling new and complicated
information (Chapman and Chapman 1967).Moreover, with their increased processing efficiency, machines have
been confirmed to have predictive advantages using novel features from alternative data sources (Lu et al. 2023a,
Zhou et al. 2021). This also explains the significant improvement achieved by machines with large information
volumes.
5.2. Why Do Humans Disagree with Machines’ Recommendations?
We next disentangled the underlying behavioral mechanisms when collaboration was employed.We noticed that
after observing the machines’ recommendations, the human evaluators sometimes adjusted their final decisions
to follow the machines’ recommendations, but not always. Table 2 shows that only with machine explanations
and large information volumes did the human evaluators contribute additional value. This value would disappear
if either of the two conditions were removed. In order to understand the human evaluators’ behavior, we conducted
regression tests using observations in which the human evaluators’ initial decisions differed from those of
the machines.
We employed Probit models, in which the DV was IfApprove and the independent variables included all
available loan features.To understand howmachines’ recommendations influenced humans’ decision-making processes,
we compared the discrepancies betweenhumanevaluators’ initial and final decisions. Empirically,we defined

a new binary indicator, IfFinal, which equaled one if the approved decision was made after a machine recommendation
was present. Again, we included this binary indicator and the interaction terms of the features to
investigate which features played a significant role in changing the human evaluators’ decisions.
We reported the results in Tables 6 and 7. With small information volumes (Groups 5 and 6 in Table 6) and
with large information volumes but no machine explanations (Group 7 in Table 7), the factors that explained the
human evaluators’ final approval decisions remained similar to those in the first stage (shown in Tables 3 and 4).
For example, with the small information volume, the interaction terms for three features (i.e., applicant age, living
city DPI, and monthly income level) presented significant coefficients in Model 2 in Table 3. This suggests that
humans rely on these three features to decide whether to change their initial decisions (i.e., whether to follow the
machines’ recommendations).Take the feature of monthly incomelevel as an example. The corresponding estimate
is positive, implying that humanswould switch from rejection to approval even if an applicant’s income is not high
enough. That is, the weight of the monthly income level in human evaluators’ credit risk assessment became larger
than before, and human evaluators were more tolerant of cases with relatively lower levels of income. To further
illustrate humans’ willingness to follow machines’ suggestions and to capture human behavior in the second stage,
we defined another dependent variable, IfFollow. The detailed empirical strategy and corresponding results are
presented in Appendix C2. All the empirical results imply that, under these experimental conditions, the reasons
(i.e., key features) explainingwhy the human evaluators disagreed with the machines initiallywere the same as those
explaining why they continued to disagree with machines after receiving the machine recommendations.
In Groups 5 to 7, the features with significant estimates of interaction terms with IfFinal (i.e., indicating
the reasons explaining the differences between initial and final decision-making processes) included both humanfamiliar
ones (i.e., those they had used in the first stage, such as living city DPI and number of outgoing contacts)
and machine-only ones (e.g., number of commercial place visits). There are two possible ways that humans
and algorithms might reach diverse decisions. One is that humans might have some uncertainty surrounding
“borderline” cases (i.e., those with important features showing values near the evaluators’ or machines’ thresholds).
Humans and machines may make inconsistent decisions on such borderline loans when their feature values
are located in such threshold gaps. When handling these relatively complicated applications, humans may lack

Table 6 Regression on Human Evaluators’ Initial and Final Approval Decisions (Groups 5 and 6; Probit Model)
Group 5 Group 5 Group 6 Group 6
(humans’ final decision) (initial vs. final) (humans’ final decision) (initial vs. final)
DV: IfApprove Model 1 Model 2 Model 3 Model 4
Loan purpose -0.025 (0.022) -0.013 (0.024) -0.113* (0.065) -0.102 (0.124)
Gender 0.160 (0.140) 0.131 (0.139) 0.037 (0.143) 0.035 (0.144)
Age 0.073** (0.034) 0.032 (0.040) 0.070** (0.032 0.024 (0.032)
Living city DPI 0.155** (0.024) 0.123*** (0.026) 0.103*** (0.027) 0.098*** (0.030)
Monthly income level 0.117*** (0.034) 0.096*** (0.033) 0.059* (0.033) 0.052* (0.028)
Education level 0.024*** (0.008) 0.020** (0.008) 0.067*** (0.015) 0.031** (0.014)
IfFinal -0.421*** (0.085) -0.598*** (0.085)
Loan purpose × IfFinal -0.012 (0.010) -0.011* (0.006)
Gender × IfFinal 0.028 (0.198) 0.002 (0.183)
Age × IfFinal 0.041* (0.024) 0.045** (0.023)
Living city DPI × IfFinal 0.023* (0.013) 0.005** (0.002)
Monthly income level × IfFinal 0.022** (0.010) 0.005* (0.003)
Education level × IfFinal 0.004 (0.003) 0.035** (0.017)
Other borrower-related variables Included Included Included Included
Other loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -306.15 -599.31 -293.04 -584.36
#obs. 468 936 461 922
a Models 1 to 4 are based on the samples in which human evaluators’ initial decisions were inconsistent with machines’ decisions (i.e.,
IfConsistent = 0).We duplicated the sample because we considered the humans’ initial and final decisions separately. Other table notes
are the same as [b] and [c] in Table 3.
confidence (Kunimoto et al. 2001) and be more likely to follow machines’ suggestions, regardless of their initial
approval or rejection decisions. Considering the following ratios and the performance improvement from Group
1 to Groups 5 and 6 and from Group 2 to Group 7, our findings indicate that the machines were relatively better
at evaluating cases with feature values near the borderline.
Conversely, it is likely that humans and machines could reach distinct conclusions about an applicant’s default
probability because of differences in evaluating important features. As a result, humans would tend to stick with
their initial opinions. Considering that the machines incorporated extra features to assess the loans, these additional
features might dominate the human-familiar ones, and human evaluators could find that the values of their familiar
features were beyond their expectations. This echoes the literature about humans’ aversion toward AI when
humans cannot successfully interpret the reasoning behind a machine’s decision (Wang and Benbasat 2016). Figure
5 provides empirical evidence with feature distributions to support these arguments. In specific, we visualized
the distributions using four sub-samples, which were separated by two standards: whether human evaluators ultimately
accepted or rejected the applications (A vs.R), and whether humans followed or continued to disagree with
the machines’ recommendations (F vs. D). Interestingly, we observed that the means of (F&A) are close to those
of (F&R), implying that when dealing with borderline cases, humans place more trust in the machines. On the

Table 7 Regression on Human Evaluators’ Initial and Final Approval Decisions (Groups 7 and 8; Probit Model)
Group 7 Group 7 Group 8 Group 8
(humans’ final decision) (initial vs. final) (humans’ final decision) (initial vs. final)
DV: IfApprove Model 1 Model 2 Model 3 Model 4
Loan purpose -0.059 (0.115) 0.111 (0.117) -0.017 (0.128) -0.011 (0.124)
Gender 0.207 (0.136) 0.045 (0.032) -0.154** (0.069) 0.032 (0.034)
Age 0.130** (0.056) 0.073 (0.059) 0.100*** (0.018) 0.051 (0.057)
Living city DPI 0.135*** (0.025) 0.098*** (0.024) 0.188*** (0.030) 0.140*** (0.033)
Monthly income level 0.076** (0.032) 0.032*** (0.010) 0.140*** (0.034) 0.110*** (0.033)
Education level 0.191** (0.081) 0.130*** (0.040) 0.032* (0.019) 0.030* (0.016)
Avg amount of game card -0.002 (0.005) -0.030 (0.057) -0.038 (0.063) -0.014 (0.014)
ATV shopping durable 0.003 (0.002) -0.001 (0.002) 0.007 (0.009) 0.008 (0.012)
ATV shopping virtual -0.005 (0.004) -0.001 (0.001) -0.020*** (0.005) -0.010 (0.008)
#Outgoing contacts -0.036*** (0.010) -0.015* (0.008) -0.025** (0.012) -0.022* (0.012)
#Office by week 0.028** (0.011) 0.067 (0.042) 0.027** (0.012) 0.019 (0.012)
#Recreational place by week -0.126 (0.100) -0.029 (0.095) -0.034 (0.117) -0.027 (0.129)
#Commercial place by week -0.044* (0.025) -0.022 (0.024) -0.111*** (0.039) -0.058 (0.036)
#Public service place by week 0.064 (0.048) 0.015 (0.048) 0.010 (0.040) -0.028 (0.040)
IfFinal -0.337*** (0.091) -0.349*** (0.091)
Loan purpose × IfFinal -0.170 (0.164) -0.006 (0.178)
Gender × IfFinal 0.149 (0.134) -0.185* (0.105)
Age × IfFinal 0.056** (0.028) 0.048** (0.025)
Living city DPI × IfFinal 0.053*** (0.014) 0.048*** (0.014)
Monthly income level × IfFinal 0.044*** (0.014) 0.030** (0.015)
Education level × IfFinal 0.061*** (0.015) 0.003 (0.017)
Avg amount of game card × IfFinal 0.027 (0.025) -0.023 (0.043)
ATV shopping durable × IfFinal -0.002 (0.004) -0.001 (0.002)
ATV shopping virtual × IfFinal -0.004 (0.005) -0.010** (0.004)
#Outgoing contacts × IfFinal -0.021** (0.010) -0.003 (0.013)
#Office by week × IfFinal 0.017* (0.010) 0.008 (0.006)
#Recreational place by week × IfFinal -0.095 (0.108) -0.007 (0.114)
#Commercial place by week × IfFinal -0.022* (0.014) -0.052*** (0.011)
#Public service place by week × IfFinal 0.049 (0.050) 0.018 (0.051)
Other borrower-related variables Included Included Included Included
Other loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -329.26 -646.33 -265.37 -546.14
#obs. 638 1,276 649 1,298
Table notes are the same as those for Table 6.
contrary, once they found the featureswere far belowor above their thresholds, they held on to their own views.We
offer additional evidence to support this assertion by considering all relevant loan features in Figure C2 (Appendix
C2).
The above result, however, was not found in Group 8. Interestingly, in Models 3 and 4 of Table 4, we noticed
that some alternative features, such as gender and the average transaction amount for purchases of virtual goods
(i.e., “ATV shopping virtual”), had significant coefficients. That is, those additional features explained why the
human evaluators shifted from their initial decisions.5 More importantly, given that those features did not reach
significance when we compared the human evaluators’ initial decisions with those of the machines, it suggests
5 It is possible that the evaluators might have strategically chosen to follow the machines’ decisions if the machine recommended either
approval or rejection.We alleviated this concern in Appendix C3.

(a) Distributions of Living City DPI (b) Distributions of #Outgoing Contacts
a All distributions are based on the samples in which human evaluators’ initial decisions were inconsistent with machines’ decisions.
b F & A: Cases wherein humans followed the machines’ recommendation and ultimately approved the loan applications; F & R: Cases wherein
humans followed the machines’ recommendation and ultimately rejected the loan applications; D & A: Cases wherein humans disagreed with
the machines’ recommendation and ultimately approved the loan applications; D & R: Cases wherein humans disagreed with the machines’
recommendation and ultimately rejected the loan applications.
Figure 5 Feature Distributions of Diverse Cases (Group 7)
that the human evaluators reconsidered their initial decisions. In other words, the presence of large information
volumes and machine explanations provoked evaluators to engage in active rethinking, which improved their final
decision accuracy.
5.3. Disagreement and Decision Quality: Decomposition of the Rethinking Process
As discussed earlier, we observed that with the presence of large information volumes and machine explanations,
humans reconsidered an interesting feature, “ATV shopping virtual”. This feature had not been used by either
humans or machines in the independent decision-making process. The prediction models might have ignored or
downplayed the values of this feature due to its correlations with other features.We conjectured that the attention
to the “ATV shopping virtual” feature stemmed from human evaluators associating it with the “average amount
spent on game cards” feature. When the human evaluators saw the machines making different decisions, they also
noticed that the loans had some irregular patterns on features that the evaluatorswere unfamiliar with (e.g., “average
amount spent on game cards”). However, such features could hardly be applied by human evaluators, as the most
common value by far across all loan applications was 0 (refer to Figure A1a in Appendix A1; the median is 0). Such
a distribution would lead to human evaluators perceiving those features as non-informative. The literature has
suggested that humans are good at building connections between given information and other relevant, familiar, or
understandable information in cognitive processing (Br˚aten and Samuelstuen 2007, Hollnagel 1987). Since game

cards are typical virtual goods and “ATV shopping virtual” had many more salient non-zero values (Figure A1b;
the median is 8.70), human evaluators are likely to attend more to this feature when making decisions.
In Appendix C4, we compared the default rates between Groups 7 and 8 after separating loans “saved” by
the machines (i.e., those that were originally rejected by human evaluators but ultimately approved due to the
machines’ approval recommendations) and those “saved” by human evaluators.We showed that using the updated
decision rules with new and correct features (i.e., significantly correlated with default behavior), human evaluators
were more likely to correctly select “good” loans from those rejected by the machines, whereas humans’ decisions to
overrule the machines resulted in no change or a decrease in efficiency (i.e., replacing some “bad” applications with
other “bad” ones) in Group 7 where humans relied on their priors.Meanwhile, the use of gender features might be
due to their relatively high correlations with “ATV shopping virtual” (refer to Table 5). Such findings also explain
the alleviation of gender bias (which we will demonstrate later, in Section 6.2).Moreover, we conducted a straightforward
post-hoc analysis in Appendix C4 to clarify the allocations of different loan types by humans, machines,
and collaborative efforts. This provided additional insights into how machines and humans could assume distinct
roles to improve overall collaborative performance.
Taking all of the findings together, our results suggest that with a proper design that invokes humans’ active
rethinking (e.g., the presence of effective machine explanations when processing complicated information), the
collaboration between humans and machines could potentially achieve “1+1>2” in practice.Machines would take
responsibility for handling borderline cases, and humans would have the potential to invoke active rethinking to
correct machines’ mistakes in the “random” cases (e.g., those without explicitly congruent feature patterns) when
they perceive that machines have made contradictory decisions, inspired by suggestive information cues.
6. Empirical Extensions
6.1. Heterogeneity by Human Evaluator Characteristics
Recent studies have shown that human agents’ degree of decision-making experience might affect their acceptance
of machine recommendations as well as their performance in collaboration with machines (Luo et al. 2019,Wang
et al. 2023b). Therefore, we decomposed the heterogeneity regarding individual evaluators’ characteristics. Below,
we focus on the evaluators’ experience, based on the length of time (in months) that they had worked in the focal
company before we started the experiment. FollowingMarcotte (1998), the experience was measured at four levels

Table 8 Heterogeneity Analysis of Human Evaluators’ Months Working (Probit Model)
Groups 1 & 2 (only human) Groups 5–8 (human + machine)
Model 1 - DV: IfDefault Model 2 - DV: IfDefault Model 3 - DV: IfConsistent Model 4 - DV: IfFollow
Large info. (L) -0.384*** (0.143) -0.093*** (0.018) -0.221*** (0.083) 0.469** (0.187)
Month of working=1 (Work=1) (baseline) (baseline) (baseline) (baseline)
Work=2 -0.237* (0.127) -0.052 (0.157) 0.141 (0.089) 0.141 (0.163)
Work=3 -0.400*** (0.140) -0.109 (0.162) 0.153* (0.084) -0.212 (0.174)
Work=4 -0.549* (0.146) -0.147* (0.087) 0.194** (0.078) -0.284 (0.186)
L ×Work=1 (baseline) (baseline) (baseline) (baseline)
L ×Work=2 0.305 (0.187) -0.103 (0.259) 0.042 (0.112) 0.093 (0.253)
L ×Work=3 0.355* (0.212) 0.383 (0.254) 0.084 (0.120) 0.229 (0.254)
L ×Work=4 0.356* (0.203) 0.057 (0.263) 0.048 (0.113) 0.359 (0.248)
Explanation (Expl) -0.150 (0.179) 0.114 (0.088) 0.146 (0.192)
Expl ×Work=1 (baseline) (baseline) (baseline)
Expl ×Work=2 0.405 (0.296) 0.036 (0.113) 0.193 (0.251)
Expl ×Work=3 0.057 (0.236) 0.013 (0.120) 0.720*** (0.273)
Expl ×Work=4 -0.021 (0.264) 0.022 (0.122) 0.442* (0.267)
L × Expl -0.278 (0.305) -0.051 (0.121) 0.159 (0.281)
L × Expl ×Work=1 (baseline) (baseline) (baseline)
L × Expl ×Work=2 -0.196 (0.395) 0.070 (0.158) -0.480 (0.366)
L × Expl ×Work=3 -0.383** (0.180) 0.059 (0.165) -0.641* (0.377)
L × Expl ×Work=4 -0.637* (0.375) 0.078 (0.172) -0.998** (0.390)
Borrower-related variables Included Included Included Included
Loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -822.87 -5,565.09 -957.51 -1,107.73
#obs. 2,716 11,727 5,603 2,216
a Models 1 and 2 are based on the approved samples. Model 3 is based on all loan samples. Model 4 is based on the samples in which human evaluators’ initial
decisions were inconsistent with the machines’ decisions (i.e., IfConsistent= 0).We introduce the definition of IfFollow in Appendix C2.
b Large info. = 1 for the treatment using large information volumes for decision making, 0 for small. Evaluator months working: 1 = not longer than 6 months, 2
= 6–12 months, 3 = 13–18 months, 4 = longer than 18 months. Interpret. = 1 for treatment of disclosing machine explanations, 0 for not.
c Standard errors are in parentheses. Significant results are in bold. *p <0.10, **p <0.05, ***p <0.01.
(1 = not longer than 6 months, 2 = 6–12 months, 3 = 13–18 months, 4 = longer than 18 months). To quantify the
impact of experience levels,we considered another Probit model, this one with three-way interaction terms including
the existence of large information volumes, the availability of machine explanations, and experience levels.We
also included all lower-level interaction terms in the regression.We presented the estimated coefficients in Table 8,
whereinModel 1 considers the default rate asDV and includes humans’ independent decisions only, whileModels
2–4 are in the human–machine collaboration modes. Specifically, we replicated our mechanism tests with heterogeneous
experience levels: whether a loan defaultedwasModel 2’sDV, whether initial decisionswere consistentwas
Model 3’s DV, and whether to follow machines’ decisions was Model 4’s DV. Note that the estimation of Model
4 incorporated only samples where human evaluators’ initial decisions were inconsistent with machine decisions.
Additionally,we offer more comprehensive heterogeneity analyses with alternative characteristics in Appendix D1.
Table 8 yields several interesting findings. First, the positive estimate of L ×Work = 3 (or 4) inModel 1 indicates
that without machine assistance, experienced human evaluators performedworse with a large information volume
than with a small one. Given the definition of work experience, evaluators with a higher experience level might
have accumulated significantly more knowledge in handling small data over a long time, and thus, they might have

found it hard to switch their mindset (i.e., experience inertia) (Becker 1995). Another plausible explanation is that
these more senior evaluators might have less trust in AI, as suggested by Wang et al. (2023b). On the other hand,
evaluators who were new to the company might have still been in the learning stage when the experiment started,
and in such cases, persistent learning could have brought more benefits. Second, we observed that experienced evaluators
tended to make more decisions that were consistent with those of the machines (as shown in the results
of Model 3), especially with small information volumes. This is reasonable because experienced evaluators were
more likely to have learned the feature values comprehensively and reached a similar level of performance as the
machines. Third, the estimates inModel 4 suggest that, when we focused on loans with different initial decisions,
experienced evaluators were more likely to follow machine explanations in a small information scenario but more
likely to overrule machines’ decisions and stick to their own opinions given the availability of large information volumes.
Combining all of these results with those inModel 2 makes it clear that the satisfaction of both conditions
encouraged experienced evaluators to initiate an active rethinking process and thereby achieve reduced borrower
credit risk. Furthermore, to deepen our understanding of how individual heterogeneity influences behavior in the
presence of machine assistance, we replicated our mechanism examinations with different experience levels. The
findings, detailed in Appendix D2, offer more nuanced and straightforward evidence indicating that experienced
evaluators were more inclined to initiate an active rethinking process when provided additional external information
in Group 8.
6.2. Decision Biases
As implied inTable 5, most of themajor variables considered in both the human evaluators’ and machines’ decisionmaking
processes were relatively highly correlated with the performance metric. This confirms the fact that both
humans and machines made decisions based on their estimated credit risk. In the meantime, it is worth noting
that some of the major variables (e.g., loan purpose, average amount spent on game cards, and number of visits to
commercial places)were also highly correlated with gender.Anatural question arises: will this correlation cause any
fairness issues? For example, will it affect the loan-approval decisions of borrowers of different genders, especially
when considering different information volumes and human–machine collaboration modes?
To address this question, we first focused on the final performance as measured by the non-default rates. We
recorded the statistics of each group inTable D5 in Appendix D3.We observed that with large information volumes

(i.e., Group 4), machines tended to favor female applicants, because the non-default rate of the approved male
applicants (98.03%) was much larger than that of female applicants (93.99%). That is, machines seemed to have
exerted a higher loan-approval criterion for males than females. The involvement of human evaluators without
machine explanations (i.e., Group 7) could not alleviate such gender bias. However, when human evaluators were
presented with machine explanations (i.e., Group 8), the final repayment performance of the approved female and
male applicants became better and similar (96.67% vs. 97.55% ), suggesting the mitigation of gender bias.
Following Teodorescu et al. (2021), we additionally applied the criterion of “equalized opportunity” (EOR),
which requires positive outcomes to be independent of the protected attribute, in order to alternatively measure
decision fairness (biases) between genders in our different experimental groups. Let G be the gender
indicator (G = 0 or 1), and Y=1 and ^Y=1 be the correct and actual positive outcomes (i.e., a loan application
being approved in our context), respectively. “Correct” here means that non-default loans (observed
from the repayment performance of the approved loans) got approved. As such, equalized opportunity means
Pr(^Y=1|G=0,Y=1)=Pr(^Y=1|G=1,Y=1). Applying this criterion to our context, EOR describes the decision
biases between genders as follows: EOR = Appr(G=0)/NonD(G=0)
Appr(G=1)/NonD(G=1) , where Appr(G=0) and Appr(G=1) refer to the
approval rates for females and males, and NonD(G=0) and NonD(G=1) refer to the non-default rates for females
and males (calculated within female or male groups), respectively. The closer EOR is to 1, the greater the fairness
is between the genders. The larger the deviation from 1, the more bias there is toward females (EOR>1) or males
(EOR <1). Figure 6 plots the values of EOR across the different experimental groups.
We learned from Figure 6 that the human evaluators treated males and females equally in terms of fairness,
regardless of the volumes of information available (EOR = 1.012 (small amounts of information) and 0.987 (large
amounts of information), both close to 1). That is, the human evaluators tended to apply relatively similar standards
in evaluating the male and female applicants. The machines, however, significantly favored females when
they had large information volumes available for decision-making (EOR = 1.201). This was due mainly to the high
correlation between the most important features used by machines and the default indicator, as shown in Table 5.
This finding is consistent with previous studies (e.g., Fuster et al. 2022) and implies that whereas machines perform
much better in general with large-scale information, they return results that are gender-biased, notwithstanding

a Refer to Table D5 for complete values.
Figure 6 Equalized Opportunity Ratio on Gender
the literature’s demonstration of the value of large-scale information in alleviating certain forms of demographic
discrimination (Lu et al. 2023a). Further, we did not observe any significant change when human evaluators were
involved in making the final decisions with small information volumes (i.e.,EOR= 1.025 and 1.040 in Groups 5 and
6, respectively). However, we did observe a significant reduction in EOR when both large information volumes
and machine explanations were available (i.e., EOR = 1.056 in Group 8). In this scenario, the increase in final decision
accuracy could be attributed to human intervention in correcting the risk evaluations of female borrowers.
Similarly to our findings in Section 5.2, human evaluators associate certain observed features to others (i.e., “ATV
shopping virtual”). Fortunately, the “ATV shopping virtual” feature positively correlates with the feature gender
(refer toTable 5) and default probability. Hence, human evaluators helped mitigate gender biases successfully. This
again highlights the value and necessity of collaboration between humans and machines. It is essential to acknowledge
that the gender bias observed in our dataset and empirical context may be specific to our circumstances. Environments
with a more balanced interaction between genders could potentially avoid this gender-related issue.We
provided a comprehensive discussion about how our findings concerning gender biases could be extrapolated to
other contexts in Appendix D3.
7. Conclusions and Discussion
7.1. Simultaneous Needs of Both Conditions
In the emerging stream of human–machine collaboration literature, there is a dearth of systematic understanding
about when, with machines’ assistance, humans can actively contribute and how they can add extra value to task

outcomes.We dived into the information processing literature, the comprehension of which affords two prerequisites
for invoking humans’ deep thinking: information complexity initially draws humans’ attention to engaging
in the tasks, and useful external cues drive humans to perform active consideration. We applied these theoretical
implications to human–machine collaboration tasks, and accordingly, against the backdrop of the microloan
industry, we devised two treatments by manipulating information volumes and displays of machine explanations.
A unique two-stage field experiment helped us to explicitly quantify the corresponding performance.
Our empirical findings shed light on the significance and compatibility of the two theory-driven conditions,
and showed that neither can be dispensed with. First, although larger information volumes mean more potential
knowledge to help gauge decision-making performance (Hu et al. 2022), our empirical comparisons demonstrated
that humans tended to utilize what they have specialized in (i.e., small information volumes, Group 1 vs. Group
2), because learning is costly and instant feedback might be uncertain.Without effective extrinsic motivation, such
distortionwould further impede humans’ acceptance of machines’ recommendations (Group 4 vs. Group 7). This
is generally detrimental as humans’ insistence on their own decision rules is very likely to result in underfitted
decisions in different tasks (Song et al. 2021).
Second, it was also no surprise to find that offering machine explanations alone, without the presence of large
information volumes, could not inspire humans’ further contribution (Group 6 vs. Group 8). This is owed to
the fact that machines’ superiority in tackling prohibitively (for humans) complex tasks to achieve satisfactory
predictionswas constrained by information availability (also refer to the comparison between Group 3 vs. Group 4
in Figure 3).Ontop of limited information, humans could not becomesmarter than machines. Notably,we noticed
that a few recent studies have focused on the value of machine explanations to human–machine collaboration (e.g.,
Bauer et al. 2023, Jacobs et al. 2021).However, our study suggests contingent factors, such as task complexity,would
impact the effect of machine explanations. Although machine explanations provide humans with more reference
information, humans may not take advantage of them due to insufficient motivation to deeply involve themselves
in decision-making (Speier 2006). Instead, humans were found to involve more trust in machines by following
their recommendations with those “borderline” loans.
Hence, only with the simultaneous presence of large information volumes and machine explanations can
human–machine collaboration realize better performance than humans or machines alone (i.e., experimental

Group 8) via initiating active rethinking. This engagement results in further improvement of decision accuracy
and mitigation of the machines’ biased decisions. Our findings, therefore, confirm the validity of generalizing the
dual-process theories of reasoning from humans’ independent or interpersonal decision-making to the realm of
machine assistance.
7.2. Managerial Implications
This study, built on our unique experimental designs, also offers non-trivial insights to practitioners. Our findings
could inform companies’ future benefit-cost analyses in managing their efforts/investment and balancing among
human agents (human capacities), data purchasing/collecting, and adoption of AI techniques. Our experiments
probe diverse possible and manageable factors that could negatively affect the desired efficiency of human–machine
collaboration, andwe showempirical evidence of those factors’ roles in the overall decision-making process. What’s
more, this paper presents practitioners with a caveat to their prevalent preference for big data, AI techniques,
and/or human–machine collaboration. Specifically, if big data is available, this collaboration can achieve both satisfactory
decision-making efficiency and fairness. However, when faced with the threat of machines taking their
jobs or the possibility of over-domination by machine intelligence, human employees across companies and even
industries might resist machine assistance or begin to rely on it excessively. Thus, we provide a scheme of machine
interpretability to encourage human agents to rely less on machines and to create additional value. If only small
data is available (e.g., affordable), a machine alone seems enough. The involvement of human efforts, regardless of
whether machine explanations are present or absent, cannot add significant value in improving prediction accuracy
or addressing gender biases in this case.Moreover, our empirical analyses not only offer guidance to platforms
in designing efficient collaboration systems but also open pathways to gaining valuable insights into hiring decisions.
In particular, our heterogeneity analyses highlight that individuals with experience possess greater potential
to attain elevated levels of collaborative performance and amend machine biases through more systematic contributions.
Nevertheless, even with experienced employees, platforms should not neglect the importance of refining
their training approaches and procedures. This includes the implementation of comprehensive data literacy training
programs (Hvalshagen et al. 2023), providing valuable cues and timely feedback for decision-making improvement
loops (Proctor and Bonbright 2021). Additionally, it is essential to incorporate modules on ethics and bias
awareness into training programs (Sellier et al. 2019).

7.3. Discussions of Generalizability
Our theory-driven experimental design and empirical findings are highly generalizable to other contexts where
the decision-making task objectives are not excessively intricate for humans or machines and/or can be clearly formulated.
Moreover, the applicability extends to scenarios where opportunities exist to acquire additional information,
whether in terms of volume or type, to enhance overall performance (Amit and Sagiv 2013). Examples
of such tasks include job candidate screening in labor hiring, supplier evaluation in procurement, and medical
treatment decision-making. On a broader note, our study suggests that machines consistently outperform human
agents when tasked with objectives that are not particularly challenging, such as classification or prediction problems
involving structured objects and features. The availability of a large amount of information might stimulate
human agents to pay attention to their tasks, but it does not guarantee that they will aid machines. Additional cues,
such as machine explanations, are crucial for guiding human agents to perform an active rethinking of complex
information to deal with uncertainties, thereby producing better outcomes.
However, it is worth discussing some caveats to practical system designs as they relate to the generalizability
of our results. Our findings regarding the two conditions essential for stimulating active information processing
in humans are contingent on many surrounding factors. For example, humans should be responsible for their
decision-making performance to some degree, thereby preventing the complete delegation of decision-making to
machines. Humans’ loan-approval capabilities should also be associated with the ultimate collaboration performance.
Also, selected AI algorithms should be suitable for tackling the specific task objectives and models need to
bewell-trained. Regarding the two focal treatments, rich information is not a panacea; any newly acquired information
must be inherently valuable to bolster decision-making performance. In addition, machine explanations must
be delivered in a clear and compelling manner. As there might be disagreement between human (expert) knowledge
and machine explanations (Krishna et al. 2022), the explanations should be suitably displayed, understandable, and
able to stimulate cognitive reasoning (e.g., enabling easy comparisons). Lastly, as proposed byWang et al. (2023b),
humanworkers’ prior knowledge of AI and their responsibilities assigned are factors associated with their attitudes
toward AI. In our specific context, human evaluators generally had limited knowledge of machine learning, and
the compensation structure within the platform (as outlined in Section 3.1) did not encourage evaluators to proactively
enhance their understanding to achieve superior performance levels. In other settings where human workers
are more proficient in AI and have stronger motivation to consistently refine their task performance, the responses
to machine recommendations (with or without machine explanations) may exhibit variation.
From the technical perspective, our empirical results also reinforce our theory-guided design approach to some
extent, as our two proposed treatments did not explicitly rely on any specific form of machine interpretability.
As long as they could offer clear signals, we deemed them potentially valuable in encouraging humans to reassess
their perspectives.Moreover, while centered on the implementation of specific machine-learning algorithms, our
empirical analyses and findings can be extrapolated to diverse applications involving advanced and more intricate
AI techniques.Onthe one hand, our targeted interventionswere guided by theory and offered insights into human
behavioral responses to factors including task complexity and reference cues. Additionally, our experimental design
deliberately withheld information about the specific machine-learning algorithms from participants, making it
possible to extend our observations to other AI models, despite potential variations in actual performance and
opportunities for human contributions.
Additionally, it is worth noting that in our primary study, we cannot evaluate the value of AI identity explicitly.
Put differently, our empirical results do not conclusively discern whether the observed effects stem from the
additional information offered by machines (or senior managers) or from the direct attitudes of humans toward
AI or machines. However, it is crucial to acknowledge that the performance of senior managers in real-world scenarios
may not exhibit the same level of stability and efficiency as machines, especially given the vast amounts of
information involved. Considering the time constraints inherent in making accurate decisions, AI or machines
tend to outperform their experienced human counterparts.Moreover, several research papers have delved into the
difficulties faced by humans when attempting to articulate or summarize the rules guiding their decision-making
processes (Hu et al. 2022). Compared to reliance on machines, relying on senior managers to provide explicit decision
cues is more challenging. In contrast, machines offer the advantage of leveraging advanced techniques such as
feature importance extraction. This underscores the significance of fostering collaboration between humans and
machines.
Finally, our experimental design emphasized the efficacy of a two-stage decision-making process wherein where
human evaluators initially make independent loan approval decisions and subsequently determine their final decisions
by opting to adopt or reject the machine recommendations. While recognizing that two-stage designs may
be practically infeasible, we suggest the potential relevance of our findings in scenarios where only a single stage
is feasible–directly presenting machine recommendations to the original human-alone decision-making scenario.
However, this adjustment may influence decision-making outcomes. For example, without a distinct independent
decision-making stage, the direct provision of machine recommendations may lead to over-reliance on machines
or foster distrust due to the absence of a clear contrast to humans’ independent decisions. The lack of explicit comparisons
may further hinder rule identification, especially among less experienced individuals, resulting in more
significant heterogeneity in decision-making performance compared to a two-stage setting.
7.4. Limitations and Directions for Future Studies
Our paper has several limitations that provide promising opportunities for future research. First, our empirical
design focused on a static scenario without human learning. However, in a real-world environment, humans
and machines might learn from each other’s decision-making processes and adjust gradually over a relatively long
period. Future research can extend our analyses to disentangle learning behavior and thereby design an optimization
strategy for both sides using techniques such as reinforcement learning models. Second, our experimental
treatment considered a binary case between small and large information volumes. Future studies can relax this
constraint and explore a continuous level of information complexity, the insights from which could offer business
managers more practical conclusions and increased value. Third, in our empirical setup,we deliberately limited the
experimental period to one or twoweeks to establish a controlled environment, which helped us mitigate potential
biases introduced by human learning behaviors evolving over time.We acknowledge the temporal constraint as a
limitation in our study, paving the way for future investigations. Extending the experimental period would enable
researchers to explore how humans process and value information conditions over an extended period, offering
valuable insights into the dynamics of long-term interactions. Fourth, divergence in terms of cultural background
or industry domain might have affected our findings. Similar studies in other countries or industries can further
validate these findings and offer novel insights into human–machine collaboration designs.






----------------------------------------------------------------------------------







Ibrahim, Rouba, et al. “Eliciting human judgment for prediction algorithms.” Management
Science, https://doi.org/10.1287/mnsc.2020.3856.

Abstract
Even when human point forecasts are less accurate than data-based algorithm predictions, they can still help boost performance
by being used as algorithm inputs. Assuming one uses human judgment indirectly in this manner, we propose
changing the elicitation question from the traditional direct forecast (DF) to what we call the private information adjustment
(PIA): how much the human thinks the algorithm should adjust its forecast to account for information the human has
that is unused by the algorithm. Using stylized models with and without random error, we theoretically prove that human
random error makes eliciting the PIA lead to more accurate predictions than eliciting the DF. However, this DF-PIA gap
does not exist for perfectly consistent forecasters. The DF-PIA gap is increasing in the random error that people make
while incorporating public information (data that the algorithm uses) but is decreasing in the random error that people
make while incorporating private information (data that only the human can use). In controlled experiments with students
and Amazon Mechanical Turk workers, we find support for these hypotheses.

Key words: laboratory experiments, behavioral operations, random error, elicitation, forecasting, prediction, discretion,
expert input, private information, judgment, aggregation

1. Introduction
Because of increased access to data and advancements in machine-learning algorithms, a common operational
improvement initiative is to replace human forecasters with data-driven prediction algorithms. For
example, in our motivating setting, a hospital needs surgery duration forecasts to schedule operating room
use, which costs $2; 190 per hour on average (Childers and Maggard-Gibbons 2018). Using surgery duration
data from that hospital, Ibrahim and Kim (2019) show that physicians’ mean absolute percent forecast
error was 33%, whereas algorithms based on available patient and surgery data reduced that error to 29%.
Nevertheless, even if humans are not better than algorithms head-to-head, their judgments can still help.
In the above example, the hospital could improve predictive accuracy even further, to under 27%, by using
the physicians’ forecasts as an input (along with the other data) to the algorithm. In other words, the best
forecasts often come not from replacing humans with algorithms, but from combining them.
In this research, we ask the following question: If we know that we are going to use human judgment not
directly, but rather indirectly, in an algorithm, should we elicit something else besides point forecasts? If so,
what human judgment should we elicit, and why might it work better?
We theorize that the primary reason why humans add value to algorithms is that they have access to
private information that the algorithm does not use, for example, because it is not in the database or there are
not sufficient historical training data for the algorithm to effectively use it. Therefore, we consider whether,
rather than asking for a human’s direct forecast (DF), it may be better to instead ask about her judgment of
this private information’s impact (even if the system designer does not know what this private information
is ahead of time). Specifically, we propose the idea of eliciting the private information adjustment (PIA)—
how much the human thinks the algorithm should adjust its forecast to account for the information that only
the human has.
Using stylized models (x2), we theorize that the PIA leads to more accurate predictions than DF only if
there is human random error. That is, from a predictive accuracy perspective, there is no difference between
eliciting a DF or the PIA if people are perfectly consistent in how they use information to make a forecast.
However, if they are inconsistent, then the PIA should help algorithms more than the DF. Furthermore, the
models shed light on how random error creates this difference by predicting which environmental conditions
would lead to greater differences in performance. Namely, they show that the PIA’s advantage, relative
to DF, is larger when “public” data—the data that the algorithm uses—are complicated for the human to
process but smaller when the human’s private information is complicated to process instead.
To test these hypotheses regarding the difference in performance between DF and PIA, we conducted
controlled experiments in which we elicited human judgments for 50 simulated surgery durations based on
predictive data. We told our participants that the hospital’s algorithm had access to only some of the data
(“public information”) and that only the participant had access to the other data (“private information”).
In one condition, we elicited judgment by asking for the participant’s DF for each surgery, while in the
other, we elicited the participant’s PIA for each surgery. Then, for each condition, we calibrated prediction
algorithms using the first 35 surgeries and tested their predictive performance using the last 15 surgeries.
In Experiment 1 (x3), conducted with university students and replicated with Amazon Mechanical Turk
(MTurk) workers, we find that prediction algorithms performed significantly better when they had access
to the participants’ PIA as inputs as opposed to their DF: their average root mean squared error (RMSE)
in the test sets was 21% lower. In Experiment 2 (x4), we manipulate random error magnitudes by making
the public or private information more or less complex: subjects must aggregate multiple factors when
the information is complex but are provided one equivalent factor when the information is not complex.
Consistent with our theoretical development, we find that the RMSE for PIA is 48% lower than for DF
when the public information is complex but only 6% lower than for DF when the private information is
complex. Finally, in x5, we discuss why private information exists in practice, implications of our findings,
and opportunities for future research.
We contribute to four main bodies of research. Management science researchers have recognized the
potential value in integrating human judgment with forecasting algorithms (see Arvan et al. 2019 for a
review). Humans often possess so-called “domain knowledge”: better and more up-to-date information than
what statistical models use (see x3 of Lawrence et al. 2006). Such domain knowledge is the generally
accepted explanation for why human judgmental forecasting sometimes even outperforms statistical models
in practice (see Lawrence et al. 2000 for sales forecasting and Alexander Jr 1995 for earnings forecasting).
The two most common integration approaches are to make judgmental adjustments to an algorithm’s point
forecast (e.g., see Carbone et al. 1983, Fildes et al. 2009) or to combine separate human and algorithm
point forecasts (e.g., see Blattberg and Hoch 1990, Goodwin 2000). We contribute by examining a different
human elicitation question from the point forecast. Notably, our proposed method is not equivalent to
judgmental adjustments because we use the PIA as an input for the prediction algorithm. In fact, in our
experiments, using PIA responses to adjust algorithm forecast outputs yields poor predictive performance.
A stream of behavioral operations management research studies the system design implications of human
random error. For example, the best way to design contracts (Su 2008, Ho and Zhang 2008), queues (Huang
et al. 2013, Tong and Feiler 2017), or auctions (Davis et al. 2014) changes once the system designer considers
human random error. Most closely related to our paper is Kremer et al. (2016). They show that human
random error causes eliciting human forecasts in a top-down fashion to be more effective in some environments
but bottom-up forecasting to be more effective in others.We contribute by showing how a forecasting
system’s elicitation design impacts performance once one considers human random error, even if it has no
effect without human random error.
Researchers in judgment and decision making have made advancements in developing strategies to
improve human judgment accuracy. Perhaps the most well-known idea is to harness the “wisdom of crowds”
(e.g., see Surowiecki 2005) through averaging multiple people’s judgments. Interestingly, because people
are so inconsistent (Kahneman et al. 2016), even averaging multiple judgments by the same person separated
by time (Vul and Pashler 2008) or with a prompt to think differently (Herzog and Hertwig 2009) helps,
albeit only about half as much as averaging judgments by two different people (Mannes et al. 2012). Most
closely related to our work in this stream is Palley and Soll (2019), who develop a new elicitation method
that improves the “wisdom of crowds” strategy by estimating the amount of shared information between
individuals. Our elicitation strategy also seeks to improve an aggregation strategy by addressing the issue
of disentangling the shared information between the human and the algorithm.
Lastly, studying the benefit of incorporating discretion from humans with local knowledge in operational
decision making has been an emerging topic of study in operations management. Various application
domains have been considered, such as capacity decisions in service operations (Campbell and Frei 2011),
sales forecasting (Osadchiy et al. 2013), price setting (Phillips et al. 2015), hospital unit admission decisions
(Kim et al. 2015), and product removal decisions in retail stores (Kesavan and Kushwaha 2020).
2. Theory Development
In this section, we leverage simple mathematical models to compare the theoretical performance of a prediction
algorithm that uses human DF and one that uses human PIA. Specifically, our focus is on showing
that the difference in performance depends critically on whether or not we assume the forecaster suffers
from random error. Our theoretical development is agnostic about the exact sources of this random error
and does not attempt to provide a detailed description of the psychology of prediction. Rather, the point
of the models is to clearly demonstrate that including random error in the forecast is sufficient to generate
differences between DF and PIA. We use these results to motivate two hypotheses about whether and how
eliciting the PIA will be more effective than DF.
2.1. Surgery Duration Assumptions
We assume an actual surgery duration, Y , is a random variable defined by the linear model
Y =v +
X
i2P[I
wiXi +; (1)
where we separate the public factors, denoted by the index set P, from the private factors, denoted by the
index set I. In (1),  is an error term, with E[]=0, which represents true environmental random shocks, i.e.,
random variations that are impossible to predict even with all public and private information. We assume
that  and (Xi)i2P[I are mutually independent.
2.2. DF and PIA Are Equivalently Effective with Consistent Forecasters (No Random Error)
We define DF and PIA for the consistent forecaster who does not suffer from random error as follows:
DF =v +
X
i2P[I
w
iXi and PIA =
X
i2I
w
iXi: (2)
Here, we assume that v and w
i , for i 2 P [ I, are deterministic, though not necessarily known by the
algorithm a priori. (Note that they can be any constants and are not necessarily “optimal”. For example,
setting w
i =0 is equivalent to assuming humans do not use that information.) Define the best-fitting models
of Y given the public factors and DF or PIA using linear regression:
(Model-DF) MDF =0 +
X
i2P
iXi +DFDF; (3)
(Model-PIA) MPIA =
0 +
X
i2P

iXi +PIAPIA: (4)
Then, the following proposition holds. We relegate the proofs of all propositions to the Appendix.
PROPOSITION 1. Model-DF and Model-PIA yield the same predictions.
That is, with consistent forecasters, predicting surgery durations using DFs as model inputs yields the same
predictions as using PIAs as model inputs; the two elicitation methods are equivalent from the algorithm’s
perspective.
2.3. PIA Outperforms DF with Inconsistent Forecasters (Random Error)
Next, we define DF and PIA for the inconsistent forecaster who does suffer from random error:
DFb =vb +
X
i2P[I
Wb
i Xi and PIAb =
X
i2I
Wb
i Xi: (5)
Here, we assume that Wb
i are random variables with E[Wb
i ] =  wb
i and Var[Wb
i ] > 0. We also assume that
Wb
i and Xi are all mutually independent, for i 2 P [ I. Thus, in contrast to (2), (5) captures inconsistencies
or random error in assigning weights to each factor. For example, these random weights could reflect
inconsistencies in the encoding of information, memory retrieval, aggregation of multiple factors, or the
translation from one domain to another. Also, note that this random weights model can capture ideas such as
inconsistency in which factors humans take into account or adding a random term to DFb but not to PIAb.
For the purposes of this paper, we make no strong claim about the psychological source of this random
error—only that it exists (e.g., see Kahneman et al. 2016) and is greater when people are asked to account
for more factors.
Define the best-fitting models of Y given the public factors and DFb or PIAb using linear regression:
(Model-DFb) MDFb =0 +
X
i2P
iXi +DFbDFb; (6)
(Model-PIAb) MPIAb =
0 +
X
i2P

iXi +PIAbPIAb: (7)
In contrast to the equivalence result in Proposition 1 for the consistent-forecaster model, the following
proposition demonstrates the benefit of eliciting the PIA compared to eliciting the DF under the inconsistentforecaster
model. (Note that all propositions hold for both MSE and RMSE; we use RMSE to report our
experiment results.)
PROPOSITION 2. The mean squared error (MSE) for predictions under Model-DFb is strictly larger
than that under Model-PIAb, i.e., E[(Y 􀀀MDFb )2]>E[(Y 􀀀MPIAb )2].
6
The intuition is that from the algorithm’s perspective, the value of the human input is the private
information—the algorithm already has the public information. The algorithm can infer the private information
equally well from DF or PIA responses without human random error. However, when there is human
random error, the algorithm can more accurately infer the private information from the PIA. Based on this
result, we formulate our first hypothesis.
Hypothesis 1 All else equal, a prediction model that is calibrated using DF yields less accurate predictions
than a prediction model that is calibrated using PIA.
2.4. The DF-PIA Gap Magnitude Depends on Random Error Location
Proposition 2 establishes our main result that because of human random errors, using PIA yields more
accurate predictions than using DF. We now investigate how the “location” of these random errors (i.e.,
whether they occur incorporating public versus private factors) affects the performance difference between
DF and PIA. To do so, we study the behavior of the DF-PIA gap, which we define as the difference in the
MSEs from Proposition 2, E[(Y 􀀀MDFb )2]􀀀E[(Y 􀀀MPIAb )2].
Random Error Incorporating Public Factors. To examine the effect of random error on the DF-PIA
gap when incorporating public factors, we consider two cases that are identical except for the degree of
variability in Wb
i for i 2 P. Specifically, we define fWb
i to be a mean preserving spread of Wb
i (Rothschild
and Stiglitz 1970). The following result establishes that increasing the variability in how people incorporate
public factors increases the DF-PIA gap:
PROPOSITION 3. The DF-PIA gap is larger when fWb
i is used in (5), for i 2 P, instead of Wb
i .
The idea behind Proposition 3 is as follows. Model-PIAb remains the same when we add variability to
Wb
i ; i 2 P because PIA responses are unaffected by the random error incorporating public factors. However,
Model-DFb is less accurate when fWb
i is used instead of Wb
i ; i 2 P. DF responses are more variable when
fWb
i is used, which makes it harder for the algorithm to learn the private factors. Combining these two
observations implies that the DF-PIA gap increases.
Figure 1 shows the results from numerical simulations (see Appendix B for details). The left panel corresponds
to Proposition 3. It varies the standard deviation of the public-factor random weight, holding
constant the standard deviation of the private-factor random weight. Observe that the DF-PIA gap increases
with the variability in the public-factor weight because the RMSE associated with Model-PIAb remains
constant, while the RMSE associated with Model-DFb increases.
7
Figure 1 Numerical Simulation Illustrations of Propositions 3 and 4.
Random Error Incorporating Private Factors. We now turn to examining the effect of random error on
the DF-PIA gap when incorporating private factors.We proceed as above, by considering two cases that are
identical except for the degree of variability in Wb
i for i 2 I.
PROPOSITION 4. The DF-PIA gap is smaller when fWb
i is used in (5), for i 2 I, instead of Wb
i .
In contrast to Proposition 3, Proposition 4 shows that adding variability to how people incorporate private
factors reduces the DF-PIA gap. Both Model-PIAb and Model-DFb lose accuracy as we add variability to
Wb
i ; i 2 I. However, the loss is more dramatic for Model-PIAb. PIA’s advantage of more directly eliciting
the private information decreases as the random error incorporating private information increases.
The right panel of Figure 1 is the corresponding figure for Proposition 4. Observe that the DF-PIA gap
decreases in the standard deviation of the private-factor random error term because while random error
incorporating the private factor increases the RMSE under both Model-DFb and Model-PIAb, the increase
is steeper in the latter.
Summary and Hypothesis. Proposition 3 shows that the DF-PIA gap increases in the random error incorporating
public factors. Proposition 4 shows that the DF-PIA gap decreases in the random error incorporating
private factors. Combined, they imply that the DF-PIA will be greater when adding random error
incorporating public information than when adding the same amount of random error incorporating private
information. Based on these results, we formulate our second hypothesis:
Hypothesis 2 The location of random error moderates the DF-PIA gap. Specifically,
(a) Inducing greater random error incorporating public information increases the DF-PIA gap.
(b) Inducing greater random error incorporating private information decreases the DF-PIA gap.
8
(c) Random error incorporating public information increases the DF-PIA gap more than random error
incorporating private information.
3. Experiment 1: Elicitation via DF versus PIA
Experiment 1 is a simple direct test of Hypothesis 1.
3.1. Experimental Design
3.1.1. Task. Participants first reviewed 30 historical surgeries, each with information about the number
of procedures, the anesthesia complexity score, and the resulting surgery duration. They then completed 50
rounds of surgery duration prediction. In each round, they were shown a new surgery’s number of procedures
and anesthesia complexity score. Then, they were asked a question about predicting its duration.
3.1.2. Conditions. Subjects were randomly assigned to one of two conditions: direct forecast (“DF”)
or private information adjustment (“PIA”). The only difference between these two conditions is that in
each of the 50 rounds, DF participants answered the question “What is your forecast for the duration of
this surgery? I think this surgery duration will be minutes.”, whereas PIA participants answered the
question “The hospital system only has the first piece of information about this surgery—the number of
procedures. You have additional information. To account for your additional information, how would you
advise the hospital system to adjust its forecast for the duration of this surgery? I would advise the hospital
system to increase/decrease (choose one) its forecasted surgery duration by minutes.” See Appendix,
Figure E.1 for screenshots.
3.1.3. Simulating Surgery Duration. We used the following equation to simulate surgery duration:
Ys =60+20XP
s +10XI
s +s. Here, Ys is the duration of surgery s; XP
s denotes the number of procedures,
an integer-valued public factor that has a uniform distribution between 1 and 10, inclusive; and XI
s denotes
the anesthesia complexity score, a private factor that has a uniform distribution between 􀀀5 and 5. Finally,
s follows a normal distribution with mean 0 and standard deviation 5. All participants observed the same 30
simulated historical surgeries. However, each participant observed a unique sequence of randomly generated
surgeries for their 50 prediction rounds.
3.1.4. User Interface and Instructions. We programmed the user interface using the online software
SoPHIE (Hendriks 2012). After receiving written instructions describing the task, participants were required
to pass a three-question comprehension test before starting the experiment. They could review the instructions
and retake the test until they answered all questions correctly. See Appendix, Figure E.2 for full
instructions.
3.1.5. Pre-registration. For all experiments, we set our target sample sizes, exclusion criteria, and
analysis plans a priori. We pre-registered to exclude participants who (1) did not complete the experiment
or (2) put the same answer more than 90% of the time. We also pre-registered our dependent variable and
analyses. We calibrate prediction algorithms using data from the participants’ training set (first 35 rounds)
and then use the algorithms to generate predictions on the test set (last 15 rounds). Our performance criterion
is the RMSE of the predictions generated on the test set. The full pre-registration document for Experiment
1 is available at https://aspredicted.org/blind.php?x=3e427n.
3.2. Results
3.2.1. Participant and Response Summary Statistics. Undergraduate and graduate students at a large
research university in the US were invited to participate through a behavioral laboratory subject pool recruitment
system. Each participant received a $10 electronic gift card for completing the online study.
A total of 120 students participated. Following our exclusion criteria, we removed 8 participants who did
not complete the experiment, leaving 112 for analysis (56 in each condition). Among the 112 participants,
75% were female, 88% were 18 to 24 years old, and 12% were 25 to 34 years old. The average of mean
response to the question was 152:6 minutes (SD 30:8) in the DF condition and 12:1 minutes (SD 23:7) in
the PIA condition.
3.2.2. Algorithm Calibration and Prediction Accuracy Calculation. For each condition, we used the
number of procedures, actual surgery duration, and participant response from all participants’ first 35 rounds
to calibrate prediction algorithms for surgery duration. The pre-registered linear regression model included
participant dummies, number of procedures interacted with participant dummies, and participant response
interacted with participant dummies. Table E.1 in the Appendix summarizes the prediction algorithms calibrated
for each condition. We used the calibrated prediction algorithms to generate the predictions, ^ Ys, for
each surgery s in the last 15 rounds for each participant.We then computed RMSE =
q
1
15
P15
s=1(Ys 􀀀 ^ Ys)2
for each participant.
3.2.3. Testing Hypothesis 1. The average RMSE (averaged across all participants) was 22:4 (SD 6:3)
in the DF condition and 17:8 (SD 7:6) in the PIA condition; see Figure 2(a). This difference of 4:6 is
significant (p=0:0008) and represents a 21% decrease. This result supports Hypothesis 1.
3.2.4. Other Benchmarks. Figure 2(a) also shows the performance of three other benchmarks:
“DF-As is” corresponds to using participant DF condition responses without any algorithms. Doing so
results in an average RMSE of 46:8 (SD 24:5), significantly worse than when we use participant responses
as inputs to algorithms.
Figure 2 Experiment 1: Performance Comparison.
0 10 20 30 40 50 60
Root Mean Square Error (RMSE)
 
DF−As is XP only DF PIA XP and XI
(a) RMSE comparison. Means and standard errors are shown.
XP is the public factor, and XI is the private factor.
0 10 20 30 40
Root Mean Square Error (RMSE)
 
−.25 0 .25 .5 .75 1
 
Correlation(XI, R)
DF condition
PIA condition
(b) Correlation(XI , R) versus RMSE. Each dot is one participant.
R is defined as the residual of response after regressing it
on XP . Red x marks show the performance of the “XP and XI”
model.
“XP only” corresponds to using only the public information in the algorithm, without the use of any
participant responses. Across the 112 participants, such an algorithm leads to an average RMSE of 29:3
(SD 3:5)—an improvement over “DF-As is” even though participants had access to the private information
in addition to the public information. However, it is worse than the average RMSE of both the DF condition
(p < 0:0001) and the PIA condition (p < 0:0001). In other words, participant responses added predictive
value in the experiment.
Lastly, “XP and XI” corresponds to allowing prediction algorithms to directly observe the private information
XI and include it in prediction algorithms. In this experiment, it is equivalent to “consistent forecasters”
and is a benchmark for the best performance possible. This algorithm results in an average RMSE
across the 112 participants of 4:9 (SD 1:1).
3.2.5. Mechanistic Evidence. The theorized mechanism driving Hypothesis 1 is that PIA responses
help the algorithm account for the private information better than the responses from the DF condition. To
examine this mechanism, we calculate the correlation of the PIA responses with XI and compare them with
the correlation of the DF responses with XI .
Naturally, because the PIA asks directly about the private information, the correlation between XI and
response was lower in the DF condition than in the PIA condition (0:39 versus 0:74). Next, we consider the
correlation between XI and R, where we define R as the residual of participant response after regressing
it on XP . That is, we take out the effect of public factor from each response to construct R. Note that if
participants did not suffer from random error, then R would be perfectly correlated with XI in both DF
and PIA conditions. In contrast, we find that it is less than 1 in both conditions. However, it is significantly
higher in the PIA condition than in the DF condition (0:76 versus 0:62, p = 0:0008). In other words, PIA
responses provide better information about XI than DF responses.
Figure 2(b) illustrates the predictive accuracy versus the correlation value above for each participant.
As expected, higher correlation between XI and R leads to better prediction performance. There are more
participants with high correlation in the PIA condition than in the DF condition, which contributes to the
better performance of the PIA condition overall. The red “x” marks indicate the hypothetical perfectlyconsistent
benchmark, with no random error for each participant, which yields perfect correlation for both
DF and PIA conditions. The deviation of the PIA and DF dots from the red marks illustrates the effect of
human random error in participant responses.
3.3. MTurk Replication
We replicated the same experiment with MTurk workers. See https://aspredicted.org/blind.
php?x=yv2vs7 for the pre-registration document. While overall, the predictions from the experiment
with MTurk workers were less accurate, the between-condition results replicated, providing evidence of
robustness across different populations. Appendix C provides details on the replication as well as a comparison
between the performances of university students and MTurk workers.
3.4. Discussion
Consistent with Hypothesis 1, Experiment 1 provides evidence that eliciting the PIA information instead of
DF leads to better prediction algorithm performance. After the effect of public factor is taken out, participant
responses are more correlated with the private factor. This tighter relationship helps prediction algorithms
to incorporate private information, leading to better predictive performance. These results were replicated
across university students and MTurk workers.
4. Experiment 2: Manipulating Information Complexity of Public versus Private
Factors
Experiment 2 was designed to test Hypothesis 2, namely how the DF-PIA gap established in Experiment
1 is moderated by random error in incorporating public versus private factors. In addition, it provides a
replication test of Hypothesis 1 using different surgery duration equations.
4.1. Experimental Design
The task was similar to that in Experiment 1. However, we changed the surgery duration equation, and
we varied the number of factors by condition. We conjectured that greater information complexity induces
greater random error. Therefore, we created higher complexity to induce more human random error by
requiring that subjects aggregate multiple factors. Otherwise, to create lower complexity to induce less
random error, we automatically pre-aggregated multiple factors into a single representative factor for the
participant.
Specifically, in the Baseline case, we pre-aggregated information so that there was only one public and
one private factor, as in Experiment 1. However, we required that participants account for two public factors
in the Public Info Complex case or two private factors in the Private Info Complex case. Thus, the experiment
had a 2 (DF, PIA) by 3 (Baseline, Public Info Complex, Private Info Complex) between-subject experimental
design.
The equations below show the underlying model we used for all conditions and the pre-aggregations we
constructed to manipulate complexity by condition:
Ys = 150+10XP1
s +10XP2
s +10XI1
s +10XI2
s +s (Underlying Model)
= 150+10XP1
s +10XP2
s +(50+20XI
s )+s (Public Info Complex)
= 150 +(50+20XP
s )+10XI1
s +10XI2
s +s (Private Info Complex)
= 150 +(50+20XP
s ) +(50+20XI
s ) +s (Baseline).
Here, XP1
s and XP2
s represent the two public factors. In the experimental task, they are the “procedure
set-up requirements” and the “procedure complexity score,” respectively. Symmetrically, XI1
s and XI2
s represent
the two private factors. In the experimental task, they are the “anesthesia set-up requirements” and
the “anesthesia complexity score,” respectively. The random generation process for public and private factors
was symmetric. For every surgery s, XP1
s and XI1
s were uniform random integers between 0 and 10,
inclusive. XP2
s and XI2
s were uniform random numbers between 􀀀5 and 5 (rounded to the nearest tenth).
We set XP
s = (XP1
s 􀀀5)=2+XP2
s =2 and XI
s = (XI1
s 􀀀5)=2+XI2
s =2, which establishes the above equalities.
In the experimental task, they are a generic “procedure score” and “anesthesia score,” respectively.
See Appendix, Figure E.3 for screenshots. The pre-registration document for Experiment 2 is available at
https://aspredicted.org/blind.php?x=9uq8dw.
4.2. Results
4.2.1. Participants, Participant Responses, and Prediction Algorithm. MTurk workers who were
located in the US, had a Human Intelligence Task (HIT) approval rate of 99% or higher, and had 100 or more
HITs approved were qualified to participate in the experiment. Participants who completed the experiment
were paid $2 for participation. A total of 480 MTurk workers participated. Following the pre-registered
exclusion criteria, we removed 174 individuals who did not complete the experiment and 54 participants
who failed to correctly answer a four-question comprehension test on their first attempt. Among the 252
remaining participants, 42% were female, and 8% were 18 to 24 years old; 36%, 25 to 34; 31%, 35 to 44;
13%, 45 to 54; and 11%, 55 or over. Columns (1) and (2) of Table 1 provide the number of participants
and the average response in each condition. We developed prediction algorithms in the same way as in
Experiment 1 (see x3.2.2). Table E.2 in the Appendix summarizes the prediction algorithms.
Table 1 Experiment 2: Summary of Experiment Results.
Information Question (1) (2) (3) (4) (5) (6)
Type Type N Response Corr(XI , response) Corr(XI , R) RMSE of test set DF-PIA gap
Baseline DF 47 236.6 (30.7) 0.52 (0.21) 0.63 (0.25) 35.2 (13.9) 12.5***
PIA 42 4.0 (19.1) 0.76 (0.23) 0.79 (0.22) 22.8 (11.4)
Public Info DF 41 241.9 (24.5) 0.28 (0.18) 0.42 (0.26) 41.2 (11.2) 19.9***
Complex PIA 38 13.8 (33.9) 0.80 (0.27) 0.80 (0.27) 21.3 (14.0)
Private Info DF 47 237.7 (31.6) 0.66 (0.15) 0.70 (0.15) 30.6 (8.8) 1.7
Complex PIA 37 48.8 (46.1) 0.72 (0.17) 0.73 (0.17) 28.9 (7.0)
Note. Means and standard deviations (in parentheses) are shown. XP is the public factor, and XI is the private factor. In column
(4), R is defined as the residual of response after regressing it onXP . In column (6), DF-PIA gap is defined as the difference between
the mean RMSEs of DF and PIA conditions. Column (6) also shows DF-PIA gap’s statistical significance from a two-sample t-test
for difference of means. * p <0:05, ** p <0:01, *** p <0:001.
4.2.2. Robustness of Hypothesis 1. Columns (5) and (6) of Table 1 summarize the prediction performance
in each of the six conditions. Consistent with Hypothesis 1, the average RMSE of all PIA participants
was 32% lower than the average RMSE of all DF participants (35:4 versus 24:2, p < 0:0001). As shown
in column (6) of Table 1, the DF-PIA gap was statistically significant at the 5% level in two of the three
information conditions. In x3.2.5, we found that better performance is associated with higher correlation
between participant response and XI after the effect of XP in the responses is taken out. Columns (3) and
(4) of Table 1 provide the average correlation between XI and response and the average correlation between
XI and R, defined as the residual of response after regressing it on XP . As expected, the correlations are
higher in the PIA conditions than in the DF conditions, which again provides mechanistic evidence for
Hypothesis 1.
4.2.3. Testing Hypothesis 2. Hypothesis 2(a) predicts the DF-PIA gap to be greater under publicinformation-
complex conditions than under baseline conditions. Consistent with this hypothesis, the gap
was 19:9 under the public-information-complex conditions and 12:5 under the baseline conditions. This
difference of 7:4 was statistically significant (p=0:036; see Table 2).
Hypothesis 2(b) predicts the DF-PIA gap to be smaller under private-information-complex conditions
than under baseline conditions. Consistent with this hypothesis, the gap was 1:7 under the privateinformation-
complex conditions and 12:5 under the baseline conditions. This difference of 10:8 was statistically
significant (p=0:002; see Table 2).
Hypothesis 2(c) predicts the DF-PIA gap to be greater under public-information-complex conditions than
under private-information-complex conditions. Consistent with this hypothesis, the gap was 19:9 under
public-information-complex conditions and 1:7 under private-information-complex conditions. This difference
of 18:2 was statistically significant (p < 0:001; see Table 2). These findings provide evidence that the
benefit of PIA over DF is greater when public information is complex than when private information is
complex.
One unpredicted pattern is that the RMSE in DF is smaller under the private-information-complex condition
than under the baseline condition (30:6 versus 35:2, p <= 0:056). A plausible explanation is that, in
addition to inducing more random error, splitting the private information into two factors causes participants
to weight the private information more in general (see Fox and Clemen (2005)).
Table 2 Experiment 2: Performance Comparison.
(1)
Root Mean Squared Error
Information type (Base is Baseline conditions)
Public Info Complex conditions 5.96 (2.43)
Private Info Complex conditions -4.64 (2.35)
Question type (Base is DF conditions)
PIA conditions -12.49 (2.42)
Interaction effects (Base is Baseline conditions  PIA conditions)
Public Info Complex  PIA -7.42 (3.52)
Private Info Complex  PIA 10.77 (3.48)
Constant 35.25 (1.66)
N 252
R2 0.27
Note. Column (1) is a linear regression model with RMSE as the dependent variable. + p < 0:1, * p < 0:05,
** p <0:01, *** p <0:001.
4.3. Discussion
In addition to replicating Hypothesis 1 under different simulation parameters and information variables,
Experiment 2 provided evidence that the location of random error matters in a manner consistent with
Hypothesis 2. Specifically, eliciting human judgment via PIA instead of DF is helpful because of random
error incorporating public information. Increasing random error incorporating private information reduces
this benefit.
5. General Discussion
5.1. Summary
Our theoretical and experimental results suggest that in some situations, there is an opportunity to substantially
improve the way prediction algorithms incorporate human judgment by applying a new PIA elicitation
method, instead of the traditional method of DF. Under DF, humans contribute random error as they account
for public information that the algorithm can already access. This random error hinders the algorithm’s
ability to infer the humans’ private information. PIA avoids this hindrance by asking more directly about
how much to adjust for the human’s private information.
5.2. What Is Private Information and Why Does It Exist?
Pragmatically speaking, private information is any predictive information the human observes that the algorithm
does not use. Because private information is context specific, rather than attempting to discuss exactly
what it is, we find it constructive to discuss reasons why humans may have information that the algorithm
does not use (i.e., why private information exists).We discuss these reasons in the context of our motivating
example of predicting surgery durations: see Ibrahim and Kim (2019). At this hospital, the public information
is the patient’s electronic medical record as well as answers to specific questions from a standardized
booking slip for surgery. All other predictive information the surgeon has is private.
5.2.1. Identification Challenges. Some theoretically easy-to-input information may be private simply
because the system fails to request it. In our experiments, private information is easy to identify because
the researcher knows all the data that exist in the environment and what data the human can access that the
algorithm cannot. In practice, such an exercise is more difficult because if information is kept private from
the algorithm, it may also be kept from the system designer. In other words, you cannot ask for what you
do not know exists. For example, a surgeon may need special anesthesia equipment that requires additional
set-up time, but there may be no place to indicate this information on the booking slip because the system
designer was unaware of this special equipment.
5.2.2. Privacy Concerns and Integration Barriers. Even if the system designer knows that humans
have certain private information that should be used in the algorithm, humans may be unwilling to input
this information. For example, the surgeon may have a mental estimate of the likelihood of making a severe
mistake during the procedure, but he/she may be unwilling to record that information for liability reasons.
Similarly, certain information may be stored somewhere that is difficult to integrate. For example, a surgeon
may know which technician is scheduled to support the surgery, but that information is stored in another,
unintegrated database.
5.2.3. Codification Challenges. Despite advancements in “big data,” it often remains impractical to
input and store certain types or large quantities of predictive information into an organization’s system. For
example, one study reports that experienced doctors use nearly two million pieces of information to treat
their patients (Pauker et al. 1976). While some of the two million pieces of information may be explicit
knowledge—knowledge that can be easily articulated, codified, stored, and accessed—they are likely to be
tacit knowledge, or knowledge that is difficult transfer, such as intuitive judgment; see Cowan et al. (2000)
for a detailed discussion. As a result, inputting this information into the system will be costly and time
consuming (Pollack et al. 2014), if not impossible.
5.2.4. Insufficient Training Data. Even if information has been inputted into the system, it may remain
unused by the prediction algorithm due to a lack of sufficient historical data for training. Macario (2006)
reports that 50% of surgeries have less than five previous cases with the same procedure and same surgeon
during the preceding year. Zhou and Dexter (1998) report that only 32% of their surgeries had two or more
previous cases with the same procedure and same surgeon. Ibrahim and Kim (2019) remove about 60% of
the surgeries from their data collected over three years to keep only the surgeries that had 30 or more cases
with the same procedure and same surgeon. The fact that each specialty, or even each procedure, has its own
meaningful variables that are specific to the specialty or the procedure exacerbates the problem (Hosseini
et al. 2015). Thus, the algorithm designer may intentionally choose to leave certain information as “private”
because of lack of training data to make it useful.
5.3. What Should System Designers Do?
5.3.1. Consider Eliciting Human Input Even If Human Forecasts Are Inaccurate. Our study was
motivated by a hospital administrator who, concerned with poor human forecasting performance, was considering
using algorithms and fully eliminating surgeons from the surgery duration forecast process. Our
results highlight the fact that even when human forecasts are significantly worse than algorithms head-tohead,
system designers can potentially significantly boost the algorithms’ performance by seeking human
input. Thus, when implementing prediction algorithms, system designers should check to see whether
human judgment can boost algorithm performance before fully replacing humans.
5.3.2. Try Adding a PIA-Type Question. When using human judgment as an algorithm input, we suggest
adding a PIA-type question, especially when the public information is complex or the system designer
knows that there exists simple private information. Note also that DF and PIA are not mutually exclusive.
Thus, if the system already elicits the DF, one may choose to add the PIA and use both as inputs to the
algorithm. Prompting people to think differently via DF and PIA may, in fact, help people better communicate
private information to the algorithm (e.g., Herzog and Hertwig 2009), although doing so requires more
effort.
5.3.3. Identify and Convert Private Information into Public Information. The results of this paper
suggest that PIA mitigates the undesirable impact of human random error relative to DF, but not completely.
Directly converting private information to public information will be superior to eliciting the PIA (e.g., see
Figure 2) once enough training data have been collected. Thus, in addition to eliciting the PIA, we suggest
attempting to learn what the underlying private information is behind humans’ answers and altering the
system to collect or directly elicit it moving forward. While it is unlikely that one will be able to fully
eliminate private information in this manner, in some contexts, one may be able to eliminate enough private
information to render the improvement due to PIA or DF negligible.
5.3.4. Experiment with and Revise the PIA Elicitation Format. Because the type of private information
is context specific, the best way to write the PIA question is also likely to be context specific. Therefore,
we suggest experimenting with and periodically revising how to write the PIA question. In Appendix D,
we have made some limited progress on this issue via an experiment. We found that changing the format
of the PIA question to make it easier for the human to translate from the domain of the private information
to the domain of the question can enhance PIA’s performance. For example, on the one hand, if the private
information is a relative assessment of complexity, then structure the PIA question as an assessment relative
to an average patient with the same public information. On the other hand, if the private information is a
delay in minutes, then format the PIA question to be in minutes.
5.4. How Can Future Research Help Improve (or Disprove) This PIA Idea?
One limitation of our laboratory study approach is that it does not directly address the question of whether
PIA will outperform DF in practice. Certainly, field experiments or even more realistic laboratory experiments
can help address this question. Nevertheless, our initial studies suggest that more investigation into
how best to write the PIA question may be beneficial before one can confidently implement it and assess its
performance relative to DF. How should one decide whether to make the PIA question in a relative domain
(e.g., relative to an average case) or an absolute domain (e.g., in dollars)? What is the best way to describe
public information in specific practical contexts? Should one decompose the PIA into multiple parts based
on known categories of private information? Does showing the algorithm’s forecast before humans provide
their PIA help or hurt? Are there algorithm aversion or incentive issues that need to be addressed before
implementation?
We also recognize that there are several other open theoretical questions. We have assumed linear relationships
and linear models in this paper. Intuitively, we believe the main directional predictions apply to
other machine-learning algorithms and non-linear relationships. However, future work may verify whether
our results do indeed generalize, which may lead to further insight. Furthermore, we have assumed a simple
model of random error that does not capture detail in how humans turn cues into predictions, where exactly
the random error occurs, or how the format of the PIA question might matter. Another potentially fruitful
direction is to incorporate further psychologically descriptive detail into a behavioral model of prediction.
In conclusion, field work, laboratory experiments, and behavioral models are all important for enhancing
the understanding and use of PIA questions. It is our hope that by defining the PIA idea and documenting
its potential improvement experimentally, we stimulate research that drives improvement in practice.






----------------------------------------------------------------------------------







Yalcin, G., Lim, S., Puntoni, S., & van Osselaer, S. M. J. (2022). Thumbs Up or Down:
Consumer Reactions to Decisions by Algorithms Versus Humans. Journal of Marketing
Research, 59(4), 696-717. https://doi.org/10.1177/00222437211070016

Abstract
Although companies increasingly are adopting algorithms for consumer-facing tasks (e.g., application evaluations), little research
has compared consumers’ reactions to favorable decisions (e.g., acceptances) versus unfavorable decisions (e.g., rejections) about
themselves that are made by an algorithm versus a human. Ten studies reveal that, in contrast to managers’ predictions, consumers
react less positively when a favorable decision is made by an algorithmic (vs. a human) decision maker, whereas this difference
is mitigated for an unfavorable decision. The effect is driven by distinct attribution processes: it is easier for consumers to internalize
a favorable decision outcome that is rendered by a human than by an algorithm, but it is easy to externalize an unfavorable
decision outcome regardless of the decision maker type. The authors conclude by advising managers on how to limit the likelihood
of less positive reactions toward algorithmic (vs. human) acceptances.

Keywords
algorithms, decision making, decision outcome favorability, attribution theory

A growing number of companies are using algorithms to make
business decisions that directly affect potential and existing customers.
For example, algorithms are now used to decide which
applicants should be admitted to platforms (e.g., Raya) and who
should receive loans (e.g., Upstart; for more examples, see Web
Appendix A). As the prevalence of algorithms in consumerfacing
decisions increases, so does the managerial importance
of understanding consumers’ reactions to algorithmic versus
human decisions. We investigate consumers’ reactions toward
a company following a decision (favorable or unfavorable)
made by an algorithmic versus a human decision maker.
Specifically, we focus on contexts where the decision outcome
is considered diagnostic of the consumer’s qualifications, deservingness,
or merit, such as when consumers submit an application
to access a valued service or other benefits.
We demonstrate that consumers react less positively when a
favorable decision (e.g., the acceptance of an application) is
made by an algorithm rather than by a human. This difference,
however, is attenuated for an unfavorable decision (e.g., the
rejection of an application). We explain this interaction
between the decision maker type and decision outcome favorability
by drawing on attribution theory (Jones and Davis
1965; Kelley 1967). Consumers are motivated to internalize
favorable decisions, but internal attribution is more difficult
when the decisions are made by an algorithm (vs. a human),
so consumers react less positively (e.g., form less positive attitudes
toward the company). By contrast, consumers are motivated
to externalize unfavorable decisions, and this is
similarly easy with algorithmic and human decision makers,
so consumers’ subsequent reaction is relatively indifferent to
the decision maker type.
The current research makes three primary contributions (for
a comprehensive literature review, see Table 1). First, our
research addresses an underexplored question: How do consumers’
attitudes (and related constructs) change as a function
of a company’s use of algorithmic versus human decision
makers in consumer-facing tasks? Previous work has focused
on consumers’ choices, such as for advice, between an algorithmic
and a human decision maker (Dietvorst, Simmons, and
Massey 2015; Longoni, Bonezzi, and Morewedge 2019).

However, companies usually decide whether to rely on algorithms
or humans for a given task; consumers are more often
in the position of decision recipients. Unlike prior research,
the current research focuses on consumers’ reactions to algorithmic
versus human decisions about themselves. This distinction
is important because the two situations may elicit different
psychological processes. Decision recipients face the task of
interpreting a decision outcome reflective of one’s worth in
the eye of others. In such a context, one’s reaction to the decision
outcome often involves self-serving interpretations and
motivated reasoning (Taylor and Brown 1988), a topic that
has not been examined in prior research on algorithmic decisions.
More generally, as consumers’ choices often diverge
from their reactions to the given options (Botti and Iyengar
2006), we argue that it is unclear whether findings about consumers’
choice behavior (e.g., reluctance to rely on algorithmic
advice) are generalizable to the reactions of consumers as decision
recipients (e.g., negative reactions to algorithmic decisions
made about the consumers themselves).
Second, we examine an important factor that influences consumers’
reactions to different decision makers: the favorability of
decision outcomes, which is known to affect people’s attitudes
and behaviors (e.g., Barry, Chaplin, and Grafeman 2006;
Rhodewalt and Davison 1986). Both types of decision outcomes
are common; companies may deliver approvals or acceptances as
well as denials or rejections to existing or potential customers—
and yet, the consequences of decision outcome favorability are
underexplored in the research on algorithmic (vs. human) decision
making.We find that most managers believe that consumers
react more positively to decisions made by humans (vs. algorithms)
regardless of the decision outcome (see the managerial
intuitions study and Web Appendix B). We demonstrate,
however, that favorable decision outcomes elicit divergent reactions
to algorithmic versus human decisionmakers, whereas such
difference is attenuated for unfavorable decision outcomes.
Third, in examining the process underlying the proposed
effect, we elucidate how consumers interpret decisions made
by algorithms versus by humans. Unlike prior work that
focuses on consumers’ diverging perceptions of humans and
algorithms (e.g., moral authenticity [Jago 2019], trustworthiness
[Lee 2018]), the current work examines consumers’ differential
attribution of a given decision outcome. Specifically, we
demonstrate that for a favorable decision, a human (vs. an algorithmic)
decision maker facilitates stronger internal attribution
of the decision outcome, whereas for an unfavorable decision,
consumers readily engage in external attribution regardless of
the type of decision maker. The current research thus marries
the psychological literature on attribution (McFarland and
Ross 1982; Okten and Moskowitz 2018) with the marketing literature
on algorithms (Castelo, Bos, and Lehmann 2019;
Puntoni et al. 2021), offering a novel contribution to both.
In the following sections, we review the extant work on algorithmic
and human decision making. We draw on attribution
theory to make theoretical predictions about how consumers
respond to favorable and unfavorable decisions made by algorithms
versus humans.
Theoretical Background
An algorithm is “a set of steps that a computer can follow to
perform a task” (Castelo, Bos, and Lehmann 2019, p. 809). A
growing number of companies rely on algorithms; the market
for artificial intelligence is expected to be worth over $300
billion by 2026 (Markets and Markets 2021). The widespread
adoption of algorithms has encouraged researchers to investigate
how consumers perceive algorithms versus humans. Existing
work has demonstrated that consumers perceive algorithmic
and human decision makers to have different strengths and weaknesses.
For instance, compared with humans, algorithms are perceived
as more objective (Lee 2018; Sundar and Nass 2001) but
also as less authentic, less intuitive, and less moral (Bigman and
Gray 2018; Jago 2019; Yeomans et al. 2019).
Extant work has examined consumers’ choices between
algorithmic and human decision makers and has documented
an aversion to algorithms (for an exception, see Logg,
Minson, and Moore [2019]). For example, consumers are often
reluctant to use algorithms to predict stock prices (Onkal et al.
2009), solicit medical advice (Cadario, Longoni, and
Morewedge 2021; Longoni, Bonezzi, and Morewedge 2019;
Promberger and Baron 2006), and predict people’s performance
(Dietvorst, Simmons, and Massey 2015). In addition, algorithm
aversion varies with contextual factors such as the nature of the
task (subjective vs. objective; Castelo, Bos, and Lehmann 2019)
and the product (hedonic vs. utilitarian; Longoni and Cian 2022).
The current research is the first to examine consumers’ attitudes
toward a company in the context in which (1) a decision
maker (algorithm vs. human) is already chosen, (2) the decision
is made by the company about the consumers themselves (i.e.,
the decision is self-diagnostic), and (3) a decision outcome is
known (see Table 1). Our research context is of managerial
importance. Companies often deliver both types of decision
outcomes—favorable (e.g., approval, acceptance) and unfavorable
(e.g., denial, rejection)—to existing or potential customers.
Our in-depth interviews with practitioners confirm the prevalence
of algorithms in many consumer-facing tasks such as consumer
application evaluations (see Web Appendix C,
interviews #1, #5 and #11) and insurance premium decisions
(#1). Such decisions are often based on personal information
provided by the consumers, and decision outcomes are thus
reflective of consumers’ qualifications.
We posit that the self-diagnostic nature of many consumerfacing
decisions motivates consumers to make different attributions
for favorable and unfavorable outcomes. The type of
decision maker (algorithm vs. human) affects consumers’ internal
and external attributions, leading to an interaction effect
between the decision maker type and decision outcome favorability
on consumers’ attitudes toward the company.
Attribution of Favorable and Unfavorable Decisions as a
Function of the Decision Maker Type
Consumers often make inferences about the causes of events,
actions, and behaviors (Heider 1958; Jones and Davis 1965)

and attribute behaviors or outcomes to either internal or external
causes (Kelley 1967). Attribution theory proposes that people
are motivated to attribute self-relevant outcomes in a selfserving
way: to maintain or enhance their self-worth, people
are motivated to attribute favorable outcomes to themselves
(i.e., “internal attribution”; Baumeister 1999; Zuckerman
1979) and to attribute unfavorable outcomes to external
factors (i.e., “external attribution”; Kelley and Michela 1980;
Miller and Ross 1975). In marketing research, attribution
theory has been used to explain consumers’ perceptions of a
company’s performance (Dunn and Dahl 2012; Folkes 1984;
Wan and Wyer 2019), other consumers’ behavior (He and
Bond 2015; O’Laughlin and Malle 2002), and one’s own behavior
(Leung, Paolacci, and Puntoni 2018; Yoon and Simonson
2008). We contribute to this literature by demonstrating that
the decision maker type (algorithm vs. human) affects how consumers
attribute favorable versus unfavorable decision outcomes.
Consumers who receive a favorable decision are motivated to
make an internal attribution (Luginbuhl, Crowe, and Kahan
1975), and we argue that they find it easier to do so when the
decision is made by a human (vs. an algorithm). Consumers
often define themselves on the basis of personal characteristics
(e.g., abilities, attitudes) that make them feel unique (Brewer
1991; Fromkin and Snyder 1980). Human (vs. algorithmic) decision
makers are perceived as more adept at considering individuals’
unique characteristics and qualifications (Longoni,
Bonezzi, and Morewedge 2019). In contrast, algorithms
usually rely on a set of precoded categories of characteristics
and qualifications that are shared by many (note that an algorithm
probably would not recognize characteristics that are unique to a
single person) and reduce individuals into a number (Newman,
Fast, and Harmon 2020). Thus, we predict that consumers
view a favorable decision made by a human (vs. an algorithm)
as more reflective of their individuality (i.e., unique self) and
deservingness (e.g., “My application was accepted because of
who I am”), so they would more easily make strong internal attributions
for a favorable decision made by a human (vs. an algorithm).
It is easier to attribute a good outcome to “me” when
the decision maker relied on characteristics and achievements
that are “uniquely me.” Put differently, it is more difficult to attribute
a positive outcome to something about oneself if those qualities
or that something is shared with many others.
In contrast, consumers who receive an unfavorable decision
are motivated to make an external attribution, and we argue that
they would find no difference in difficulty to do so regardless of
whether the decision is made by a human or an algorithm. The
decision maker is easily blamed for making a bad decision
regardless of whether that decision maker is a human or an algorithm,
but for different reasons. For instance, an algorithm can
be easily blamed for ignoring consumers’ uniqueness (Longoni,
Bonezzi, and Morewedge 2019), while a human can easily be
blamed for not being objective (Lee 2018).
If the type of decision maker affects consumers’ ability to
make attributional inferences for different decision outcomes,
this should be expected to have repercussions for consumer attitudes.
Causal reasoning—reasoning about what or who is
responsible for a given outcome—is a key factor in attitude formation
and change (e.g., Forsyth 1980; Kelley 1973) and the
marketing literature contains many demonstrations that attributions
are an important determinant of attitudes toward companies
(e.g., Dunn and Dahl 2012). In the context of automation,
Leung, Paolacci, and Puntoni (2018) show that the extent to
which the consumption context enables people to make internal
or external attributions explains their product preferences. For
example, in their Study 6, the authors demonstrate that framing
an automated product in a way that makes it easier for people
to internally attribute favorable consumption outcomes leads to
more positive attitudes toward the product.
Summary of Key Predictions and Overview of Studies
We present ten studies that examine our theory (for a summary,
see Table 2). Whereas most managers predict (in interviews and
surveys) that consumers react more positively to human (vs. algorithmic)
decision makers regardless of decision outcome favorability,
we demonstrate a robust interaction effect on consumers’
attitudes toward the company (Studies 1a–8) and their
word-of-mouth (WoM) intentions (specifically, the net promoter
score measure, Study 1b). Furthermore, we examine the underlying
attribution processes through both mediation (Studies 4 and 6) and
moderation (Study 5). We also rule out alternative explanations
including attention (Study 2), social presence (Study 7), and perceived
fairness (a follow-up study in the “General Discussion”
section). Finally, we offer managerial insights into strategies for
improving reactions to favorable decisions made by algorithms
(Study 8). We report all conditions and all measures. Some
studies included an exploratory measure, for which we report analyses
in the Web Appendix. For some of our studies, we screened
participants beforethe study by using an attention check, such as
an instructional manipulation check (Oppenheimer, Meyvis, and
Davidenko 2009), and those who failed the attention check were
not allowed to proceed to the actual study. Our reports of these
studies include only those who participated in the actual study.
Sample sizes were determined prior to data collection. All data
and study materials are available on OSF (osf.io/3bnsz).
Managerial Intuitions
To evaluate the managerial importance of our findings, we
examined how practitioners would predict customers’ reactions
to favorable versus unfavorable decisions made by humans
versus algorithms. We started with a series of in-depth interviews
with 14 managers, and none correctly predicted our
hypothesized interaction effect. Motivated by this preliminary
result, we conducted a survey with a larger group of experienced
professionals. We report the results of the in-depth interviews
in Web Appendix C and the results of the survey next.
Method
We recruited 88 managers (Mage =35.05 years; 24 women;
Mwork experience =11 years) from an executive master of business administration program at a major European business
school.
We described a business situation involving consumer applications
(see Web Appendix B) and asked the managers to
predict how the type of decision maker would influence customer
satisfaction in response to an acceptance and in response
to a rejection (getting [accepted/rejected] by an algorithm
would be better than getting [accepted/rejected] by an employee
vs. getting [accepted/rejected] by an algorithm would be
equally good as getting [accepted/rejected] by an employee
vs. getting [accepted/rejected] by an employee would be
better than getting [accepted/rejected] by an algorithm).
Results and Discussion
Managers expected that an algorithmic (vs. a human) decision
maker would lead to lower satisfaction regardless of decision
outcome favorability (B=−.09, z=−. 31, p=.758). Specifically,
61% of the managers predicted that participants would be less satisfied
with an acceptance from an algorithm (vs. a human; see
Figure 1). Similarly, 59% of themanagers predicted that consumers
would be less satisfied with a rejection from an algorithm (vs. a
human).
Only 5% (i.e., four managers) generated our predicted interaction
effect: consumers would react more favorably to an
acceptance made by a human (vs. an algorithm) and would be
similarly satisfied with a rejection made by a human and by
an algorithm. Interestingly, managers also predicted that the
decision maker type would matter less for acceptance decisions
than for rejection decisions (choice share of “algorithm =
human”: Mfavorable =23.9% vs. Munfavorable=10.2%; B=
−1.39, z=−2.28, p=.023), the opposite of our hypothesized
pattern.
How Are Consumers’ Attitudes Toward the
Company Affected by the Decision Maker
Type and Decision Outcome Favorability?
The first set of studies tested the managers’ prediction (i.e., consumers
respond more positively to a human [vs. an algorithmic]
decision maker regardless of the outcome) against our own (an
interaction effect). In Studies 1a–b, we examined our hypothesized
interaction effect on two dependent variables: consumers’
attitudes toward the company and WoM intentions. We predicted
that consumers would react less positively when a favorable
decision was made by an algorithm (vs. a human); the
differential reaction would be mitigated for an unfavorable
decision.
Study 1a: Effect of the Decision Maker Type as a
Function of Decision Outcome Favorability: Attitudes
Toward the Company
Method. In this preregistered study (aspredicted.org/j7da3.pdf),
we randomly assigned 993 Amazon Mechanical Turk (MTurk)
workers (Mage=40.06 years; 531 women)1 to one of four conditions
in a 2 (decision maker type: algorithm vs. human) × 2
(decision outcome favorability: favorable vs. unfavorable)
between-participants design.
Participants read that they were applying for membership at
Violethall Country Club (see Web Appendix D). Participants
learned that their applications were either accepted (favorable
decision condition) or rejected (unfavorable decision condition);
we told participants that the decision was made by
either a country club algorithm (algorithm condition) or a
country club coordinator (human condition). We also told all
participants that the decision was final and could not be
appealed. After learning the outcome, participants indicated
their attitudes toward the country club (“What is your general
opinion about Violethall Country Club?”) on three bipolar
items (1=“dislike a great deal”/“very negative”/“not favorable
at all,” and 11=“like a great deal”/“very positive”/“very favorable”;
α=.99; adapted from Park et al. 2010).
Results. A 2 (decision maker type) ×2 (decision outcome favorability)
analysis of variance (ANOVA) revealed a significant
main effect of the decision maker type (Malgorithm=5.16, SD=
3.10 vs. Mhuman=5.38, SD=3.29; F(1, 989)=4.98, p=.026,
η2p
=.01) and of decision outcome favorability (Mfavorable=7.49,
SD=2.61 vs. Munfavorable=3.07, SD=1.96; F(1, 989)=924.46,
p < .001, η2p
=.48). Consistent with our theory and inconsistent
with the managers’ predictions, we found a significant interaction
effect (F(1, 989)=8.46, p=.004, η2p
=.01; see Figure 2): attitudes
toward the country club were less positive among participants
whose applications were accepted by the algorithm than among
participants whose applications were accepted by the club coordinator
(Malgorithm=7.13, SD=2.59 vs. Mhuman=7.88, SD=2.58;
F(1, 989)=13.15, p < .001, η2p
=.01).Meanwhile, the effect of the
decision maker type was significantly mitigated when participants’
applications were rejected (Malgorithm=3.12, SD=2.11
vs. Mhuman=3.02, SD=1.82; F(1, 989)=.23, p=.632).
Study 1b: Effect of Decision Maker Type as a Function of
Decision Outcome Favorability: WoM Intentions
With Study 1b, we aimed to replicate Study 1a with two key
changes. First, we tested whether our effect generalizes to a
nonsocial context: business loan applications. To further
remove social cues, we used “approved” and “denied” instead
of “accepted” and “rejected.” Second, we measured participants’
WoM intentions, another managerially important dependent
variable.

Method. We randomly assigned 500 Prolific workers (Mage=
33.97 years; 264 women) to one of four conditions in a 2 (decision
maker type: algorithm vs. human) × 2 (decision outcome
favorability: favorable vs. unfavorable) between-participants
design.
Participants read that they were applying for a business loan
(see Web Appendix E). We told participants that their loan
applications were either approved or denied by either a loan
algorithm or a loan officer. Next, participants indicated their
attitudes toward the bank (α=.99), as in Study 1a. We also
measured participants’ WoM intentions using the item made
famous by the net promoter score (“On a scale from 0–10,
how likely are you to recommend this bank to a friend or colleague?”;
0=“extremely unlikely,” and 10=“extremely likely”).
Results. We first conducted a 2 (decision maker type) × 2 (decision
outcome favorability) ANOVA on attitudes toward the
bank. We found a significant main effect of the decision
maker type (Malgorithm =5.74, SD =3.20 vs. Mhuman =6.51,
SD =3.35; F(1, 496)=22.17, p < .001, η2p
=.04) and of decision
outcome favorability (Mfavorable =8.72, SD=2.18 vs.
Munfavorable =3.55, SD=1.89; F(1, 496)=853.09, p < .001, η2p
=.63). Crucially, we replicated the significant interaction
effect on consumers’ attitudes (F(1, 496)=8.21, p=.004, η2p
=.02): attitudes toward the bank were less positive among participants
whose applications were approved by the algorithm
than among participants whose applications were approved by
the loan officer (Malgorithm =8.06, SD =2.51 vs. Mhuman =
9.40, SD=1.51; F(1, 496) =28.56, p < .001, η2p
=.05).
Meanwhile, the effect of the decision maker type was significantly
attenuated when the applications were denied
(Malgorithm =3.39, SD =1.79 vs. Mhuman =3.71, SD =1.97;
F(1, 496)=1.71, p=.192).
Next, we conducted an analogous ANOVA on WoM intentions.
We found a significant main effect of the decision maker
type (Malgorithm=4.34, SD=3.02 vs. Mhuman=5.10, SD=3.29;
F(1, 496)=21.06, p < .001, η2p
=.04) and of decision outcome
favorability (Mfavorable=7.15, SD=2.12 vs. Munfavorable=2.31,
SD=2.01; F(1, 496)=722.11, p < .001, η2p
=.59). More importantly,
we found a significant interaction effect (F(1, 496)=
5.04, p=.025, η2p
=.01; see Figure 3): the bank was less likely
to be recommended to others by participants whose applications
were approved by the algorithm than participants whose applications
were approved by the loan officer (Malgorithm=6.54, SD=
2.27 vs. Mhuman=7.77, SD=1.75; F(1, 496)=23.25, p < .001,
η2p
=.04). Again, however, the effect of the decision maker type
onWoMintentions was significantly mitigated when participants’
applications were denied (Malgorithm=2.10, SD=1.81 vs.Mhuman
=2.52, SD=2.18; F(1, 496)=2.76, p=.097, η2p
=.01).
Discussion of Studies 1a–b. Studies 1a–b demonstrated that the
effect of the decision maker type (algorithm vs. human) on consumers’
reactions is a function of decision outcome favorability.
When participants received a favorable decision outcome,
the algorithm (vs. human) decision maker led to less positive
reactions toward the company. However, this effect was significantly
mitigated when participants received an unfavorable
decision outcome.
We note the robustness of our effect thus far: it held in both
social (club membership application) and nonsocial (bank loan
application) contexts and with two managerially relevant measures
of consumers’ reactions (attitudes toward the company
and WoM intentions). In addition, we demonstrated that our
effect is not driven by an assumption that an algorithmic (vs.
human) decision would not be the final decision. We consistently
observed the key interaction effect regardless of
whether we explicitly emphasized that the decision is final.
Note that our findings contradict the managers’ intuitions,
so they are managerially informative. Furthermore, it is
noteworthy that our interaction effect cannot be explained by
the algorithm aversion literature (e.g., Longoni, Bonezzi, and
Morewedge 2019), which documents consumers’ avoidance
of algorithms (vs. humans) without consideration of decision
outcome favorability. The interaction effect is therefore distinct
from prior findings on general algorithm aversion.
Study 2: Replication with a Real Application Process
The purpose of Study 2 was twofold. First, we aimed to provide
a field test of the predicted effect. Participants applied to join a
research participant pool run by a research company, Johnson
Customer Insight. We examined participants’ attitudes toward
the research company when their applications were accepted
or rejected by either a human or an algorithm. Second, we
aimed to rule out an alternative account: inattention to unfavorable
information. People tend to avoid unfavorable information
that can hurt their self-esteem (Trope and Neter 1994), so they
may pay less attention to information (including the decision
maker type) that is related to an unfavorable decision
outcome. Accordingly, inattention may explain the apparent
indifference to the decision maker type for unfavorable decision
outcomes. To address this possibility, we directed participants’
attention to the decision maker type in all conditions before
measuring attitudes toward the company.
Method. We randomly assigned 303 Prolific workers (Mage=
34.19 years; 184 women) to one of four conditions in a 2 (decision
maker type: algorithm vs. human) × 2 (decision outcome
favorability: favorable vs. unfavorable) between-participants
design.
We created a Prolific researcher account under the name
Johnson Customer Insight and told Prolific workers (who are
essentially gig economy workers whose gig is to be a paid
research participant) that the company was creating a research
participant pool. Furthermore, we told participants that
Johnson Customer Insight was dedicating that particular day
to determining the eligibility of applicants for future surveys
with generous compensation (see Web Appendix F).
Participants were invited to complete an application form,
which included questions about their cognitive abilities and
their Prolific history; participants were told that the information
reflected their diligence and attractiveness as a research participant.
After submitting the application, each participant
received an application number and was asked to wait while
their applications were evaluated; after a few minutes, they
received either an acceptance (favorable decision outcome) or
a rejection (unfavorable decision outcome). Participants then
rated their overall attitude toward the research company
(“What is your overall evaluation of Johnson Customer
Insight?”) on a scale from one to ten stars.
On the next page, we informed participants of the type of
decision maker: either one of the coordinators or a computer
program designed by the information technology team.
Participants completed another measure of attitude: “How do
you feel about Johnson Customer Insight now?” (1=“less positive,”
and 7=“more positive”). Finally, we thanked and
debriefed participants (including telling them that Johnson
Customer Insight was a fictitious company) and paid the promised
bonus to all participants.
Attitudes before receiving information about the decision maker
type. As we expected, a 2 (decision maker type) ×2 (decision
outcome favorability) ANOVA on the initial rating of the
research company indicated a significant main effect of decision
outcome favorability (Mfavorable=8.30, SD=1.67 vs.Munfavorable
=4.20, SD=2.78; F(1, 299)=243.30, p < .001, η2p
=.45).
Unsurprisingly, as this measure was taken before the manipulation
of the decision maker type, we found neither a main effect
of the decision maker type (F(1, 299)=1.29, p=.256) nor an
interaction effect between the decision maker type and decision
outcome favorability (F(1, 299)=.53, p=.466), indicating successful
random assignment.
Core Results. Central to our hypothesis, we tested how the decision
maker type affected participants’ attitudes as a function of
decision outcome favorability. An ANOVA revealed a significant
main effect of the decision maker type (Malgorithm =3.72,
SD = 1.59 vs. Mhuman = 4.26, SD =1.85; F(1, 299) =11.04, p
= .001, η2p
=.04) and of decision outcome favorability
(Mfavorable =5.03, SD =1.29 vs. Munfavorable =2.95, SD=
1.50; F(1, 299) =175.69, p < .001, η2p
=.37). Crucially, we replicated
the key interaction effect (F(1, 299)=6.11, p=.014, η2p
=.02; Figure 4): attitudes toward the research company were
less positive among participants whose applications were
accepted by the algorithm than among participants whose applications
were accepted by the coordinator (Malgorithm=4.57,
SD =1.23 vs. Mhuman =5.47, SD =1.19; F(1, 299)=16.84, p <
.001, η2p
=.05). Meanwhile, the effect of the decision maker
type on the attitudes was significantly mitigated when participants’
applications were rejected (Malgorithm =2.88, SD =1.45
vs. Mhuman =3.01, SD =1.54; F(1, 299)=.36, p=.548). The
key interaction effect remained significant after controlling for
the initial rating of the research company (F(1, 298) =5.90, p
=.016, η2p
=.02).
Discussion. Study 2 replicated our key findings in a realistic
setting where participants ostensibly were applying to a
research company. Furthermore, Study 2 ruled out the alternative
account based on inattention to unfavorable information by
separating the decision outcome from the decision maker,
thereby ensuring attention to the latter.
Studies 3a and 3b: Effect of (Not) Disclosing the
Decision Maker
Studies 3a and 3b focused on favorable decision outcomes (as we
did not observe a significant effect of the decision maker type for
unfavorable decision outcomes in our previous studies). We
aimed to clarify whether the effect of the decision maker type
on reactions is driven by a positive effect of the human decision
maker, a negative effect of the algorithmic decision maker, or
both. The distinction is important from the perspectives of managers
and business ethics because it has implications for the consequences
of disclosing (vs. not disclosing) the decision maker
type. Studies 3a–b included a third condition in which consumers
are not informed of the decision maker, creating a baseline for
assessing the effect of the decision maker type.
Methods. We randomly assigned 403 (Study 3a: Mage=32.75
years; 251 women) and 402 (Study 3b: Mage =34.98 years;
259 women) Prolific workers to one of three conditions (decision
maker type: algorithm vs. human vs. unspecified) in a
between-participants design.
In Study 3a, participants were applying for membership at
Violethall Country Club (see Web Appendix G); depending
on the condition, participants learned that their applications
were accepted by a club algorithm (algorithm condition),
accepted by a club coordinator (human condition), or simply
accepted (unspecified decision maker condition). Participants
completed the same attitude items (a =.98) as in Study 1a.
Study 3b was a conceptual replication of Study 3a with one difference:
participants read that they were applying for a bank
loan (see Web Appendix G). Similar to Study 3a, participants
learned that their applications were accepted by a loan algorithm,
accepted by a loan officer, or accepted by an unspecified
decision maker. Participants rated their attitudes toward the
bank (a =.98).
Study 3a Results. We observed a significant effect of the decision
maker type (F(2, 400)=6.78, p=.001, η2p
=.03).
Replicating our previous findings, attitudes toward the
country club were less positive among participants whose applications
were accepted by the algorithm than among participants
whose applications were accepted by the club coordinator
(Malgorithm =6.04, SD =2.70 vs. Mhuman =7.21, SD =2.83;
F(1, 400)=12.14, p < .001, η2p
=.03). Attitudes were significantly
less positive in the algorithm condition than in the
unspecified condition (Malgorithm = 6.04, SD = 2.70 vs.
Munspecified = 6.97, SD = 2.69; F(1, 400) = 7.79, p = .006, η2p
= .02), but attitudes were similar in the human and unspecified
conditions (Mhuman = 7.21, SD = 2.83 vs. Munspecified = 6.97,
SD = 2.69; F < 1, p = .482; Figure 5).
Study 3b Results. We observed a significant effect of the decision
maker type (F(2, 399)=13.79, p < .001, η2p
=.06).
Participants whose loan applications were accepted by the algorithm
indicated less positive attitudes toward the bank than both
participants whose loan applications were accepted by the loan
officer (Malgorithm =7.38, SD =2.39 vs. Mhuman =8.50, SD =
1.94; F(1, 399) =18.85, p < .001, η2p
=.05) and participants
whose loan applications were accepted by an unspecified decision
maker (Malgorithm =7.38, SD =2.39 vs. Munspecified =8.59,
SD =1.98; F(1, 399)=22.29, p < .001, η2p
=.05). Again, the
difference between the human and unspecified conditions was
not significant (Mhuman =8.50, SD =1.94 vs. Munspecified =
8.59, SD =1.98; F < 1, p=.711; Figure 5).
Discussion of Studies 3a and 3b. Studies 3a and 3b clarify that
the effect of the decision maker type in favorable decisions
occurs because the disclosure of an algorithmic decision
maker hurts consumers’ attitudes relative to a baseline of an
undisclosed decision maker. These findings have implications
for decision transparency, which we discuss in the “General
Discussion” section.
What Psychological Mechanisms
Differentiate Consumers’ Reactions to
Algorithmic and Human Decision Makers?
We proposed that consumers react less positivelywhen their applications
are accepted by algorithms (vs. humans) because they find
it relatively more difficult to internalize an acceptance made by an
algorithm (vs. by a human). By contrast, when a decision outcome
is unfavorable, consumers readily externalize the decision
outcome, so they react similarly toward the company regardless
of the decision maker. We directly examined this attribution
mechanism through mediation (Studies 4 and 6) and moderation
(Study 5).
Study 4: Mediation by Internal Attribution
Study 4 examined the mediating role of attribution. We predicted
that algorithmic (vs. human) decision makers would
elicit distinct attributions as a function of decision outcome
favorability, and the attributions would mediate our key interaction
effect on attitudes toward the company.
Method. We randomly assigned 600 Prolific workers to one
of four conditions in a 2 (decision maker type: algorithm
vs. human) × 2 (decision outcome favorability: favorable
vs. unfavorable) between-participants design. Our final data
set consisted of 571 participants (Mage=33.84 years; 249
women) who passed our attention check.2
As in Study 3a, participants read that they were applying for
membership at a country club (seeWeb AppendixH); participants
learned that their applications were either accepted or rejected by
either the country club algorithm or the country club coordinator,
and they indicated their attitudes toward the country club (α=.99)
as in Study 3a. Next, we measured internal attributions
(adapted from Russell [1982]): “To what extent do you feel this
decision [reflects something about yourself/can be attributed to
something about yourself/is due to your personal qualities or
behaviors]?” (1=“not at all,” and 11=“very much”; α=.91).
Results. We conducted a 2 (decision maker type) ×2 (decision
outcome favorability) ANOVA on attitudes toward the country
club. We found a significant main effect of the decision maker
type (Malgorithm=5.01, SD=2.95 vs. Mhuman=5.91, SD=3.11;
F(1, 567)=8.62, p=.003, η2p
=.01) and of decision outcome
favorability (Mfavorable=7.06, SD=2.88 vs. Munfavorable=3.85,
SD=2.30; F(1, 567)=211.62, p < .001, η2p
=.27). Again, we
found a marginally significant interaction between the decision
maker type and decision outcome favorability (F(1, 567)=
3.66, p=.056, η2p
=.01; see Figure 6): attitudes toward the
country club were less positive among participants whose applications
were accepted by the algorithm than among participants
whose applications were accepted by the coordinator (Malgorithm
=6.49, SD=2.88 vs. Mhuman=7.54, SD=2.79; F(1, 567)=
11.82, p < .001, η2p
=.02). Meanwhile, this difference was significantly
mitigated among participants whose applications were
rejected (Malgorithm=3.74, SD=2.36 vs. Mhuman=3.97, SD=
2.23; F < 1, p=.471).
We conducted an analogous ANOVA on internal attributions.
We found a significant effect of the decision maker
type (Malgorithm =6.54, SD =2.79 vs. Mhuman =7.27, SD =
2.69; F(1, 567) =8.01, p=.005, η2p
=.01) and of decision
outcome favorability (Mfavorable =7.54, SD =2.47 vs.
Munfavorable =6.27, SD =2.90; F(1, 567)=30.15, p < .001, η2p
=.05). Importantly, we found a significant interaction effect
(F(1, 567)=10.11, p=.002, η2p
=.02; see Figure 6): the internal
attribution was weaker when the acceptance decision was made
by the algorithm than when it was made by the club coordinator
(Malgorithm =6.82, SD =2.54 vs. Mhuman =8.15, SD =2.24;
F(1, 567)=18.17, p < .001, η2p
=.03). The effect of the decision
maker type was significantly mitigated for the internal attribution
of the rejection decision (Malgorithm =6.30, SD =2.98 vs.
Mhuman =6.22, SD=2.81; F < 1, p=.806).
Finally, we ran a moderated mediation analysis (PROCESS
Model 8, 10,000 bootstrapped samples; Hayes 2013) with attitudes
toward the country club as the dependent variable, decision maker
type (−1=algorithm, 1=human) as the independent variable,
decision outcome favorability (−1=unfavorable, 1=favorable)
as the moderator, and internal attribution as the mediator (see
Figure 7).3 As we predicted, we found a significant moderated
mediation effect (B=.16, 95% confidence interval [CI] =
[.0536, .2780]). For a favorable decision outcome, the indirect
effect of the decision maker type through internal attribution
was significant (B=.15, 95% CI = [.0720, .2392]), suggesting
decision from an algorithm (vs. a human) was driven by the
weaker internal attribution of the favorable decision. For an unfavorable
decision outcome, however, the corresponding indirect
effect was not significant (B=−.01, 95% CI = [−.0864, .0694]).
In summary, Study 4 directly examined the proposed mechanism
and found evidence that decision outcome favorability
affects the internal attribution process of algorithmic versus
human decisions, thereby leading to divergent reactions to the
decisions made by the different decision makers.
Study 5: Moderated Mediation by Internal Attribution of
a Favorable Outcome
We proposed that consumers react more positively when a
favorable decision is made by a human (vs. an algorithm)
because a human decision maker facilitates the internal attribution
of the decision outcome more. If this is the case, this effect
should be mitigated when the decision outcome is not diagnostic
of consumers’ personal characteristics (e.g., the decision was
made at random), in which case there is little justification for
internal attribution regardless of the decision maker type.
Study 5 tested this prediction by manipulating selfdiagnosticity;
the decision was based on either an evaluation
of the consumer’s application or a raffle. Furthermore, Study
5 increased the generalizability of our effect by replicating it
in another managerially relevant context: networking platforms.
Method. We randomly assigned 501 Prolific workers to one of
four conditions in a 2 (decision maker type: algorithm vs.
human) ×2 (decision method: evaluation vs. raffle) betweenparticipants
design. Our final data set consisted of 443 participants
(Mage=39.33 years; 222 women) who passed our attention check.
Participants read that they were applying to join a business
networking community, NetWorkLink (see Web Appendix I).
Participants learned that their applications were accepted by
either the club algorithm or the club coordinator, and the decision
method involved either an evaluation of the applications or
a raffle (i.e., random selection). Finally, we measured participants’
attitudes toward the networking club (α=.98) and internal
attributions (α=.95) by using the same items as in Study 4.
Results. A 2 (decision maker type) ×2 (decision method)
ANOVA on attitudes revealed no significant main effect of the
decision maker type (Malgorithm=6.44, SD=2.59 vs. Mhuman=
6.55, SD=2.82; F(1, 439)=1.12, p=.290), but a significant
effect of the decision method (Mevaluation=7.30, SD=2.43 vs.
Mraffle=5.69, SD=2.73; F(1, 439)=44.54, p < .001, η2p
=.09).
Importantly, we found a marginally significant interaction
effect (F(1, 439)=3.44, p=.064, η2p
=.01; see Figure 8).
When the acceptance decision was based on an evaluation of
the applications (i.e., when the decision was self-diagnostic),
we replicated our previous findings: attitudes toward the networking
club were less positive among participants whose applications
were accepted by the algorithm than among participants
whose applications were accepted by the club coordinator
(Malgorithm=6.98, SD=2.34 vs. Mhuman=7.70, SD=2.49;
F(1, 439)=4.25, p=.040, η2p
=.01). However, when the acceptance
decision was based on a raffle (i.e., when the decision was
not self-diagnostic), the decision maker type did not significantly
affect participants’ attitudes (Malgorithm=5.80, SD=2.74 vs.
Mhuman=5.60, SD=2.73; F < 1, p=.574).
An analogous ANOVA on internal attribution revealed a significant
main effect of the decision maker type (Malgorithm=5.59,
SD=2.99 vs. Mhuman=6.07, SD=3.29; F(1, 439)=9.93, p=
.002, η2p
=.02) and of the decision method (Mevaluation=7.47,
SD=2.26 vs. Mraffle=4.17, SD=3.05; F(1, 439)=179.62, p <
.001, η2p
=.29). Crucially, we again found a significant interaction
effect (F(1, 439)=6.01, p=.015, η2p
=.01; Figure 8): when the
decision was based on an evaluation of the applications, the internal
attribution of the acceptance was weaker among participants
whose applications were accepted by the algorithm than among participants
whose applications were accepted by the club coordinator
(Malgorithm=6.84, SD=2.29 vs. Mhuman=8.25, SD=1.96; F(1,
439)=15.69, p < .001, η2p
=.03). However, when the acceptance
decision was based on a raffle, the decision maker type did not significantly
affect the internal attribution made by participants (Malgorithm
=4.08, SD=3.04 vs. Mhuman=4.25, SD=3.07; F < 1, p=.621).
To test whether our key effect ismediated by the internal attribution
of the favorable decision outcome, we conducted a moderated
mediation analysis (PROCESS Model 8, 95% CI, 10,000
bootstrapped samples; Hayes 2013) with attitudes toward the
networking club as the dependent variable, decision maker
type (−1=algorithm, 1=human) as the independent variable,
decision method (−1=raffle, 1=evaluation) as the moderator,
and internal attribution as the mediator. In line with our theory,
we found a significant moderated mediation effect (B=.26,
95% CI = [.0516, .4765]): when the decision was based on an
evaluation of the applications and thus self-diagnostic (such
that participants were motivated or able to internally attribute
the favorable outcome), the indirect effect through internal attribution
was significant (B=.29, 95% CI = [.1668, .4344]),
suggesting that the more positive attitude toward the networking
club after receiving a decision from the human (vs. algorithm)
was driven by the stronger internal attribution of the favorable
decision. When the decision was based on a raffle and thus
was not self-diagnostic, however, the indirect effect was not significant
(B=.04, 95% CI = [−.1327, .2088]).
In summary, Study 5 corroborates our attribution mechanism
by demonstrating moderation by the self-diagnosticity of the
decision. Together, the results of Studies 4 and 5 provide converging
evidence that supports our attribution mechanism.
Study 6: External Attribution of an Unfavorable Decision
Outcome
We proposed that the decision maker type has an attenuated effect
on consumers’ reactions following an unfavorable decision
outcome because consumers can readily engage in external attribution
of an unfavorable decision outcome regardless of the decision
maker. Consumers perceive both algorithmic and human decision
makers to have weaknesses: humans are less objective (Lee 2018),
and algorithms neglect the uniqueness of each individual (e.g.,
Longoni, Bonezzi, and Morewedge 2019). Accordingly, when
consumers receive an unfavorable decision outcome, they can
blame a human decision maker for a lack of objectivity and
blame an algorithmic decision maker for neglecting their individual
uniqueness. We argued that these countervailing effects cancel
each other out, resulting in consumers’ relative indifference to the
type of decision maker. In Study 6, we tested this proposition by
measuring consumers’ perceptions of the decision maker’s objectivity
and consideration of individual uniqueness.
Method. In this preregistered study (aspredicted.org/ah2sc.pdf),
we randomly assigned 626 MTurk workers (Mage=35.51
years; 332 women) to one of two conditions (decision maker
type: algorithm vs. human) in a between-participants design.4
Participants read that they were applying for membership at a
country club and their applications were rejected by either the
club algorithm or the club coordinator (see Web Appendix J).
Participants then assessed the decision maker’s objectivity and
consideration of the applicant’s uniqueness (the order of the measures
was randomized). Specifically, participants answered three
items about the decision maker’s objectivity: “To what extent do
you think [this algorithm/club coordinator] [made an unbiased
assessment of your application/made an unemotional assessment
of your application/assessed your application rationally]?” (1=
“not at all,” and 11=“very much”; α=.71). Participants also
answered three items about the decision maker’s consideration
of their application’s uniqueness: “To what extent do you think
this [algorithm/club coordinator] [recognized the uniqueness
of your application/considered the unique aspects of your application
/ tailored the decision to your unique case]?” (adapted 
from Longoni, Bonezzi, and Morewedge [2019]; 1=“not at all,”
and 11=“very much”; α=.93). Lastly, participants completed
the same attitude items as in Study 5 (α=.97).
Results. In line with our previous findings, and as preregistered,
there was no significant effect of the decision maker type on
attitudes toward the country club (Malgorithm =4.84, SD =2.62
vs. Mhuman =4.78, SD =2.61; F(1, 624) < 1, p=.782; see
Figure 9). Crucially, we found a significant effect of the decision
maker type on participants’ perceptions of the decision
maker’s objectivity and consideration of uniqueness: the club
coordinator (vs. algorithm) was perceived as less objective
(Mhuman=5.95, SD=2.47 vs. Malgorithm=7.07, SD=2.28;
F(1, 624)=34.76, p < .001, η2p
=.05), whereas the algorithm
(vs. club coordinator) was perceived as less sensitive to the applicant’s
uniqueness (Malgorithm=4.41, SD=2.81 vs. Mhuman=
5.35, SD=2.74; F(1, 624)=17.67, p < .001, η2p
=.03).
Finally, we conducted a mediation analysis (PROCESS
Model 4; 95% CI, 10,000 bootstrapped samples; Hayes 2013)
with attitudes toward the country club as the dependent variable,
the decision maker type (−1=algorithm, 1=human) as
the independent variable, and perceived objectivity and uniqueness
consideration as the two mediators. In line with the preregistered
prediction, the indirect effects of the decision maker type
via the two mediators were significant in opposite directions
(perceived objectivity: B=−. 19, 95% CI = [−.2905, −.1144];
uniqueness consideration: B=.17, 95% CI = [.0847, .2547]),
explaining the relative indifference to the decision maker type
for unfavorable decision outcomes. The direct effect was not significant
(B=.00, 95% CI = [−.1803, .1790]; see Figure 10).
In summary, Study 6 corroborates our theory that consumers
make external attributions about unfavorable decision outcomes
for both human and algorithmic decision makers, facilitated by
the perceived weakness of the decision maker—human decision
makers have poor objectivity, while algorithmic decision
makers do not consider each applicant’s unique characteristics.
In addition, these results contradict an alternative explanation
based on psychological numbness following a rejection, which
could plausibly lead to an indifference to the type of decision
maker for unfavorable decision outcomes. However, the psychological
numbness account predicts psychological deactivation (including
disengagement from attributional processes), which does not
explain the parallel mediation processes that we found in Study 6.
Study 7: Effect of Human Decision Making Versus Mere
Human Observation
One could argue that participants in our previous studies reacted
more positively to acceptance by humans due to social presence
(Argo, Dahl, and Manchanda 2005; McFerran and Argo 2014);
when an algorithm makes an acceptance decision, no social
agent is aware of the outcome. By contrast, the social presence
of the human decision maker might lead participants to feel
more positive about the outcome and thus react more positively
toward the company.
Although it cannot explain several findings in the previous
studies (e.g., the moderation in Study 5), we conducted Study
7 to directly test the alternative account of social presence by
adding a new condition in which a human monitored (but did
not interfere with) the algorithm’s decisions. If social presence
accounts for our effect, consumers should react similarly when
a human makes the decision versus merely observes the favorable
outcome. If our effect is due to distinct attributions under
human versus algorithmic decision makers, however, then reactions
should be similar when an algorithm makes the decision
with versus without a human monitoring the decision process.
Method. We randomly assigned 597 MTurk workers (Mage =
35.42 years; 318 women) to one of six conditions in a 3 (decision
maker type: algorithm only vs. human vs. algorithm with
human monitoring) × 2 (decision outcome favorability: favorable
vs. unfavorable) between-participants design. The procedure
of this study was similar to that of Study 1a with the
addition of the third decision maker condition, in which the
club coordinator ran and monitored the algorithm’s evaluation
of applications (see Web Appendix K). Participants completed
the same attitude scale as in Study 6 (α=.98).
Results. We found a significant main effect of the decision maker
type (Malgorithm only=5.50, SD=2.98 vs. Mhuman=5.76, SD=
3.17 vs. Malgorithm w/ human monitoring=5.17, SD=2.83; F(2, 591)
=4.52, p=.011, η2p
=.02) and of decision outcome favorability
(Mfavorable=7.18, SD=2.67 vs. Munfavorable=3.76, SD=2.24;
F(1, 591)=295.05, p < .001, η2p
=.33). The interaction effect
was marginally significant (F(2, 591)=2.45, p=.087, η2p
=.01;
see Figure 11).5
In the favorable decision outcome condition, the simple
effect of the decision maker type was significant (F(2, 591)=
5.67, p=.004, η2p
=.02). Replicating our previous studies, attitudes
toward the country club were less positive among participants
whose applications were accepted by the algorithm than
among participants whose applications were accepted by the
club coordinator (Malgorithm only=7.09, SD=2.68 vs. Mhuman
=7.82, SD =2.59; F(1, 591)=4.36, p=.037, η2p
=.01).
Moreover, we found a significant difference in attitudes
between the human condition and algorithm-with-humanmonitoring
conditions; attitudes toward the country club were
less positive in the latter condition (Mhuman =7.82, SD =2.59
vs. Malgorithm w/ human monitoring =6.68, SD=2.65; F(1, 591)=
11.13, p < .001, η2p
=.02). There was no significant difference
in attitudes between the algorithm-only and algorithm-withhuman
monitoring conditions (Malgorithm only=7.09, SD =2.68
vs. Malgorithm w/ human monitoring =6.68, SD=2.65; F(1, 591)=
1.40, p=.238). In the unfavorable decision outcome condition,
attitudes toward the country club were not influenced by the
decision maker type (F(2, 591) =1.37, p=.255).
Discussion. Consistent with our attribution account and inconsistent
with the social presence account, we found that consumers
react more positively when an acceptance decision is made
by a human than by an algorithm, regardless of whether a
human monitors the algorithm’s decisions. At first glance, our
findings may seem contradictory to those of Study 9 in
Longoni, Bonezzi, and Morewedge (2019), in which individuals
were more likely to use a medical algorithm if it was complemented
by a human dermatologist (i.e., a dermatologist reviewed
the algorithm’s diagnosis andmade a final decision). The studies,
however, have a key difference: a human was actively engaged in
the decision-making process in Longoni, Bonezzi, and
Morewedge’s study, whereas a human merely observed the algorithm
and could not alter its decisions in our study.
What Can Managers Do to Mitigate the
Negative Effects of Algorithms?
We consistently observed that consumers react less positively
when a favorable decision is made by an algorithm (vs. a
human). Study 8 examined a potential solution: anthropomorphizing
the algorithm. Extant work suggests that humanizing
a nonhuman agent (e.g., referring to an object with a personal
name) leads people to attribute human-like abilities to it
(Crolic et al. 2022; Epley 2018). We proposed that humanizing
an algorithm should more closely align consumers’ perceptions
of a human decision maker and an algorithmic decision maker,
enabling the human-like algorithm to lead to more positive
reactions than the non-human-like algorithm.
Study 8: Humanizing Algorithms to Mitigate Negative
Consequences: Attitudes Toward the Company
Method. We randomly assigned 601 Prolific workers (Mage=
33.52 years; 316 women) to one of three conditions (decision
maker type: algorithm vs. human vs. human-like algorithm)
in a between-participants design.
The procedure of Study 8 was similar to that of Study 1a.
Participants were told they were applying for membership at
a country club (see Web Appendix L); depending on the condition,
the decision maker was described as a country club algorithm
(depicted as a robot), a country club coordinator named
Sam (depicted as a woman), or a country club algorithm
named Sam (depicted as a cartoonized version of the picture
of the woman from the human condition). All participants
were informed that their applications were accepted. We
asked participants to indicate their attitudes toward the
country club using the same items as in Study 7 (α=.98).
Pretest. We conducted a separate pretest to examine whether a
human-like algorithm seems more human than a non-human-like
algorithm. We presented 100 Prolific workers (Mage=30.23
years; 41 women) with the information from the algorithm and
human-like algorithm conditions in the main study. We then
asked participants, “To what extent do you think that [the
country club algorithm/Sam] has some human-like qualities?”
and “To what extent do you think [the country club algorithm/
Sam] seems like a person?” (1=“not at all,” and 7=“very
much”; r=.80; adapted from Kim and McGill [2018]). Results
confirmed that our manipulation was successful: participants perceived
the human-like algorithm to be more human than the algorithm
was (Mhuman-like algorithm=3.92, SD=1.56 vs. Malgorithm=
2.65, SD=1.29; F(1, 598)=19.86, p < .001, η2p
=.17).
Results. In our main study, we conducted a one-way ANOVA on
participants’ attitudes toward the country club. Replicating our previous
findings, the decision maker type had a significant effect
(F(2, 598)=4.69, p=.009, η2p
=.02). Attitudes toward the club
were less positive among participants whose applications were
accepted by the algorithm than among participants whose applications
were accepted by the club coordinator (Malgorithm=7.07, SD
=2.67 vs. Mhuman=7.87, SD=2.52; F(1, 598)=8.88, p=.003,
η2p
=.01). Importantly, humanizing the algorithm led to significantly
more positive attitudes toward the country club (Malgorithm
=7.07, SD=2.67 vs. Mhuman-like algorithm=7.64, SD=2.82; F(1,
598)=4.46, p=.035, η2p
=.01) such that attitudes were similar
whether the decision maker was the human-like algorithm or the
club coordinator (Mhuman-like algorithm=7.64, SD=2.82 vs.
Mhuman=7.87, SD=2.52; F < 1, p=.384).
Discussion. Building on our prior studies’ finding that consumers
react less positively when a favorable decision is made by an
algorithm (vs. a human), Study 8 tested a potential solution:
anthropomorphizing the algorithm. Attitudes toward the
company were more positive when the favorable decision was
made by a human-like (vs. a non-human-like) algorithm.
General Discussion
The current research reveals that consumers react differently to
a company that uses algorithmic (vs. human) decision makers
as a function of decision outcome favorability: Consumers
react less positively toward a company when they receive a
favorable decision made by an algorithm than by a human;
however, this difference is significantly mitigated when the
decision outcome is unfavorable. The effect is driven by different
attributions: consumers find it relatively more difficult to
internalize a favorable decision made by an algorithm (vs. a
human), while it is similarly easy to externalize an unfavorable
decision made by either type of decision maker. Finally, we
demonstrate that humanizing the algorithm can mitigate the relatively
less positive reaction to an algorithmic (vs. a human)
decision maker in the setting of favorable decision outcomes.
Alternative Accounts
Several alternative accounts merit discussion. We review these
accounts and discuss how our findings and study design rule
them out. In addition, we have direct evidence, in the form of
both mediation and moderation, that supports attribution processes
(Studies 4–6).
First, one might argue that consumers care about a favorable
outcome being witnessed by (rather than made by) another
human and that it is this mere human presence that leads to
more positive reactions to human decision makers. Against
such a social presence account, however, participants in
Study 7 reacted more positively only when a human (vs. an
algorithm) made the favorable decision on them, but not
when a human merely observed the algorithm and thus knew
about the favorable decision outcome.
Second, our results might be explained by social cues. For
instance, being accepted by a human might create a sense of
social belonging, while being evaluated by an algorithm
might engender feelings of disrespect. However, we observed
the key interaction effect even in contexts in which social relationships
are less salient (i.e., business loan application, market
research participant panel). Moreover, if algorithmic (vs.
human) evaluation creates feelings of disrespect, we should
have found a main effect of the decision maker type, but not
necessarily the interaction effect between the decision maker
type and decision outcome favorability.
Third, consumers might pay less attention to unfavorable
information about the self because they inherently avoid information
that can hurt their self-esteem (Trope and Neter 1994).
In this regard, consumers might be inattentive to any unfavorable
information about the self, including the type of decision
maker that was involved in the unfavorable decision.
However, we replicated our interaction effect even when we
explicitly directed participants’ attention to the decision
maker type (Study 2), ruling out the inattention account. In
addition, the inattention account does not explain the two
opposing mediation processes for unfavorable decisions in
Study 6.
Fourth, one could argue that psychological numbness
explains the relative indifference to the decision maker type for
unfavorable decision outcomes. An experience of social exclusion
(e.g., ostracism) can impair people’s emotional sensitivity
and cognitive function (Williams 2007), and even social rejections
by nonhuman agents (e.g., robots) can lead to negative psychological
consequences (Nash et al. 2018). If psychological
numbness explains our effect, however, then it should be
limited to contexts in which social relationships are salient—
but our effect is significant in nonsocial contexts as well.
Moreover, the psychological numbness account would predict
that consumers who receive unfavorable decision outcomes
should be less likely to engage in any cognitive processes including
attributions, but in Study 6, participants’ reactions to unfavorable
decisions were due to external attribution processes.
Fifth, one could argue that the perceived fairness of algorithmic
versus human decision makers explains our results.
Consumers are known to perceive decisions made by algorithms
(vs. humans) as less fair (Lee 2018). Differential perceptions of
decision fairness should produce a main effect of the decision
maker type, but not necessarily the interaction effect that we
observed consistently. Nonetheless, we conducted a follow-up
study (seeWeb Appendix N) that measured the perceived fairness
of the decision.We found amain effect of the decision maker type:
participants perceived the human to be fairer than the algorithm
(Mhuman=4.12, SD=1.55 vs. Malgorithm=3.28, SD=1.57; F(1,
317)=23.63, p < .001, η2p
=.07). However, this effect was not
moderated by decision outcome favorability (F < 1, p=.578),
ruling out perceived fairness as a viable explanation for our effect.
Finally, one could be concerned about scale insensitivity as an
explanation for the interaction between the decision maker type
and decision outcome favorability. Specifically, one could
argue that there could be differences in consumers’ reactions to
unfavorable decision outcomes by different decision maker
types, but that our measures are not sensitive enough to
capture these differences (e.g., because such reactions are in
general quite negative). Study 2 rules out this concern. In
Study 2, we first elicited a response to the outcome (favorable
or unfavorable) and then provided information about the decision
maker to probe how the information of the decision maker type
changes participants’ attitudes toward the company. In this
study, we still observed that participants reacted to rejections
by humans and algorithms similarly.
Theoretical Implications
The current research makes several theoretical contributions.
Extending prior research on how consumers decide between algorithms
and humans (Dietvorst, Simmons, and Massey 2015;
Longoni, Bonezzi, and Morewedge 2019), we shed light on
how consumers’ reactions to a self-diagnostic decision (i.e., decisions
about the consumers themselves) are affected by the decision
maker type (human vs. algorithm). Second, our work identifies a
theoretically and managerially relevant moderator (decision
outcome favorability) that has been underexplored in the existing
literature on algorithmic decision making. Finally, we extend the
existing work on consumers’ perceptions of the different decision
makers (e.g., Lee 2018) by examining how algorithmic (vs.
human) decisions prompt different attributions as a function of
decision outcome favorability. In doing so, our research marries
the social psychology literature on attribution processes with the
marketing literature on algorithmic decision making.
Our article opens several avenues for future research. First,
future research could examine consumers’ perceptions of decisions
that are made through human–algorithm collaboration.
Consumers may react differently depending on the nature of
the collaboration (e.g., who conducts the first round of screening
vs. makes a final decision). Second, future research could
examine whether our interactive effect is influenced by the nature
of decision criteria. Companies use a variety of criteria to accept
or reject consumers (e.g., high/low performance, passing/failing
a threshold). In our studies, we did not specify why an application
was accepted or rejected.We encourage researchers to investigate
whether specific reasoning affects the interaction between
the decision maker type and decision outcome favorability.
Third, even though big data has improved the quality of decisions
made by both humans and algorithms, there are still concerns
about the representativeness of data used by firms (Bolukbasi
et al. 2016). Given that minority groups are often underrepresented
in data sets (Sheikh 2006), the effect of the decision
maker type on consumers’ reactions may differ for consumers
from a minority versus majority group. Future research can incorporate
consumer demographics to understand such differences.
Fourth, our work focuses on consumers’ attitudes toward the
company, but more research is needed to understand how algorithmic
decisionmaking impacts consumers’ psychological
security. For instance, future research can investigate how the
decision maker type affects consumers’ perceived threat and
anxiety (Mende et al. 2019). Fifth, future research could investigate
whether consumers’ reactions change depending on whether
the decision outcome is communicated by a person or through a
nonhuman medium (e.g., email). Although we manipulated only
the decision maker type and held all other communication about
the decision outcome constant, future research could examine the
effect of how decisions are communicated to consumers (e.g.,
Campbell 2007; price tag vs. store owner). Sixth, although the
current research primarily focuses on consumers’ attitudes
toward the company, which is a managerially important consumer
indicator, future research can extend our findings to
other behavioral measures.
Lastly, it is interesting to consider under which conditions
algorithmic (vs. human) decisions might be more likely to
facilitate internal attributions. Although we observed a consistent
pattern across different consumer contexts, responses,
and procedures, it is possible that in some situations algorithmic
acceptance might offer an especially salient cue of diagnosticity
and facilitate internal attributions to a larger extent.
In general, more research is needed to understand how our
effects can be moderated by the nature of the evaluation
context. For instance, if a decision process is based on a
simple objective criterion (e.g., if one’s grade point average
is above the 80th percentile), a favorable decision might facilitate
internal attributions regardless of whether the decision
maker is an algorithm or a human, mitigating the effect of
the decision maker type.
Managerial Implications
The current work has several managerial implications. First, our
results offer insights—perhaps surprising to many managers—
into how the adoption of algorithms for consumer-facing decisions
may affect consumers’ reactions toward the company. We
found that some managers hesitate to automate consumerfacing
decisions because they are concerned about exacerbating
consumers’ negative reactions to unfavorable decision outcomes
(see in-depth interviews #2, #3, and #12 in Web
Appendix C; Dietvorst, Simmons, and Massey 2015; Luo
et al. 2019). Our results, however, demonstrate that an algorithmic
(vs. a human) decision maker hurts consumers’ reactions
for favorable outcomes, not for unfavorable ones.
Second, in our interviews with managers, some managers
expected that consumers would respond more positively when
human and algorithmic decision makers collaborate, and
some mentioned that their companies are already using this
strategy (see in-depth interviews #2 and #9 in Web Appendix
C). Our results indicate that consumers may not necessarily
respond more positively to companies if humans are merely
observing the algorithms without active involvement in decision
making (Study 7). By showing this, we offer managerial
insights on how companies can design their evaluation processes.
In addition, we demonstrate that the effect of the decision
maker type is mitigated when the favorable decision
outcome is not self-diagnostic (i.e., when the decision was
based on a raffle; Study 5). Managers can leverage these findings
to improve consumers’ reactions to companies that use
algorithms for consumer-facing decisions.
Third, we explored a possible approach to mitigate the risk
of less positive reactions following algorithmic acceptance:
making the algorithm more human-like. In Study 8, the addition
of simple anthropomorphic cues eliminated the effect of the
decision maker type in the case of an acceptance decision.
We also observed a similar pattern in field data from a financial
services company. These data provide click-through rates on a
link to the company’s services after receiving financial feedback
from human-like algorithms (vs. non-human-like algorithms;
for details, see Web Appendix M). Once consumers
answered a questionnaire, the company provided feedback
based on an algorithmic assessment of the consumer’s financial
health. Some consumers received feedback that was highly
favorable (good financial health with just a check-up needed),
mimicking the favorable outcome condition of Study
8. Replicating the effect with a behavioral measure, these consumers
were more likely to seek information about the company’s
services when the favorable feedback came from a
human-like (vs. non-human-like) algorithm. These preliminary
findings mimic those in Study 8 and corroborate the conclusion
that negative consequences of algorithmic decision making may
be averted by making algorithms more human-like (using, e.g.,
a more conversational format, a human name, a human-like
photo).
Finally, we offer insights for policy makers. When the decision
maker type is not disclosed, consumers are likely to react
similarly as they do to a human decision maker (Studies 3a–
b), offering firms an incentive to avoid transparency, which is
not in the interest of consumers. Our results align with recent
movements calling practitioners to be more transparent about
their use of algorithms (Davenport et al. 2020; Rai 2020) and
laws in the United States and European Union that require companies
to disclose whether they use algorithms in consumerrelated
tasks (Castelluccia and Le Métayer 2019; Smith 2020).
