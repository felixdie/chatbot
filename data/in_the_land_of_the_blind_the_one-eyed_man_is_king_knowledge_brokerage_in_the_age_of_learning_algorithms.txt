VWaardenburg, L., Huysman, M., & Sergeeva, A. V. (2022). In the land of the blind, the
one-eyed man is king: Knowledge brokerage in the age of learning algorithms. Organization
science, 33(1), 59-82.

Abstract

This paper presents research on how knowledge brokers attempt to translate opaque
algorithmic predictions. The research is based on a 31-month ethnographic study of the
implementation of a learning algorithmby the Dutch police to predict the occurrence of crime
incidents and offers one of the first empirical accounts of algorithmic brokers. We studied a
group of intelligence officers, who were tasked with brokering between a machine learning
community and a user community by translating the outcomes of the learning algorithm to
police management. We found that, as knowledge brokers, they performed different translation
practices over time and enacted increasingly influential brokerage roles, namely, those of
messenger, interpreter, and curator. Triggered by an impassable knowledge boundary
yielded by the black-boxed machine learning, the brokers eventually acted like “kings in the
land of the blind” and substituted the algorithmic predictions with their own judgments. By
emphasizing the dynamic and influential nature of algorithmic brokerage work, we contribute
to the literature on knowledge brokerage and translation in the age of learning algorithms.

Keywords: learning algorithms, artificial intelligence, algorithmic brokers, knowledge brokerage, knowledge sharing, knowledge translation


Introduction
From healthcare to recruitment, litigation, and law enforcement,
learning algorithms are increasingly prevalent
in everyday work (e.g., Brayne 2020, Rezazade
Mehrizi et al. 2020, Zhang et al. 2020, Lebovitz et al.
2021, Van den Broek et al. 2021). By combining large
data sets with advanced computational and statistical
methods to make connections between data points—a
process that is called “machine learning” (Burrell
2016, Brynjolfsson and McAfee 2017, Davenport
2018)—learning algorithms generate algorithmic predictions
(Faraj et al. 2018). Learning algorithms deserve
specific scholarly attention, as we cannot rely on
the existing understanding of “intelligent technologies”
in organizations (Von Krogh 2018, Bailey
and Barley 2020, Huysman 2020, Pachidi et al. 2021).
Earlier “rule-based” technologies, such as expert systems,
reflected the expert knowledge that was coded
into them (Forsythe 1993), and developers could explain
their outputs. In contrast, through machine
learning, the input data and the knowledge of developers
are autonomously transformed into algorithmic
predictions. The downside of machine learning is that
it is difficult for humans to discern how and which
connections between data points are made, which
makes it challenging to understand how algorithmic
predictions are generated. This problem is often
referred to as the “opaque nature” (Burrell 2016,
Christin 2020) or “black box problem” (Pasquale 2015,
Anthony 2021) of learning algorithms.
The opaque nature of learning algorithms makes
trusting and using algorithmic predictions in practice
problematic (Bader and Kaiser 2019, Lebovitz et al.
2019, Glikson and Woolley 2020). As a potential solution,
recent studies posit that “algorithmic brokers”
(Kellogg et al. 2020) could emerge to facilitate the use
of these systems by translating predictions to users
(Henke et al. 2018, Gal et al. 2020, Sachs 2020). The
work of algorithmic brokers should therefore resemble
what is referred to in organizational theory as
“knowledge brokers” (e.g., Carlile 2004, Pawlowski
and Robey 2004, Meyer 2010)—actors who enact
translation practices to solve knowledge boundaries
between communities (Brown and Duguid 1998).
These knowledge boundaries are defined by the
practices that are easily shared by actors within communities
and equally difficult to share by actors from
different communities. For algorithmic brokers, this
means solving a knowledge boundary between a
machine learning community and a user community.
59

A machine learning community can be defined by the
shared practices of developing algorithmic predictions;
a user community represents actors who share
domain knowledge and intend to use algorithmic predictions.
Previous studies argue that for brokers to
translate between communities requires a thorough
understanding of the practices of both communities
(Brown and Duguid 1998, Sturdy and Wright 2011,
Røvik 2016). However, in the case of learning algorithms,
algorithmic predictions are generated by combining
inputs (i.e., data and developers’ knowledge)
with machine learning (Von Krogh 2018). Because of
the black-boxed nature of learning algorithms, these
aspects of the machine learning community remain
hidden, even for developers (e.g., Faraj et al. 2018),
which leads to a puzzle that goes beyond the current
understanding of knowledge brokerage: How do
brokers translate algorithmic predictions when they
cannot understand how these are generated?
To answer this question, we offer a 31-month ethnographic
study of a Dutch police department that implemented
predictive policing—that is, the use of a learning
algorithm to predict where and when a crime is
likely to occur. By analyzing the implementation process
over an extended period, we found that a group of
“intelligence officers” performed different translation
practices through which they enacted increasingly influential
knowledge brokerage work (i.e., in the form of
messenger, interpreter, and curator). Our study offers
an integrative perspective on organizational theory and
emerging technologies and reveals the emergence of a
new phenomenon: the algorithmic broker with its dynamic
and influential nature. Through our process perspective
on knowledge brokerage work, we offer new
insights into the literature on knowledge brokers (e.g.,
Brown and Duguid 1998, Pawlowski and Robey 2004,
Meyer 2010, Burgess and Currie 2013). The study
shows that the translation practices that knowledge
brokers enact over time afford them a unique position
in which they can grow to become increasingly influential.
Moreover, this case highlights that knowledge brokerage
work is more complex than resolving a knowledge
boundary between communities (e.g., Dougherty
1992, Carlile 2004, Boari and Riboldazzi 2014), because,
in their efforts to resolve such boundaries, brokers
can generate new boundaries between themselves and
the communities they are intended to connect. In addition,
our findings contribute to translation theory (e.g.,
Czarniawska and Sev´on 2005, Mueller and Whittle
2011, Nielsen et al. 2014, Røvik 2016). Whereas current
translation theories mainly focus on how knowledge is
translated to specific fields and organizations, we show
the importance of unpacking how knowledge is translated
from its original source and provide insights into
what happens to translation in the case of opaque machine
learning.
Research on Knowledge Brokers
Sharing knowledge between actors coming from diverse
professional or organizational settings is considered
a key organizational competence and the topic
has occupied many organizational scholars (e.g., Nonaka
1994, Von Hippel 1994, Østerlund and Carlile
2005, Pachidi et al. 2021, Safadi et al. 2021). Initially,
knowledge was mainly considered as an object that
had to be made explicit before it could be transmitted
(e.g., Nelson and Winter 1982, Teece 1998). However,
organizational scholars started to counter this perspective
by arguing that knowledge is embedded in
practices (e.g., Cook and Brown 1999, Brown and Duguid
2001, Tsoukas 2003), which means that knowledge
is shared through sharing practices. The
practice-based perspective on knowledge has gained
traction ever since and has triggered many interesting
research avenues, such as how practice-based knowledge
can be distributed, managed, and supported by
technology (Orlikowski 2002, Levina and Vaast 2005,
Faraj et al. 2016).
Taking a practice-based perspective on knowledge
helps us to see that knowledge sharing is a complex
process, since the tacit elements of knowledge that are
“rooted in action, procedures, routines, commitment,
ideals, values, and emotions” (Nonaka and von Krogh
2009, p. 636) can lead to interpretative differences
between actors residing in different communities
(Lave 1988, Brown and Duguid 1998, Orlikowski
2002, Carlile 2004). For example, Barley (1986) has
shown how interpretative differences can emerge and
grow when the implementation of a CT scanner requires
knowledge sharing between technology developers
and technology users. In such situations, a
so-called “semantic boundary” (Carlile 2004) hinders
knowledge sharing as the communities’ practicebased
knowledge1 continues to reproduce this boundary.
Crossing a semantic boundary therefore requires
creating shared understandings (Dougherty 1992).
Some scholars argue that diverse actors can develop
a shared understanding when they participate in
shared practices (Brown and Duguid 1991, Lave and
Wenger 1991, Orr 1996). However, most studies argue
that such shared practices are unusual and emphasize
that crossing a semantic boundary requires a particular
group of “knowledge brokers” to operate
in-between communities by becoming familiar with
them in order to gather and disseminate information
and knowledge (e.g., Hargadon and Sutton 1997,
Brown and Duguid 1998, Carlile 2004, Evers and
Menkhoff 2004, Burgess and Currie 2013, Chiambaretto
et al. 2019). Accordingly, organizational scholars
have paid attention to the role of knowledge brokers
in areas such as engineering (Johri 2008), science (Barley
1996, Kissling-Naf 2009), information technology
(Pawlowski and Robey 2004), and recently regarding



emerging technologies, such as learning algorithms
(Kellogg et al. 2020). Knowledge brokers perform a
kind of “boundary work” (e.g., Soundarajan et al.
2018, Langley et al. 2019), yet differ from what are
known as “boundary spanners” (e.g., Ancona and
Caldwell 1992, Levina and Vaast 2005) in that knowledge
brokers do not belong to or come from the communities
they intend to connect (Gould and Fernandez
1989, Fleming and Waguespack 2007, Meyer 2010,
Haas 2015).
The concept of knowledge brokers is derived from
the field of broker studies (e.g., Gould and Fernandez
1989, Burt 1992, Obstfeld 2005, Stovel and Shaw 2012,
Heaphy 2013) and traditionally resides in the structural
network approach (e.g., DiMaggio 1993, Fernandez
and Gould 1994, Reagans and McEvily 2003, Leonardi
and Bailey 2017). Taking this perspective, brokers are
considered to occupy a “structural hole” (Burt 1992)
between disconnected actors and to benefit from
unique access to various communities and knowledge
sources (DiMaggio 1993, Fernandez and Gould 1994).
Studies on brokerage work move away from the structural
network perspective, in which a broker’s role is
determined by one’s position in a network, to take a
more practice-based perspective on how brokerage
roles are enacted in practice (e.g., Wenger 1999,
Fernandez-Mateo 2007, Lingo and O’Mahony 2010,
Obstfeld et al. 2014, Edacott and Leonardi 2020). These
studies emphasize how brokerage work emerges
when new tasks are created that existing communities
are unwilling or unable to take on (Barley 1996,
Heimer and Stevens 1997, O’Mahony and Bechky
2008, Huising and Silbey 2011). For example, Kellogg
(2014) examined how, in the face of organizational reform
at a hospital, brokers took on tasks that medical
professionals and lawyers did not consider to be part
of their occupational domain.
In the case of knowledge brokerage, these new tasks
are typically related to translating knowledge in order
to ensure that actors across different communities can
understand each other. Knowledge brokers therefore
enact translation practices through which they present
the knowledge of one community in such a way that
it gains a shared understanding and can be put in
practice (Tushman and Katz 1980, Barley 1996, Grady
and Pratt 2000, Paul and Whittam 2010, Boari and
Riboldazzi 2014). To enact such translation practices,
knowledge brokers depend on their interactions with
the communities with the aim to (1) decontextualize
knowledge in order to translate knowledge from one
community into more abstract representations (e.g.,
words or texts) and to (2) contextualize the abstract representations
in order to translate to another community
(Callon 1984, Latour 1986, Czarniawska and Joerges
1996, Doorewaard and van Bijsterveld 2001, Nielsen
et al. 2014, Røvik 2016). Translation thus requires a
deep understanding of both communities, which
makes performing translation a knowledge-intensive
practice in itself.
For knowledge brokers, who are not members of
any of the communities they intend to connect (Gould
and Fernandez 1989, Fleming and Waguespack 2007,
Meyer 2010, Haas 2015), understanding these communities
can thus be a challenging and extensive task
(Brown and Duguid 1998). This becomes even more
problematic with the recent emergence of “algorithmic
brokers.” Below, we unpack why the unique, blackboxed
nature of learning algorithms offers new challenges
to knowledge brokerage work.
Brokering Learning Algorithms
Learning algorithms are calculative devices used for
“machine learning,” a subfield of the broader field of
“artificial intelligence” (AI). Whereas AI technologies,
on the surface, appear to mainly consist of abstract
statistical equations, there is always a community of
computer scientists behind it, who construct the abstract
mathematical representations through situated,
embodied, and social practices (Lave 1988). AI technologies
consist of three parts: task inputs, task processes,
and task outputs (Von Krogh 2018). Task inputs
comprise the input data and the knowledge of
the developers, for example, of constructing, cleaning,
and preparing data sets and of coding the initial decision
logic of the learning algorithm. Task processes refer
to machine learning, that is, making and adjusting
connections between a large number of data points,
which changes the decision logic as initially coded.
The task outputs are the algorithmic predictions,
which can be put in practice by users and are often
used as new data points for the learning algorithm.
Whereas the task inputs are relatively transparent
(e.g., data sets can be looked into, and developers can
be asked about their practices), the task processes (i.e.,
machine learning) present a challenging new phenomenon,
as machine learning becomes increasingly difficult
for humans to understand (Brynjolfsson and
McAfee 2017, Faraj et al. 2018, Gal et al. 2020). This is
referred to as the “opaque nature” (Burrell 2016,
Christin 2020) or the “black-box problem” (Pasquale
2015, Introna 2016, Ajunwa 2020, Anthony 2021) of
learning algorithms and mainly occurs because the
procedures used for machine learning differ fundamentally
from “demands of human-scale reasoning
and styles of semantic interpretation” (Burrell 2016,
p. 2). To illustrate this point, Burrell (2016, p. 9) discussed
a spam filter: “Humans likely recognize and
evaluate spam according to genre: the phishing scam,
the Nigerian 419 email, the Viagra sales pitch. By contrast,
the ‘bag of words’ approach [i.e., machine learning]
breaks down texts into atomistic collections of



units, words whose ordering is irrelevant.” Thus,
whereas humans use their ability to interpret and put
a message into context to assess if an email is spam, a
learning algorithm uses words commonly associated
with spam (e.g., click, dollar, price), is trained to rank
these words by weight, flags an email based on the aggregate
of the weights of all words, and becomes better
at doing this over time through machine learning.
Understanding how algorithmic predictions are generated
therefore requires not only discerning the task inputs
(e.g., the data used to develop and train the model
or the internal decision logic as coded by developers)
but also unpacking the machine learning, that is, how
the learning algorithm’s internal decision logic changes
when the algorithm learns from data. However, the inherent
difference between machine learning and human
reasoning makes the opaque nature of learning
algorithms a fundamental issue and keeps even developers
in the dark about how the internal decision logic
of these systems evolves over time (Michalski et al.
2013, Faraj et al. 2018). As a consequence, black-boxed
machine learning is a specific area of concern in the
field of computer science, which triggered these scholars
to study “explainability issues” and how to alleviate
them (e.g., Doran et al. 2017, Kirsch 2017, Lipton
2018, Preece et al. 2018, Miller 2019, Mittelstadt et al.
2019, Robbins 2019, Barredo et al. 2020). They argue
that the nature of learning algorithms is a doubleedged
sword: their key strength (i.e., learning from
large data sets to arrive at predictions) is simultaneously
their main problem. The explainability issues
are also gaining traction with organizational scholars,
who increasingly emphasize that when users are confronted
with algorithmic predictions that cannot be explained
or understood, they experience difficulties
trusting, using, and maintaining control over the role
of learning algorithms in their decision-making processes
(Zarsky 2016, Bader and Kaiser 2019, Lebovitz
et al. 2019, Christin and Brayne 2020, Gal et al. 2020,
Glikson and Woolley 2020, Dur´an and Jongsma 2021).
To overcome the explainability issues of learning algorithms
in organizations, organizational scholars emphasize
the need to make algorithmic predictions
comprehensible and actionable to users (Bolin and
Andersson Schwarz 2015). This requires new tasks related
to translating algorithmic predictions in practice
(Henke et al. 2018, Gal et al. 2020, Kellogg et al. 2020,
Sachs 2020, Shestakofsky and Kelkar 2020). As the
practices of developing algorithmic predictions of the
machine learning community and the domain practices
of the user community are not easily shared, a
semantic boundary creates an opportunity for knowledge
brokers to step in and take up translation tasks
(Carlile 2004). By translating algorithmic predictions
to users, algorithmic brokers (Kellogg et al. 2020)
are seen as providing a potential solution to the
explainability problem established in computer science
(Henke et al. 2018).
Yet, an interesting puzzle arises regarding the ability
to translate algorithmic predictions. As we have
discussed, theories on translation taught us that to enact
translation practices requires knowledge brokers
to interact with and understand the communities involved
(e.g., Brown and Duguid 1998, Røvik 2016).
Such interaction is significantly hindered in the case
of learning algorithms. More precisely, whereas
brokers can interact with developers to discover the
knowledge of the machine learning community, the
black-boxed machine learning prevents them from
fully understanding how algorithmic predictions are
generated. In translating predictions to users, algorithmic
brokers are thus confronted with a new situation
in which understanding the input data and the knowledge
of developers is not enough to comprehend how
algorithmic predictions are generated. Accordingly,
our aim is to analyze which practices algorithmic
brokers enact when they encounter black-boxed learning
algorithms—in other words, when they operate
“in the land of the blind.”
Methods
Research Setting
Our study focuses on the implementation of the
so-called “Crime Anticipation System” (CAS), which
was internally developed by a team of so-called “data
scientists” at the Dutch police. The development of
CAS was initiated by the national police management
to allocate police resources (e.g., patrol officers, specialized
teams, material resources) more effectively
and efficiently by predicting where and when a crime
was most likely to occur. In contrast to, for example,
the fragmented organizational structure of the U.S.
police force (see e.g., Van Maanen 1973, Brayne 2020),
the Dutch police is nationally organized and coordinated,
which facilitated the nationwide implementation
of CAS. The Dutch police started the predictive
policing project in 2012 by hiring three data scientists
and, between 2012 and 2017, gradually expanded the
data science team to about 20 members. Maintaining
CAS remained one of the responsibilities of these data
scientists, although after its implementation at local
police departments, most of the members of the data
science team were also actively involved in other projects,
such as developing counterterrorism learning algorithms
and image recognition for investigating and
preventing child sexual abuse. One data scientist
(Dennis2) took the lead in the development of CAS
and was therefore the main “brains” behind the learning
algorithm. All other data scientists were responsible
for maintaining the system and performing
updates.




of CAS, which predicted a week in advance where
and when a crime was most likely to occur. During
the test phase, the work of “intelligence officers,” who
could help local police managers to use the crime predictions,
emerged. In the Findings section, we will go
into detail about the emergence of these intelligence
officers as algorithmic brokers. Here, it is important to
emphasize that the implementation of CAS therefore
included data scientists as developers, intelligence officers
as algorithmic brokers, and local police managers
(hereafter “police managers”) as users. The interaction
between intelligence officers, data scientists,
and police managers in the implementation and use
of CAS was influenced by the siloed and hierarchical
organizational structure of the Dutch police. The “user
community” consisted of the police managers who
were intended to use the crime predictions in their operational
decision-making practices, such as allocating
police resources. They transferred data-related tasks
to intelligence officers, and, because the nature of police
work is action-oriented and police managers considered
CAS to be extremely complex and “foreign,”
they did not feel the need to engage with CAS directly
and trusted intelligence officers to do so. As one police
manager responded to an intelligence officer, “You
lost me at http.”
The “machine learning community” consisted of
the data scientists, the data, and the CAS learning algorithm.
To create CAS, the data scientists were inspired
by the U.S. version “PredPol.” Whereas the police
could have bought into the external PredPol
algorithm, national management decided that the
in-house data scientists could better develop a new
version so that it would not require the police to share
vulnerable data with external sources. Moreover,
through in-house development, the police planned to
hold a grip on which data and variables were included
in the learning algorithm (e.g., to prevent profiling,
they decided not to include individual-level data).
The data scientists used logistic regression analysis
as the technique for the CAS learning algorithm. Logistic
regression analysis is a very popular method in
machine learning, specifically for binary classification
tasks (i.e., a problem with two class values, such as
“crime” and “no crime”). It is used to predict, for example,
whether an email should be classified as spam
or not, whether a tumor is benign or malignant, or
whether a loan will or will not be repaid. Because
learning algorithms are trained using large amounts
of data, CAS was developed with data of crimes with
the highest reporting numbers, which are called
“high-impact crimes” (e.g., burglary, car theft, robbery).
Such crimes are relatively easy to carry out and
thus happen frequently, and they have a high impact
on citizens, which means that they are often reported.
The reporting of these crimes results in a large number
of data points, which makes them specifically suited
for developing and training learning algorithms.
For the CAS algorithm to learn, the data scientists
constructed a data set with historic high-impact crime
data. They divided the country into squares of 125 m2
and used three years of historical data for every
square. Across the three years, they used biweekly reference
moments, which resulted in 76 lines of data per
square. Each line of data consisted of eight technical
variables and 47 predictive variables (limited by strict
data regulations). The technical values included, for
example, time indicators, the name of the police station,
and the name of the police district. The 47 predictive
variables consisted of 19 population-related variables
(e.g., number of one-parent households, total
number of addresses, average house price, number of
male and female inhabitants, and average age of inhabitants)
and 28 crime-specific variables (e.g., for
burglary, variables such as time since the last burglary
and number of burglaries in the last two weeks). In
addition, each line included whether the specific
crime happened in the two weeks between the reference
moment. To predict the probabilities of future
crimes, the logistic regression model of CAS was
trained to learn a mapping between the 47 predictive
variables and whether a crime happened or not. To
transform the numerical probabilities into a visualization
of the crime predictions on a map, threshold values
were added to determine whether and in what
color predicted squares appeared on the map; the
darker the color, the higher the predicted probability
(see Figure 1). Data extraction, model building, and
map generation were automated and happened on a
Figure 1. (Color online) Visualization of Predictions as Perceived
in the User Interface




with the size of the data set and the high number
of predictions, made the internal decision logic of
predictions opaque in practice, even for the data
scientists.
The data scientists were located in a different building,
far removed from daily police operations and the
intelligence officers. They were hired for their expertise
in computer science and were expected to create
systems that would generate new insights for police
operations across the country. The data scientists were
not bothered by their distance from daily police operations.
They considered algorithmic predictions fundamentally
different from police occupational knowledge
and were convinced that these predictions could
and should be generated away from the police. As a
result, the data scientists only occasionally interacted
with intelligence officers (via email or organized meetings
held on average twice a year) and rarely spoke
with police managers.
Data Collection
We performed ethnographic research with the aim of
theory elaboration to make theoretical advancements
(Fisher and Aguinis 2017). We conducted our fieldwork
with the Dutch police over 31 months, from October
2016 to April 2019. During these three years, the
first author observed and took part in the daily work
at the intelligence department and the emergency response
department. In this study, we report on our
data of the intelligence department only. We followed
the intelligence officers over these three years, with an
intensive observation period in the second year of the
study, in which the first author joined the intelligence
department approximately three days a week, observing
and taking part in the intelligence officers’ work.
All observations were conducted when CAS was already
in use, and details about CAS were obtained
through (retrospective) interviews with data scientists
and archival documents. Our interest in the role of the
intelligence officers was triggered when, at the start of
our fieldwork, we were surprised to see that the police
managers did not directly interact with CAS and that
the intelligence officers performed this work instead.
The first author had unrestricted access to the intelligence
department—which consisted of about 15 fulltime
employees—of a police station in a large Dutch
city. She shadowed the intelligence officers in all their
work, including their interactions with CAS, data scientists,
police managers, and police officers. Her main
focus was on the intelligence officers, but joining the
various interactions also gave her thorough insights
into the other groups involved. She would usually sit
at the desk next to one of the intelligence officers and
write down in detail which features they used when
working with CAS, how they tried to make sense of
the learning algorithm and the crime predictions, and
how they reasoned and went about representing the
predictions to police managers. Through her prolonged
presence at the intelligence department, she
gained the trust of the intelligence officers to perform
some of the intelligence activities herself, which gave
her deep insights in the efforts involved in performing
intelligence officers’ work. For example, they asked
her to help out with extensive database searches, and
she was given access to the CAS user interface to go
through crime predictions and eventually even helped
new intelligence officers settle in by explaining how to
use CAS.
The first author also followed other activities of the
intelligence officers, which gave her a rich contextual
understanding of the empirical site. For example, participating
in briefings at the start of police shifts, joining
management meetings and meetings with data
scientists, and accompanying the intelligence officers
for lunch and occasional festivities, such as their yearly
team outing and Christmas party. Finally, the first
author joined one of the intelligence officers appointed
as spokesperson to regional (once a month)
and national (once every six months) gatherings of intelligence
officers at police stations across the country.
Because the intelligence officers all worked at different
police stations, these meetings were used to reflect
and learn from each other. Initially during these meetings,
the intelligence officers shared best practices and
their struggles with translating algorithmic predictions.
This further established the first author’s observations
of the challenges faced by the intelligence officers.
Near the end of the fieldwork, the first author
observed that the intelligence officers collectively emphasized
the need to substitute predictions, which
validated her observations of how the role of intelligence
officers changed over time. By actively participating
in all facets of the intelligence officers’ work,
the first author became fully socialized into the intelligence
department, by which she developed a holistic
perspective of intelligence officers’ work and their
relationship to other stakeholders, a deep understanding
of the work practices performed, as well as the underlying
feelings and experiences, such as confusion,
stress due to time pressure, and tiredness, and the
pride and joy of being able to come up with a fitting
recommendation.
In addition, the first author also conducted 33 formal
semistructured interviews. Voice recording was possible
for 25 interviews, which were transcribed verbatim.
For the other eight, detailed notes were taken during
the interview and expanded afterward into an elaborate
summary. We explicitly searched for and contacted
people who could provide rich details and
reasoning into how CAS development, implementation,



data scientists who were closely involved with CAS
for the longest time, intelligence officers who were at
the intelligence department already before the implementation
of CAS, and police managers who were
closely involved in the implementation of the learning
algorithm. Moreover, for a deeper understanding of
the police occupational world, the first author interviewed
five patrol officers, who needed to have at
least 10 years of experience to make sure they could
deeply reflect on their work. The main questions
asked to data scientists were about the techniques
used in CAS to get in-depth, retrospective insight into
the development and reasoning behind CAS. After
one of these interviews, the first author sat with the
data scientist to have a close look at the learning algorithm
of CAS, which gave her a better understanding
of the methods used. Intelligence officers and police
managers were asked to describe their occupational
trajectory, their daily activities, and what role CAS
played in these activities to get an in-depth understanding
of the influence of the learning algorithm on
their everyday work. In addition, police managers
were asked about their views on the usefulness of
CAS for allocating police resources and crime prevention
to understand their motivation behind working
with the system. At the very end of the fieldwork
(April 2019), the first author conducted retrospective
interviews with two intelligence officers, where she
asked them to reconstruct how their work practices
and responsibilities changed from the introduction of
CAS in 2015 to their current role.
Finally, during the fieldwork, countless informal
conversations took place with all groups involved.
These informal conversations allowed the first author
to ask questions to solicit interpretations of specific
events or decisions. For retrospective details, we also
collected documentation data that were either internally
or externally available. These materials were
very valuable, as they gave us additional information
about the technical specifications of CAS (e.g., the
complete list of variables used) and insight into, for
example, the evaluations of the CAS implementation,
strategic plans, reasoning and expectations about role
transformations, and meeting details. We summarized
each of the data sources in Table 1.
Data Analysis
Throughout the data collection, we engaged in regular
conversations to reflect on observations, ask ourselves
what these meant, and link them to related literature.
The coding was performed by the first and second authors,
with the first author taking the lead and the second
author frequently checking in and adding input.
We began coding by reading field notes and interview
transcripts, adding potential codes in the margins.
This helped us to identify important themes. For example,
we were struck by how the intelligence officers
frequently referred to unexpected changes in their
work and role and remarks about their growing influence
on police managers. To trace how this growing
influence came about, we performed a temporal analysis
of our data, broadly mapping the changes. We
also noted the struggles of intelligence officers with
understanding and interpreting algorithmic predictions.
This triggered us to further scrutinize the nature
of algorithmic predictions and how this related to the
intelligence officers’ brokerage work.
We used open coding (parsing out the data to understand
the underlying dynamics) to conduct a more
formalized analysis of the field notes and transcripts
(Strauss and Corbin 1990). We initially focused on
specifying in detail the activities and interactions of
the three groups involved. We categorized the codes
by the occupational group to maintain oversight (i.e.,
“data scientists,” “intelligence officers,” and “police
managers”) and used these groups to construct a visual
map that portrayed how certain activities triggered
specific events (Langley 1999).3 We then engaged in
further rounds of axial coding, that is, unraveling
more thematic relationships and contrasts through
coding across concepts (Strauss and Corbin 1990), and
noticed that the intelligence officers’ efforts to understand
both the machine learning community and the
police community played a central role in how their
work changed over time. We compared and contrasted
the intelligence officers’ interactions with the learning
algorithm, the developers, and the associated algorithmic
predictions, as well as with the police community,
through which five key translation practices emerged:
(1) extracting, (2) examining, (3) transferring, (4) domesticating,
and (5) substituting (see Figure 2).
Using the literature on knowledge brokerage work
and translation theory helped us to better understand
what these five brokerage practices exemplified.
Based on theories on translation (Røvik 2016), we
grouped the practices “extracting” and “examining”
under the theoretical category “translating from (machine
learning community)” and the practices
“transferring,” “domesticating,” and “substituting”
under the theoretical category “translating to (user
community).” Together, these two theoretical categories
formed the basis for our understanding of algorithmic
brokerage work. This structure, and its associated practices,
also helped us to see how the algorithmic brokerage
work evolved through a cumulative process, in
which new types of practices built on earlier ones. In
this cumulative process, we identified three algorithmic
brokerage roles: (1) messenger, (2) interpreter, and (3)
curator. In what follows, we use these roles to explain
the cumulative efforts to translate algorithmic predictions
in practice.
Findings
After a two-year development period, in 2015, the
data scientists performed a test to see whether the
CAS could be nationally implemented. They deployed
the learning algorithm for several months in five large
Dutch cities, which was closely monitored by evaluators
from the Dutch police academy. After the test,
which was considered a success, the evaluators wrote
a report in which they indicated an occupational
group called “intelligence officers,” who emerged as
important actors who “supported police managers” at
local police stations by “being able to generate CAS
predictions” (internal document). The importance of
intelligence officers was surprising to the evaluators,
since, before the introduction of CAS, the work of intelligence
officers mainly involved supporting police
officers by searching the numerous police databases
when the police themselves did not have direct access
to them (e.g., finding crime numbers, suspect data, or
information about criminal networks). Intelligence officers
were “hidden” in a back office, the work was
generally regarded as low-status, the education level
required for the position was low—it did not require
one to be knowledgeable of technology or police
work—and it was considered to offer an opportunity
for those who “wanted to join the police without
wanting to work on the street” (intelligence officer
Louisa).
The evaluators, however, saw the potential benefits
of tasking intelligence officers, who were used to
working with police data, with translating algorithmic
predictions to make them meaningful for police work.
They ended their report with suggestions for a new
work process for contextualizing algorithmic predictions.
According to the evaluation report, the work
process should include three steps: actualizing, interpreting,
and explaining. Actualizing meant adjusting
predictions to local changes (e.g., when a burglar was
captured). Interpreting meant adding more information
to the crime predictions, such as the most-used
crime methods. Explaining meant deeply analyzing
why a crime is predicted (i.e., finding causal explanations
for the algorithmic predictions). The data science
team agreed with the suggestion of the evaluators and
gathered that intelligence officers could, for example,
contextualize a burglary prediction by adding information
about the kind of houses in the targeted area:
You need to have somebody [i.e., an intelligence officer]
who looks at the maps and thinks about the
causes of high risk and how to prevent them. How to
take the cause away so that you are not fighting the
symptoms but taking away the cause of the problem.
(Data scientist Dennis)
The intelligence officers were thus expected to find underlying
causes for predictions, but the data scientists
assumed that they did not need to understand how the
learning algorithm generated predictions to perform
their translation tasks and that access to police databases
would be enough. As one of the data scientists explained,
“Intelligence officers don’t have to interpret
model parameters or any kind of technical stuff; they
just get the maps.” The intelligence officers were thus
asked to fulfill brokerage work without full insight into
how algorithmic predictions were generated.
Below, we analyze the efforts of a group of intelligence
officers at one police station to translate crime
predictions for police managers and how they thereby
Figure 2. Conceptual Scheme



enacted three consecutive roles—namely, those of
messenger, translator, and curator. We discuss how
these efforts were hindered by the inability to understand
machine learning and how this eventually led
the intelligence officers to believe that the predictions
should be substituted by their own alternatives.
Algorithmic Broker Acting as Messenger
The main aim of intelligence officers’ work was to
make abstract algorithmic predictions meaningful for
local police managers. The predictions were available
to the intelligence officers by means of an interactive
map, where they could select the location, the crime
type, and the time frame. Because police managers
never looked at the map, they asked the intelligence
officers to generate a weekly overview of the predictions,
which could be used as input for scheduling
police resources. Generating such an overview was a
laborious task for the intelligence officers. For example,
they had to click on every time frame in a drop-down
menu,4 and since the system generated predictions for
four different crime types per police station, the intelligence
officers went through this cycle four times, selecting
a time frame in the drop-down menu a total of
168 times. When a prediction appeared on the map in
the form of a colored block, they translated the predictions
into words and added it to a Word document—
for example, “burglary, Monday, between 12:00 and
16:00, [street name].” Per crime type, the final list made
in Word included on average one predicted time frame
and one or two predicted areas a day.
Through this process of extracting predictions, a
comprehensive list of predictions was generated.
However, because the map did not offer any insights
into the causes, they had little clue about the meaning
of these predictions in the context of the police. Moreover,
since their new tasks caused them to be “in
search of their identity as intelligence officers and
sometimes didn’t know where their work ended” (intelligence
officer Wendy), their insecurity grew toward
the information needs of the police managers.
Afraid to leave out a prediction that might turn out to
be right, or add irrelevant information, the intelligence
officers decided to stick to comprehensive reporting
of all crime predictions. Better safe than sorry, the intelligence
officers gathered that transferring a full
overview of potential crimes would be best to support
police managers’ decision making and assumed that
“all police managers probably know what’s behind
the predictions” (intelligence officer Eva).
Even though it took the intelligence officers quite
some time and effort to construct exhaustive lists of
predictions, the police managers did not receive the
lists with much enthusiasm; the document was too
long, and the causes were unknown. For example, police
manager Rudy reflected that the long lists were
difficult to use because they lacked a specific focus:
“If you keep the [algorithmic predictions] too broad,
then we are quick to ignore them. I think the more
concrete you are, the more feeling we have for it.”
The data scientists also acknowledged that simply listing
crime predictions was not enough, because the
“quantitative” predictions needed “qualitative insights”
(data scientist Dennis). They emphasized the
need to “add color to” and “enrich” the crime predictions.
As Dennis explained:
Intelligence officers have to take the predictions and
enrich them with qualitative information. For example
[for burglary predictions], adding who could do it
or why burglaries might occur in that area or at that
time. Intelligence officers could say: “we have some
narcotics-related issues here, so maybe it could be
junkies?” Most of the time, junkies aren’t wellprepared
criminals, so maybe it’s just very easy for
them to burglarize that area. So maybe those houses
have very bad hinges and locks and you can just enter
them with a very easy trick. That’s the kind of
context the intelligence officers should provide.
In sum, confronted with a map that did not provide
any background, such as the causes of crime predictions,
together with largely unknown requirements
from the police community, intelligence officers initially
tried to determine whether the algorithmic predictions
would make sense to police managers if they
extracted them from the system and transferred them
as a list (see Table 2). By performing translation practices
in the form of “extracting” and “transferring,” the
intelligence officers enacted a brokerage role that can
best be described as a “messenger.” It soon became
clear, however, that the differences between algorithmic
predictions and the knowledge of police managers
were larger than the intelligence officers initially
expected. Both the police managers and the data scientists
criticized the efforts of the intelligence officers
and pushed them to deepen their knowledge brokerage
work by not just listing but further interpreting
the predictions. In other words, the intelligence officers
had to better decontextualize the algorithmic predictions
from the machine learning community in order
to contextualize them to the police.
Algorithmic Broker Acting as Interpreter
To translate algorithmic predictions to the police, the
intelligence officers realized they lacked a deep understanding
of the machine learning community and the
police community and invested in learning more
about both.
Learning About the Machine Learning Community.
The intelligence officers recognized that they had to
better understand the computational and statistical
techniques used in CAS. As intelligence officer
Richard reflected, “There are so many indicators that
CAS uses to make these calculations. And then CAS
turns a square red on the map. But why does it turn
that square red?” Consequently, the first step was to
interact with the data scientists to find out more about
their practices and to see if the causes of predictions
could be made transparent. They asked the data scientists
to create a tool that would make the decision logic
of crime predictions visible. The assumption was
that such a tool would make it possible for the intelligence
officers to trace how a crime prediction was calculated.
However, the data scientists insisted that “the
algorithm did not easily display why something was
predicted” (data scientist Jules) and that generating
the best predictions required complex techniques for
pattern recognition in vast amounts of data, which
made the learning algorithm opaque. As a consequence,
the data scientists claimed that pattern recognition
through machine learning, which combines
many different variables and theories, required “such
complex mathematical reasoning that it probably extends
beyond human reasoning.”5 Data scientist Dennis
further explained this belief as follows:
If you want to have the perfect set of selection rules,
it means that you have to study a lot of variances for
a long time. And this is the reason why [data scientists]
don’t do it in a common-sense way [using human
reasoning] because there are too many possible
variations. You have to do it by computer [using machine
learning].
To help the intelligence officers understand the data
science practices, the data scientists did explain the
techniques they used for developing CAS. For example,
they showed the variables that were included in
the learning algorithm. Such a list of variables still,
however, did not give insight into which variable was
considered most important for a given prediction and
for what reason, as this was determined by the learning
algorithm and unknown even to the data scientists.
These explanations therefore did not satisfy the
intelligence officers’ need to understand how the
crime predictions were generated and gradually they
gave up on their quest to gain deep insights into the
practices of the data scientists.
Dedicated to fulfilling their tasks as brokers, they
decided to leave the data scientist aside and started to
examine the predictions by inspecting the input they
had direct access to: the police data. As intelligence officer
Eva reflected, “How predictions come about
technically might be a guess but you can have a look
at the police data of past years and find quite some
reasons.” For example, to understand why burglaries
were often predicted in the morning, insight into how
the time frame of crime predictions was calculated
was needed, which triggered the intelligence officers
to dig into the police database and look for time
stamps in burglary reports. It appeared that, if a burglary
occurred in a period when people were away
from home, the report included a time frame (e.g.,
08:00 to 18:00) instead of one time stamp (e.g., 08:30).
So, they reasoned that the time the data scientists decided
to use was the so-called “starting time” of an incident
(in this case 08:00) instead of including the full
time frame.
Taking their assignment to create connections between
the machine learning community and the police
community seriously, the intelligence officers unsuccessfully
tried to share their findings from the police
data with the data scientists. For example, when they
suggested a different method for calculating time
frames, the data scientists maintained their belief in
the machine learning techniques they had applied and
said that this was the “only scientifically proven method”
for calculating time predictions (data scientists
Dennis and Mary). In another instance, when one of
the intelligence officers emailed the data scientists
to share that CAS generated predictions for car burglaries
in areas where cars were not permitted, data
scientist Dennis continued to believe in the CAS predictions
and answered that “it really was a parking
area.”
These interactions with the data scientists made the
intelligence officers realize there was a serious boundary
between machine learning and their human interpretations,
which blocked a mutual understanding
between them and the data scientists. According to
the intelligence officers, the data scientists were
“trying to develop better tools” (intelligence officer
Fred) but “did not understand what they [intelligence
officers] wanted” (intelligence officer Bart). They grew
more and more skeptical of how algorithmic predictions
were developed. As intelligence officer Wendy
remarked, “Data scientists don’t have a clue about police
work. CAS is just a tool with some kind of science
behind it. Well, if you reason like that, you don’t get
our reasoning.” Moreover, no matter how much effort
they put into examining the data to better understand
where the crime predictions came from, most of the
time they “just could not deduce from the data why a
prediction appeared” (intelligence officer Joey), which
was considered to be a serious bottleneck in performing
their work as knowledge brokers. As intelligence
officer Fred explained:
Understanding CAS is especially important for getting
to the final step, for putting the predictions in
the context of the police. If I know that the reason behind
a prediction is just that a lot of crimes happened
there in the past, then I can suggest that the police officers
drive around in that area so that they can prevent
the predicted crimes from coming true. If the
prediction appears because of demographic data,


something like that, then police officers have to take
another approach. Then they have to warn the residents
and make them prevent these crimes from happening
[e.g., by improving their locks].
The inability to fully comprehend the decision logic
of CAS had fundamental consequences for translating
predictions from the learning algorithm to the police.
To better understand how this was so influential, we
first turn to how the intelligence officers also put efforts
into better understanding the police community.
Learning About the User Community. Initially, the intelligence
officers also struggled with translating the
crime predictions to the police. To solve this issue,
they started to interact more directly with the police
to gain a better understanding of the police community.
By printing a crime prediction, sitting down with
police officers, and asking them to make sense of that
prediction from their occupational perspective (see
Figure 3), they learned that “more concrete” (police
manager Rudy) or contextualized predictions included
specific details of the area or of potential suspects.
For example, the police managers told the intelligence
officers that algorithmic predictions would start to
make sense to them if the intelligence officers “dared
to add suspects” (Rudy). To create these more contextualized
predictions, the intelligence officers relied
on police data; navigating the police databases and
reading police reports (e.g., DNA matches, burglary
reports, pictures of criminals sent to the police via
community WhatsApp groups). They also learned
from interacting with police managers that short and
action-oriented descriptions best fit the police community.
“We gave the police managers a couple of options
and asked for their opinion,” intelligence officer
Wendy reflected, “and eventually they said ‘give us
as little as possible.’”
Using their improved understanding of police
work, the intelligence officers changed the way they
handled crime predictions and started deleting, editing,
and interpreting them. The request for a concise
document triggered the intelligence officers to limit
the number of predictions they presented to five time
frames (from an average of 28) and two locations
(from an average of 56) and to delete all predictions
they thought did not make sense. For example, they
removed burglary predictions when no burglaries
happened the week before. Moreover, even though
they could not comprehend the decision logic of the
crime predictions, the intelligence officers tried to include
details that they could link to the predictions
without knowing the exact causes, such as area characteristics
(e.g., “rehabilitation center for ex-convicts
in the vicinity”), housing conditions (e.g., “mainly student
houses” or “outdated locks”), or even adding potential
suspects who had been criminally active in the
area before. Intelligence officer Ben summarized their
knowledge brokerage work as follows:
We add an interpretation to the algorithmic predictions
so police managers can do something with
them. In other words: “It is like this for these reasons.”
You can also give police managers advice, like: “I
would focus on this or that person,” or “I wouldn't
do anything about that type of crime because it’s way
too unpredictable.”
The police managers appreciated the new way of
domesticating algorithmic predictions and perceived
the brokerage work as more relevant and valuable.
They expressed, for example, that, thanks to the intelligence
officers’ interpretations, the algorithmic predictions
gave more “direction to their decisionmaking
work” (police manager Harry) and also recognized
the increased value of intelligence officers’ work
for “coordinating police work” (police manager
Rudy). Moreover, during the time that the intelligence
officers became more knowledgeable of police work
and the police managers started using the crime predictions
to inform their operational decisions, the police
managers observed an overall decline in the number
of high-impact crimes (e.g., burglary and car
theft). The decrease in the number of burglaries was
even so spectacular that the police station won a national
award called “Harm Alarm” for the largest burglary
reduction (minus 47% compared with the year
before). In their internal communication, the police
managers attributed this achievement largely to the
Figure 3. (Color online) Police Officer and Intelligence Officer
TogetherMaking Sense of a Prediction




Even though the declining crime numbers could
have reasons unrelated to the use of algorithmic predictions
(e.g., criminals being less interested in doing
“laborious” burglaries and moving toward cybercrime
instead), the police managers felt they had reasons to
believe that the use of algorithmic predictions was paying
off. Happy with the work of the intelligence officers,
the police managers decided to give more weight
to the brokerage work. They appointed the intelligence
officers as key figures for informing their operational
and strategic decisions by inviting them into their management
meetings. To “make crime predictions more
central” (police manager Harry), they scheduled about
20 minutes at the beginning of these meetings for intelligence
officers to present their advice.
In sum, to translate algorithmic predictions to the police,
the intelligence officers realized that they themselves
first had to better understand how these predictions
were generated and how police work was
performed. In their efforts to find out more about the
decision logic of crime predictions, they encountered
the opaque nature of learning algorithms, which solidified
a knowledge boundary between the machine learning
community and the intelligence officers. On the other
hand, due to the consistent interactions with the
police, the access to the police data, and the police managers’
increased belief in the value of crime predictions,
the knowledge differences between the intelligence officers
and the police community was slowly fading. This
allowed the intelligence officers to contextualize the algorithmic
predictions in such a way that they made
sense to the police managers (see Table 2). By performing
translation practices in the form of “examining”
and “domesticating,” the intelligence officers enacted a
knowledge brokerage role that can best be described as
an “interpreter.” However, even though their contextualizing
efforts seemed to work for the police managers,
the intelligence officers continued to struggle with understanding
the black-boxed machine learning.
Algorithmic Broker Acting as Curator
Now that the intelligence officers became used to their
ascribed expertise as algorithmic brokers, they
searched for ways to deal with the opaque algorithmic
predictions and discussed this with their head of department.
He suggested that the difference between
machine learning and their human interpretation was
in fact so large that it could not be overcome and that
they should therefore use their own expertise:
Intelligence work is not only about CAS. You can include
your input there as well. Human intelligence is
by definition smarter than algorithmic systems. (Head
of intelligence department Rick)
By now, the intelligence officers were so knowledgeable
of the police community that they felt confident
enough to leave CAS aside and focus only on
helping police managers to not be disturbed by
“useless” issues and emphasize the “really important”
ones (intelligence officer Richard). Moreover, a side effect
from their efforts to deduce details about machine
learning from police data were that they realized that
they used many more data sources in their knowledge
brokerage work than those included in CAS. Intelligence
officer Joey expressed a shared sentiment: “To
be honest, I trust CAS less than I trust the information
I can gather from the police databases.” They also became
increasingly vocal among each other about the
centrality of their work for guiding police managers.
For example, in one of their department meetings,
they agreed that intelligence work should not be
about “figuring out how systems work, but making
meaningful data combinations for police managers.”
Whereas data scientists believed that the intelligence
officers continued to make the crime predictions
meaningful to the police and helped police managers
to make their operational processes “smarter
and better” (data scientist Jules), in the meantime, the
intelligence officers substituted CAS with more explainable
solutions that supported their human judgments.
For example, the intelligence officers requested
that their local IT desk develop an archival and analysis
tool. This tool operated on Excel and used all data
sources the intelligence officers worked with previously
to make sense of algorithmic predictions. It did
not include a learning algorithm but was merely there
to help the intelligence officers store and add codes to
police reports, which facilitated quick and easy information
retrieval and analysis. Since the tool did not
use a learning algorithm, it was possible to scrutinize
the calculated patterns, which facilitated their knowledge
brokerage work. For example, they requested
that the tool include a new method for calculating
crime time frames, by using and visualizing a weighted
average of the time windows of past crimes. When
the intelligence officers compared the times calculated
by their tool with the times predicted by CAS, they
considered their “own” times “more explainable” (intelligence
officer Louisa). Their new tool only gave the
intelligence officers insights into past crime patterns
and had no predictive capacity for an example of CAS
predictions compared with outcomes of their new
tool, but the transparent and explainable nature of
their new tool helped them in to make substitutes that
they thought would best fit the police. As intelligence
officer Wendy reflected:
We already see the problem and then we go and
double-check it with CAS and say: “Oh, well, it supports
our judgment, we can point police managers’



attention there.” The problem is already clear, it’s already
evident, so we don’t need CAS that much
anymore.
Interestingly, whereas they pushed the learning
algorithm to the background and constructed explainable
alternatives that aligned with their human judgments,
only the intelligence officers themselves were
aware of this shift. Driven by police management’s
pushback to being disturbed by the complex technology
and pushed by their encouragements to come up
with “concise” predictions, for example, to “give them
as little as possible” (police manager Rudy), the intelligence
officers shielded the police managers from the
process through which they generated the substitutes.
“We should keep these choices away from police
management,” said the head of intelligence Rick during
one of their department meetings, “they just need
a clear recommendation; we shouldn’t bother them
with what kind of tools we used for it.”
This was also reinforced by the intelligence officers’
experiences during their presentations at management
meetings. During these presentations, police managers
did not pay attention to slide handouts or explanations
and were instead checking their phones. Yet,
they plainly followed the intelligence officers’ recommendations.
“We give our advice,” intelligence officer
Wendy reflected, “and most of the time the police
managers allocate police resources accordingly.”
Eventually, these occurrences during meetings made
them believe that the police managers took their advice
seriously without the need for any references,
and they decided to just offer the substitutes without
the need to “back up their suggestions to police managers
with numbers” (intelligence officer Aileen).
Wendy explained:
In the beginning, we had this whole document with a
long interpretation [of the algorithmic predictions].
Now, I only present the problem and our advice. Police
managers just don’t care at all what the numbers
look like.
In the end, the intelligence officers presented their
recommendations using just one slide, which only included
a direct and short piece of advice without its
source, such as: “Due to incidents with disorderly conduct
because of alcohol/narcotics use, the intelligence
department advises police management to conduct
alcohol/narcotics tests on traffic participants during
the nightly hours over the weekend. Mainly at locations
[anonymized].” Being able to substitute the crime
predictions with their own alternatives that were willingly
accepted by the police managers, the intelligence
officers felt they had grown more equal to them:
We are now considered more as a partner of police
managers. Before, we would usually wait for police
managers to give us a task. Now, it’s just: we are a
department and we have something to say too. And
we have good suggestions. That’s the difference. We
changed into an intelligence department having a seat
at the table. (Intelligence officer Wendy)
In sum, the intelligence officers eventually realized
that the boundary between machine learning and
their human interpretation of crime predictions was
impassable. As a consequence, they pushed back the
learning algorithm and substituted it with explainable
alternatives that aligned with their human judgments
and that they considered most suitable for the police
managers (see Table 2). As such, by performing translation
practices in the form of “substituting,” the intelligence
officers enacted a brokerage role that can be
best described as a “curator,” in which they grew to
become more influential and were eventually considered
more as a partner to the police managers.
Discussion
Building on the findings of our case, we offer a general
explanation of how algorithmic brokers translate
predictions to users (see Figure 4). In particular, we
observed how brokers perform translation practices
that allow them to enact increasingly influential algorithmic
brokerage roles. The brokerage work changes
over time, because when they attempt to translate algorithmic
predictions, knowledge differences emerge
between the brokers and the communities they intend
to connect. At the start of the brokerage work, brokers
lack sufficient understanding of the machine learning
community and the user community and cannot do
more than act as messengers. They do so by interacting
with the learning algorithm’s task outputs, extracting
and transferring predictions. This, however, leads
to failed attempts of decontextualizing the algorithmic
predictions from the machine learning community
and of contextualizing them to the user community.
To solve this, realization sets in that translation requires
deeper insights into both communities. This
means a move away from merely acting as a messenger
to an interpreter role, aiming to examine the algorithmic
predictions and domesticating them in the
user community. Although it is possible for the
brokers to reach a deeper understanding of the learning
algorithms’ task inputs, that is, the developers’
knowledge and the input data, the opaque nature of
machine learning prevents them from fully understanding
how algorithmic predictions are generated.
Because of this, the brokers experience an impassable
knowledge boundary between them and the machine
learning community, which triggers them to act as curators
and substitute the algorithmic predictions with
their own human judgments.
This study shows that bringing together the fields
of emerging technologies and organizational theory
allows for the emergence of a new phenomenon, that
of algorithmic brokerage work with its dynamic and
influential nature. More specifically, the current divide
between the two academic fields has resulted in a
scholarly understanding of knowledge brokerage in
which the need to comprehend the communities to be
able to translate has been taken more or less for
granted (Barley 1996, Vogel and Kaghan 2001, Meyer
2010). Our study shows that the recent rise of learning
algorithms with its black-boxed machine learning
brings to the fore the need for uniting the two fields.
Particularly, our case of knowledge brokerage in the
age of learning algorithms highlights the complex and
important practice of translating from the machine
learning community for knowledge brokerage. Studying
the translation practices of algorithmic predictions
reveals that knowledge brokerage work can become
increasingly influential, even to the extent that brokers
can eventually substitute the algorithmic predictions,
and gives us a better understanding of how and why
this growth in influence happens. Below, we offer the
key contributions of our study.
Creating New Knowledge Boundaries Through
Algorithmic Brokerage Work
One of the core findings of the research presented in
this paper is that algorithmic brokers enact different
translation practices over time and can thereby create
new knowledge boundaries between them and the
communities they intend to connect. This dynamic
perspective on brokerage work offers new insights
into the literature on knowledge brokerage and to
translation theory.
Previous studies argued that knowledge brokerage
tasks emerge when a semantic boundary hinders two
communities from sharing knowledge (Dougherty
1992, Carlile 2004, Boari and Riboldazzi 2014) and reasoned
that knowledge brokers could resolve boundaries
and align perspectives by enacting translation
practices (Tushman and Katz 1980, Barley 1996,
Wenger 1999, Grady and Pratt 2000, Paul and Whittam
2010). We contribute to the knowledge brokerage
literature by providing a more fine-grained and dynamic
perspective on how knowledge brokers enact
translation practices over time and in relation to
opaque algorithmic predictions. Building on Røvik
(2016) and based on our empirical findings, we
Figure 4. TheoreticalModel of Brokering Algorithmic Predictions


translate from the machine learning community, and
“transferring,” “domesticating,” and “substituting” as
practices to translate to the user community, which offers
a more refined insight into the complexity of brokerage
work.
For brokers to resolve a semantic boundary and to
translate knowledge, prior research has emphasized
the need to understand the communities involved
(Brown and Duguid 1998, Carlile 2004, Sturdy and
Wright 2011, Gal et al. 2020). Our research reveals
that, in the case of learning algorithms, such
“contextual bilingualism” (Røvik 2016, p. 299) cannot
be obtained, because gaining a deep understanding of
machine learning is impossible. Through the brokers’
translation practices, which are influenced by a lack of
understanding of how algorithmic predictions are
generated, a knowledge boundary solidified between
the machine learning community and the brokers. As
we mentioned earlier, most research on knowledge
brokerage focuses on the semantic boundary that
brokers should be able to resolve (Dougherty 1992,
Carlile 2004, Boari and Riboldazzi 2014). Our case
shows that, in the efforts to resolve a semantic boundary
through translation practices, knowledge boundaries
can solidify between knowledge brokers and the
communities they intend to connect. This added complexity
regarding knowledge boundaries uncovers an
additional understanding of knowledge brokers; by
translating knowledge, they can create their own
boundaries.
By unpacking the practices through which brokers
translate predictions from one community to another,
this study also emphasizes the dynamic and changing
nature of translation, which offers a contribution to
translation theory (e.g., Latour 1986, 2005; Law 2002;
Czarniawska and Sev´on 2005; Røvik 2016). Moreover,
whereas translation theory scholars have paid extensive
attention to how ideas are translated to specific
fields and organizations (e.g., Saka 2004, Mueller and
Whittle 2011, Nielsen et al. 2014, Ciuk and James
2015), only a few studies have focused on how knowledge
is translated from its original source (Furusten
1999, Suddaby and Greenwood 2001, Heusinkveld
and Benders 2005). These studies, so far, have not addressed
what translation entails if knowledge boundaries
are impassable, such as in the case of learning
algorithms.
Finally, by uncovering the emergence of an impassable
knowledge boundary between the algorithmic
brokers and the machine learning community, our
study points to the importance of including technology
in our understanding of knowledge sharing. Our
study points out that the practices commonly understood
to contribute to knowledge sharing (such as personal
interaction and shared activities) can become
counterproductive in the case of black-boxed machine
learning and can even lead to communities moving
further apart.
Becoming Influential Curators Through
Algorithmic Brokerage Work
Another core finding of this study is how, by performing
different translation practices, the knowledge
brokers enact roles that change to become more influential
over time. Especially the emergence of algorithmic
brokers as curators, acting as “kings in the land of
the blind” adds to our understanding of the work of
knowledge brokers as influential and consequential.
Research on knowledge brokerage has largely regarded
the work to be that of neutral intermediaries
dealing with the knowledge of others (Barley and
Bechky 1994, Barley 1996). However, to better understand
the influential nature of algorithmic brokerage
work, the analogy of art curators provides a useful
lens. Around the 16th century, with the materialization
of “cabinets of curiosity,” art curators emerged
and became responsible for taking care of works of art
and valuable objects. In that time, they were leveraging
the direct connection between artists and collectors.
The cabinets were closed to the public and
housed the private art collections of wealthy citizens.
Stemming from the Latin word cura, the art curators’
work at that time was to take care of art objects behind
closed doors and was not considered to have a recognizable
status. Interestingly, with the rise of public
museums, the curators’ caretaking practices triggered
the public to consider them as experts of art objects
(Teather 1990). Over time, art progressed into “too
many artists, too many movements, too many artworks
in too many shows, too much discussion”
(Balzer 2014, p. 65). The direct connection between artists
and collectors thus was vanishing, and knowledge
about art became increasingly abstract and difficult
to understand. Given their knowledge of art
sources, art curators stepped in as key figures in the
translation of art toward the wider public and were
usually blindly trusted by collectors. The story of art
curators is particularly helpful, because it reveals
how, through caretaking practices, the curators
changed from hidden caretakers to a highly influential
and independent occupation. The historical journey of
curators helps us to understand that, in contrast to
our previous understanding of knowledge brokerage
work as neutral, the algorithmic brokerage work in
our study becomes so influential that brokers can substitute
outputs with their own judgments.
It is interesting to note that, by using the analogy of
the historical trajectory of the work of art curators,
this study departs from the current interest in “data
curators” who are mainly considered to act as content
creators, data cleaners, or data editors (e.g., Karasti




“behind the scenes” of technology development
and is therefore usually invisible (Sachs 2020). For example,
Gray and Suri (2019) described how “ghost
workers” emerged because of the need to review the
content and quality of the data that is used for training
learning algorithms. As the current focus of curation
is mainly on the input of technology, our case of algorithmic
brokers who enact the role of curators shifts
this perspective toward the output of learning algorithms,
just like the output of art. This study therefore
emphasizes the need to acknowledge that algorithmic
brokers acting as curators can occupy a much more influential
role than what was previously assumed in
the invisible “ghost work” of data curators and to unpack
the consequences of curation for how algorithmic
predictions are (re)presented to users.
Practical Implications and Future Research
This study offers practical implications for domain experts,
managers, and technology developers engaged
in the development and implementation of emerging
technologies in organizations. In various fields and
parts of organizations, dealing with issues around explainability
of technology is becoming an important
topic. As we have seen so far, on the side of technology
developers and regulatory bodies these issues are
mainly assumed to reside in the “translating from”
side and technical solutions are offered (e.g., Doran
et al. 2017, Kirsch 2017, Lipton 2018, Preece et al. 2018,
Miller 2019, Mittelstadt et al. 2019, Robbins 2019, Barredo
et al. 2020). On the other hand, organizations are
generally interested in the “translating to” side when
confronted with issues of algorithmic (in)transparency
and push for more contextualization toward user
community without recognizing the need for explaining
how algorithmic predictions are generated (Henke
et al. 2018, Kellogg et al. 2020). Our study emphasizes,
however, that one cannot exist without the other,
which requires involving both the technology developers
and domain experts, for example, through mutual
reflection and adaptation already during the development
and implementation process (Zhang et al.
2020, Van den Broek et al. 2021). Involvement in terms
of understanding each other’s thought worlds requires
more long-term investments and new skills for
developers, brokers, and domain experts or users
(Waardenburg et al. 2021). For example, developers
need social skills to understand the user needs, domain
experts need technical skills to understand the
reasoning behind and limits of these technologies,
and algorithmic brokers need skills that allow them
to cope with the black-boxed machine learning without
feeling the need to completely substitute predictions
when they cannot understand how they are
generated. Developing such skills will provide a first
step to overcome the knowledge boundary between
the machine learning community and the user
community.
This study also shows that algorithmic brokerage
work does not neutrally and objectively represent algorithmic
predictions but likely includes the human
interpretations of the brokers. Whereas brokerage
work can be crucial for using learning algorithms in
practice, it needs clear demarcations through, for example,
regulation and close monitoring to prevent the
work from going beyond translating into substituting.
As Røvik (2016, p. 300) emphasized, “The more the
transfer process is regulated by authorities, the less
transformable the transferred construct is for the
translator.” Also, our case highlights data access as an
important resource that enables brokers to translate
algorithmic predictions to the user community. Yet,
whereas data access can offer transparency, this study
shows that unguided data access can also trigger
brokers to trust their own interpretations more than
algorithmic predictions and set aside the learning
algorithm.
In addition, it is worth noting several boundary
conditions of our study, which also open up opportunities
for further research. Our case shows that occupational
values matter for how desirable access to explanations
may be from the perspective of the user. In
our study, the users (i.e., police managers) did not feel
the need for explanations of algorithmic predictions
and blindly left the responsibilities of translating with
the intelligence officers. Although brevity and action
orientation are virtues in the police community, this
might be different in other communities, such as radiology,
where the decision-making practices of the
users might require as much evidence as possible
(e.g., Kim et al. 2021). We encourage future studies to
look at other communities to further understand the
differences in explanations required and to provide
insights into who or what is accountable in the age of
learning algorithms. Also, we presented a case of the
use of a learning algorithm within a highly hierarchical
and siloed organizational structure, which hindered
the interaction between the different communities.
Due to this hierarchical setting, the knowledge
brokerage work turned out to be largely onedirectional.
It would be interesting for advancing our
knowledge on algorithmic brokering to also include
more innovative or flat research settings, in which different
relationships exist between developers and
users (such as cocreation or agile technology development),
which opens possibilities to study more unidirectional
exchanges. Finally, our study focused on a
relatively basic and simple version of a learning algorithm,
which nevertheless had fundamental consequences
for work and organizing. With the emergence



of more advanced and even more opaque learning algorithms

further enlarged. We encourage future research to
continue to unpack algorithmic brokerage work to
provide deep insights into the organizational consequences
of emerging technologies that are increasingly
opaque.
Conclusion
Learning algorithms, because of the black-boxed
machine learning, offer an extreme case for understanding
how knowledge brokers enact translation
practices. In this study, we provided a case of knowledge
brokers who aimed to translate algorithmic predictions
from a machine learning community to a user
community. Translation has always been the core of
knowledge brokerage work, yet so far has been mainly
taken for granted in organizational literature. It is
now, in the age of learning algorithms, of significant
importance to question how knowledge brokers are
able to translate from a machine learning community,
since machine learning has become increasingly difficult
to understand. As this study shows, when the
outputs of one community are opaque to all actors
involved, brokers can become “kings in the land
of the blind” and decide to substitute algorithmic predictions
with their own judgments. The case of learning
algorithms therefore highlights that knowledge
brokers should not be considered as merely instrumental
in solving knowledge boundaries but even
more so as highly influential curators of knowledge.







