Wang, L., Huang, N., Hong, Y., Liu, L., Guo, X., & Chen, G. (2023). Voice‐based AI in call
center customer service: A natural field experiment. Production and Operations
Management, 32(4), 1002-1018.

Voice-based AI in Call Center Customer Service:
A Natural Field Experiment
Abstract
Voice-based artificial intelligence (AI) systems have been recently deployed to replace traditional
interactive voice response (IVR) systems in call center customer service. However, there is little
evidence that sheds light on how the implementation of AI systems impacts customer behavior, as well
as AI systems’ effects on call center customer service performance. By leveraging the proprietary data
obtained from a natural field experiment in a large telecommunication company, we examine how the
introduction of a voice-based AI system affects call length, customers’ demand for human service, and
customer complaints in call center customer service. We find that the implementation of the AI system
temporarily increases the duration of machine service and customers’ demand for human service;
however, it persistently reduces customer complaints. Furthermore, our results reveal interesting
heterogeneity in the effectiveness of the voice-based AI system. For relatively simple service requests,
the AI system reduces customer complaints for both experienced and inexperienced customers.
However, for complex requests, customers appear to learn from the prior experience of interacting with
the AI system, which leads to fewer complaints. Moreover, the AI-based system has a significantly
larger effect on reducing customer complaints for older and female customers as well as for customers
who have had extensive experience using the IVR system. Finally, we find that the speech-recognition
failures in customer-AI interactions lead to increases in customers’ demand for human service and
customer complaints. The results from this study provide implications for the implementation of an AI
system in call center operations.
Keywords: Artificial intelligence; customer service; service flexibility; natural field experiment;
difference-in-differences
Received: February 2021; accepted: June 2022 by Subodha Kumar after two revisions
1. Introduction
Advances in machine learning (ML) technology have accelerated the application of voice-based
artificial intelligence (AI) systems in various business functions, performing tasks such as speech
recognition and natural language processing.1 With the intention of improving customer experience as
well as reducing service costs, an increasing number of companies are deploying voice-based AI to
complement or replace current systems and services provided by human agents (Xiao and Kumar
2021). According to Markets and Markets (2021), the global conversational AI market size is predicted
to grow from $6.8 billion in 2021 to $18.4 billion by 2026, and AI-supported customer service is a
major factor driving the growth. Moreover, the value of global call center AI market reached 959.80
million in 2020 and is predicted to reach $ 9,949.61 Million by 2030 (Valuates Reports 2022).
Our study examines the implementation of a voice-based AI system that replaces the traditional
interactive voice response (IVR) system in a customer service call center. In the absence of the AI
system, customer calls are first connected to the IVR system and customers communicate with the IVR
system through phone keypads to obtain specific services. The service requests that the IVR system
cannot handle are then transferred to human agents. Upon the rollout of a voice-based AI system,
customers communicate with the AI in natural dialogues, and the AI system performs tasks, such as
processing natural language in a manner that resembles human intelligence. Note that the voice-based
AI system is different from the traditional IVR system in several ways, as summarized in Table 1. To
begin with, the AI system continuously evolves with the accumulation of a large amount of service data,
enhancement in computing power, and improvement in learning algorithms (LeCun et al. 2015). In
contrast, the IVR system was designed by industry experts based on their service experiences, does not
change with service data, and requires customers to strictly follow pre-set rules when interacting with
the system (Resnick and Virzi 1995). Through speech recognition in the AI system, customers can tell
the system which kinds of services they require; on this basis, they are directly routed to certain services
(Tang et al. 2003). In contrast, the IVR system typically relies on a hierarchical structure that directs
customers in a step-by-step manner to locate specific services (Suhm et al. 2002). To switch services,
customers are required to return to the main service menu and repeat the above-mentioned actions to
select another service.
Table 1. Voice-based AI System vs. IVR System
Voice-Based AI System IVR System
Inputs for Building the System Large amounts of service data Expert knowledge
Technological Characteristics
Natural language processing
Speech recognition
Improves with service data
Pre-designed services transferring rules
Remains the same as originally designed
Customer-System Interaction Interacting in natural dialogues Inputting specific information
Service Organization Direct routing Hierarchical structure
Considering the technological advantages of the voice-based AI system, the implementation of
such a system as a replacement for the traditional IVR system might significantly influence customer
service experiences. First, the AI system improves the flexibility of service flows and enables
personalized customer service. Instead of strictly following pre-defined service flows like in the IVR
system, customers can actively control the pace of service when they interact with an AI-based system.
For example, because of the AI system’s flexible navigation structure, customers can skip over the
layers of IVR structures and directly access the desired services. Second, the voice-based AI system can
adapt to customers’ interaction preferences to improve their service experiences. While the IVR system
provides highly structured and limited choices for customers, the AI system can interact with customers
in natural dialogues, allowing customers to express their needs adequately (Fountain et al. 2019).
Moreover, the voice-based AI system has the ability to learn from prior interactions with the input data
from customers and iteratively improve its performance. In scenarios wherein the AI system gets stuck,
it can tag the problems with the help of human agents and learn from the scenarios to resolve similar
problems in the future (Wilson and Daugherty 2018).
According to Forbes Insights, call centers are predicted to be the new sandbox for AI-powered
customer experience, as they are expected to deploy AI-based tools to boost retention, loyalty, and
profit.2 AI technology has gained widespread adoption in call center services over the past few years.3
Nonetheless, despite the growing interests in AI, its implementation has a
proof-of-concept-to-production gap (Perry 2021). In other words, AI can work well theoretically or on
test data, but it may fail to reach expectations in practical settings. In our study context, an IVR system
operates with pre-designed fixed logic, while an AI-based system relies on a complex algorithmic
structure. In turn, given the high variability of customer interactions in call center service, the service
efficacy of an AI system is likely to be subject to variation (Brynjolfsson and Mcafee 2017). For
example, customers may speak with accents or dialects while communicating with AI, thereby resulting
in speech-recognition failures that influence the effectiveness of the systems. However, without the
rigid logic of an IVR system, an AI-based system might learn to adapt to the customer interactions and
even be more effective than an IVR system. The high variability of tasks within customer service
requires more flexible, human-like responses, and the AI system may work better than the strictly
programmed IVR systems that are inflexible. Therefore, we believe it is important for researchers to
empirically test the effectiveness of AI in real-life settings.
Most prior studies on AI in operation management (OM) have mainly focused on the effects of
AI-supported automation and smartness, and examine how related technologies are deployed to
facilitate operation decisions or redesign operation process in product pricing (Karlinsky-Shichor and
Netzer 2019), order decision-making (Li and Li 2022), and quality management (Senoner et al. 2021).
Limited work explores the role of AI in interactions between customers and service systems,
particularly in service contact design scenarios (Roth and Mentor 2003). With few exceptions, for
example, Cui et al. (2021) examined the role of AI in buyer price requests and its influence on seller
price quotations in business-to-business (B2B) wholesaling. Contributing to this knowledge gap in
prior literature, our study focuses on the effects of implementing a voice-based AI system in the
business-to-consumer (B2C) customer service setting. Specifically, analyzing data from a natural field
experiment in a large telecommunication company’s customer service call center, we seek to answer the
following research questions:
How does the introduction of voice-based AI systems impact call length, customer’s
demand for human service, and customer complaints in call center customer services? How do the
effects of AI implementation in customer service vary for different customers?
To this end, we examine a natural field experiment4 with a voice-based AI system implemented
in a telecommunication company’s call center customer service operation. In the experiment, the
company’s customer service operation rolls out the AI system to replace the IVR system in different
phases, serving a portion of its customers based on the last digit of the customer’s phone number,
4 A natural field experiment (NFE) is the type of experiment ―where the environment is one where the subjects naturally
undertake these tasks and where the subjects do not know that they are participants in an experiment. Such an exercise
represents an approach that combines the most attractive elements of the lab and naturally-occurring data: randomization and
realism. (List 2007)
thereby allowing them to engage in customer service calls through natural dialogues with AI. We then
use difference-in-differences (DID) estimations to identify the effects of the AI-based system on key
outcomes. Our results reveal that the duration of machine service and customers’ demand for human
service increases temporarily after the introduction of the voice-based AI system, suggesting a possible
novelty effect. Meanwhile, the AI system significantly and persistently reduces customer complaints.
Moreover, we find interesting heterogeneity in the main effects of the AI system. To begin
with, the effects of the AI system on customer complaints appear to depend on the complexity of the
service requests. Compared with the customers who continue to use the traditional IVR system, the
customers assigned to use the AI-based system tend to make fewer complaints when they have
relatively simple service requests. In contrast, with relatively complex service requests (i.e., service
calls transferred to human agents), customers learn from their prior interactions with AI; this learning
effect leads to fewer complaints. Lastly, we find that the AI-based system exerts a significantly greater
effect on reducing customer complaints for older customers, female customers, and for customers with
longer user tenure.
Our study makes several important contributions to the related literature on AI applications and
call center operations. First, our study adds to research on AI applications by extending the scope to the
call center customer service setting and offers useful insights into how AI-powered service flexibility
impacts different outcomes in human-AI interactions (Luo et al. 2019, Sun et al. 2019, Cui et al. 2021).
Second, we contribute to the literature on call center customer service operations by empirically
examining how the implementation of the AI system affects customer behavior and the performance of
customer service, responding to calls for research on using disruptive technologies like AI to address
OM problems in general (Kumar et al. 2018, Karlinsky-Shichor and Netzer 2019) and to explore the
direct effects of technology-mediated customer-involved service contact designs in particular (Roth and
Menor 2003). In addition, building on related OM literature on call center operations, which views
customers as mostly homogenous and uses a single metric to represent the performance of operation
systems for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), our study further explores
customer heterogeneity in responding to the operations of the voice-based AI system.
Furthermore, our findings also offer useful implications for practice. We demonstrate that using
the voice-based AI system to replace the IVR system does not result in customer aversion to the AI
system, thereby validating the effectiveness of using voice-based AI systems in call center customer
service. We also demonstrate the novelty effect of implementing an AI system and find significant
heterogeneity in the effectiveness of voice-based AI systems on reducing customer complaints based on
the complexity of customer requests as well as customers’ age, gender, and tenure with the company
service, respectively. These results provide actionable insights into the implementation and further
development of voice-based AI systems. For example, companies must consider the possible short-term
increases in the duration of machine service and customers’ demand for human service while
scheduling for a service system that applies voice-based AI to replace the IVR system. Instead of
relying on customers’ self-learning, companies could educate their customers on using an AI system
with relatively complex requests. Moreover, the details obtained from customer-AI conversations
reveal that speech-recognition failures may lead to negative consequences. Therefore, it is necessary for
companies to continuously improve the capability of their AI systems to cater to a diverse customer
base.
2. Related Literature
2.1. Application of Artificial Intelligence (AI) Systems
Following prior literature, we define AI systems as algorithms that perform perceptual, cognitive, and
conversational functions typical of the human mind (Longoni et al. 2019). In recent years, the
significant development of AI systems has led to wide adoptions and applications in various domains.
Specifically, in the OM literature (see the summary of related literature in E-Companion A), from the
technical perspective, some prior work attempted to design AI-based algorithms to solve operational
problems such as demand or sales forecasting (Cui et al. 2018), product pricing (Yang et al. 2022), and
quality inferring (Senoner et al. 2021). Meanwhile, scholars have also explored how AI-enabled
automation and smartness features facilitate or support operational decisions in contexts such as price
request (Cui et al. 2021), order decision-making (Li and Li 2022), and automated pricing
(Karlinsky-Shichor and Netzer 2019).
Recently, a few studies on the application of AI systems have begun to understand the use of
such systems in commerce operations, where the AI system directly interact with individuals. For
example, Cui et al. (2021) examined how AI chatbots affect suppliers’ price quoting strategies. They
found that automation of chatbots alone leads to discrimination against chatbot buyers, but signaling the
use of a smart recommendation enabled by an AI system effectively reduces suppliers’ price quote for
chatbot buyers. In addition, Sun et al. (2019) suggested that the use of voice-based AI in online
shopping significantly affects consumers’ search behavior and purchase decisions. These prior studies
primarily focused on the role of AI systems in facilitating sales, but not much is known regarding the
application of voice-based AI systems in post-sales—that is, the scenario of customer service. In this
regard, our study aims at addressing this research void in the stream of work on AI system applications
by empirically examining the impact of implementing a voice-based AI system as a replacement for an
IVR system for customer service on the key outcomes related to customer experience and call center
operations.
2.2. Information Technology and Service Operation
Information technology plays an important role in improving service operations (Roth and Menor
2003). Companies increasingly rely on technology-based services to reduce service costs (Krishnan et
al. 1999) and increase service efficiency (Beckman and Sinha 2005). In recent years, with the
accumulation of large amounts of data on customers and transactions, companies have gradually
applied data-driven algorithms to automatically process service-related tasks—such as customer
segmentation, pattern identification, service instruction, and real-time personalization—which, in turn,
help companies to improve service quality (Sodhi et al. 2022, Sun et al. 2022).
Customers play an essential role in the delivery of services (Roth and Menor 2003). The design
of customer contact—the interaction between a customer and a service provider—is important for
shaping customers’ service experiences (Kellogg and Chase 1995). Prior research has empirically
examined the contact between customers and employees (Kellogg and Chase 1995, Soteriou and Chase
1998) and demonstrated that the physical service environment significantly influences customers’
perceptions and behavior (Bitner 1992). However, the advancement of information technology is
changing the ways in which customers interface with service providers. For example, companies
commonly establish self-service systems to cater to customers’ real-time service needs (Tezcan and
Behzad 2012). More recently, AI is being implemented to replace or complement conventional service
providers (Xiao and Kumar 2021). Froehle and Roth (2004) extended the customer contact perspective
to technology-mediated services and called for research on exploring the effects of virtual service
contact designs. Therefore, this paper focuses on the effects of different virtual service contact designs
(i.e., IVR and AI systems) in the context of call center customer service and specifically investigates
how replacing IVR systems with voice-based AI directly influences customers’ interaction outcomes.
2.3. Information Technology and Call Center Customer Service
Call center customer service has been an essential channel through which customers interact with firms
(Aksin et al. 2007, Tezcan and Behzad 2012). New developments in information technology provide an
opportunity to redesign and improve service-delivery operations in call centers. For example,
information technology supports a call center to expand to a larger scale (Adria and Chowdhury 2004).
In such contexts, researchers have examined the effects of call center centralization (Adria and
Chowdhury 2004) and discussed the risks caused by large-scale service systems (Pang and Whitt 2009).
Meanwhile, capacity management translates into a complex process in modern call centers. Researchers
have thus investigated the impacts of flexible labor resources (Kesavan et al. 2014) and attempted to
develop real-time schedule adjustment frameworks (Mehrotra et al. 2010). Another prevalent
technology-enabled change in call center operation is outsourcing; a wealth of research has explored
issues related to outsourcing (Kocaga et al. 2015) and call-routing (Gans and Zhou 2007) strategies in
such contexts. In the above studies, researchers have mainly focused on optimizing system designs in
contexts where technologies have been deployed to facilitate service operations (e.g., Aksin et al.
2007). However, little research has explored the effects of different technology-mediated contact
designs with customers directly involved in service delivery (Roth and Mentor 2003).
Specifically, in call center customer service, one typical technology-mediated service contact
design is the IVR system, which enables self-service at the front end of phone calls (Tezcan and Behzad
2012). Well-implemented IVR systems have the potential to automate a significant portion of services
and lead to improved customer service experiences (Tezcan and Behzad 2012). Thus, ample prior work
has examined the design of IVR-equipped service systems (Khudyakov et al. 2010, Suhm and Peterson
2002). Meanwhile, related studies from the user’s perspective reveal that customers often feel
when they interact with an IVR system because they perceive the services provided by IVR systems
be less customized and report that such systems occasionally do not understand their needs (Dean
2008). Consequently, customers often attempt to avoid IVR systems due to the lack of personalized
services or social interactions; instead, they seek direct interaction with human agents (Tezcan and
Behzad 2012).
Recent developments in AI technologies have enabled its applications in various contexts
(Brynjolfsson et al. 2019, Cui et al. 2021, Sodhi et al. 2022). For example, in 2017, Google’s machine
learning algorithms achieved a 95% accuracy rate for speech recognition in the English language, a
level that is close to actual human dialogue.5 In the customer service setting, voice-based AI systems
can understand customer needs through their voice inputs and can interact with customers in a
human-like manner (Van Doorn et al. 2017, Wilson and Daugherty 2018, Xiao and Kumar 2021).
However, considering the complexity of AI technology, it is challenging to determine the
effectiveness of AI systems in a real-world setting that goes beyond training data (Brynjolfsson and
Mcafee 2017). Once an AI system is deployed, it is expected to handle a large variety of situations
that may be unforeseen in training data. For example, customers may speak with accents or dialects
while communicating with AI, thereby resulting in speech-recognition failures that influence the
effectiveness of systems. Therefore, how AI implementation affects customer behaviors and the
performance of customer services remains an important empirical question that warrants further
investigation.
3. The Effects of the Implementation of AI on Call Center Customer Service
Based on the above discussions, in this section, we seek to discuss a few predictions on how the
implementation of a voice-based AI system in call center customer service will affect three key
metrics that are of interest to OM researchers: call length, demand for human service, and customer
complaints. While we do not provide any directional hypotheses in this section, the discussion serves
as a theoretical basis that guides our empirical analyses, which we report in subsequent sections.
5 Google’s ability to understand natural language is almost equivalent to that of humans.
Call length. Call length represents the duration of a customer’s service call (Gans et al. 2003),
which is important in the management of call center customer service operations because it directly
impacts scheduling and routing designs (Gans et al. 2003). In a traditional IVR system, the services
are organized in a tree-like hierarchical structure, whereby the leaves represent different services, the
nodes indicate customer states in the system, and the connections among different nodes indicate the
paths to specific services. All paths are pre-designed and customers can move only from one node to
another by inputting information in accordance with the guidance of the system. Typically, customers
must pass through several nodes before reaching certain services. Meanwhile, they must pay attention
to obtaining information on how to move from one node to another. An IVR system design typically
entails a time-consuming service experience. In contrast, with an AI-based service system, customers
can skip all the layers of IVR structures and directly access intended services by briefly summarizing
their needs to the system, which is likely to result in shorter call lengths, compared to a traditional
IVR system.
Conversely, it is also possible that an AI system leads to an increase in call lengths, as
compared to IVR systems, due to the characteristics of the speech-based interaction mode. To begin
with, when using the AI system, customers need to take time to summarize their needs in the form of
dialogues for the AI system to predict the intended services. Second, according to prior research on
communication modes, individuals interacting with text-based service systems (e.g., by inputting
numbers in the IVR systems) follow the cognitive economy principle, such that they are more likely
to focus on service requests and use keyword commands to improve communication efficiency (Le
Bigot et al. 2007). In contrast, the speech-based interaction mode enhances users’ involvement; users
tend to use quest-irrelevant expressions, such as politeness expressions in their interactions (Chafe
1982, Le Bigot et al. 2007), which could make information exchanges less effective (Le Bigot et al.
2007). In addition, when in conversational mode with an AI, users are expected to adapt their
behavior to the interaction system (Le Bigot et al. 2007, Cowan et al. 2015). Consequently, users
might devote more time and cognitive effort to formulate their speech and repeat information heard
during the interactions in order to share a common lexicon and syntactic structure with the interaction
system (Le Bigot et al. 2007). Therefore, speech-based AI service interactions may have longer
service durations than services handled by an IVR system. Based on the above discussions, it is
challenging to clearly predict the direction as well as the magnitude of changes in call length after the
introduction of the voice-based AI system; thus, the effect of the voice-based AI system (vs. IVR
system) remains an open question that warrants further empirical investigation.
Demand for human service. Customers’ demand for human service has direct implications
for staffing problems and operational costs within call center customer service (Tezcan and Behzad
2012). The introduction of AI may have mixed effects on customers’ demand for human service. On
the one hand, prior studies on AI applications demonstrate that, in certain contexts, individuals have a
subjective perception against AI and, thus, might be reluctant to interact with it even though AI now
offers high-level performance (Dietvorst et al. 2015; Longoni et al. 2019; Luo et al. 2019). When the
AI system offers the flexibility of transferring to human agents, customers may skip interacting with
AI and turn directly to human agents. Therefore, the AI-based system may increase customers’
demand for human service.
On the other hand, previous research also suggests that providing individuals with even a
slight amount of control over the AI’s behavior has the potential to mitigate their aversion to it
(Dietvorst et al. 2018), and this could be the case in our study. For example, the AI system enables
customers to control the pace of service and customers have the freedom to decide when to transfer to
human agents. Such a user-friendly design may mitigate customers’ aversion to interacting with AI
systems as well as mitigate any potential increase in customers’ demand for human service.
Considering both sides of the arguments, it is unclear whether and to what extent the implementation
of a voiced-based AI system would influence customers’ demand for human services; thus, we seek to
test this relationship empirically.
Customer complaints. Customer complaints are manifestations of customers’ negative
service experience (Singh 1988). Firms expend significant effort to improve customer service
experience and reduce customer complaints. According to the service operations literature, firms
create standardized service routines to control service delivery and ensure a uniform service level
(Leidner 1993). Standardized service routines reflect the preference of service providers with regard
to the manner in which customer needs must be met, with the process steps being organized in a
particular order. The service processes are largely determined by average customer demands and
preferences (Victorino et al. 2013). Since they follow service routines designed for an average
customer, service systems lack flexibility, cannot spontaneously react to unforeseen situations (Groth
et al. 2009), and are likely to overlook customer heterogeneities (Ashforth and Fried 1988). Dealing
with customer heterogeneities (e.g., request and preference variations) was a major challenge for
service operations (Frei 2006), and flexibility is one of the important capabilities in operational design
(De Groote 1994, Aksin et al. 2007).
A diverse environment and heterogenous needs are best fitted with flexible technology (de
Groote 1994). Specifically, improving the degree of flexibility in how service systems react to
customer requests enables the delivery of customized services (Tansik and Smith 1991), thereby
enabling an enhancement of customers’ service experiences (Roth et al. 2006). For example,
Victorino et al. (2013) reported that customers’ perceived lower service quality from dinner
recommendation services provided by an employee who rigidly follows the service script. In contrast,
customers gave high ratings to service interactions in which the employee offers the flexibility of
reacting to customer varieties (Victorino et al. 2013). In addition, Heim and Sinha (2002) showed that
the flexibility of the service process in electronic retailing is positively associated with customer
satisfaction. However, increased flexibility in a rule-based service system could potentially be
accompanied by an increased complexity of the system, which makes it more likely to result in
subjective service failures.
In the context of our study, the voice-based AI system accommodates customers’
communication preference heterogeneities by enabling customers to express their service needs in
ways that are most suitable for them. Meanwhile, compared with the IVR system, the AI system
enhances the flexibility of service flows so that customers can directly locate their intended services,
switch among different services, and transfer to human agents whenever they want. Therefore, we
expect the implementation of the voice-based AI system to enhance customers’ experience and reduce
customer complaints; moreover, we also seek to empirically evaluate this effect.
4. Background and Data
4.1. Natural Field Experiment
Our study considers a natural field experiment conducted by a large telecommunication company’s
call center customer service, which serves as an important channel for customer-firm interaction
(Aksin et al. 2007). The company was established in 1995 and now has 14 branches and over 8,000
employees, providing services to over three million customers in a major city (covering an area of
approximately 53,100 km2) in northeast China, with a market share of 33%. The company rolled out
its voice-based AI system in its call center customer service system based on the last digit of customer
phone numbers. Figure 1 summarizes the timeline of the natural field experiment. Before Dec. 19,
2018, all service calls were connected to the IVR system. From Dec. 19, 2018, the AI system was
implemented to replace the IVR system for a certain portion of the company’s customers, which was
chosen on the basis of a set of randomly selected last digits of the customer’s phone number.
Specifically, service calls from phone numbers with the last digit 1 or 7 were connected to AI, while
calls from other phone numbers remained connected to the IVR system. Between Dec. 19 and Dec.
31, 2018, the updated service system was in the beta-testing phase and was not connected to the
internal databases. Therefore, no service records were stored. Beginning on Jan. 1, 2019, the AI
system was connected to internal databases with phone records stored. Thereafter, beginning Jan. 10,
2019, in addition to service calls from phone numbers ending in 1 or 7, service calls from phone
numbers ending in 3, 5, or 9 were also connected to the AI system. After Jan. 15, 2019, the AI system
completely replaced the IVR system.
Figure 1. Timeline of the Natural Field Experiment
While interacting with the AI-based system, customers verbally state their requests briefly
and the AI system provides instant responses based on the analysis of information input by the
customers. If customers do not describe their requests clearly, the AI system asks specific questions to
guide customers to providing more information so that the AI can route them to the specific services
in order to meet their needs (e.g., payments, check balance, temporarily stop service). If the IVR
system or AI system is unable to provide specific services that customers are looking for, the
customers have the option to be transferred to human agents. When interacting with the IVR system,
customers need to strictly follow pre-designed service flows. After navigating the entire voice
guidance on possible services, the system tells customers to press a specific number to be transferred
to human agents. In contrast, the AI-based system sets no restrictions on when and how customers can
transfer to human agents. At the beginning of the service, the AI system tells customers ―If you need
help from human agents, please say ‘Transfer to human agents’.‖
4.2. Data & Measures
The implementation of the voice-based AI system in call center customer service provides exogenous
variations on the type of service system (voice-based AI vs. IVR system) that a customer experiences.
The observation duration in our study was 30 days, including a 21-day pre-treatment period (Nov. 28
to Dec. 18, 2018) and a 9-day treatment period (Jan. 1–9, 2019).6 We exclude the beta-testing phase
between Dec. 19 and Dec. 31, 2018, because we were unable to observe the outcome variables during
this phase.
Our data set contains timestamps of customers’ phone call records, such as the start and end
times of service calls, customers’ profile information, such as age and gender, when a customer began
using the telecommunication service, as well as the transcripts of customer-AI conversations from all
the customers served by the company. We constructed the variable Call Length to measure a service
call’s duration using the difference between a call’s start and end times. Considering the skewed
distribution of Call Length (Gans et al. 2003), we log-transformed this variable in our estimations. In
addition, we constructed the variable Human Service to capture whether a customer transferred to
human agents in the service call. Further, extending the related research on customer service
6 In this natural field experiment, we have a nine-day treatment period (between Jan. 1 and Jan. 9, 2019, when the customers
with the last digit of phone number 7 connected to AI system vs. those with the last digit of phone number 9 connected to
IVR system). Also, a three-week pre-treatment period allows us to check the parallel trend of our key outcome variables
before the experiment (from Nov. 28 to Dec. 18, 2018, when all service calls were connected to the IVR system). In
addition, the cooperating telecommunication company designs its services on a monthly basis (e.g., customers choose a
monthly service package, charge bills for the next month). Taken together, we chose a 30-day observation window.
experiences in OM (Aksin et al. 2007), we considered customer complaints as one typical
consequence of negative service experiences. Here, we constructed the variable Customer Complaint
to capture whether the customer complained about the service within 30 minutes after a service call.
Following prior research on technology acceptance (Venkatesh et al. 2012), we also included
a few additional variables on the observable individual characteristics to understand whether
individual differences (e.g., age, gender, experience) moderate users’ acceptance and use of the
voice-based AI system. Specifically, we observed user age and gender. In addition, we constructed the
variable Service Tenure to measure how many years a customer has been using his or her phone
number and consider it a proxy for the customer’s experience using the IVR system. Table 2 presents
a summary of the operationalization of the main variables.
Table 2. Variables and Definitions
Variables Definitions
Call Length The duration of a service call. It is measured by the difference between a call’s
end time and start time
Human Service Whether a customer chose to transfer to human agents, with Yes = 1 and No =
0
Customer
Complaint
Whether a customer complained about the service within 30 minutes, with Yes
= 1 and No = 0
Age The actual age of a customer calculated based on his/her birthday information
Gender Female = 1 and Male = 0
Service Tenure The number of years a customer has been using his/her phone number. It is
regarded as a proxy for the customer’s experience using the IVR system
5. Methodology & Results
5.1. Econometric Identification
First, we processed the data to ensure comparability between our treatment and control groups in the
company’s natural field experiment. In the experiment, the group assignment hinges on the last digit
of the customers’ phone number, instead of randomly assigning customers to either the treatment or
control group. To ensure comparability of observations, given the lack of individual random
assignment, we tried to balance our samples before data analyses. If certain individuals prefer an even
number as the last digit, it may result in significant differences between the two groups (i.e., even
numbers vs. odd numbers); this is also observed in our data (see E-Companion B). To address this
issue, we first excluded data from customers with phone numbers that end in even numbers (i.e., 0, 2,
4, 6, and 8). Further, we conducted a series of pairwise comparisons of the observable covariates (i.e.,
Age, Gender, Service Tenure) and the pre-treatment values of outcome variables by the last digit of
the customers’ phone numbers (see Table B1 in E-Companion B). We found no significant
differences in the observable covariates across the groups of customers whose phone numbers end
with 7 and 9, as indicated in Table 3. Table 4 reports the descriptive statistics of the main variables.
Table 3. Comparisons of Observable Covariates and Pre-Treatment Values of Outcome
Variables
Last Digit Age Gender
Service
Tenure
Log (Call
Length)
Human
Service
Customer
Complaint
7 43.579
(11.516)
0.639
(0.480)
8.507
(3.740)
4.453
(0.953)
0.320
(0.466)
0.014
(0.116)
9
43.954
(11.590)
0.648
(0.478)
8.650
(3.728)
4.470
(0.922)
0.316
(0.465)
0.012
(0.109)
p-value 0.466 0.993 0.207 0.345 0.658 0.455
Notes: Standard errors are given in parentheses. We observed insignificant differences on the observable
covariates and the pre-treatment values of the outcome variables, thereby suggesting comparability of the groups
with customers whose phone numbers end with 7 vs. 9.
Table 4. Descriptive Statistics
Variables Observations Mean SD Min Max Median
Log (Call Length) 18,580 4.446 0.869 2.996 7.365 4.407
Human Service 18,580 0.288 0.453 0 1 0
Customer Complaint 18,580 0.013 0.114 0 1 0
Age 18,580 43.765 11.554 15 70 43
Gender 18,580 0.643 0.479 0 1 1
Service Tenure 18,580 8.578 3.734 2 23 8
To provide additional model-free evidence for the treatment effect and comparability of the
two groups, we also show the pre-treatment parallel trends of the outcome variables in Figures C1–C3
in E-Companion C. Overall, the evidence indicates that customer groups with the last digit as 7 vs. 9
are comparable. Thus, we opt to use the customer groups whose phone numbers ended with 7
(treated) and 9 (control) to estimate the effects of the AI-based system on the outcome variables in our
main analyses.
Thereafter, we performed difference-in-differences (DID) analyses to estimate the effects of
the voice-based AI system on service call length, customers’ need for human service, and customer
complaints. We specify the DID estimations with Equations (1), (2), and (3). In addition, we also
incorporate customer-level random effects in these estimations.7
( )
. (1)
( )
( )
. (2)
7 In this paper, we selected random-effects models for two reasons. First, in our field experiment, randomization occurred
based on the last digit of the phone number. Thus, for the same customer, all his/her service records were either in the
treatment group or the control group. When conducting regressions with customer fixed effects, a few variables would have
been subsumed, like the variable AI_agent that indexes whether a record was in the treatment or control group, and dummies
that capture the features of specific customers (e.g., Age, Gender, Service Tenure) in the regression models. Second, for
dummy variables Human Service and Customer Complaint, the records of these variables with a value of 1 are relatively
sparse. If we chose fixed-effects models, many observation groups would have been omitted because of all negative
outcomes (i.e., Human Service = 0 or Customer Complaint = 0). In order to check the robustness of our results, we also
present the estimation results of fixed-effects models.
( )
( )
.
(3)
In the equations above, i denotes customers, and t denotes observation time; AI_agent is a
dummy variable, with 1 representing service calls from customers in the treatment group (i.e.,
customers whose phone numbers end with 7) and 0 representing service calls from customers in the
control group (i.e., customers whose phone numbers end with 9). After_AI is a dummy variable that
equals 1 for observations that took place after the introduction of the AI system and 0 for observations
on or prior to Dec. 18, 2018; Day Dummyt is a vector of time dummies representing each day during
our observational period; ui represents customer-specific random effects; and is the error term. In
Equation (1), the dependent variable is Log (Call Length). In Equations (2) and (3), we observed the
binary indicators of whether a call is transferred to human service and whether a call service
eventually received a customer complaint; we estimated these outcomes with logistic regressions. We
are interested in the coefficients of the interaction term, AI_agent * After_AI, as they capture the
effects of the AI system (compared with the IVR system) on the outcomes. For example, if the
coefficient in Equation (1) (i.e., ) is positive and statistically significant, it suggests that compared
with control customers—who used the IVR system and did not access the AI system—the treated
customers, who did use the AI system experienced longer average call length after the implementation
of the AI system.8
5.2. Main Findings
The regression estimations are presented in Tables 5 and 6, demonstrating the effects of the AI system
on Log (Call Length), Human Service, and Customer Complaint, respectively. To explore how the
durations of machine and human calls change after the implementation of the AI system, we separated
8 We also conducted a series of placebo tests to check whether the identified effects existed before the introduction of the AI
system or between customer groups who did not get access to the AI system. The results of the placebo tests are reported in
E-Companion D.
the duration of machine service (Machine_Call Length) from human service (Human_Call Length)9
and then conducted regressions. The results presented in Columns 1–4 in Table 5 suggest that the
implementation of voice-based AI significantly increases the duration of machine service by 5.65%
(i.e., 100 × ( −1) %) but exerts no effect on the duration of human service. These results indicate
that customers tend to spend more time interacting with the AI system as compared to the IVR
system. The results echo previous literature stating that, compared to mechanical self-service systems,
customers tend to engage more with natural language-based service robots (Huang and Rust 2021).
Specifically, diving into the records of customer-AI conversations, we found evidence for the wide
use of quest-irrelevant characteristics (e.g., the use of first- or second-person pronouns, politeness
expressions, and hesitation expressions), suggesting enhanced involvement during speech-based
interactions, which can lead to an increase in the service duration (Hauptmann and Rudnicky 1988, Le
Bigot et al. 2007).10 Meanwhile, per the results in Columns 5 and 6 in Table 5, we found inconclusive
evidence for the effects of the voice-based AI system on the total call length (i.e., Log (Call Length)).
Table 5. Effects of AI Implementation on Call Length
Variables (1) Log
(Machine_
Call
Length)
(2) Log
(Machine_
Call
Length)
(3) Log
(Human_C
all Length)
(4) Log
(Human_C
all Length
(5) Log
(Call
Length)
(6) Log
(Call
Length)
AI_agent 0.003
(0.018)
0.021
(0.071)
0.005
(0.022)
AI_agent * After_AI 0.055**
(0.021)
0.041*
(0.022)
0.082
(0.082)
0.012
(0.083)
0.050**
(0.023)
0.032
(0.023)
Age -0.003***
(0.001)
-0.027**
(0.003)
-0.006***
(0.001)
9 Machine_Call Length measures the duration of a machine (i.e., AI or IVR) service, while Human_Call Length captures the
duration of a service delivered by human agents.
10 We examined the records of customer-AI conversations to provide possible explanations for the increase in call length
(particularly in machine call length) and found that, in 28.8% of the conversations, customers use first- or second-person
pronouns (e.g., ―I‖, ―you‖), suggesting enhanced involvement during conversations. In addition, approximately 21.3% of the
conversations include at least one utterance to express politeness (e.g., ―Thank you‖) and 16.0% of the conversations contain
hesitation expressions (e.g., ―Uh‖) when customers form utterances during conversations. Consistent with research on
interaction mode, all these quest-irrelevant characteristics in speech-based interactions—although making the interaction
more natural—can also lead to an increase in the service duration (Hauptmann and Rudnicky 1988, Le Bigot et al. 2007).
Gender -0.024
(0.017)
-0.022
(0.065)
-0.027
(0.021)
Service Tenure -0.004*
(0.002)
-0.022**
(0.009)
-0.006**
(0.003)
Observations 18,580 18,580 18,580 18,580 18,580 18,580
Between R-square 0.080 0.061 0.045 0.006 0.043 0.013
Number of Customers 3,625 3,625 1,818 5,359 3,625 3,625
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 3 and 4, the
values of Human_Call Length are 0 for services successfully handled by the AI system or IVR system and we
calculated Log (Human_Call Length) = log (Human_Call Length + 1).
Table 6 reports the results from estimating Equations (2) and (3). Specifically, the estimates
presented in Columns 1 and 2 in Table 6 suggest that the introduction of a voice-based AI system
does not appear to exert a significant effect on customers’ demand for human service, even though it
is easier for customers to transfer to human agents when interacting with the voice-based AI system.
The above results provide null evidence on the possible negative consequence of implementing
voice-based AI in supporting customer service. When interacting with the AI system, customers can
choose to transfer to human agents at the beginning of the services, which may increase the workload
of human agents. However, we did not observe such an effect in our results. One possible explanation
for the results is that the AI-based service system in our research context enables customers to control
the service pace by transferring to human agents anytime they want. Giving customers the freedom to
control AI can reduce their aversion against AI (Dietvorst et al. 2018).
Table 6. Effects of AI Implementation on Human Service and Customer Complaint
Variables (1) Human
Service
(2) Human
Service
(3) Customer
Complaint
(4) Customer
Complaint
AI_agent 0.007 (0.083) 0.129 (0.369)
AI_agent * After_AI 0.103 (0.089) 0.036 (0.092) -1.037** (0.406) -1.101** (0.438)
Age -0.035*** (0.004) -0.069*** (0.017)
Gender -0.030 (0.079) 0.249 (0.358)
Service Tenure -0.025** (0.011) 0.117** (0.048)
Observations 18,580 10,621 18,169 959
Number of Customers 3,625 1,658 3,625 107
Day Dummies Y Y Y Y
Customer Random Effects Y - Y -
Customer Fixed Effects - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 2 and 4, some
observations were excluded when we conducted logistic regressions considering customer fixed effects.
We also considered the effects of AI implementation on Customer Complaint. In Columns 3
and 4 in Table 6, we observed that the AI system significantly reduces customers’ likelihood of filing
complaint reports. We also quantified the magnitude of this effect in accordance with Hosmer et al.
(2013). Specifically, compared with the average Customer Complaint before the implementation of AI
in the sample (M = 0.013), we estimated that the implementation of AI reduces the probability of
customer complaints to 0.005 (i.e., 0.013 * / (1 + 0.013 * )), with a decrease of 61.54%
in customer complaints. As an extension to the literature that examines the impacts of AI-enabled
features in operation management, such as automation (Cui et al. 2021, Li and Li 2022) and smartness
(Cui et al. 2021), our results reveal that the service flexibility (a reflection of AI smartness) enhanced
by the AI systems does, indeed, improve the overall service performance. In E-Companion E, we
replicated our main analysis and found similar results from the data from customers with phone
numbers ending in odd numbers.
5.3. Additional Analyses
In the additional analyses, we first explored the heterogeneity in our main results (Section 5.3.1). Prior
work has demonstrated that user characteristics play critical roles in affecting the performance of
technology designs (Venkatesh and Morris 2000, Venkatesh et al. 2012), and we thus tested the
moderating effects of customer characteristics (e.g., age, gender, and service tenure). Next, we dived
into the customer-AI conversations and considered the consequences of possible AI service failure
(Section 5.3.2). Since AI cannot work perfectly to handle all service tasks, we tried to understand how
customer-AI interaction and AI’s speech-recognition failures influence customer service outcomes.
In addition, prior studies have found that individuals are more likely to accept and use new
technologies when they get used to them (Taylor and Todd 1995). In our research context, a customer
can use the call service system several times during our observation period. Therefore, in
E-Companion F, we further analyzed whether the effects of AI implementation change as customers
accumulate experience in interacting with the AI system. That is, the learning effects in customer-AI
interactions. Our results suggest that, for relatively simple requests, the implementation of
voice-based AI directly increases machine service duration and reduces customer complaints. When
dealing with complex requests (service calls handled by human agents), the AI system only reduces
customer complaints for customers who are experienced in using the AI system.
Finally, one potential explanation for the observed effects of the voice-based AI system on the
outcomes of interests is the novelty effect. For example, customers may be unfamiliar with the AI
system when the system is first introduced in the call center. In such a scenario, they are more likely
to spend a longer amount of time interacting with the AI system or they may be more tolerant of the
services provided by the AI system. In order to understand the possible novelty effect, in
E-Companion G, we re-estimated our regression equations in the main analysis by considering or
eliminating records on the voice-based AI system’s first- and second-time services for each customer.
The results suggest that the implementation of the AI system persistently reduces customer
complaints. However, possible novel effects of the AI system during the period of its early
introduction indicate that the duration of machine service and customer demand for human service
increases only temporarily after the introduction of the AI system and these effects are not significant
in the long term.
5.3.1. Heterogeneity by Customer Characteristics
After estimating the main effects of the AI system, we further examined how customer-level
covariates—including age, gender, and experience of using the IVR system—moderate the effects of
the voice-based AI system. The variable Service Tenure measures the number of years that a customer
has been using his/her phone number and we used it as a proxy for customers’ experience using the
IVR system. In terms of continuous variables, including Age and Service Tenure, we first
mean-centered these variables before constructing the interaction terms. Table 7 presents the results of
the moderation analyses.
As indicated in Panel A of Table 7, we found that Age moderates the effects of the AI system.
Specifically, older (vs. younger) customers may benefit more from the dialogue-based services
supported by AI, such that after the implementation of voice-based AI system, they spend less time on
call services, have less demand for human service, and register fewer complaints. In line with Meuter
et al. (2005), who found that older customers are not proficient at using traditional IVR systems and
thus are more reluctant to interact with these systems. Consequently, the convenience and flexibility
enabled by the voice-based AI system are more helpful for improving the service experience of older
customers. With regard to the moderating role of Gender in Panel B, the AI-based system is more
effective in reducing complaints from female customers (i.e., Gender = 0). Studies on the use of
self-service technology indicated that females are strongly influenced by their perceptions of ease of
use of technologies (Venkatesh and Morris 2000). Therefore, they experience a significant
improvement in AI-supported flexible services. Furthermore, we find that customers’ experience of
using the IVR system, Service Tenure, moderates the effect of the AI system on Customer Complaint
(Panel C). For customers who have more (vs. less) experience using the IVR system, the
implementation of the AI system has a greater effect in reducing their complaints. One possible
explanation for this is that experienced users are more familiar with the drawbacks of the IVR system
and more likely to appreciate the benefits of the AI system, thereby tending to have fewer complaints.
As an extension of prior literature that assumes service systems have the same service performance
for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), our results indicate that the
effects of AI-based systems vary in terms of customer gender, age, and service tenure.
Table 7. Heterogeneity by Customer Characteristics
Panel A.
Moderation by Age
(1)
Log (Call
Length)
(2) Log
(Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.004
(0.022)
0.009
(0.084)
0.332
(0.398)
AI_agent * After_AI 0.053**
(0.023)
0.035
(0.023)
0.084
(0.090)
0.003
(0.093)
-1.441***
(0.443)
-1.613***
(0.486)
AI_agent * After_AI * Age -0.004*
(0.002)
-0.004**
(0.002)
-0.014*
(0.008)
-0.022**
(0.009)
-0.109***
(0.041)
-0.146***
(0.047)
Age -0.009***
(0.001)
-0.036***
(0.005)
-0.104***
(0.027)
AI_agent * Age 0.001
(0.002)
0.000
(0.007)
0.058
(0.036)
After_AI * Age 0.008***
(0.001)
0.008***
(0.001)
0.008
(0.006)
0.010
(0.006)
0.055**
(0.028)
0.060**
(0.029)
Gender -0.027
(0.021)
-0.031
(0.079)
0.250
(0.364)
Service Tenure -0.006**
(0.003)
-0.025
(0.011)
0.119**
(0.049)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Panel B.
Moderation by Gender
(1)
Log (Call
Length)
(2) Log
(Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.005
(0.037)
-0.028
(0.140)
0.657
(0.673)
AI_agent * After_AI 0.038
(0.039)
0.015
(0.039)
0.053
(0.152)
-0.057
(0.156)
-2.669***
(0.904)
-2.873***
(0.990)
AI_agent * After_AI *
Gender
0.017
(0.478)
0.025
(0.049)
0.075
(0.188)
0.143
(0.193)
2.118**
(1.015)
2.306**
(1.116)
Age -0.006***
(0.001)
-0.035***
(0.004)
-0.069***
(0.017)
Gender -0.043
(0.033)
-0.054
(0.124)
0.599
(0.587)
AI_agent * Gender 0.001
(0.046)
0.056
(0.174)
-0.738
(0.809)
After_AI * Gender 0.031
(0.034)
0.025
(0.034)
-0.048
(0.133)
-0.070
(0.138)
-0.634
(0.584)
-0.700
(0.629)
Service Tenure -0.006**
(0.003)
-0.025**
(0.011)
0.118**
(0.049)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Panel C. Moderation by
Service Tenure
(1)
Log (Call
Length)
(2)
Log (Call
Length)
(3)
Human
Service
(4)
Human
Service
(5)
Customer
Complaint
(6)
Customer
Complaint
AI_agent 0.005
(0.022)
0.002
(0.083)
0.117
(0.402)
AI_agent * After_AI 0.050**
(0.023)
0.032
(0.023)
0.111
(0.090)
-0.049
(0.092)
-0.991**
(0.423)
0.998**
(0.445)
AI_agent * After_AI *
Tenure
-0.007
(0.006)
-0.006
(0.006)
0.033
(0.025)
0.036
(0.025)
-0.257**
(0.110)
-0.217*
(0.116)
Age -0.006***
(0.001)
-0.035***
(0.004)
-0.073***
(0.019)
Gender -0.027
(0.021)
-0.030
(0.079)
0.246
(0.390)
Service Tenure -0.015***
(0.004)
-0.037**
(0.017)
0.023
(0.082)
AI_agent * Service Tenure 0.002 -0.010
(0.023)
0.124
(0.108)
(0.006)
After_AI * Service Tenure 0.021***
(0.004)
0.020***
(0.004)
0.026
(0.018)
0.025
(0.018)
0.198***
(0.075)
0.170**
(0.078)
Observations 18,580 18,580 18,580 10,621 18,169 959
Number of Customers 3,625 3,625 3,625 1,658 3,625 107
Day Dummies Y Y Y Y Y Y
Customer Random Effects Y - Y - Y -
Customer Fixed Effects - Y - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 4 and 6, some
observations were excluded when we conducted Logistic regressions considering customer fixed effects.
5.3.2. Speech-Recognition Failures in Customer-AI Interactions
We analyzed the transcripts of customer-AI conversations to examine how AI’s speech recognition
failures in customer-AI interactions may affect customers’ demand for human service and customer
complaints. To this end, we calculated the variable Failure_Count to measure the number of times
that the AI system failed to recognize a customer’s intention during a service call by counting the
number of times the AI system repeated the same question. On average, during our observational
window, approximately 28.5% of the customer-AI system service sessions involved
speech-recognition failures. We also measured Conversation_Count to quantify the rounds of
interaction between the AI system and a customer during a service call. Considering the skewed
distributions of the variables Failure_Count and Conversation_Count, we used the log-transformed
values of these variables in our regression estimations.11 Table 8 presents the results pertaining to
speech recognition failures of AI. The results suggest that Log(Conversation_Count) is negatively
related to Human Service and Customer Complaint. One possible explanation is that, for service
requests that can be handled by the AI system, customers tend to have more interaction rounds with
the AI system and are less likely to turn to human agents and complain about the service. Meanwhile,
Log(Failure_Count) is positively related to Human Service and Customer Complaint, thereby
indicating that speech-recognition failures in customer-AI conversations can lead to significant and
11 In cases when variables include 0, we added 1 before the logarithm transformation. For example, we calculated Log
(Failure_Count) = log (Failure_Count+1).
negative effects on call center service performance by increasing customers’ demand for human
service, thus leading to more customer complaints.
Table 8. Effects of Details in Customer-AI Interactions
Variables (1) Human
Service
(2) Human
Service
(3) Customer
Complaint
(4) Customer
Complaint
Log (Conversation_Count) -3.858*** (0.096) -1.971*** (0.104) -3.347*** (0.608) -1.122** (0.461)
Log (Failure_Count) 0.966*** (0.090) 0.242* (0.128) 1.610*** (0.594) 0.213 (0.585)
Age -0.033*** (0.003) -0.027 (0.017)
Gender -0.122* (0.067) 0.136 (0.388)
Service Tenure -0.042*** (0.009) -0.057 (0.052)
Observations 17,274 6,950 17,274 366
Number of Customers 9,042 1,880 9,042 78
Day Dummies Y Y Y Y
Customer Random Effects Y - Y -
Customer Fixed Effects - Y - Y
Notes: Standard errors are given in parentheses. ***p < 0.01, **p < 0.05, and *p < 0.1. In Columns 2 and 4,
most observations were excluded when we conducted logistic regressions considering customer fixed effects. In
Column 4, the coefficient of Log (Failure_Count) is not significant and one possible reason for the result may
be that some observations were excluded when we conducted Logistic regressions considering customer fixed
effects.
6. Discussion
6.1. Key Findings
This study investigates how the implementation of a voice-based AI system in call center customer
services affects customer behavior and call center performance. The results reveal several interesting
findings. First, we find that the voice-based AI system temporarily increases the duration of machine
service and customers’ demand for human service when the system is first introduced to customers,
but these effects were not significant after the customers gained experience with the AI system.
Second, the effects of the AI system on customer complaints vary in accordance with the complexity
of the customers’ service requests and the customers’ experience of using the AI system. Specifically,
the AI-system effectively reduces customer complaints for both experienced and inexperienced
customers when customers have relatively simple requests. In contrast, with regard to complex
requests, the AI system improves customers’ service experience only after they accrue sufficient
experience interacting with the AI system. Third, we explore how customer characteristics moderate
the effects of the AI system. The results reveal that the AI system is more helpful in reducing
complaints for older customers, female customers, and customers who are experienced in using the
IVR system.
6.2. Theoretical and Practical Implications
Our study contributes to the related literature on the application of AI systems and call center
customer service operations. To begin with, this work extends the literature on AI applications by
improving the understanding of the effects of AI systems in the customer service setting. Previous
studies have either focused on deploying AI-based algorithms to support or optimize operational
processes from the technical perspective (Senoner et al. 2021, Sun et al. 2022, Yang et al. 2022) or
examined the effects of different AI-enabled features in contexts such as price request (Cui et al.
2021), order decision-making (Li and Li 2022), and automated pricing (Karlinsky-Shichor and Netzer
2019). These studies mainly investigated certain advantages and drawbacks of AI-enabled automation
(Karlinsky-Shichor and Netzer 2019, Li and Li 2022, Cui et al. 2021) or smartness (Cui et al. 2021).
As an extension, our study explores the effectiveness of AI-enabled service flexibility—a specific
reflection of AI smartness—in call center customer service. Technology-based self-service systems
(e.g., ATMs) have already been demonstrated to work well in dealing with highly structured service
tasks (Barua et al. 1991), while our study suggests that AI-enabled service flexibility is more likely to
improve the service effectiveness when dealing with tasks with high variability (e.g., call center
services). Thus, IT investment decisions in service operations should match the features of both
service tasks and technologies.
In addition, prior studies have indicated that the realization of AI’s value in the context of
human-AI interaction crucially depends on institutional settings and the role that customers play in
using AI applications (Dietvorst et al. 2018, Luo et al. 2019). Often, customers are reluctant to interact
with AI when they passively receive AI-supported marketing information (Luo et al. 2019),
forecasting results (Dietvorst et al. 2015), or medical care (Longoni et al. 2019). However, our study
suggests that when customers have the freedom to control the flow and direction of the service, the
AI-based service system does not result in a significant increase in demand for human service, even
though the AI system allows customers to transfer to human agents at any time during the interaction.
Furthermore, our results offer preliminary insight into the negative effects of speech-recognition
failures on customer-AI system interactions, thereby enriching research on imperfect AI (Dietvorst et
al. 2015, 2018).
Our study also contributes to the literature on call center customer service operations by
investigating how AI technologies impact customer behavior and the performance of call center
customer service. Previous research has examined changes in call center customer service operations
elicited by technological advances, such as call center centralization (Adria and Chowdhury 2004),
flexible resource management (Kesavan et al. 2014), and outsourcing (Kocaga et al. 2015), but
limited attention has been paid to exploring the effects of the contact designs of different
technology-mediated services with direct customer involvement (Roth and Mentor 2003, Froehle and
Roth 2004). Moreover, the OM literature mainly focuses on measuring the performance of call center
customer services from the firm’s perspective, using easily trackable metrics, such as operational
costs (Tezcan and Behzad 2012) and wait time (Khudyakov et al. 2010, Singhal et al. 2019). There is
limited research on customers’ service experience (Aksin et al. 2007). Through the customers’
perspective, our study examines the effects of an AI-based service system on customer complaints,
which is a key consequence of customers’ negative service experiences. We find that customers tend
to make fewer complaints when served by the voice-based AI system. In addition, enriching prior
studies that treat customers as homogeneous and use a single metric to represent the performance of
service systems for all customers (Khudyakov et al. 2010, Tezcan and Behzad 2012), this study
further examines how the effects of the AI-based system vary in accordance with customers
characteristics, such as age, gender, and experience in using the traditional IVR system.
Furthermore, our findings also have important practical implications. First, we find that the
implementation of the voice-based AI system in call center customer services helps improve customer
service experiences (i.e., reduces customer complaints) and that the flexibility of transferring to
human agents, enabled by the AI system, does not lead to a significant increase in customers’ demand
for human service in the long term. These findings showcase the value of voice-based AI systems in
the provision of customer service. Thus, companies can continue implementing AI systems to support
customer services. Second, our findings also shed light on bridging the
proof-of-concept-to-production gap (Perry 2021). We find that the effectiveness of the AI system is
closely dependent on the service tasks (e.g., the complexity of customers’ service requests) and
customers’ experience of using AI systems. As predicted, the implementation of the AI system
directly improves customers’ service experience in relatively simple service tasks. In terms of
handling complex requests, the voice-based AI system only operates effectively to reduce complaints
from customers who have gained enough knowledge about the interacted AI system. Thus, users may
suffer the proof-of-concept-to-production gap in the early stages of adoption, particularly when
dealing with complex tasks (Sodhi et al. 2022). Correspondingly, customer service operations that are
equipped with AI systems can initially distinguish simple customer requests from complex ones based
on historical service records and then encourage customers with simple requests to use AI-assisted
services. Platforms can consider guiding customers to establish appropriate expectations of the AI
system and transfer customers with complex requests to human agents as quickly as possible. Third,
our results indicate the possible novelty effect of a newly implemented AI system. We find that the
duration of machine service and customers’ need for human service temporally increases after the
introduction of the AI system. Companies can take these effects into account when scheduling
resources for their newly implemented AI-based service systems. Moreover, in our study, suggestive
evidence from customer-AI conversations reveals that customers are more likely to turn to human
agents and complain about services after experiencing speech-recognition failures. This is likely not a
huge concern in the longer run, as AI-based services are likely to improve in terms of speech
recognition, given their learning capabilities.
6.3. Limitations and Future Research
Our study has several limitations, which also indicate ample opportunities for future research. First,
our experimental randomization is based on the last digit of customers’ phone numbers, rather than
being performed at the individual level, and thus, we balanced our samples before data analysis.
Second, due to data limitations, we could not observe detailed records of specific service requests
handled by the IVR system; therefore, we were unable to categorize service requests based on
objective service types. It will be interesting for future researchers to extend our findings based on the
objective complexity of customer service requests. Third, the current study focuses on the effects of a
voice-based AI system on call length, customers’ demand for human service, and customer
complaints. Future research could explore other outcome variables, such as service satisfaction,
customer retention, and future customer engagement, which reflect the value of AI implementation for
businesses. Moreover, leveraging the transcripts of customer-AI conversations, we conducted a few
preliminary analyses on customer-AI interactions by examining the negative effects of
speech-recognition failures. It would be interesting for future research to explore other factors in
human-AI interactions, such as emotions expressed by AI and service tones used by AI in
conversations, which inform the design of voice-based AI systems. Lastly, we analyze the
effectiveness of deploying a voice-based AI system to replace the traditional IVR system in the
telecommunication customer service setting in China. The generalizability of our findings might be
subject to the technical designs of the AI and IVR systems, cultural variation, as well as differences in
levels of technology development among countries, all of which may affect users’ attitude to and
adoption of AI. Thus, we encourage future research to further explore the implications of voice-based
AI systems among different user populations or in other service settings.







----------------------------------------------------------------------------------







Lu, T., & Zhang, Y. (2024). 1+ 1> 2? information, humans, and machines. Information
Systems Research.

AbstractWith the explosive growth of data and the rapid rise of artificial intelligence (AI) and automated working processes, humans
inevitably fall into increasingly close collaboration with machines, either asemployees or consumers. Problems in human-machine
interaction arise as a consequence, not to mention the dilemmas posed by the need to manage information on ever-expanding
scales. Considering the general superiority of machines in this latter respect, compared to human performance, it is essential to
explore whether human–machine collaboration is valuable, and if so, why. Recent studies have proposed diverse explanation
methods to uncover machine learning algorithms’ “black boxes,” aiming to reduce human resistance and enhance efficiency.However,
the findings of this literature stream have been inconclusive. Little is known about the influential factors involved or the
rationale behind their impacts on human decision processes.We aimed to tackle the above issues in the present study by specifically
examining the joint impact of information complexity and machine explanations. Specifically, we cooperated with a large Asian
microloan company to conduct a two-stage field experiment. Drawing upon studies in dual-process theories of reasoning that have
proposed different conditions necessary to arouse humans’ active information processing and systematic thinking,we tailored the
treatments to vary the level of information complexity, the presence of collaboration, and the availability of machine explanations.
We observed that with large volumes of information and with machine explanations alone, human evaluators could not add extra
value to the final collaborative outcomes. However, when extensive information was coupled with machine explanations, human
involvement significantly reduced the default rate compared with machine-only decisions.We disentangled the underlying mechanisms
with three-step empirical analyses.We revealed that the co-existence of large-scale information and machine explanations
can invoke humans’ active rethinking, which in turn, shrinks gender gaps and increases prediction accuracy. In particular, we
demonstrated that humans could spontaneously associate newly emerging features with others that had been overlooked but had
the potential to correct the machine’s mistakes. This capacity not only underscores the necessity of human-machine collaboration
but also offers insights into system designs. Our experiments and empirical findings provide non-trivial implications that are
both theoretical and practical.

1. Introduction
Given the fast rate of artificial intelligence (AI) commercialization and its penetration into daily life, humans have
started to closely collaborate with machines, both as employees and consumers (Alibaba 2018,Wang et al. 2023a).
For example, many companies have introduced AI-based coaching systems to assist humans and improve their
decision-making effectiveness and efficiency (Loutfi 2019). In reality, humans and machines can complement each
other. Previous research has found that the decision-making accuracy of machine-learning algorithms is generally
higher than that of humans under normal circumstances (Grove et al. 2000). However, humans are more likely to
use experience to identify and process low-frequency cases that are difficult to include in machine-learning algorithms;
humans also have more advantages than machines in terms of flexibility (Sawyer 1966).More importantly,
humans’ deep thinking is awell-established andwell-understood tool for augmenting performance on independent
or team tasks (Amit and Sagiv 2013).
Unfortunately, there are various constraints, such as information opacity, machine-learning algorithms’ complexity,
and personnel’s lack of experience with or understanding of advanced technologies. Accordingly, the realized
performance of human-machine collaborations falls short of the expectation due to distrust of machines
(Jacovi et al. 2021) or over-reliance on them (F¨ ugener et al. 2021). Even worse, without properly designed collaboration
systems, humans’ involvement could reduce the collaborative performance for various reasons, such as their
being over-cautious (Lu et al. 2023b) or hyper-focused on details (Wang et al. 2023c).
To address the urgent, essential question regarding how to efficiently change humans’ responses to machines
from either aversion or over-reliance to active contribution, researchers have recently begun to turn to machinelearning
model explanations (Schmidt et al. 2020, Bauer et al. 2023). However, previous investigations in this vein
have predominantly concentrated on technical solutions and lacked a comprehensive examination of the conditions
and underlying mechanisms that influence the solutions’ impact on human decision processes. This omission
introduces certain limitations, as not all model explanations prove effective in every scenario (Chen et al. 2023).
In this study,emphasis is placed on task complexity, particularly information complexity, a contingent factor that
plays a pivotal role in shaping the effectiveness of machine explanation implementations.We posit that task complexity
and machine explanations shouldwork concurrently to foster deep thinking in humans, thereby contributing
to the efficacy of human-machine collaborations. Specifically, task complexity and information richness engage
humans in deliberate information processing by capturing their attention and interest in complex decision tasks
(Levin et al. 2000). The presentation of machine explanations that serve as valuable cues and decision-making references
prompt humans to carefully reassess decisions, address conflicts, and actively process information through
cognitive reasoning (Mantel and Kardes 1999). Through the alignment of these conditions, humans are more likely
to employ enhanced decision-making strategies, ultimately improving the performance of human-machine collaborations.
Notably, prior studies exploring the value of machine explanations have typically conducted lab experiments
or simulations alone. This approach proves challenging, as participants tend to present differently and participate
more actively in a controlled lab environment (Keil et al. 2000). Consequently, there is a compelling need to adopt
a more pragmatic approach––a realization that led us to design and implement field experiments. These experiments
serve as a crucial means of observing and analyzing human behavior in more authentic, real-world scenarios,
particularly with regard to their ability to navigate and respond to varying levels of information complexity and
cues.
Therefore, in this paper,we apply field experiments to determine whether and howhumans’ potential to achieve
“1 + 1 > 2” can be realized, particularly in the context of increasing technological development and humanmachine
collaboration. Our three research questions are: (1) What is the realized performance when humans and
machines collaborate under different levels of information complexity and different system designs? (2) What are
the underlying mechanisms? (3) How do human characteristics affect collaborative performance?
We focused on the microloan industry and partnered with a large Asian microloan company to conduct a twostage
field experiment. We dove into the dual-process theories of reasoning (Evans 2003), suggesting two prerequisites
for invoking humans’ deep thinking: information complexity initially draws humans’ attention and engages
them in the tasks, and useful cues drive humans to actively consider the task. Accordingly,we experimentally manipulated
how much information about borrowers was provided to evaluators, whether evaluators got to see the
machine’s recommendation, and whether the machine’s recommendation was explained to the evaluators.
Our empirical analyses yielded several interesting findings. First, with small information volumes, human evaluators
could not add extra value to the final outcome (i.e., the default rate prediction accuracy). Second, the human
evaluators outperformed the machines when the human evaluators were allowed to observe the machine’s suggestions
before making their final decisions and when the machine explanations were offered and the information
volume was large. In these cases, human evaluation resulted in a 2.02% reduction in the default rate (from 5.15 to
3.13%). However, this improvement disappeared if either machine explanations or information complexity were
not given. Third, we observed that when humans and machines made decisions independently, a certain amount
of disagreement was inevitable. In the human-machine collaboration modes, a disagreement of 62.82% resulted
from a small information volume without machine explanations, compared with 85.67% disagreement resulting
from large amounts of information and disclosure of machine explanations.
To disentangle the potential mechanisms and explain the above findings, we employed a three-step analytical
framework. Our findings suggested several important insights. First, human evaluators tended to stick with traditionally
important features such as income or education level, while machines explored more possibilities using
other sources of information, including shopping and offline trajectory behavior. This explains why machines, in
general, performed better than humans, especially when large amounts of information were offered. Second, with
the availability of machine explanations and large information volumes, evaluators performed active rethinking
when inconsistent decisionswere made. This improved their final decision accuracy by, for example, correcting the
risk evaluation of female borrowers. However, such a rethinking process did not occur if either condition was not
satisfied. Third, we disentangled the “rethinking” procedure in which humans associate the machine explanations
with other features if they considered the displayed features to be “non-informative”.
Furthermore, when considering individual heterogeneity among human evaluators,we found that though more
experienced evaluatorswere less likely to followthe machines’ suggestions, theywere stimulated in their rethinking
by the machines’ suggestions and explanations, and this, in turn, improved company performance. In addition,
we compared repayment behavior to examine the existence of potential gender-based decision biases. Our findings
suggest that with more data and machine explanations, human-machine collaboration could potentially shrink the
inter-gender default rate gap, which was initially and unintentionally produced by machine-learning algorithms.
This further highlights the value and necessity of collaboration between humans and machines.
The contributions of our study are multi-fold. First, it adds to the emerging literature on human–machine collaboration.
Whereas a few of the most recent studies have investigated whether humans and machines complement
each other in decision-making in different contexts (e.g., Cao et al. 2021, Luo et al. 2019, Zhang et al. 2023), the
majority have suggested outcomes only implicitly or ostensibly. Through in-depth mechanism detection analyses,
our study unravels how and why properly designed collaboration can invoke humans to contribute. Thus, we
advance this stream of literature by revealing the existence and value of humans’ rethinking processes, both theoretically
and empirically. Second, we contribute to the recent literature on the value of offering machine explanations
within the context of human-machine collaboration. The existing literature has not reached a consensus on how
humans respond to machines’ advice in the case of machine explanations (Krishna et al. 2022). Our study proposes
and verifies one reason of inconclusive findings in prior literature: the outcome of providing machine explanations
is related to other conditions such as humans’ perception of the environmental or task complexity. Whereas previous
studies have largely suggested that displaying (feature-based) machine explanations would invoke humans’
System 1 thinking (i.e., heuristics or rules-of-thumb for making quick judgments) rather than System 2 (active
reasoning and rethinking) (Chen et al. 2023), we demonstrate that with a proper collaboration design, machine
explanations can prompt humans’ rethinking and improve human–machine collaboration. Third, we add to the
recent stream of literature regarding machine biases. Recent studies have proposed the utilization of multi-source
data to alleviate algorithmic discrimination and sample biases. In fact, there is evidence that alternative data sources
would eliminate biases related to race and socioeconomic factors (Lu et al. 2023a). However, machine failure has
already been proven (Fuster et al. 2022,Huet al. 2022), so this paper not only identifies the sources of gender biases
but also uncovers the value and necessity of human involvement to make up for machine failure.
2. Related Studies
This section first summarizes three related streams of literature, then offers an introduction to the theoretical framework
underpinning experimental treatment design.
2.1. Human CollaborationWith and Aversion to Machines
AI applications require human intervention and assistance. Previous studies have explored the pros and cons of
human–machine collaboration in decision-making. For example, studies have shown that most statistical models
exceed or approach the judgment accuracy of the average clinician (Camerer et al. 2019).Machine algorithms have
been extensively shown to manage substantial amounts of data more proficiently than humans (Peukert et al. 2023,
Wang et al. 2023c). However, despite the fact that machines can make highly accurate predictions, it is difficult for
them to handle random or uncertain cases and boundary cases whose features show contradictory patterns on the
prediction objectives (labels) (Guo andWang 2015). By contrast, humans are found to be better at identifying rare
cases (Sawyer 1966) and to perform more effectively in innovative areas such as new product development (Lou and
Wu 2021). Recent studies have shown the superiority of human–machine collaborations over both full machine
automation and human-only operations (F¨ ugener et al. 2022), and have shed light on the merits of “the humanin-
the-loop” (F¨ ugener et al. 2021). On the one hand, machines can augment the capabilities of humans, such as
managers (Davenport et al. 2020); and on the other hand, humans can complement machines by contributing their
general intelligence (Te’eni et al. 2023) and diverse ideas (Wang et al. 2023d, Zhang et al. 2023) and incorporating private
information (i.e., data that only humans can use such as in-house data) (Choudhury et al. 2020, Ibrahim et al.
2021, Sun et al. 2022). Cao et al. (2021) showed that when analysts are given access to a small amount of alternative
data and in-house machine resources, combining machines’ computational power and humans’ understanding of
soft information produces the best performance in generating accurate forecasts.
However, recent research has also revealed that humans might resist the adoption or usage of machines, resulting
in low efficiency of human–machine collaboration (Allen and Choudhury 2022, de V´ericourt and Gurkan
2023,Wang et al. 2023b). This resistance exists not only among those who accept machines’ advice (e.g., Commerford
et al. 2022, Liu et al. 2023), but also among machine-based service targets, namely ordinary consumers. For
example, the adoption of chatbots has had negative effects on user acceptance and efficiency due to consumers’
insufficient knowledge and relative lack of empathy from chatbots (Luo et al. 2019). However, this negative impact
may be mitigated by users’ experience levels (Luo et al. 2021, Tong et al. 2021), flexibility, and willingness to make
adjustments based on machines’ predictions (Dietvorst et al. 2018). Human aversion to machines could also be
due to the potential of machines to threaten human jobs. AI robots have replaced and will replace human labor
in different ways in various fields (Brynjolfsson and Mitchell 2017, Lu et al. 2018). Machines have outperformed
humans in many jobs, especially low-skilled, repetitive, and dangerous ones (Autor and Dorn 2013). Conversely,
F¨ ugener et al. (2021) warned that we must also attend to humans’ over-reliance on machines, which would render
human–machine collaboration useless.
2.2. Machine Explanations
The lack of model explanations could result in human aversion to machines, stemming from a sense of distrust
(Siau andWang 2018). To avoid such negative outcomes, the existing literature has examined multiple approaches.
Acommonlyadopted approach improves trust in human–machine collaboration settings by offering more detailed
information of machine-learning decisions (Lu et al. 2019, Rai 2020). Through various post-hoc explanation methods,
human participants can be assisted in constructing suitable mental models under diverse conditions, thereby
enhancing their trust and the model efficiency (Mohseni et al. 2020). However, this approach should be employed
with caution. Schmidt et al. (2020) indicated that offering unintuitive explanations (i.e., those dealing with features
humans are unfamiliar with) may fail to boost humans’ trust in machines. Rudin (2019) also cautioned that
post-hoc explanations tend to offer incomplete and biased information regarding the mechanisms underlying algorithms.
This may lead participants to overestimate their ability to explain decisions declaratively, resulting in misinformation.
Our research aligns with this common practice. However, while some previous studies have explored the impact
of machine explanations on human–machine collaboration, few have delved into the specific mechanisms of how
and why such an approachworks in influencing human decision processes. The most similar study to ours is Bauer
et al. (2023), which revealed that humans can dynamically adjust the importance they attribute to available information
and adapt their mental models based on machine explanations. Additionally, their findings highlighted
that the provision of machine explanations might reinforce confirmation bias, potentially resulting in suboptimal
or biased decisions.However, our study differs from Bauer et al. (2023) in at least two key aspects. First, while Bauer
et al. (2023) only attended to a limited number of borrower features,we additionally consider information complexity.
As outlined in Section 2.4, we contend that the effectiveness of machine explanations in shaping individuals’
information processing depends on the complexity of the information presented to them.Machine explanations
stimulate active cognitive information processing only under specific conditions of information complexity. Furthermore,
under certain conditions, the overall performance of human–machine collaboration may see improvement
rather than deterioration. Second, the findings of the study by Bauer et al. (2023) could have been influenced
by their use of online lab experiments. By their nature, lab experiments present challenges related to sample representativeness
(Compeau et al. 2012). Of greater significance is the potential for participants to react differently
within the confines of a lab setting, which is characterized by specific monitoring and anchoring conditions. Participants
might naturally respond more actively and attentively to the experimental manipulations, potentially
leading to an overestimation of their behavioral outcomes (Keil et al. 2000). In contrast, our study adopts a field
experiment approach within a real-world micro-finance context to examine individuals’ decision-making in a more
natural setting.
2.3. Investors’ Decision-Making in Micro-finance
Many scholars have focused on individual investors’ decision-making in micro-finance businesses, including P2P
lending, crowdfunding, and microloans.Asubset of the literature has revealed the important factors that investors
consider in their decision-making (e.g., Gonzalez and Loureiro 2014, Tao et al. 2017, Wang et al. 2019). Studies
have also identified biases in micro-finance investors’ decisions, including preferences regarding gender (Chen et al.
2017) or location (Lin and Viswanathan 2016). Recent research has paid attention to the value of machine-assisted
tools in financial decision-making. For example, Ge et al. (2021) found that P2P lending investors experiencing
more defaulted loans are more likely to perceive the market to be risky and thus tend to rely more on their own
judgment rather than a robot advisor. Additionally, some investors attempt to intervene in machine usage. They
may be more concerned about returns and less likely to lose confidence in machines immediately after observing
a machine failure (Germann andMerkle 2019), or they may tend to adjust their machine usage based on the latest
performance (Ge et al. 2021). In our study, we also delve into both decision-making accuracy and potential biases
within the micro-finance context. However, unlike existing studies, our emphasis lies in examining how machine
decisions function as recommendations to influence users’ decision-making.
2.4. Theoretical Underpinning: The Dual-Process Theories of Reasoning
Humans’ and machines’ respective advantages in decision-making and their collaborative value lie in their complementarity
(Feuerriegel et al. 2022). However, humans fall easily into aversion toward or over-reliance on machines;
neither situation yields better decision outcomes than either human-only or machine-only decision-making.
Therefore, one key to promoting the value of collaboration between humans and machines is to invoke humans’
deep thinking in their co-working with machines. The literature on dual-process theories of reasoning (Evans
2003), our theoretical underpinning, raises the question of howhumans’ deep thinking can be aroused in machineassisted
tasks. The dual-process theories of reasoning propose the existence of two cognitive systems, “System 1”
and “System 2”, that underlie thinking and reasoning. System 1 processes information and reasoning fast, automatically,
and with minimal effort, leading to quick and instinctive decision-making as a rapid response to familiar
situations and stimuli. In contrast, System 2 operates at a slower pace, involves deliberate thought, and requires
conscious effort. It incorporates logical reasoning and analysis and involves the application of cognitive resources
(Kahneman 2011).
Several factors can determine whether individuals opt for System 1 or System 2 information processing and reasoning.
To encourage individuals to embrace System 2 processing, certain conditions must be met. Specifically,
since System 2 is typically involved in complex tasks, problem-solving, critical thinking, and decision-making in
novel or challenging situations, task complexity is a primary condition.Task complexity, often represented by information
complexity (Amit and Sagiv 2013), stimulates deep thinking in individuals by capturing their attention
and interest in decision tasks (Levin et al. 2000). As proposed by Endsley (1995), being well-informed about the
situation at hand is a prerequisite for subsequent deep reasoning and action selection. Information complexity,
manifested as multiple alternatives and/or numerous attributes, influences users’ situational processing of observed
information (Bauer et al. 2023, Sun and Taylor 2020). Specifically, new attributes provide novel pieces of information
that enhance one’s recognition of the decision tasks and domain (He et al. 2020). Faced with greater volumes
of more diverse, unfamiliar information, individuals are inclined to invest more effort in reasoning through more
ambiguous task situations (Van der Schalk et al. 2010). In other words, although more complex information may
not necessarily result in increased decision-making accuracy, it does enhance individual’s willingness to actively participate
in decisions (Oskamp 1965). With complex information, people are more willing to perceive the increase
in information as useful and desirable, even if it comes with a certain level of burden (Amit and Sagiv 2013). In
contrast, with simple information, people tend to make rapid decisions via System 1 processing (Speier 2006).
The second condition for motivating people to engage in high-quality System 2 processing (i.e., active consideration
and systematic deep thinking) is the presence of useful cues for reference. A well-designed reference cue has
the potential to prompt individuals to meticulously reassess their decisions and compare them with the provided
references (Weiss 1982). Consequently, individuals can rectify their initial decisions, address conflicts, and even generate
novel ideas through cognitive reasoning, association, and imagination (Hollnagel 1987). Several approaches
can be effective in fostering such deep thinking. First, high information quality leads to elevated epistemic motivation
(Cacioppo et al. 1996). For instance, structured and concrete information can encourage individuals to
engage more deeply in a task and, therefore, process information more actively and positively (Mantel and Kardes
1999). Additionally, when individuals are provided with explicit reference points (Chernev 2003), they maintain
high motivation to engage in cognitive reasoning and adopt superior information-processing strategies to navigate
complex decision-making. Moreover, the decision to employ System 2 processing can be influenced by individuals’
experience and expertise. When faced with novel and unfamiliar situations, individuals are more inclined to
activate System 2 processing to tackle challenges and gain new knowledge (Smerek 2014).
Applying the lens of this theoretical literature stream to human–machine collaboration,we propose two designs,
each of which corresponds to one of the two conditions mentioned above: (1) offering humans and machines rich
information for decision-making, and (2) exposing humans to structured machine explanations for final decisions.
Specifically, decision-making with rich information requires strong cognitive abilities for information processing
(Icard 2018); this arouses humans’ perception of the task complexity (Sun and Taylor 2020). We thus posit that,
compared with limited information, offering rich information could enhance and maintain humans’ awareness
of decision-making tasks and their willingness to participate in the tasks, regardless of their capability for handling
large information volumes. Furthermore, presenting machines’ decisions as recommendations along with
proper machine explanations showing how the prediction outcomes were obtained by machines in a faithful and
human-interpretable manner (Krishna et al. 2022) can trigger individuals’ active cognitive reasoning. For example,
if machine explanations are provided, humans can learn from machines’ decision-making rationale, trace back their
own decision rules, and double-check whether the new knowledge from machines fits and actually improves decision
accuracy (Mohseni et al. 2020).We call this the rethinking process. In this paper, rethinking or reconsideration
refers to the process of carefully reviewing a decision or conclusion that has previously been made to determine
whether the initial decision should be changed. It is usually an inquiry into, or reflection on, the most basic given
information, or the asking of fundamental questions such aswhy and howbreakthrough improvementswere made
after observing new signals or outcomes (Jain and Pagrut 2001). Such a self- and system-monitoring process aligns
with the concept of active consideration (i.e., Pattern 5) developed by Jussupow et al. (2021), which was concluded
to be the best practice for achievement of satisfactory outcomes from human–machine collaboration.
Broadly speaking, notwithstanding the many and broad investigations into human–machine collaboration,
there is a dearth of literature unraveling the decision-making process during human interactions with machine
assistants under diverse conditions. This paper aims to bridge that void. Particularly, we focus on the role of information
complexity and machine explanations in prompting humans to actively rethink and improve the consequent
decision outcomes. Given the complex environments covering interactions among information volumes,
machine explanations, human experience, and behavioral biases, this question might not have a fixed and intuitive
answer.We also reveal the scenarios that can leverage humans’ and machines’ respective advantages to realize 1 + 1
> 2.
3. Experimentation
3.1. Experimental Background
We partnered with a large Asian microloan company to conduct a field experiment. The microloan company was
founded in 2011 and served over 250,000 borrowers by 2018, with unsecured microloans of approximately US$465.
The company uses only the owner’s money for lending, and their loans are mostly used for temporary financial
needs such as supplementary cash flow for small businesses and irregular shopping needs. The loans have a term of
1–7 months and are repaid in monthly installments starting one month after their issuance. The company sets its
annual interest rate from 12 to 16%.1
To apply for a loan, a borrower is required to provide their basic personal information such as name, phone
number, gender, age, educational level, and income level. Subsequently, borrowers are required to choose the loan
amount (US$46.5—US$1,240, US$465 by default) and loan term as well as check the annual interest rate. In this
study, we focused only on loans with a term of 1, 2, or 3 months.2 In addition, borrowers are required to clearly
state the purpose of the loan. They can then submit their application. Every new application is randomly assigned
1 The annual interest rate is set on a daily basis, rather than assigning different rates to borrowers with different assessed credit risks. This
daily interest rate is generally determined at a mediate level in the online lending market. The company does not announce the actual rates to
the market in advance, and borrowers are therefore less likely to decide strategically when to apply to get a lower rate. Such a design allowed
us to tease out the potential endogeneity issues brought by interest rates.
2 This was to facilitate our experimental observation, because it takes a long time to observe and confirm the repayment or default behavior
when the loan term is long. We compared the repayment performance among loans of different terms with the historical data and found
that the loan term was not highly related to repayment performance.
to a human evaluator who assesses the borrower’s credit risk (i.e., default probability) based on the collected information,
and makes the final loan-approval decision accordingly. The focal company’s loan-approval rate is approximately
47%, similar to the competitors in the market. The main goal of loan screening is to minimize the number
of defaulted cases while maintaining the approval rate specified above.
3.2. Experimental Setup
3.2.1 Implementation of Treatment I: Information Complexity
Inspired by the dual-process theories of reasoning, we introduced two factors that could influence human evaluators’
decision-making in collaboration with machines in Section 2.4. As the first step,we utilized the focal empirical
setup to incorporate variations in information complexity. Before our experiment, the focal platform granted loans
based entirely on human evaluators’ decisions. Evaluators only accessed borrowers’ basic information, loan history,
and current loan attributes (12 variables [features] in total) to make their credit risk evaluation. Thus, this information
comprises the first level of information complexity: small information volumes. To construct an alternative
information scenario (i.e., with large information volumes),we asked the focal company to collect additional information
from the borrowers starting June 1, 2017. The additional information included recent (past six months)
online shopping activities on the largest e-commerce platform in the focal country and cellphone usage information
collected from the pertinent communication carriers.3 Previous studies have suggested that shopping and cellphone
usage may be correlated with borrowers’ socioeconomic status and credit behaviors (e.g., Blumenstock et al.
2015). Therefore, based on the relevant literature and canonical behavioral theories (Lu et al. 2023a), we extracted
32 features for each source in order to comprehensively describe borrowers’ online shopping and cellphone usage
and mobility trace characteristics. Table A1 in Appendix A1 describes these features.
3.2.2 Machine Preparations and Implementation of Treatment II: Machine Explanations
Since the focal company had not sought any machine assistance before our collaboration, it was necessary for us
to design and train prediction models for each of the two information scenarios. Our training samples comprised
borrowers who submitted loan applications June 1–30, 2017. For these sampled borrowers, the human evaluators
3 The groups with large information volumes had access to multi-sourced information, emphasizing information diversity (i.e., new
attributes). Labeling one treatment as “large information volumes” is intentionally contrasting it with the small-sized demographic feature
set used in the other groups. Therefore, our manipulation is intricately aligned with the concept of information complexity, as elucidated
in Section 2.4.
assessed their credit risks and made loan-approval decisions using small-scale information, as usual. At this stage,
the human evaluators did not have access to the additional information collected. We then gathered repayment
information for the approved borrowers from more than 9,000 training sample loans made between July 1 and
November 30, 2017. Since the loan term was no longer than 3 months, a 5-month observation period was sufficient
for us to confirm borrowers’ repayment and default behaviors. Default is defined as the failure to fully repay the loan
at least 60 days after the loan due date. At the end of November, we obtained the borrowers’ basic and additional
information, as well as their repayment behaviors.
Based on the above information, we then trained machine-learning algorithms. For both information scenarios,
we implemented standard operationalizations (e.g., 10-fold cross-validation, out-of-sample prediction, and hyperparameter
tuning) and replicated the training procedures multiple times until they achieved stable loan default
prediction performance.We tried diverse, widely accepted machine-learning models, including logistic regression,
support vector machine, k-nearest neighbor, multi-level perceptron, random forest, and extreme gradient boosting
(XGBoost).XGBoost achieved the best performance, sowe employed it in our experiment.To maintain a relatively
comparable performance across experimental groups,we did not updateXGBoost during the experimental period.
Meanwhile,we leveraged the same training samples to train the human evaluators. Specifically,we randomly separated
the human evaluators into two groups: one group maintained the previous loan evaluation process with the
small information volume, and the other group evaluated credit risks and made loan-approval decisions with the
large information volume. After a 7-day training period, all human evaluators reached a stable evaluation performance.
Please refer to Appendix A2 for detailed information on the human evaluators and the training procedure.
With the pre-trained prediction models, we were able to design the second treatment. Specifically, to prepare
the machine explanation information based on the above machine-learning algorithms, we implemented a SHAP
analysis method, which yields Shapley values representing the average expected marginal contribution to predicting
the default probability of one feature after all possible combinations have been considered (Roth 1988). In Figure 1,
we present the most important features under the two information volume scenarios.
3.3. Experimental Design
To identify the loan approval decision performance under human-only, machine-only, and human–machine collaboration
decision-making scenarios, we designed and implemented a two-stage experiment, as illustrated in Figure
2.
(a) Decisions with Small Information Volume (b) Decisions with Large Information Volume
These features rank in the top 5 or 7 in respective analyses. The other features play only limited roles in machine-learning-based predictions
(i.e., they have very small absolute scores). Positive (negative) values mean that the features are positively (negatively) related to default
behavior.
Figure 1 Important Features in Machines’ Decision-making Processes
Figure 2 Experimental Process
Experimental Stage 1. The first stage began on December 8, 2017, and lasted for oneweek. The relatively short term
of the treatments helped tease out the potential confounders stemming from the substantial evolution (learning
or change) of the human evaluators, machine-learning algorithms, and borrower-characteristic distributions with
long-term experience. At this stage, the company collected basic and additional information from every new borrower,
and we randomly assigned the borrowers to one of the four groups. In Groups 1 (H & S) and 2 (H & L), a
credit risk assessment was completed by human evaluators. They had access to the small (Group 1) or large (Group
2) information volumes to inform their approval or rejection of each loan application. The two human evaluator
groups were consistent with those in the training process described earlier. In Groups 3 (M & S) and 4 (M &
L), we employed the corresponding pre-trained XGBoost to predict each application’s default probability based
on a small (Group 3) or large (Group 4) number of features and to make loan-approval decisions by ranking the
predicted default probability from lowest to highest. Following the company’s usual practice, we maintained the
loan-approval rate at 47% in all four experimental groups. For all granted loans,we continued tracing and collecting
their repayment behavior from January 8 toMay 14, 2018.
Experimental Stage 2. We spent another two weeks (from December 15 to 28, 2017) conducting the second stage
of our experiment. The two-week period ensured that the evaluation workload was similar to that in the first stage.
Again, we randomly assigned each new loan application to one of the four groups. In all groups, the human evaluators
were instructed to collaborate with the machine. Specifically, human evaluators in Group 1 were randomly
assigned to Groups 5 and 6 and those in Group 2 were assigned to Groups 7 and 8, with an equal number of evaluators
in each group to manage the same amount of information. As illustrated in Figure A2 in Appendix A3, the
loan-approval decision process had two steps. In the first step, human evaluators made credit risk evaluation and
loan-approval decisions independently with small (Groups 5 and 6) or large (Groups 7 and 8) information volumes;
this is identical to the situation in Stage 1. In the second, decision-making step, the machine-learning algorithm’s
loan-approval decision for the same loan was presented to the human evaluators. In Groups 5 and 6, the machinelearning
algorithm used the trained model with a small number of features (corresponding to Group 3), and in
Groups 7 and 8, it used a large number of features (corresponding to Group 4). The human evaluators did not
have much knowledge of the applied machine-learning algorithm; theywere simply notified that machine-learning
algorithms usually have strong decision-making abilities.
Next, we incorporated the second treatment, the existence of machine explanations. Specifically, in Groups 5
((H + M)&S&w/o Expl) and 7 ((H + M)&L&w/o Expl),we gave only the machine’s loan-approval decisions
to the human evaluators, without explanations regarding how the decision had been reached (see Figure A2a). In
Groups 6 ((H + M)&S&w/ Expl) and 8 ((H + M)&L&w/ Expl), the human evaluators could see not only the
machine’s loan-approval decisions but also the post-hoc explanations (i.e., the most important features presented
in Figure 1). For these features, the human evaluators could find and compare the values of the fixed features of
the focal borrower and the average values of non-defaulters (see Figure A2b). The human evaluators in Groups
6 and 8 were provided this information at the beginning of experimental stage 2. We conjecture, based on our
theoretical framework, that this information (strengthened by the value comparison) served as an ideal reference
due to machines’ superior capability (Chernev 2003). Then, human evaluators were required to make their final
loan-approval decisions. When their initial decisions were incongruent with the machine’s, they could either insist
on their own decisions or adjust them to followthe machine’s recommendations. As mentioned before, the human
evaluators were told to maintain a consistent approval rate before and during the experiment, and so the approval
rates in all of our experimental groups were maintained at approximately 47%. Similarly, we continued to collect
the Stage 2 borrowers’ repayment performance data over the subsequent 5 months.
3.4. Experimental Data
We obtained our experimental data after completing repayment information collection. The dataset contained
the borrowers’ basic and additional information, the human evaluators’ and machines’ initial approval decisions
(Groups 1 to 8), the human evaluators’ final approval decisions (Groups 5 to 8), and the repayment performance
(default or not) of the approved loans. Additionally, we collected background information on the human evaluators,
including their gender, education level, number of months’ experience (discretized by six month period), and
historical decision accuracy (i.e., the ratio of defaulted loans to all approved loans in the three months before our
experiment).
Table 1 Randomization Check
Loan characteristics Borrower characteristics
Group #Obs. Loan amount (US$) Interest rate (%) Loan purpose Gender Age Living city DPI (US$) Monthly income level Education level
1.H& S 2,924 472.8 13.888 0.446 0.235 25.17 6,528.9 4.886 4.252
2.H& L 2,930 473.7 13.911 0.437 0.241 25.18 6,505.0 4.886 4.256
3.M& S 3,001 472.9 13.930 0.431 0.249 25.12 6,524.7 4.902 4.201
4.M& L 3,020 472.8 13.913 0.430 0.237 25.19 6,565.2 4.942 4.9206
5. (H + M) & S & w/o Expl 2,885 474.7 13.920 0.437 0.245 25.07 6,545.5 4.960 4.223
6. (H + M) & S & w/ Expl 2,918 470.7 13.902 0.437 0.241 25.15 6,588.1 4.884 4.218
7. (H + M) & L & w/o Expl 2,978 475.2 13.924 0.428 0.233 25.09 6,563.7 4.874 4.216
8. (H + M) & L & w/ Expl 2,946 475.4 13.904 0.434 0.240 25.11 6,571.6 4.943 4.257
Group #Unique evaluators Evaluator gender Evaluator education level Evaluator months working Evaluator historical (decision) accuracy
1.H& S 31 0.774 4.452 2.516 2.000
2.H& L 31 0.774 4.452 2.516 2.065
a H= human decision,M= machine decision,H+M= human + machine decision, w/o Expl = without AI explanations, w/ Expl = with AI explanations.
b Loan purpose: 1 = consumption, 0 = others (e.g., for emergency). Gender: 1 = female, 0 = male.
c Monthly income level: 1 = US$150 or below, 2 = US$150–US$300, 3 = US$300–US$450, ..., 8 = US$1,050–US$1,200, 9 = US$1,200 or above.
d Education level: 1 = middle school or below, 2 = vocational school, 3 = high school, 4 = technical school, 5 = undergraduate, 6 = graduate or above.
e Evaluator months working: 1 = not longer than 6 months, 2 = 6–12 months, 3 = 13–18 months, 4 = longer than 18 months.
f Evaluator historical (decision) accuracy: 1 = low (default rate>15%), 2 = medium (10%<default rate<15% ), 3 = high (default rate<10%). Refer to Table A2 for descriptive statistics on evaluator
historical accuracy.
g Groups 3 and 4 did not involve human evaluators. In experimental stage 2, the human evaluators in Group 1 (or 2) were randomly and equally assigned to Groups 5 and 6 (or Groups 7 and 8).
h For every feature, the values show no significant differences across the groups based on the F-test.
Figure 3 Default Rates of Experimental Groups

There were a total of 23,805 loans in the 8 groups involved in our experiment.We removed 203 repeat borrowers
from the company to avoid interference from the previous experience. The final experimental sample size was
23,602. Table 1 reports the sample size and the major characteristics of borrowers, loans, and evaluators across the
experimental groups.Most of the borrowerswere men (>75%); 28.43% of the borrowers had received an undergraduate
education, and the average (self-reported) monthly income ranged between US$450 and US$600. Approximately
44% of the loans were for personal consumption purposes. Regarding the human evaluators, most were
female (77%) with a technical school or undergraduate-level education background. On average, the human evaluators
had been working for the company for approximately 1 year, and those with high, medium, and low levels of
historical decision performances were evenly distributed between the groups (i.e., around 1/3 each).We detected
no statistically significant differences between the groups, which suggested that the randomization had been successful.
4. Empirical Findings
Our key variable of interest was borrowers’ default rates. This is a common metric in the microloan industry (Fu
et al. 2021) and within the focal company. We defined it as the ratio of defaulted loans to the total number of
approved loans. Figure 3 plots the default rates across all groups, and Table 2 calculates the inter-group differences
with between-group t tests. The default rate in Group 1 was 12.83%, echoing the average performance of the focal
company before our experiment. The comparison yielded several interesting patterns. First, as expected, when making
decisions separately, the human evaluators performedworse than the machines, and the large-scale information
volumes increased the performance gap (i.e., Comparisons B and D). Second, the human evaluators did not add
additional value when jointly deciding based on a small information volume, regardless of whether the machine
explanationswere offered (i.e., Comparisons E, G, andHwith insignificant differences in the mean value of default
rates). Third, we observed different outcomes in the scenarios with large information volumes. In particular, when
the human evaluatorswere presented with the machines’ suggestions and the machine explanations before making
their final decisions, they performed better than the machines’ independent decisions, showing a 2.02% reduction
in default rates, from 5.15 to 3.13% (i.e., Comparison J). This suggests that the human evaluators contributed additional
value to the evaluation process that only they, as humans, could provide. However, this improvement disappeared
if no machine explanation was provided (i.e., Comparison I). In sum, the collaborative values were only

Table 2 Comparison of Default Rates among Different Experimental Groups
Comparison Experimental groups Difference in means p-values
A 1.H& S vs. 2.H& L 0.0228 0.0650*
B 1.H& S vs. 3.M& S 0.0268 0.0275**
C 3.M& S vs. 4.M& L 0.0500 0.0000***
D 2.H& L vs. 4.M& L 0.0539 0.0000***
E 5. (H+M) & S & w/o Expl vs. 6. (H+M) & S & w/ Expl 0.0032 0.7833
F 7. (H+M) & L & w/o Expl vs. 8. (H+M) & L & w/ Expl 0.0287 0.0003***
G 3.M& S vs. 5. (H+M) & S & w/o Expl -0.0048 0.6779
H 3.M& S vs. 6. (H+M) & S & w/ Expl -0.0016 0.8892
I 4.M& L vs. 7. (H+M) & L & w/o Expl -0.0085 0.3215
J 4.M& L vs. 8. (H+M) & L & w/ Expl 0.0202 0.0071***
a As our experiment comprised multiple treatments, we followed multiple hypothesis testing in experimental economics (List et al.
2019) to address the potential bias. Thus, p-values are multiplicity-adjusted values based on between-group t tests. *p < 0.10, **p <
0.05, ***p <0.01.
realized if the two conditions, information complexity and useful cues, were satisfied. We also considered profit
gains and evaluated the dollar values of the different factors. The results in Figure B1 in Appendix B1 confirmed the
consistency.
Noticing the above diverse patterns, we then further decomposed the decision-making behavior of human evaluators
after they had observed machines’ suggestions. Specifically, in Figure 4, we compared the decision consistency
between humans (initial decision) and machines (in Figure 4a), and calculated the adjustment ratios when
inconsistency arose (in Figure 4b). Our results indicate that when the human evaluators were making decisions
independently (i.e., before observing the machines’ suggestions), there were a certain number of cases in which
the humans disagreed with the machine’s decisions. As shown in Figure 4a, the agreement proportion was smaller
with the large information volume (83.78% consistency in Group 5 vs. 78.58% consistency in Group 7). This pattern
was similar regardless of whether machine explanations were available. In the human–machine collaboration
scenario, the human evaluators adjusted their decisions by following the machines’ recommendations. The proportion
of adjustment, however, varied across the experimental groups. In particular,we observed that only 62.82%
disagreement was eliminated with a small information volume and no machine explanation. The adjustment rates
significantly increased when the information volume was large or machine explanations were offered. For example,
compared with limited information, the availability of large amounts of information could mitigate the human
evaluators’ unwillingness to follow machines, decreasing it by 18.21% (Group 5 vs. Group 7, p-value<0.001).Meanwhile,
machine explanations also encouraged human evaluators to accept the machines’ decisions by improving
the ratio of following from 81.03 to 85.67% (Group 7 vs. Group 8, p-value = 0.026).

(a) Ratio of Decision Consistency between Humans andMachines (b) Ratio of FollowingMachines’ Decisions
Notes: p-values are multiplicity-adjusted values (List et al. 2019) based on between-group t tests.
Figure 4 Consistency and Following between Humans and Machines
5. Mechanism Examinations
This section aims to disentangle the potential mechanisms driving the differences in performance between humans
and machines and the contributions made by humans when collaborating with machines. This part consists of
three steps.We first examined empirically why humans and machines decided differently when making decisions
separately and how decision inconsistency explained the performance differences. Second, we isolated the underlying
behavioral mechanisms explaining why humans disagreed with the machines’ recommendations when collaboration
was allowed. Third, we discussed how disagreement affects decision quality, and decomposed the human
evaluators’ “rethinking” procedure in the collaborative mode.
5.1. Why Do Humans and Machines Behave Differently?
To answer this question, we explored decision-making processes by identifying the important features involved.
First, to determine the information that had played a part in either the human evaluators’ or the machines’ decisionmaking
processes, we considered a (loan-)application-level Probit model with all available information as independent
variables and defined the dependent variable (DV) using a dummy variable, IfApprove, which equaled one
if the loan was approved. We derived two sets of Probit models using all loan applications with either small or
large information volumes. The estimated coefficient of each information variable suggested the predictive power,
which served as a proxy for feature importance in the humans’ or the machines’ decision-making process. Features
with significant coefficients in the regressions were important features.

Table 3 Regressions on Humans’ and Machines’ Approval Decision (Groups 1, 3, 5, and 6; Probit Model)
Groups 3, 5, 6 (machines’ decision) Groups 1, 3, 5, 6 (humans vs. machines)
DV: IfApprove Model 1 Model 2
Loan purpose -0.161*** (0.034) 0.042 (0.048)
Gender 0.030 (0.029) 0.062 (0.058)
Age 0.087*** (0.005) 0.068 (0.062)
Living city DPI 0.212*** (0.007) 0.139*** (0.010)
Monthly income level 0.126*** (0.008) 0.082*** (0.008)
Education level 0.163*** (0.021) 0.055*** (0.028)
MInd -0.086 (0.203)
Loan purpose × MInd -0.208*** (0.040)
Gender × MInd -0.030 (0.045)
Age × MInd 0.024*** (0.006)
Living city DPI × MInd 0.066 (0.060)
Monthly income level × MInd 0.043 (0.036)
Education level × MInd 0.103 (0.087)
Other borrower-related variables Included Included
Other loan-related variables Included Included
Evaluator-related variables Included Included
Log likelihood -4,951.40 -10,363.18
#obs. 8,804 17,531
a Model 2 considers human evaluators’ initial decisions before displaying machines’ recommendations to them when using Groups 5 and
6.We duplicated the sample for Groups 5 and 6 to consider the humans’ initial decisions and machines’ decisions, respectively.
b The variables concretely reported in the table are those that might be useful in this paper’s analyses (although they may be insignificant
here).Most of the other variables were insignificant, and we do not report their details. Living city DPI was divided by 1,000.
c Standard errors are in parentheses. Significant results are in bold. *p <0.10, **p <0.05, ***p <0.01.
Furthermore, to compare the decision-making processes between humans and machines in a more explicit way,
we ran two additional Probit models, in which we included all related loan-level features as well as their interaction
terms and a new binary indicator, MInd, denoting whether the approval decision was made by a machine learning
model (=1 if yes,=0 otherwise).We reported the estimation results in Tables 3 and 4 for the small and large information
volumes scenarios. Model 1 in both tables reports the estimates of machine-only decisions. We estimated
the coefficients using samples from Groups 3, 5, and 6 for the small information volume scenario and from Groups
4, 7, and 8 for the large information volume scenario.Model 2 in both tables reports the models with interaction
terms. We included all human-only decisions (i.e., humans’ initial decisions without machine interventions) and
machine-only decisions in Groups 1, 3, 5, and 6 (in Table 3) and Groups 2, 4, 7, and 8 (in Table 4). The coefficients
of the interaction terms in Model 3 elaborate on whether and to what extent the corresponding features explain
the divergence between humans’ and machines’ decision-making processes.4
4 In Appendix C1, we conducted multiple robustness checks. First, we reran our regressions within each experimental group using different
samples. The results indicated that thehumanevaluators’ initial decisions did not involve any learning from the machines’ recommendations.
This also confirmed that the comparisons between the two stages in our experiment were reasonable. As another robustness check, we
employed decision tree approaches to infer the decision rules implemented by human evaluators. The results in Figure C1 confirm the
consistency. Additionally, we incorporated an alternative DV to offer more insights into how humans and machines reached the same or
different initial decisions.

Table 4 Regressions on Humans’ and Machines’ Approval Decision (Groups 2, 4, 7, and 8; Probit Model)
Groups 4, 7, 8 (machines’ decision) Groups 2, 4, 7, 8 (humans vs. machines)
DV: IfApprove Model 1 Model 2
Loan purpose -0.028*** (0.004) 0.021 (0.049)
Gender 0.045 (0.032) 0.070 (0.053)
Age 0.088*** (0.005) 0.060 (0.074)
Living city DPI 0.154*** (0.007) 0.091*** (0.011)
Monthly income level 0.121*** (0.009) 0.065*** (0.014)
Education level 0.075*** (0.023) 0.072*** (0.036)
Avg amount of game card -0.018*** (0.001) -0.009 (0.016)
ATV shopping durable 0.001 (0.003) 0.005 (0.005)
ATV shopping virtual -0.001 (0.001) -0.002 (0.002)
#Outgoing contacts -0.052*** (0.010) -0.050*** (0.018)
#Office by week 0.077*** (0.003) 0.004 (0.005)
#Recreational place by week -0.026 (0.028) -0.026 (0.042)
#Commercial place by week -0.097*** (0.008) -0.034 (0.069)
#Public service place by week 0.014 (0.014) 0.042 (0.043)
MInd -0.083 (0.225)
Loan purpose × MInd -0.064** (0.030)
Gender × MInd -0.022 (0.048)
Age × MInd 0.028*** (0.006)
Living city DPI × MInd 0.056 (0.049)
Monthly income level × MInd 0.006 (0.011)
Education level × MInd 0.006 (0.008)
Avg amount of game card × MInd -0.008*** (0.002)
ATV shopping durable × MInd 0.001 (0.001)
ATV shopping virtual × MInd -0.001 (0.001)
#Outgoing contacts × MInd -0.002* (0.001)
#Office by week × MInd 0.063*** (0.004)
#Recreational place by week × MInd 0.001 (0.002)
#Commercial place by week × MInd -0.063*** (0.011)
#Public service place by week × MInd -0.027 (0.029)
Other borrower-related variables Included Included
Other loan-related variables Included Included
Evaluator-related variables Included Included
Log likelihood -4,155.33 -9,642.50
#obs. 8,944 17,798
a Model 2 considers the human evaluators’ initial decisions before the machines’ recommendations were displayed to them when using
Groups 7 and 8.We duplicated the sample when using Groups 7 and 8 to consider the humans’ initial decisions and the machines’ decisions,
respectively. Other table notes are the same as [b] and [c] in Table 3.
Several interesting patterns explain the differences in performance between the human evaluators’ and machines’
individual decisions. When decisionswere made with the small information volume, the human and machine evaluators
considered similar features (i.e., living city DPI, monthly income level, and education level). The machines
additionally captured the applicants’ age and the loan purpose, which is known to have a relatively high correlation
with default behavior (refer toTable 5). This explains why the machines performed slightly better than the humans
with the small information volume. When a large information volume was available, the human and machine evaluators
deviated. Interestingly, we found that the human evaluators generally tended to stick with traditionally
important features (e.g., living city DPI, monthly income level, education level); the only new feature that human
evaluators adopted was the frequency of outgoing contacts. In contrast, the machines explored additional sources

Table 5 Correlations of Major Variables
(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12)
(1) IfDefault 1
(2) Gender 0.025 1
(3) Living city DPI -0.195 -0.010 1
(4)Monthly income level -0.164 -0.041 0.022 1
(5) Age -0.120 -0.054 0.047 0.104 1
(6) Education level -0.103 -0.036 0.002 0.028 0.091 1
(7) Loan purpose 0.093 0.178 -0.036 -0.033 -0.065 -0.026 1
(8) Avg amount of game card 0.169 -0.247 0.001 -0.004 0.014 -0.017 -0.002 1
(9) #Outgoing contacts 0.104 -0.054 -0.023 0.036 0.012 0.033 0.001 -0.027 1
(10) #Office by week -0.115 -0.013 -0.034 -0.010 -0.021 0.017 0.009 0.046 0.049 1
(11) #Commercial place by week 0.090 0.096 -0.030 -0.016 -0.026 0.032 0.014 -0.089 0.019 -0.055 1
(12) ATV shopping virtual 0.094 0.098 -0.005 -0.070 -0.078 0.035 0.045 -0.038 0.010 -0.028 0.013 1
a Correlations are based on all loan samples. Relatively large values are in bold.
of information, with a particular focus on factors potentially linked to default behavior (refer to Table 5). These
factors included shopping behavior (e.g., average amounts spent on game cards), cellphone call behavior (e.g., the
frequency of outgoing contacts), and offline trajectory behavior (e.g., frequency of visiting the office or commercial
places per week). This is reasonable because humans might resist or be incapable of handling new and complicated
information (Chapman and Chapman 1967).Moreover, with their increased processing efficiency, machines have
been confirmed to have predictive advantages using novel features from alternative data sources (Lu et al. 2023a,
Zhou et al. 2021). This also explains the significant improvement achieved by machines with large information
volumes.
5.2. Why Do Humans Disagree with Machines’ Recommendations?
We next disentangled the underlying behavioral mechanisms when collaboration was employed.We noticed that
after observing the machines’ recommendations, the human evaluators sometimes adjusted their final decisions
to follow the machines’ recommendations, but not always. Table 2 shows that only with machine explanations
and large information volumes did the human evaluators contribute additional value. This value would disappear
if either of the two conditions were removed. In order to understand the human evaluators’ behavior, we conducted
regression tests using observations in which the human evaluators’ initial decisions differed from those of
the machines.
We employed Probit models, in which the DV was IfApprove and the independent variables included all
available loan features.To understand howmachines’ recommendations influenced humans’ decision-making processes,
we compared the discrepancies betweenhumanevaluators’ initial and final decisions. Empirically,we defined

a new binary indicator, IfFinal, which equaled one if the approved decision was made after a machine recommendation
was present. Again, we included this binary indicator and the interaction terms of the features to
investigate which features played a significant role in changing the human evaluators’ decisions.
We reported the results in Tables 6 and 7. With small information volumes (Groups 5 and 6 in Table 6) and
with large information volumes but no machine explanations (Group 7 in Table 7), the factors that explained the
human evaluators’ final approval decisions remained similar to those in the first stage (shown in Tables 3 and 4).
For example, with the small information volume, the interaction terms for three features (i.e., applicant age, living
city DPI, and monthly income level) presented significant coefficients in Model 2 in Table 3. This suggests that
humans rely on these three features to decide whether to change their initial decisions (i.e., whether to follow the
machines’ recommendations).Take the feature of monthly incomelevel as an example. The corresponding estimate
is positive, implying that humanswould switch from rejection to approval even if an applicant’s income is not high
enough. That is, the weight of the monthly income level in human evaluators’ credit risk assessment became larger
than before, and human evaluators were more tolerant of cases with relatively lower levels of income. To further
illustrate humans’ willingness to follow machines’ suggestions and to capture human behavior in the second stage,
we defined another dependent variable, IfFollow. The detailed empirical strategy and corresponding results are
presented in Appendix C2. All the empirical results imply that, under these experimental conditions, the reasons
(i.e., key features) explainingwhy the human evaluators disagreed with the machines initiallywere the same as those
explaining why they continued to disagree with machines after receiving the machine recommendations.
In Groups 5 to 7, the features with significant estimates of interaction terms with IfFinal (i.e., indicating
the reasons explaining the differences between initial and final decision-making processes) included both humanfamiliar
ones (i.e., those they had used in the first stage, such as living city DPI and number of outgoing contacts)
and machine-only ones (e.g., number of commercial place visits). There are two possible ways that humans
and algorithms might reach diverse decisions. One is that humans might have some uncertainty surrounding
“borderline” cases (i.e., those with important features showing values near the evaluators’ or machines’ thresholds).
Humans and machines may make inconsistent decisions on such borderline loans when their feature values
are located in such threshold gaps. When handling these relatively complicated applications, humans may lack

Table 6 Regression on Human Evaluators’ Initial and Final Approval Decisions (Groups 5 and 6; Probit Model)
Group 5 Group 5 Group 6 Group 6
(humans’ final decision) (initial vs. final) (humans’ final decision) (initial vs. final)
DV: IfApprove Model 1 Model 2 Model 3 Model 4
Loan purpose -0.025 (0.022) -0.013 (0.024) -0.113* (0.065) -0.102 (0.124)
Gender 0.160 (0.140) 0.131 (0.139) 0.037 (0.143) 0.035 (0.144)
Age 0.073** (0.034) 0.032 (0.040) 0.070** (0.032 0.024 (0.032)
Living city DPI 0.155** (0.024) 0.123*** (0.026) 0.103*** (0.027) 0.098*** (0.030)
Monthly income level 0.117*** (0.034) 0.096*** (0.033) 0.059* (0.033) 0.052* (0.028)
Education level 0.024*** (0.008) 0.020** (0.008) 0.067*** (0.015) 0.031** (0.014)
IfFinal -0.421*** (0.085) -0.598*** (0.085)
Loan purpose × IfFinal -0.012 (0.010) -0.011* (0.006)
Gender × IfFinal 0.028 (0.198) 0.002 (0.183)
Age × IfFinal 0.041* (0.024) 0.045** (0.023)
Living city DPI × IfFinal 0.023* (0.013) 0.005** (0.002)
Monthly income level × IfFinal 0.022** (0.010) 0.005* (0.003)
Education level × IfFinal 0.004 (0.003) 0.035** (0.017)
Other borrower-related variables Included Included Included Included
Other loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -306.15 -599.31 -293.04 -584.36
#obs. 468 936 461 922
a Models 1 to 4 are based on the samples in which human evaluators’ initial decisions were inconsistent with machines’ decisions (i.e.,
IfConsistent = 0).We duplicated the sample because we considered the humans’ initial and final decisions separately. Other table notes
are the same as [b] and [c] in Table 3.
confidence (Kunimoto et al. 2001) and be more likely to follow machines’ suggestions, regardless of their initial
approval or rejection decisions. Considering the following ratios and the performance improvement from Group
1 to Groups 5 and 6 and from Group 2 to Group 7, our findings indicate that the machines were relatively better
at evaluating cases with feature values near the borderline.
Conversely, it is likely that humans and machines could reach distinct conclusions about an applicant’s default
probability because of differences in evaluating important features. As a result, humans would tend to stick with
their initial opinions. Considering that the machines incorporated extra features to assess the loans, these additional
features might dominate the human-familiar ones, and human evaluators could find that the values of their familiar
features were beyond their expectations. This echoes the literature about humans’ aversion toward AI when
humans cannot successfully interpret the reasoning behind a machine’s decision (Wang and Benbasat 2016). Figure
5 provides empirical evidence with feature distributions to support these arguments. In specific, we visualized
the distributions using four sub-samples, which were separated by two standards: whether human evaluators ultimately
accepted or rejected the applications (A vs.R), and whether humans followed or continued to disagree with
the machines’ recommendations (F vs. D). Interestingly, we observed that the means of (F&A) are close to those
of (F&R), implying that when dealing with borderline cases, humans place more trust in the machines. On the

Table 7 Regression on Human Evaluators’ Initial and Final Approval Decisions (Groups 7 and 8; Probit Model)
Group 7 Group 7 Group 8 Group 8
(humans’ final decision) (initial vs. final) (humans’ final decision) (initial vs. final)
DV: IfApprove Model 1 Model 2 Model 3 Model 4
Loan purpose -0.059 (0.115) 0.111 (0.117) -0.017 (0.128) -0.011 (0.124)
Gender 0.207 (0.136) 0.045 (0.032) -0.154** (0.069) 0.032 (0.034)
Age 0.130** (0.056) 0.073 (0.059) 0.100*** (0.018) 0.051 (0.057)
Living city DPI 0.135*** (0.025) 0.098*** (0.024) 0.188*** (0.030) 0.140*** (0.033)
Monthly income level 0.076** (0.032) 0.032*** (0.010) 0.140*** (0.034) 0.110*** (0.033)
Education level 0.191** (0.081) 0.130*** (0.040) 0.032* (0.019) 0.030* (0.016)
Avg amount of game card -0.002 (0.005) -0.030 (0.057) -0.038 (0.063) -0.014 (0.014)
ATV shopping durable 0.003 (0.002) -0.001 (0.002) 0.007 (0.009) 0.008 (0.012)
ATV shopping virtual -0.005 (0.004) -0.001 (0.001) -0.020*** (0.005) -0.010 (0.008)
#Outgoing contacts -0.036*** (0.010) -0.015* (0.008) -0.025** (0.012) -0.022* (0.012)
#Office by week 0.028** (0.011) 0.067 (0.042) 0.027** (0.012) 0.019 (0.012)
#Recreational place by week -0.126 (0.100) -0.029 (0.095) -0.034 (0.117) -0.027 (0.129)
#Commercial place by week -0.044* (0.025) -0.022 (0.024) -0.111*** (0.039) -0.058 (0.036)
#Public service place by week 0.064 (0.048) 0.015 (0.048) 0.010 (0.040) -0.028 (0.040)
IfFinal -0.337*** (0.091) -0.349*** (0.091)
Loan purpose × IfFinal -0.170 (0.164) -0.006 (0.178)
Gender × IfFinal 0.149 (0.134) -0.185* (0.105)
Age × IfFinal 0.056** (0.028) 0.048** (0.025)
Living city DPI × IfFinal 0.053*** (0.014) 0.048*** (0.014)
Monthly income level × IfFinal 0.044*** (0.014) 0.030** (0.015)
Education level × IfFinal 0.061*** (0.015) 0.003 (0.017)
Avg amount of game card × IfFinal 0.027 (0.025) -0.023 (0.043)
ATV shopping durable × IfFinal -0.002 (0.004) -0.001 (0.002)
ATV shopping virtual × IfFinal -0.004 (0.005) -0.010** (0.004)
#Outgoing contacts × IfFinal -0.021** (0.010) -0.003 (0.013)
#Office by week × IfFinal 0.017* (0.010) 0.008 (0.006)
#Recreational place by week × IfFinal -0.095 (0.108) -0.007 (0.114)
#Commercial place by week × IfFinal -0.022* (0.014) -0.052*** (0.011)
#Public service place by week × IfFinal 0.049 (0.050) 0.018 (0.051)
Other borrower-related variables Included Included Included Included
Other loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -329.26 -646.33 -265.37 -546.14
#obs. 638 1,276 649 1,298
Table notes are the same as those for Table 6.
contrary, once they found the featureswere far belowor above their thresholds, they held on to their own views.We
offer additional evidence to support this assertion by considering all relevant loan features in Figure C2 (Appendix
C2).
The above result, however, was not found in Group 8. Interestingly, in Models 3 and 4 of Table 4, we noticed
that some alternative features, such as gender and the average transaction amount for purchases of virtual goods
(i.e., “ATV shopping virtual”), had significant coefficients. That is, those additional features explained why the
human evaluators shifted from their initial decisions.5 More importantly, given that those features did not reach
significance when we compared the human evaluators’ initial decisions with those of the machines, it suggests
5 It is possible that the evaluators might have strategically chosen to follow the machines’ decisions if the machine recommended either
approval or rejection.We alleviated this concern in Appendix C3.

(a) Distributions of Living City DPI (b) Distributions of #Outgoing Contacts
a All distributions are based on the samples in which human evaluators’ initial decisions were inconsistent with machines’ decisions.
b F & A: Cases wherein humans followed the machines’ recommendation and ultimately approved the loan applications; F & R: Cases wherein
humans followed the machines’ recommendation and ultimately rejected the loan applications; D & A: Cases wherein humans disagreed with
the machines’ recommendation and ultimately approved the loan applications; D & R: Cases wherein humans disagreed with the machines’
recommendation and ultimately rejected the loan applications.
Figure 5 Feature Distributions of Diverse Cases (Group 7)
that the human evaluators reconsidered their initial decisions. In other words, the presence of large information
volumes and machine explanations provoked evaluators to engage in active rethinking, which improved their final
decision accuracy.
5.3. Disagreement and Decision Quality: Decomposition of the Rethinking Process
As discussed earlier, we observed that with the presence of large information volumes and machine explanations,
humans reconsidered an interesting feature, “ATV shopping virtual”. This feature had not been used by either
humans or machines in the independent decision-making process. The prediction models might have ignored or
downplayed the values of this feature due to its correlations with other features.We conjectured that the attention
to the “ATV shopping virtual” feature stemmed from human evaluators associating it with the “average amount
spent on game cards” feature. When the human evaluators saw the machines making different decisions, they also
noticed that the loans had some irregular patterns on features that the evaluatorswere unfamiliar with (e.g., “average
amount spent on game cards”). However, such features could hardly be applied by human evaluators, as the most
common value by far across all loan applications was 0 (refer to Figure A1a in Appendix A1; the median is 0). Such
a distribution would lead to human evaluators perceiving those features as non-informative. The literature has
suggested that humans are good at building connections between given information and other relevant, familiar, or
understandable information in cognitive processing (Br˚aten and Samuelstuen 2007, Hollnagel 1987). Since game

cards are typical virtual goods and “ATV shopping virtual” had many more salient non-zero values (Figure A1b;
the median is 8.70), human evaluators are likely to attend more to this feature when making decisions.
In Appendix C4, we compared the default rates between Groups 7 and 8 after separating loans “saved” by
the machines (i.e., those that were originally rejected by human evaluators but ultimately approved due to the
machines’ approval recommendations) and those “saved” by human evaluators.We showed that using the updated
decision rules with new and correct features (i.e., significantly correlated with default behavior), human evaluators
were more likely to correctly select “good” loans from those rejected by the machines, whereas humans’ decisions to
overrule the machines resulted in no change or a decrease in efficiency (i.e., replacing some “bad” applications with
other “bad” ones) in Group 7 where humans relied on their priors.Meanwhile, the use of gender features might be
due to their relatively high correlations with “ATV shopping virtual” (refer to Table 5). Such findings also explain
the alleviation of gender bias (which we will demonstrate later, in Section 6.2).Moreover, we conducted a straightforward
post-hoc analysis in Appendix C4 to clarify the allocations of different loan types by humans, machines,
and collaborative efforts. This provided additional insights into how machines and humans could assume distinct
roles to improve overall collaborative performance.
Taking all of the findings together, our results suggest that with a proper design that invokes humans’ active
rethinking (e.g., the presence of effective machine explanations when processing complicated information), the
collaboration between humans and machines could potentially achieve “1+1>2” in practice.Machines would take
responsibility for handling borderline cases, and humans would have the potential to invoke active rethinking to
correct machines’ mistakes in the “random” cases (e.g., those without explicitly congruent feature patterns) when
they perceive that machines have made contradictory decisions, inspired by suggestive information cues.
6. Empirical Extensions
6.1. Heterogeneity by Human Evaluator Characteristics
Recent studies have shown that human agents’ degree of decision-making experience might affect their acceptance
of machine recommendations as well as their performance in collaboration with machines (Luo et al. 2019,Wang
et al. 2023b). Therefore, we decomposed the heterogeneity regarding individual evaluators’ characteristics. Below,
we focus on the evaluators’ experience, based on the length of time (in months) that they had worked in the focal
company before we started the experiment. FollowingMarcotte (1998), the experience was measured at four levels

Table 8 Heterogeneity Analysis of Human Evaluators’ Months Working (Probit Model)
Groups 1 & 2 (only human) Groups 5–8 (human + machine)
Model 1 - DV: IfDefault Model 2 - DV: IfDefault Model 3 - DV: IfConsistent Model 4 - DV: IfFollow
Large info. (L) -0.384*** (0.143) -0.093*** (0.018) -0.221*** (0.083) 0.469** (0.187)
Month of working=1 (Work=1) (baseline) (baseline) (baseline) (baseline)
Work=2 -0.237* (0.127) -0.052 (0.157) 0.141 (0.089) 0.141 (0.163)
Work=3 -0.400*** (0.140) -0.109 (0.162) 0.153* (0.084) -0.212 (0.174)
Work=4 -0.549* (0.146) -0.147* (0.087) 0.194** (0.078) -0.284 (0.186)
L ×Work=1 (baseline) (baseline) (baseline) (baseline)
L ×Work=2 0.305 (0.187) -0.103 (0.259) 0.042 (0.112) 0.093 (0.253)
L ×Work=3 0.355* (0.212) 0.383 (0.254) 0.084 (0.120) 0.229 (0.254)
L ×Work=4 0.356* (0.203) 0.057 (0.263) 0.048 (0.113) 0.359 (0.248)
Explanation (Expl) -0.150 (0.179) 0.114 (0.088) 0.146 (0.192)
Expl ×Work=1 (baseline) (baseline) (baseline)
Expl ×Work=2 0.405 (0.296) 0.036 (0.113) 0.193 (0.251)
Expl ×Work=3 0.057 (0.236) 0.013 (0.120) 0.720*** (0.273)
Expl ×Work=4 -0.021 (0.264) 0.022 (0.122) 0.442* (0.267)
L × Expl -0.278 (0.305) -0.051 (0.121) 0.159 (0.281)
L × Expl ×Work=1 (baseline) (baseline) (baseline)
L × Expl ×Work=2 -0.196 (0.395) 0.070 (0.158) -0.480 (0.366)
L × Expl ×Work=3 -0.383** (0.180) 0.059 (0.165) -0.641* (0.377)
L × Expl ×Work=4 -0.637* (0.375) 0.078 (0.172) -0.998** (0.390)
Borrower-related variables Included Included Included Included
Loan-related variables Included Included Included Included
Evaluator-related variables Included Included Included Included
Log likelihood -822.87 -5,565.09 -957.51 -1,107.73
#obs. 2,716 11,727 5,603 2,216
a Models 1 and 2 are based on the approved samples. Model 3 is based on all loan samples. Model 4 is based on the samples in which human evaluators’ initial
decisions were inconsistent with the machines’ decisions (i.e., IfConsistent= 0).We introduce the definition of IfFollow in Appendix C2.
b Large info. = 1 for the treatment using large information volumes for decision making, 0 for small. Evaluator months working: 1 = not longer than 6 months, 2
= 6–12 months, 3 = 13–18 months, 4 = longer than 18 months. Interpret. = 1 for treatment of disclosing machine explanations, 0 for not.
c Standard errors are in parentheses. Significant results are in bold. *p <0.10, **p <0.05, ***p <0.01.
(1 = not longer than 6 months, 2 = 6–12 months, 3 = 13–18 months, 4 = longer than 18 months). To quantify the
impact of experience levels,we considered another Probit model, this one with three-way interaction terms including
the existence of large information volumes, the availability of machine explanations, and experience levels.We
also included all lower-level interaction terms in the regression.We presented the estimated coefficients in Table 8,
whereinModel 1 considers the default rate asDV and includes humans’ independent decisions only, whileModels
2–4 are in the human–machine collaboration modes. Specifically, we replicated our mechanism tests with heterogeneous
experience levels: whether a loan defaultedwasModel 2’sDV, whether initial decisionswere consistentwas
Model 3’s DV, and whether to follow machines’ decisions was Model 4’s DV. Note that the estimation of Model
4 incorporated only samples where human evaluators’ initial decisions were inconsistent with machine decisions.
Additionally,we offer more comprehensive heterogeneity analyses with alternative characteristics in Appendix D1.
Table 8 yields several interesting findings. First, the positive estimate of L ×Work = 3 (or 4) inModel 1 indicates
that without machine assistance, experienced human evaluators performedworse with a large information volume
than with a small one. Given the definition of work experience, evaluators with a higher experience level might
have accumulated significantly more knowledge in handling small data over a long time, and thus, they might have

found it hard to switch their mindset (i.e., experience inertia) (Becker 1995). Another plausible explanation is that
these more senior evaluators might have less trust in AI, as suggested by Wang et al. (2023b). On the other hand,
evaluators who were new to the company might have still been in the learning stage when the experiment started,
and in such cases, persistent learning could have brought more benefits. Second, we observed that experienced evaluators
tended to make more decisions that were consistent with those of the machines (as shown in the results
of Model 3), especially with small information volumes. This is reasonable because experienced evaluators were
more likely to have learned the feature values comprehensively and reached a similar level of performance as the
machines. Third, the estimates inModel 4 suggest that, when we focused on loans with different initial decisions,
experienced evaluators were more likely to follow machine explanations in a small information scenario but more
likely to overrule machines’ decisions and stick to their own opinions given the availability of large information volumes.
Combining all of these results with those inModel 2 makes it clear that the satisfaction of both conditions
encouraged experienced evaluators to initiate an active rethinking process and thereby achieve reduced borrower
credit risk. Furthermore, to deepen our understanding of how individual heterogeneity influences behavior in the
presence of machine assistance, we replicated our mechanism examinations with different experience levels. The
findings, detailed in Appendix D2, offer more nuanced and straightforward evidence indicating that experienced
evaluators were more inclined to initiate an active rethinking process when provided additional external information
in Group 8.
6.2. Decision Biases
As implied inTable 5, most of themajor variables considered in both the human evaluators’ and machines’ decisionmaking
processes were relatively highly correlated with the performance metric. This confirms the fact that both
humans and machines made decisions based on their estimated credit risk. In the meantime, it is worth noting
that some of the major variables (e.g., loan purpose, average amount spent on game cards, and number of visits to
commercial places)were also highly correlated with gender.Anatural question arises: will this correlation cause any
fairness issues? For example, will it affect the loan-approval decisions of borrowers of different genders, especially
when considering different information volumes and human–machine collaboration modes?
To address this question, we first focused on the final performance as measured by the non-default rates. We
recorded the statistics of each group inTable D5 in Appendix D3.We observed that with large information volumes

(i.e., Group 4), machines tended to favor female applicants, because the non-default rate of the approved male
applicants (98.03%) was much larger than that of female applicants (93.99%). That is, machines seemed to have
exerted a higher loan-approval criterion for males than females. The involvement of human evaluators without
machine explanations (i.e., Group 7) could not alleviate such gender bias. However, when human evaluators were
presented with machine explanations (i.e., Group 8), the final repayment performance of the approved female and
male applicants became better and similar (96.67% vs. 97.55% ), suggesting the mitigation of gender bias.
Following Teodorescu et al. (2021), we additionally applied the criterion of “equalized opportunity” (EOR),
which requires positive outcomes to be independent of the protected attribute, in order to alternatively measure
decision fairness (biases) between genders in our different experimental groups. Let G be the gender
indicator (G = 0 or 1), and Y=1 and ^Y=1 be the correct and actual positive outcomes (i.e., a loan application
being approved in our context), respectively. “Correct” here means that non-default loans (observed
from the repayment performance of the approved loans) got approved. As such, equalized opportunity means
Pr(^Y=1|G=0,Y=1)=Pr(^Y=1|G=1,Y=1). Applying this criterion to our context, EOR describes the decision
biases between genders as follows: EOR = Appr(G=0)/NonD(G=0)
Appr(G=1)/NonD(G=1) , where Appr(G=0) and Appr(G=1) refer to the
approval rates for females and males, and NonD(G=0) and NonD(G=1) refer to the non-default rates for females
and males (calculated within female or male groups), respectively. The closer EOR is to 1, the greater the fairness
is between the genders. The larger the deviation from 1, the more bias there is toward females (EOR>1) or males
(EOR <1). Figure 6 plots the values of EOR across the different experimental groups.
We learned from Figure 6 that the human evaluators treated males and females equally in terms of fairness,
regardless of the volumes of information available (EOR = 1.012 (small amounts of information) and 0.987 (large
amounts of information), both close to 1). That is, the human evaluators tended to apply relatively similar standards
in evaluating the male and female applicants. The machines, however, significantly favored females when
they had large information volumes available for decision-making (EOR = 1.201). This was due mainly to the high
correlation between the most important features used by machines and the default indicator, as shown in Table 5.
This finding is consistent with previous studies (e.g., Fuster et al. 2022) and implies that whereas machines perform
much better in general with large-scale information, they return results that are gender-biased, notwithstanding

a Refer to Table D5 for complete values.
Figure 6 Equalized Opportunity Ratio on Gender
the literature’s demonstration of the value of large-scale information in alleviating certain forms of demographic
discrimination (Lu et al. 2023a). Further, we did not observe any significant change when human evaluators were
involved in making the final decisions with small information volumes (i.e.,EOR= 1.025 and 1.040 in Groups 5 and
6, respectively). However, we did observe a significant reduction in EOR when both large information volumes
and machine explanations were available (i.e., EOR = 1.056 in Group 8). In this scenario, the increase in final decision
accuracy could be attributed to human intervention in correcting the risk evaluations of female borrowers.
Similarly to our findings in Section 5.2, human evaluators associate certain observed features to others (i.e., “ATV
shopping virtual”). Fortunately, the “ATV shopping virtual” feature positively correlates with the feature gender
(refer toTable 5) and default probability. Hence, human evaluators helped mitigate gender biases successfully. This
again highlights the value and necessity of collaboration between humans and machines. It is essential to acknowledge
that the gender bias observed in our dataset and empirical context may be specific to our circumstances. Environments
with a more balanced interaction between genders could potentially avoid this gender-related issue.We
provided a comprehensive discussion about how our findings concerning gender biases could be extrapolated to
other contexts in Appendix D3.
7. Conclusions and Discussion
7.1. Simultaneous Needs of Both Conditions
In the emerging stream of human–machine collaboration literature, there is a dearth of systematic understanding
about when, with machines’ assistance, humans can actively contribute and how they can add extra value to task

outcomes.We dived into the information processing literature, the comprehension of which affords two prerequisites
for invoking humans’ deep thinking: information complexity initially draws humans’ attention to engaging
in the tasks, and useful external cues drive humans to perform active consideration. We applied these theoretical
implications to human–machine collaboration tasks, and accordingly, against the backdrop of the microloan
industry, we devised two treatments by manipulating information volumes and displays of machine explanations.
A unique two-stage field experiment helped us to explicitly quantify the corresponding performance.
Our empirical findings shed light on the significance and compatibility of the two theory-driven conditions,
and showed that neither can be dispensed with. First, although larger information volumes mean more potential
knowledge to help gauge decision-making performance (Hu et al. 2022), our empirical comparisons demonstrated
that humans tended to utilize what they have specialized in (i.e., small information volumes, Group 1 vs. Group
2), because learning is costly and instant feedback might be uncertain.Without effective extrinsic motivation, such
distortionwould further impede humans’ acceptance of machines’ recommendations (Group 4 vs. Group 7). This
is generally detrimental as humans’ insistence on their own decision rules is very likely to result in underfitted
decisions in different tasks (Song et al. 2021).
Second, it was also no surprise to find that offering machine explanations alone, without the presence of large
information volumes, could not inspire humans’ further contribution (Group 6 vs. Group 8). This is owed to
the fact that machines’ superiority in tackling prohibitively (for humans) complex tasks to achieve satisfactory
predictionswas constrained by information availability (also refer to the comparison between Group 3 vs. Group 4
in Figure 3).Ontop of limited information, humans could not becomesmarter than machines. Notably,we noticed
that a few recent studies have focused on the value of machine explanations to human–machine collaboration (e.g.,
Bauer et al. 2023, Jacobs et al. 2021).However, our study suggests contingent factors, such as task complexity,would
impact the effect of machine explanations. Although machine explanations provide humans with more reference
information, humans may not take advantage of them due to insufficient motivation to deeply involve themselves
in decision-making (Speier 2006). Instead, humans were found to involve more trust in machines by following
their recommendations with those “borderline” loans.
Hence, only with the simultaneous presence of large information volumes and machine explanations can
human–machine collaboration realize better performance than humans or machines alone (i.e., experimental

Group 8) via initiating active rethinking. This engagement results in further improvement of decision accuracy
and mitigation of the machines’ biased decisions. Our findings, therefore, confirm the validity of generalizing the
dual-process theories of reasoning from humans’ independent or interpersonal decision-making to the realm of
machine assistance.
7.2. Managerial Implications
This study, built on our unique experimental designs, also offers non-trivial insights to practitioners. Our findings
could inform companies’ future benefit-cost analyses in managing their efforts/investment and balancing among
human agents (human capacities), data purchasing/collecting, and adoption of AI techniques. Our experiments
probe diverse possible and manageable factors that could negatively affect the desired efficiency of human–machine
collaboration, andwe showempirical evidence of those factors’ roles in the overall decision-making process. What’s
more, this paper presents practitioners with a caveat to their prevalent preference for big data, AI techniques,
and/or human–machine collaboration. Specifically, if big data is available, this collaboration can achieve both satisfactory
decision-making efficiency and fairness. However, when faced with the threat of machines taking their
jobs or the possibility of over-domination by machine intelligence, human employees across companies and even
industries might resist machine assistance or begin to rely on it excessively. Thus, we provide a scheme of machine
interpretability to encourage human agents to rely less on machines and to create additional value. If only small
data is available (e.g., affordable), a machine alone seems enough. The involvement of human efforts, regardless of
whether machine explanations are present or absent, cannot add significant value in improving prediction accuracy
or addressing gender biases in this case.Moreover, our empirical analyses not only offer guidance to platforms
in designing efficient collaboration systems but also open pathways to gaining valuable insights into hiring decisions.
In particular, our heterogeneity analyses highlight that individuals with experience possess greater potential
to attain elevated levels of collaborative performance and amend machine biases through more systematic contributions.
Nevertheless, even with experienced employees, platforms should not neglect the importance of refining
their training approaches and procedures. This includes the implementation of comprehensive data literacy training
programs (Hvalshagen et al. 2023), providing valuable cues and timely feedback for decision-making improvement
loops (Proctor and Bonbright 2021). Additionally, it is essential to incorporate modules on ethics and bias
awareness into training programs (Sellier et al. 2019).

7.3. Discussions of Generalizability
Our theory-driven experimental design and empirical findings are highly generalizable to other contexts where
the decision-making task objectives are not excessively intricate for humans or machines and/or can be clearly formulated.
Moreover, the applicability extends to scenarios where opportunities exist to acquire additional information,
whether in terms of volume or type, to enhance overall performance (Amit and Sagiv 2013). Examples
of such tasks include job candidate screening in labor hiring, supplier evaluation in procurement, and medical
treatment decision-making. On a broader note, our study suggests that machines consistently outperform human
agents when tasked with objectives that are not particularly challenging, such as classification or prediction problems
involving structured objects and features. The availability of a large amount of information might stimulate
human agents to pay attention to their tasks, but it does not guarantee that they will aid machines. Additional cues,
such as machine explanations, are crucial for guiding human agents to perform an active rethinking of complex
information to deal with uncertainties, thereby producing better outcomes.
However, it is worth discussing some caveats to practical system designs as they relate to the generalizability
of our results. Our findings regarding the two conditions essential for stimulating active information processing
in humans are contingent on many surrounding factors. For example, humans should be responsible for their
decision-making performance to some degree, thereby preventing the complete delegation of decision-making to
machines. Humans’ loan-approval capabilities should also be associated with the ultimate collaboration performance.
Also, selected AI algorithms should be suitable for tackling the specific task objectives and models need to
bewell-trained. Regarding the two focal treatments, rich information is not a panacea; any newly acquired information
must be inherently valuable to bolster decision-making performance. In addition, machine explanations must
be delivered in a clear and compelling manner. As there might be disagreement between human (expert) knowledge
and machine explanations (Krishna et al. 2022), the explanations should be suitably displayed, understandable, and
able to stimulate cognitive reasoning (e.g., enabling easy comparisons). Lastly, as proposed byWang et al. (2023b),
humanworkers’ prior knowledge of AI and their responsibilities assigned are factors associated with their attitudes
toward AI. In our specific context, human evaluators generally had limited knowledge of machine learning, and
the compensation structure within the platform (as outlined in Section 3.1) did not encourage evaluators to proactively
enhance their understanding to achieve superior performance levels. In other settings where human workers
are more proficient in AI and have stronger motivation to consistently refine their task performance, the responses
to machine recommendations (with or without machine explanations) may exhibit variation.
From the technical perspective, our empirical results also reinforce our theory-guided design approach to some
extent, as our two proposed treatments did not explicitly rely on any specific form of machine interpretability.
As long as they could offer clear signals, we deemed them potentially valuable in encouraging humans to reassess
their perspectives.Moreover, while centered on the implementation of specific machine-learning algorithms, our
empirical analyses and findings can be extrapolated to diverse applications involving advanced and more intricate
AI techniques.Onthe one hand, our targeted interventionswere guided by theory and offered insights into human
behavioral responses to factors including task complexity and reference cues. Additionally, our experimental design
deliberately withheld information about the specific machine-learning algorithms from participants, making it
possible to extend our observations to other AI models, despite potential variations in actual performance and
opportunities for human contributions.
Additionally, it is worth noting that in our primary study, we cannot evaluate the value of AI identity explicitly.
Put differently, our empirical results do not conclusively discern whether the observed effects stem from the
additional information offered by machines (or senior managers) or from the direct attitudes of humans toward
AI or machines. However, it is crucial to acknowledge that the performance of senior managers in real-world scenarios
may not exhibit the same level of stability and efficiency as machines, especially given the vast amounts of
information involved. Considering the time constraints inherent in making accurate decisions, AI or machines
tend to outperform their experienced human counterparts.Moreover, several research papers have delved into the
difficulties faced by humans when attempting to articulate or summarize the rules guiding their decision-making
processes (Hu et al. 2022). Compared to reliance on machines, relying on senior managers to provide explicit decision
cues is more challenging. In contrast, machines offer the advantage of leveraging advanced techniques such as
feature importance extraction. This underscores the significance of fostering collaboration between humans and
machines.
Finally, our experimental design emphasized the efficacy of a two-stage decision-making process wherein where
human evaluators initially make independent loan approval decisions and subsequently determine their final decisions
by opting to adopt or reject the machine recommendations. While recognizing that two-stage designs may
be practically infeasible, we suggest the potential relevance of our findings in scenarios where only a single stage
is feasible–directly presenting machine recommendations to the original human-alone decision-making scenario.
However, this adjustment may influence decision-making outcomes. For example, without a distinct independent
decision-making stage, the direct provision of machine recommendations may lead to over-reliance on machines
or foster distrust due to the absence of a clear contrast to humans’ independent decisions. The lack of explicit comparisons
may further hinder rule identification, especially among less experienced individuals, resulting in more
significant heterogeneity in decision-making performance compared to a two-stage setting.
7.4. Limitations and Directions for Future Studies
Our paper has several limitations that provide promising opportunities for future research. First, our empirical
design focused on a static scenario without human learning. However, in a real-world environment, humans
and machines might learn from each other’s decision-making processes and adjust gradually over a relatively long
period. Future research can extend our analyses to disentangle learning behavior and thereby design an optimization
strategy for both sides using techniques such as reinforcement learning models. Second, our experimental
treatment considered a binary case between small and large information volumes. Future studies can relax this
constraint and explore a continuous level of information complexity, the insights from which could offer business
managers more practical conclusions and increased value. Third, in our empirical setup,we deliberately limited the
experimental period to one or twoweeks to establish a controlled environment, which helped us mitigate potential
biases introduced by human learning behaviors evolving over time.We acknowledge the temporal constraint as a
limitation in our study, paving the way for future investigations. Extending the experimental period would enable
researchers to explore how humans process and value information conditions over an extended period, offering
valuable insights into the dynamics of long-term interactions. Fourth, divergence in terms of cultural background
or industry domain might have affected our findings. Similar studies in other countries or industries can further
validate these findings and offer novel insights into human–machine collaboration designs.