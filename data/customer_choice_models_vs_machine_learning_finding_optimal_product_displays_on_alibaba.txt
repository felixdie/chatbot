Feldman J, Zhang DJ, Liu X, Zhang N (2022) Customer choice models vs. machine learning:
Finding optimal product displays on Alibaba. Operations Research 70(1):309‚Äì328.

We 
compare 
the 
performance 
of 
two 
approaches 
for 
nding 
the 
optimal 
set 
of 
products 
to 
display 
to 
customers 
landing 
on 
Alibaba's 
two 
online 
marketplaces, 
Tmall 
and 
Taobao. 
Both 
approaches 
were 
placed 
online 
simultaneously 
and 
tested 
on 
real 
customers 
for 
one 
week. 
The 
rst 
approach 
we 
test 
is 
Alibaba's 
current 
practice. 
This 
procedure 
embeds 
thousands 
of 
product 
and 
customer 
features 
within 
a 
sophisticated 
machine 
learning 
algorithm 
that 
is 
used 
to 
estimate 
the 
purchase 
probabilities 
of 
each 
product 
for 
the 
customer 
at 
hand. 
The 
products 
with 
the 
largest 
expected 
revenue 
(revenue 
√ó 
predicted 
purchase 
probability) 
are 
then 
made 
available 
for 
purchase. 
The 
downside 
of 
this 
approach 
is 
that 
it 
does 
not 
incorporate 
customer 
substitution 
patterns; 
the 
estimates 
of 
the 
purchase 
probabilities 
are 
independent 
of 
the 
set 
of 
products 
that 
eventually 
are 
displayed. 
Our 
second 
approach 
uses 
a 
featurized 
multinomial 
logit 
(MNL) 
model 
to 
predict 
purchase 
probabilities 
for 
each 
arriving 
customer. 
In 
this 
way 
we 
use 
less 
sophisticated 
machinery 
to 
estimate 
purchase 
probabilities, 
but 
we 
employ 
a 
model 
that 
was 
built 
to 
capture 
customer 
purchasing 
behavior 
and, 
more 
specically, 
substitution 
patterns. 
We 
use 
historical 
sales 
data 
to 
t 
the 
MNL 
model 
and 
then, 
for 
each 
arriving 
customer, 
we 
solve 
the 
cardinality-constrained 
assortment 
optimization 
problem 
under 
the 
MNL 
model 
online 
to 
nd 
the 
optimal 
set 
of 
products 
to 
display. 
Our 
experiments 
show 
that 
despite 
the 
lower 
prediction 
power 
of 
our 
MNL-based 
approach, 
it 
generates 
signicantly 
higher 
revenue 
per 
visit 
compared 
to 
the 
current 
machine 
learning 
algorithm 
with 
the 
same 
set 
of 
features. 
We 
also 
conduct 
various 
heterogeneous-treatment-eect 
analyses 
to 
demonstrate 
that 
the 
current 
MNL 
approach 
performs 
best 
for 
sellers 
whose 
customers 
generally 
only 
make 
a 
single 
purchase. 


Key 
words 
: 
choice 
models, 
product 
assortment, 
machine 
learning, 
eld 
experiment, 
retail 
operations 


1. 
Introduction 
The 
assortment 
optimization 
problem 
has 
come 
to 
be 
one 
of 
the 
most 
well-studied 
problems 
in 
the 
eld 
of 
revenue 
management. 
In 
this 
problem, 
a 
retailer 
seeks 
the 
revenue-maximizing 
set 
of 
products 
(or 
assortment) 
to 
oer 
each 
arriving 
customer. 
In 
its 
simplest 
form, 
the 
assortment 
optimization 
problem 
does 
not 
place 
any 
restrictions 
on 
the 
set 
of 
feasible 
assortments 
the 
retailer 


* 
The 
rst 
two 
authors 
contributed 
equally 
and 
are 
ranked 
alphabetically. 
1 








can 
oer. 
This 
version 
of 
the 
problem 
is 
often 
referred 
to 
as 
the 
uncapacitated 
assortment 
optimization 
problem. 
A 
well-studied 
extension 
of 
this 
simplest 
version 
adds 
a 
cardinality 
constraint, 
which 
limits 
the 
total 
number 
of 
products 
the 
retailer 
can 
include 
in 
any 
oered 
assortment. 
When 
each 
displayed 
product 
consumes 
the 
same 
amount 
of 
space 
(physical 
or 
virtual), 
this 
constraint 
is 
akin 
to 
a 
limit 
on 
the 
available 
shelf 
space 
or 
a 
restriction 
on 
the 
number 
of 
products 
that 
can 
be 
displayed 
on 
a 
single 
web 
page. 
For 
example, 
on 
the 
mobile 
version 
of 
Amazon, 
there 
is 
a 
section 
labeled 
\customers 
who 
bought 
this 
also 
bought,‚Äù 
where 
at 
most 
three 
products 
can 
be 
recommended. 


The 
central 
diculty 
in 
each 
variant 
of 
the 
assortment 
problem 
is 
that 
the 
retailer 
must 
carefully 
balance 
the 
appeal 
of 
her 
assortment 
as 
a 
whole 
in 
addition 
to 
the 
relative 
appeal 
of 
the 
individual 
products 
that 
are 
most 
protable. 
Adding 
a 
product 
to 
an 
assortment 
diversies 
a 
retailer's 
display 
and 
thus 
increases 
her 
market 
share, 
but 
this 
additional 
product 
can 
cannibalize 
sales 
of 
the 
products 
that 
previously 
were 
a 
part 
of 
the 
assortment. 
The 
exact 
nature 
of 
this 
trade-off 
is 
dictated 
by 
the 
underlying 
customer 
choice 
model 
through 
which 
the 
retailer 
models 
customer 
purchasing 
behavior. 
For 
a 
xed 
assortment, 
these 
models 
map 
product 
and 
customer 
features 
to 
the 
individual 
purchase 
probabilities 
of 
products 
included 
in 
this 
assortment. 
A 
variety 
of 
customer 
choice 
models 
have 
been 
developed 
in 
the 
‚Äò 
‚Äò 
‚Äò 
`economics 
and 
marketing 
literature 
to 
capture 
dierent 
nuances 
in 
customer 
purchasing 
behavior. 
Unfortunately, 
there 
is 
no 
perfect 
choice 
model, 
since 
the 
models 
that 
capture 
the 
most 
general 
forms 
of 
customer 
behavior 
are 
precisely 
those 
with 
an 
overwhelming 
number 
of 
parameters 
to 
estimate 
and 
whose 
corresponding 
assortment 
problems 
are 
dicult 
to 
solve. 


The 
necessity 
to 
better 
understand 
this 
trade-off 
with 
regard 
to 
the 
wealth 
of 
existing 
choice 
models 
has 
given 
rise 
to 
two 
general 
research 
problems 
that 
over 
the 
last 
decade 
have 
guided 
much 
of 
the 
work 
in 
the 
eld 
of 
revenue 
management. 
These 
two 
problems 
are 
summarized 
below. 


1. 
Estimation: 
Can 
a 
choice 
model's 
parameters 
be 
eciently 
and 
accurately 
estimated 
from 
historical 
data? 
2. 
Assortment 
Optimization: 
Given 
a 
fully 
specied 
customer 
choice 
model, 
is 
it 
possible 
to 
develop 
ecient 
algorithms 
with 
provable 
performance 
guarantees 
for 
the 
uncapacitated 
assortment 
problem 
and 
variants 
thereof? 
In 
developing 
answers 
to 
the 
questions 
above, 
the 
eld 
progresses 
towards 
the 
ultimate 
goal 
of 
developing 
revenue 
management 
systems 
that 
use 
assortment 
optimization 
to 
guide 
product 
oering 
decisions. 
However, 
the 
recent 
success 
of 
machine 
learning 
methods 
as 
powerful 
tools 
for 
prediction 
calls 
into 
question 
the 
practical 
relevance 
of 
customer 
choice 
models 
and 
their 
corresponding 
assortment 
problems. 
In 
other 
words, 
if 
it 
is 
indeed 
the 
case 
that 
machine 
learning 
models 
can 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 

their 
ability 
to 
accurately 
predict 
customer 
purchasin
g 
patterns, 
then 
why 
would 
a 
retailer 
ever 
adopt 
the 
latter? 
The 
answer, 
we 
nd, 
is 
that 
accurate 
predictions 
alone 
are 
not 
enough 
to 
guarantee 
that 
subsequent 
operational 
decisions 
made 
from 
these 
estimates 
will 
be 
protable. 
Of 
equal 
importance, 
is 
the 
sophistication 
with 
which 
the 
subsequent 
optimization 
problem 
captures 
key 
operational 
trade-os. 


We 
derive 
these 
insights 
by 
implementing 
and 
testing 
two 
distinct 
large-scale 
product 
recommender 
systems 
in 
collaboration 
with 
the 
Alibaba 
Group, 
the 
largest 
Chinese 
online 
and 
mobile 
commerce 
company 
whose 
Gross 
Merchandise 
Value 
(GMV) 
has 
surpassed 
US$485 
billion 
as 
of 
2016. 
More 
specically, 
we 
consider 
a 
setting 
where 
Alibaba 
must 
present 
customized 
six-product 
assortments 
to 
inquiring 
customers, 
with 
the 
goal 
of 
maximizing 
revenue. 
The 
customers 
are 
presented 
with 
these 
personalized 
six-product 
displays 
after 
receiving 
discount 
coupons, 
which 
can 
be 
applied 
to 
any 
of 
the 
six 
oered 
products. 
Henceforth, 
we 
refer 
to 
this 
problem 
as 
the 
Alibaba 
Product 
Display 
Problem. 
The 
rst 
approach 
that 
we 
develop 
and 
test 
uses 
the 
classic 
multinomial 
logit 
(MNL) 
model 
(Luce 
1959, 
McFadden 
1974) 
to 
capture 
customer 
preferences, 
and 
then 
solves 
a 
cardinality 
constrained 
assortment 
optimization 
to 
guide 
product 
display 
decisions. 
The 
second 
approach 
is 
Alibaba's 
current 
practice, 
which 
utilizes 
sophisticated 
machine 
learning 
methods 
to 
understand 
customer 
purchasing 
patterns. 
The 
implementation 
of 
both 
approaches 
unfolds 
in 
two 
steps 
that 
sequentially 
address 
how 
to 
estimate 
demand, 
and 
then 
how 
to 
use 
these 
estimates 
to 
identify 
protable 
six 
product 
displays. 
We 
refer 
to 
this 
rst 
step 
as 
the 
estimation 
problem, 
and 
the 
second 
as 
the 
assortment 
problem. 


Contributions. 
Our 
foremost 
contribution 
is 
that 
we 
show 
that 
product 
recommendation 
systems 
built 
on 
the 
framework 
of 
the 
MNL 
model 
have 
the 
potential 
to 
outperform 
machine-learning-based 
approaches. 
We 
show 
this 
result 
by 
conducting 
a 
large-scale 
eld 
experiment 
for 
one 
week 
in 
March 
2018 
involving 
more 
than 
5 
million 
customers. 
This 
experiment 
compares 
the 
performance 
of 
our 
MNL-based 
approach 
with 
Alibaba's 
state-of-the-art 
machine-learning-based 
approach, 
when 
both 
approaches 
use 
the 
top 
25 
features 
that 
are 
most 
predictive 
of 
customers‚Äô 
purchasing 
behaviors. 
Interestingly, 
we 
nd 
that 
the 
tted 
machine 
learning 
models 
produce 
far 
more 
accurate 
estimates 
of 
the 
purchase 
probabilities 
than 
the 
tted 
MNL 
models, 
yet 
the 
MNL-based 
approach 
generates 
28% 
higher 
revenue 
per 
visit. 
Furthermore, 
in 
September 
2018, 
we 
conduct 
another 
ve-day-long 
large-scale 
experiment 
involving 
more 
than 
3 
million 
customers, 
which 
compares 
our 
MNL-based 
approach 
with 
Alibaba's 
state-of-the-art 
machine-learning-based 
approach 
on 
all 
available 
features. 
In 
this 
full-feature 
setting, 
we 
demonstrate 
that 
our 
MNL-based 
approach 
generates 
3:37% 
higher 
revenue 
per 
visit 
compared 
to 
Alibaba's 
current 
machine-learning-based 
recommender 
system. 
Due 
to 
data 
security 
issues, 
we 
were 
not 
given 
access 
to 
visit-level 
data 
from 
Alibaba 
for 
this 
second 
experiment 
so 
we 
cannot 
compute 
the 
accuracy 
of 
the 
tted 
model 
for 
each 
approach. 
However, 








we 
were 
given 
the 
day-level 
aggregated 

approach. 


As 
mentioned 
above, 
the 
ever-growing 
suite 
of 
machine 
learning 
algorithms 
are 
powerful 
tools 
for 
prediction; 
they 
help 
up 
us 
uncover 
and 
understand 
complex 
patterns 
in 
our 
data. 
However, 
the 
estimates 
derived 
from 
these 
approaches 
might 
not 
capture 
critical 
problem 
specic 
nuances 
due 
to 
the 
fact 
that 
they 
were 
developed 
as 
general 
tools. 
In 
contrast, 
the 
MNL 
model 
is 
simple, 
but 
it 
was 
specically 
created 
to 
capture 
customer 
purchasing 
behavior, 
and 
more 
specically, 
to 
account 
for 
substitution 
patterns; 
the 
phenomenon 
that 
describes 
the 
event 
in 
which 
a 
customer 
settles 
for 
a 
suitable 
alternative 
when 
she 
does 
not 
nd 
her 
most 
preferred 
product 
available 
for 
purchase. 
Consequently, 
when 
this 
classic 
model 
is 
used 
to 
capture 
the 
demand 
for 
each 
product, 
we 
ultimately 
consider 
a 
more 
nuanced 
version 
of 
the 
assortment 
problem, 
in 
which 
substitution 
eects 
are 
critically 
accounted 
for. 


Our 
second 
contribution 
comes 
in 
the 
form 
of 
a 
novel 
approximation 
scheme 
for 
a 
special 
constrained 
version 
of 
the 
assortment 
problem 
under 
the 
MNL 
model. 
Our 
eld 
experiments 
only 
consider 
the 
problem 
of 
choosing 
optimal 
six 
product 
displays, 
however 
there 
are 
other 
operational 
levers 
that 
Alibaba 
could 
exploit 
in 
this 
discount 
coupon 
setting 
to 
increase 
revenues. 
In 
Appendix 
B, 
we 
consider 
two 
such 
levers, 
namely 
price 
and 
icon 
size. 
For 
the 
pricing 
problem, 
Alibaba 
must 
simultaneously 
decide 
which 
products 
to 
oer 
as 
well 
as 
the 
prices 
to 
charge 
for 
each 
of 
these 
oered 
products. 
For 
the 
icon 
size 
problem, 
we 
allow 
for 
Alibaba 
to 
choose 
the 
size 
of 
the 
product 
icon 
for 
each 
displayed 
product. 
In 
this 
setting, 
there 
is 
a 
limit 
on 
the 
total 
available 
screen 
space 
for 
all 
displayed 
products, 
but 
we 
do 
not 
enforce 
that 
exactly 
six 
products 
have 
to 
oered. 
Our 
work 
in 
this 
section 
builds 
off 
the 
results 
in 
Davis 
et 
al. 
(2013) 
and 
Feldman 
and 
Paul 
(2017), 
who 
both 
provide 
general 
schemes 
for 
solving 
constrained 
assortment 
problems. 
We 
present 
these 
results 
in 
Appendix 
B 
in 
an 
eort 
to 
avoid 
breaking 
up 
the 
exposition 
of 
the 
two 
approaches 
that 
we 
develop 
and 
test, 
which 
are 
the 
backbone 
of 
our 
work. 


Finally, 
while 
we 
make 
substantial 
progress 
in 
establishing 
the 
practical 
underpinning 
for 
assortment 
optimization, 
there 
is 
still 
plenty 
of 
work 
to 
be 
done 
to 
cement 
this 
notion. 
Along 
these 
lines, 
an 
important 
contribution 
of 
our 
work 
is 
that 
it 
sheds 
light 
on 
new 
directions 
for 
work 
on 
assortment 
optimization 
that 
is 
focused 
on 
shifting 
research 
in 
this 
realm 
closer 
to 
the 
sphere 
of 
practicality. 
We 
delay 
a 
detailed 
description 
of 
these 
new 
problems 
until 
Section 
7, 
since 
their 
importance 
is 
magnied 
once 
the 
details 
of 
our 
current 
system, 
along 
with 
its 
accompanying 

aws, 
are 
understood. 
Nonetheless, 
we 
highlight 
the 
potential 
for 
future 
work 
early 
on 
to 
emphasize 
that 
even 
though 
our 
current 
MNL-based 
approach 
is 
quite 
fruitful, 
there 
are 
many 
opportunities 
for 
improvement. 








Organization. 
The 
majority 
of 
the 
remaining 
content 
is 
centered 
around 
providing 
all 
of 
the 
necessary 

2, 
we 
provide 
a 
detailed 
overview 
of 
the 
discount 
coupon 
setting 
that 
we 
consider 
on 
Alibaba, 
which 
is 
the 
canvas 
for 
our 
eld 
experiments. 
Sections 
3 
and 
4 
describe 
how 
we 
address 
the 
respective 
estimation 
and 
assortment 
problems 
within 
the 
two 
approaches 
that 
we 
test. 
Our 
goal 
in 
these 
two 
sections 
is 
not 
only 
to 
provide 
important 
implementation 
details, 
but 
also 
to 
highlight 
advantages 
and 
disadvantages 
inherent 
to 
each 
approach. 
This 
discussion 
helps 
us 
explain 
the 
superior 
performance 
of 
our 
MNL-
based 
approach, 
and 
also 
sheds 
light 
on 
additional 
advantages 
of 
using 
choice 
models 
to 
capture 
customer 
purchasing 
patterns. 
The 
full 
details 
and 
results 
of 
our 
eld 
experiments 
regarding 
the 
25-feature 
versions 
of 
both 
approaches 
are 
given 
in 
Sections 
5 
and 
6 
respectively. 
Due 
to 
the 
data 
availability 
issues 
mentioned 
above, 
the 
results 
for 
the 
experiments 
comparing 
the 
full-feature 
versions 
of 
the 
two 
approaches 
is 
relegated 
to 
Appendix 
D. 


1.1. 
Related 
Literature 
There 
is 
an 
expansive 
collection 
of 
previous 
works 
regarding 
customer 
choice 
models 
and 
their 
accompanying 
assortment 
and 
estimation 
problems. 
Consequently, 
it 
is 
beyond 
the 
scope 
of 
this 
work 
to 
provide 
a 
full 
summary 
of 
all 
the 
past 
studies 
with 
related 
themes. 
Instead, 
since 
the 
focus 
of 
this 
paper 
is 
on 
the 
MNL 
choice 
model, 
we 
review 
past 
work 
that 
primarily 
relates 
to 
logitbased 
choice 
models, 
including 
the 
MNL, 
mixed 
multinomial 
logit 
(Mixed-MNL) 
and 
nested 
logit 
choice 
models. 
In 
doing 
so, 
we 
highlight 
the 
advantages 
and 
disadvantages 
of 
using 
each 
of 
these 
choice 
models 
with 
regard 
to 
the 
tractability 
of 
their 
corresponding 
assortment 
and 
estimation 
problems. 
We 
also 
include 
a 
summary 
of 
past 
work 
on 
the 
network 
revenue 
management 
problem, 
and 
in 
doing 
so, 
we 
illustrate 
that 
the 
revenue 
management 
community 
has 
implicitly 
considered 
the 
trade-os 
between 
using 
machine 
learning 
models 
versus 
customer 
choice 
models 
to 
capture 
demand 
for 
many 
years. 
However, 
they 
have 
done 
so 
without 
any 
sort 
of 
empirical 
test. 


The 
MNL 
choice 
model 
and 
mixtures 
thereof. 
The 
MNL 
choice 
model 
is 
perhaps 
the 
most 
well-
studied 
choice 
model 
in 
the 
revenue 
management 
literature. 
As 
mentioned 
earlier, 
the 
MNL 
originally 
was 
conceived 
by 
Luce 
(1959) 
and 
its 
practical 
use 
later 
was 
most 
notably 
established 
by 
McFadden 
(1974), 
who, 
among 
other 
things, 
shows 
that 
the 
log-likelihood 
function 
is 
concave 
in 
the 
model 
parameters. 
To 
the 
best 
of 
our 
knowledge, 
Vulcano 
et 
al. 
(2012) 
are 
the 
rst 
to 
explicitly 
consider 
estimating 
the 
parameters 
of 
an 
MNL 
choice 
model 
in 
a 
revenue 
management 
context. 
Instead 
of 
directly 
maximizing 
the 
log-likelihood, 
they 
develop 
an 
iterative 
expectation-
maximization 
(EM) 
approach 
that 
is 
based 
on 
uncensoring 
the 
most 
preferred 
product 
of 
each 
customer. 
Later, 
Vulcano 
and 
Abdullah 
(2018) 
use 
a 
similar 
minorization-maximization 
(MM) 
algorithm 
to 
estimate 
the 
parameters 
of 
an 
MNL 
model 
from 
historical 
transaction 
data. 
They 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 

accurate 
estimates 
while 
being 
computationally 
superior 
to 
the 
previously 
mentioned 
EM 
approach. 


The 
seminal 
works 

(2004) 
establish 
that 
the 
assortment 
optimization 
problem 
under 
the 
MNL 
admitted 
an 
optimal 
polynomial-time 
algorithm. 
These 
works 
show 
that 
the 
optimal 
assortment 
in 
this 
setting 
is 
a 
so-called 
revenue-ordered 
assortment 
that 
consists 
of 
some 
subset 
of 
the 
highest 
revenue 
products. 
When 
a 
cardinality 
constraint 
is 
added 
to 
the 
assortment 
problem, 
Rusmevichientong 
et 
al. 
(2010) 
provide 
a 
purely 
combinatorial 
polynomial-time 
algorithm, 
which 
is 
able 
to 
identify 
the 
optimal 
assortment. 
Building 
on 
this 
result, 
Davis 
et 
al. 
(2013) 
show 
that 
the 
MNL 
assortment 
problem 
subject 
to 
any 
set 
of 
totally 
uni-
modular 
(TU) 
constraints 
can 
be 
formulated 
as 
a 
concise 
linear 
program. 
They 
go 
on 
to 
show 
that 
a 
variety 
of 
realistic 
operational 
constraints 
can 
be 
encoded 
as 
TU 
constraint 
structures, 
including 
various 
forms 
of 
the 
aforementioned 
cardinality 
constraint. 


The 
mixed 
MNL 
choice 
model 
segments 
the 
customer 
population 
into 
multiple 
customer 
types 
whose 
buying 
decisions 
are 
each 
governed 
by 
a 
unique 
MNL 
model. 
Interestingly, 
McFadden 
and 
Train 
(2000) 
show 
that 
this 
mixed 
MNL 
model 
is 
the 
most 
general 
choice 
model 
built 
on 
the 
classic 
framework 
of 
random 
utility 
maximization 
(RUM), 
in 
which 
arriving 
customers 
associate 
random 
utilities 
with 
each 
oered 
product 
and 
then 
purchase 
the 
product 
with 
the 
largest 
positive 
utility. 
Given 
that 
the 
mixed 
MNL 
model 
has 
the 
potential 
to 
capture 
a 
broad 
spectrum 
of 
consumer 
purchasing 
behavior, 
there 
has 
been 
much 
work 
in 
recent 
years 
that 
studies 
its 
corresponding 
estimation 
and 
assortment 
problems. 
For 
example, 
Subramanian 
et 
al. 
(2018) 
formulate 
the 
estimation 
problem 
as 
an 
innite 
dimensional 
convex 
program 
and 
then 
provide 
a 
conditional 
gradient 
approach 
that 
exploits 
the 
structure 
of 
the 
choice 
probabilities 
to 
yield 
a 
local 
maximum 
of 
the 
log-likelihood. 
This 
modeling 
richness 
comes 
at 
a 
cost, 
however, 
as 
Desir 
et 
al. 
(2014) 
show 
that 
it 
is 
NP-Hard 
to 
approximate 
the 
assortment 
problem 
under 
the 
mixed 
MNL 
model 
within 
a 
factor 
of 
O(n1ÙÄÄÄ) 
for 
any 
> 
0. 
In 
fact, 
Rusmevichientong 
et 
al. 
(2014) 
show 
that 
the 
problem 
remains 
NP-Hard 
even 
when 
the 
underlying 
population 
is 
described 
by 
only 
two 
customer 
types. 
On 
the 
positive 
side, 
Desir 
et 
al. 
(2014) 
provide 
an 
fully 
polynomial-time 
approximation 
scheme 
(FPTAS) 
for 
the 
assortment 
problem 
whose 
run 
time 
scales 
exponentially 
in 
the 
number 
of 
customer 
types. 
When 
the 
preferences 
of 
the 
customer 
types 
take 
on 
a 
special 
nested 
structure, 
Feldman 
and 
Topaloglu 
(2017) 
show 
that 
an 
FPTAS 
can 
be 
salvaged 
whose 
run 
time 
is 
polynomial 
in 
all 
input 
parameters. 


The 
nested 
logit 
choice 
model. 
Under 
the 
nested 
logit 
choice 
model, 
the 
products 
are 
partitioned 
into 
nests 
and 
each 
customer's 
purchasing 
process 
unfolds 
in 
two 
steps: 
the 
customer 
rst 
selects 
a 
nest 
and 
then 
makes 
a 
purchase 
from 
among 
the 
products 
in 
the 
chosen 
nest. 
We 
point 
the 
reader 
to 
Train 
(2009) 
for 
an 
excellent 
formal 
description 
of 
this 
model 
as 
well 
as 
an 
iterative 
maximum 








likelihood 
approach 
for 
estimating 
its 
underlying 

the 
uncapacitated 
assortment 
problem 
under 
the 
nested 
logit 
model. 
They 
show 
that 
the 
problem's 
hardness 
depends 
on 

making 
a 
purchase 
after 
having 
selected 
a 
nest. 
In 
cases 
where 
a 
customer 
must 
make 
a 
purchase, 
the 
authors 
show 
that 
the 
problem 
admits 
a 
polynomial-time 
algorithm, 
which 
cleverly 
exploits 
the 
structure 
of 
the 
choice 
probabilities. 
On 
the 
other 
hand, 
the 
assortment 
problem 
is 
shown 
to 
be 
NP-Hard 
in 
cases 
where 
the 
customer 
can 
leave 
the 
store 
without 
making 
a 
purchase 
at 
either 
of 
the 
two 
steps 
in 
the 
purchasing 
process. 
Gallego 
and 
Topaloglu 
(2014) 
and 
Feldman 
and 
Topaloglu 
(2015) 
devise 
constant 
factor 
approximations 
for 
various 
constrained 
versions 
of 
the 
assortment 
problem 
under 
the 
nested 
logit 
model. 
Li 
et 
al. 
(2015) 
provide 
an 
exact 
polynomial-time 
algorithm 
for 
assortment 
optimization 
under 
the 
d-level 
nested 
logit 
model, 
in 
which 
nesting 
of 
the 
products 
is 
d-levels 
deep. 


Network 
revenue 
management. 
The 
revenue 
management 
community 
has 
implicitly 
considered 
the 
trade-os 
between 
using 
machine 
learning 
models 
versus 
customer 
choice 
models 
to 
capture 
demand 
for 
many 
years 
but 
without 
a 
formal 
empirical 
test. 
Consider, 
for 
example, 
the 
classical 
network 
revenue 
management 
problem, 
where 
the 
goal 
is 
to 
adjust 
the 
set 
of 
oered 
products 
over 
a 
selling 
horizon 
when 
the 
sale 
of 
each 
product 
consumes 
a 
combination 
of 
resources. 
On 
the 
one 
hand, 
there 
are 
numerous 
papers 
(Talluri 
and 
Van 
Ryzin 
1998, 
1999, 
Topaloglu 
2009, 
Adelman 
2007) 
that 
consider 
this 
problem 
under 
the 
so-called 
independent 
demand 
model, 
where 
the 
probability 
that 
product 
j 
is 
purchased 
in 
time 
period 
t 
is 
given 
by 
pjt. 
On 
the 
other 
hand, 
there 
are 
many 
other 
works 
(Zhang 
and 
Cooper 
2005, 
Zhang 
and 
Adelman 
2009, 
Talluri 
2010, 
Gallego 
et 
al. 
2011) 
that 
consider 
the 
same 
problem 
when 
customer 
purchasing 
patterns 
are 
guided 
by 
a 
choice 
model, 
and 
so 
the 
probability 
that 
product 
j 
is 
purchased 
in 
time 
period 
t 
is 
Pj 
(St),where 
St 
is 
the 
assortment 
of 
products 
oered 
in 
time 
period 
t. 
In 
practice, 
it 
is 
unclear 
which 
of 
these 
two 
approaches 
will 
be 
most 
lucrative. 
Further, 
the 
fundamental 
trade-off 
between 
the 
two 
approaches 
is 
exactly 
the 
one 
we 
consider 
when 
evaluating 
the 
two 
approach 
we 
implement 
for 
the 
Alibaba 
Display 
Problem. 
Namely, 
when 
customer 
behavior 
is 
governed 
by 
the 
independent 
demand 
model, 
the 
purchase 
probabilities 
can 
be 
accurately 
estimated 
with 
sophisticated 
machine 
learning 
methods. 
However, 
when 
these 
estimates 
seed 
the 
subsequent 
optimization 
problem 
whose 
solution 
dictates 
the 
set 
of 
products 
oered 
in 
each 
time 
period, 
this 
decision 
will 
once 
again 
be 
made 
without 
accounting 
substitution 
eects 
between 
products. 


Recommender 
Systems 
We 
also 
contribute 
to 
a 
broad 
literature 
that 
studies 
how 
to 
design 
recommender 
systems 
in 
digital 
platforms 
(for 
a 
comprehensive 
review, 
please 
refer 
to 
Ricci 
et 
al. 
(2011)). 
The 
traditional 
recommender 
system 
literature 
often 
focuses 
on 
collaborative 
ltering; 
a 
matrix 
factorization 
technique 
that 
helps 
infer 
a 
customer's 
preference 
towards 
a 
product 
based 
on 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 

1998). 
Recently, 
researchers 
have 
started 
to 
consider 
ensemble 
ties 
of 
each 
customer 
on 
recommender 
platforms 
(Jahrer 
et 
al. 
2010). 
Covington 
et 
al. 
(2016) 
detail 
YouTube's 
deep 
ough 
rate, 
which 
then 
feeds 
into 
the 
videos 
recommendations 
that 
this 
user 
receives. 
Similar 
to 
the 
machine-learning-based 
approach 
employed 
by 
Alibaba, 
YouTube's 
recommender 
system 
relies 
more 
heavily 
on 
the 
estimation 
phase, 
and 
then 
solves 
a 
simple 
optimization 
problem 
to 
determine 
the 
personalized 
set 
of 
recommended 
videos. 


Retailing 
operations. 
Lastly, 
our 
paper 
relates 
closely 
to 
the 
literature 
that 
studies 
assortment 
(Caro 
and 
Gallien 
2007, 
Gallego 
et 
al. 
2016), 
inventory 
(Caro 
and 
Gallien 
2010, 
Cachon 
et 
al. 
2018), 
and 
pricing 
(Ferreira 
et 
al. 
2015, 
Papanastasiou 
and 
Savva 
2016) 
problems 
in 
a 
retailing 
context. 
Caro 
and 
Gallien 
(2010) 
provide 
early 
seminal 
work 
in 
this 
stream, 
designing 
and 
implementing 
a 
system 
to 
help 
fashion 
retailer 
Zara 
distribute 
limited 
inventory 
across 
stores. 
Ferreira 
et 
al. 
(2015) 
incorporate 
machine 
learning 
with 
optimization 
and 
work 
with 
online 
retailer 
Rue 
La 
La 
to 
design 
a 
dynamic 
pricing 
system. 
Cachon 
et 
al. 
(2018) 
estimate 
the 
impact 
of 
inventory 
on 
sales 
at 
car 
dealerships 
and 
propose 
an 
inventory 
policy 
to 
maximize 
variety. 
Golrezaei 
et 
al. 
(2014) 
considers 
a 
dynamic 
assortment 
problem 
in 
which 
a 
retailer 
is 
allowed 
to 
personalize 
the 
assortment 
of 
products 
oered 
to 
each 
arriving 
customer 
in 
response 
to 
just-revealed 
features 
and 
the 
current 
inventory 
levels 
of 
each 
product. 


2. 
Alibaba's 
Retail 
Setting 
and 
Product 
Display 
Problem 
In 
this 
section, 
we 
begin 
by 
detailing 
the 
retail 
context 
on 
Alibaba 
where 
we 
run 
our 
eld 
experiments. 
After 
introducing 
and 
describing 
this 
setting, 
we 
provide 
a 
general 
formulation 
of 
the 
Alibaba 
Product 
Display 
Problem 
along 
with 
a 
high-level 
overview 
of 
the 
two 
approaches 
that 
ultimately 
are 
implemented. 
The 
fundamental 
dierence 
between 
these 
two 
approaches 
is 
the 
manner 
in 
which 
customer 
demand 
is 
modeled 
and 
estimated. 
In 
the 
rst 
approach, 
which 
is 
Alibaba's 
current 
practice, 
machine 
learning 
models 
are 
used 
to 
estimate 
customer 
buying 
patterns 
and 
then 
a 
simple 
optimization 
problem 
is 
solved 
to 
choose 
which 
products 
to 
display. 
The 
second 
approach 
captures 
customer 
buying 
patterns 
through 
the 
MNL 
choice 
model, 
whose 
estimated 
parameters 
seed 
the 
assortment 
optimization 
problem 
that 
we 
then 
solve 
to 
guide 
product 
display 
decisions. 


2.1. 
The 
Alibaba 
Platform 
We 
begin 
by 
broadly 
discussing 
the 
two 
online 
marketplaces, 
Taobao.com 
and 
Tmall.com, 
that 
Alibaba 
has 
fostered 
to 
help 
connect 
third-party 
sellers 
to 
consumers; 
these 
are 
the 
platforms 
where 
we 
conduct 
our 
experiments. 
To 
help 
motivate 
our 
goal 
of 
maximizing 
revenue, 
we 
also 
describe 
how 
Alibaba 
monetizes 
from 
these 
two 
marketplaces. 








Figure 
1 
Recommendation 
on 
Alibaba 
in 

Taobao.com 
is 
China's 
largest 
peer-to-peer 
retailing 
platform 
for 

listing 
fees 
on 
Taobao.com, 
and 
hence 
Alibaba 
monetizes 
its 
services 
on 
Taobao.com 
by 
charging 
fees 
for 
advertisements 

Tmall.com 
is 
China's 
largest 
third-party 
business-toconsumer 
platform 
for 
branded 
goods, 
such 
as 
Nike 
and 
Adidas. 
Sellers 
on 
Tmall.com 
are 
required 
to 
pay 
a 
minimum 
deposit 
when 
opening 
a 
store 
and 
an 
annual 
commission 
fee 
to 
Alibaba 
based 
on 
their 
revenue 
on 
the 
platform. 
This 
commission 
fee 
ranges 
from 
0.5 
to 
5 
percent 
depending 
on 
a 
seller's 
product 
category.1 


With 
regard 
to 
Tmall.com, 
it 
is 
clear 
that 
Alibaba 
garners 
a 
larger 
prot 
when 
customers 
spend 
more, 
since 
Alibaba 
collects 
a 
small 
fraction 
of 
each 
seller's 
revenue 
for 
this 
marketplace. 
In 
the 
case 
of 
Taobao.com, 
it 
is 
also 
generally 
believed 
that 
Alibaba's 
prots 
are 
proportional 
to 
revenue 
because 
sellers 
whose 
revenues 
are 
largest 
are 
also 
those 
who 
are 
likely 
to 
spend 
more 
on 
advertising. 
Consequently, 
Alibaba 
primarily 
uses 
total 
revenue 
to 
assess 
the 
protability 
of 
product 
display 
algorithms, 
even 
though 
the 
company 
does 
not 
take 
commission 
fees 
from 
sellers 
on 
Taobao.com. 
Hence 
throughout 
this 
paper, 
our 
objective 
is 
always 
to 
maximize 
the 
total 
revenue 
of 
each 
arriving 
customer. 
For 
the 
sake 
of 
brevity, 
we 
hereafter 
refer 
to 
Taobao.com 
and 
Tmall.com 
jointly 
as 
\the 
Alibaba 
platform.‚Äù 


1 
http://about.tmall.com/tmall/fee 
schedule 








2.2. 
The 
Alibaba 
Platform's 
Product 
Display 
Problem 
We 
begin 
with 
a 
high-level 
overview 
of 
product 
display 
systems 
on 
Alibaba 
before 
focusing 
on 
the 
exact 
nature 
of 
the 
setting 
that 
we 
study. 
Throughout 
the 
paper, 
we 
use 
the 
terms 
\product 
display 
system‚Äù 
and 
\product 
recommendation 
system‚Äù 
synonymously. 
Given 
that 
the 
Alibaba 
platform 
essentially 
is 
a 
two-sided 
marketplace 
that 
matches 
customers 
with 
 
to 
developing 
optimal 
product 
display 
algorithms 
to 
ensure 
customers 
are 
shown 
products 
that 
are 
protable. 
Broadly 
speaking, 
Alibaba's 
recommendation 
algorithms 
cater 
to 
two 
distinct 
settings, 
which 
we 
refer 
to 
as 
\the 
public 
domain‚Äù 
and 
\the 
private 
domain.‚Äù 
Product 
recommendation 
algorithms 
applied 
in 
the 
public 
domain 
are 
applied 
across 
the 
entire 
platform 
and 
hence 
are 
not 
seller-specic. 
For 
example, 
the 
Tmall 
marketplace 
front 
page 
on 
Alibaba's 
mobile 
application 
(as 
shown 
in 
the 
left 
panel 
of 
Figure 
1) 
is 
considered 
public 
domain. 
The 
product 
recommendation 
problem 
in 
this 
case 
is 
that 
of 
nding 
the 
optimal 
set 
of 
products 
across 
all 
sellers 
on 
the 
platform 
for 
each 
arriving 
customer. 
On 
the 
other 
hand, 
the 
private 
domain 
refers 
to 
pages 
that 
are 
specic 
to 
a 
particular 
seller. 
For 
example, 
the 
front 
page 
of 
Hstyle, 
the 
largest 
online 
women's 
apparel 
company 
on 
Alibaba's 
platform, 
is 
considered 
private 
domain 
(as 
shown 
in 
the 
right 
panel 
of 
Figure 
1). 
Product 
recommendation 
algorithms 
on 
the 
private 
domain 
only 
promote 
products 
specic 
to 
an 
individual 
seller. 
We 
reiterate 
that 
all 
recommendation 
algorithms 
on 
Alibaba 
are 
highly 
personalized; 
if 
a 
customer 
lands 
on 
the 
front 
page 
of 
Tmall 
twice 
in 
a 
single 
day, 
for 
example, 
it 
is 
possible 
the 
recommended 
products 
may 
change 
as 
a 
result 
of 
this 
customer's 
interactions 
(clicks, 
searches, 
purchases, 
etc. 
. 
. 
. 
) 
within 
the 
app 
between 
arrivals. 


Our 
Alibaba 
Product 
Display 
Problem 
falls 
within 
the 
realm 
of 
private-domain 
product 
recommendation 
algorithms. 
In 
particular, 
we 
focus 
on 
a 
product 
recommendation 
problem 
that 
results 
when 
customers 
are 
given 
seller-specic 
discount 
coupons. 
Customers 
acquire 
these 
coupons 
by 
clicking 
on 
a 
coupon 
icon 
that 
is 
presented 
at 
the 
top 
of 
each 
seller's 
front 
page. 
Upon 
acquiring 
the 
coupon, 
customers 
enter 
a 
coupon 
sub-page 
that 
contains 
six 
displayed 
products, 
each 
of 
which 
can 
be 
purchased 
at 
a 
discount 
using 
the 
coupon. 
Alibaba 
chooses 
to 
display 
only 
six 
products 
since 
this 
is 
the 
largest 
number 
of 
products 
that 
can 
be 
displayed 
within 
a 
single 
page 
on 
a 
mobile 
device. 
Figure 
2 
shows 
how 
a 
customer 
progresses 
from 
a 
seller's 
front 
page 
to 
the 
coupon 
sub-page 
to 
the 
six 
displayed 
products. 
We 
note 
that 
this 
coupon 
feature 
is 
only 
available 
on 
Alibaba's 
mobile 
app, 
but 
this 
does 
not 
limit 
the 
scope 
of 
our 
experiments 
since 
the 
majority 
of 
Alibaba 
customers 
use 
the 
mobile 
app 
to 
shop. 
As 
evidence, 
in 
scal 
year 
2017, 
the 
mobile 
GMV 
(i.e., 
revenue 
generated 
through 
mobile 
devices) 
was 
RMB 
2,981 
trillion 
(equivalent 
to 
USD 
436 
trillion), 
representing 
79% 
of 
total 
GMV 
through 
all 
channels.2 


2 
https://www.alibabagroup.com/en/news/press 
pdf/p170518.pdf 








Figure 
2 
The 
Process 
of 
Landing 
on 
Our 
Recommendation 
Page 



Our 
eld 
experiments 
focus 
exclusively 
on 
two 
competing 
approaches 
for 
nding 
the 
revenue-
maximizing 
set 
of 
six 
products 
to 
make 
available 
to 
each 
customer 
who 
visits 
a 
coupon 
sub-page, 
as 
shown 
in 
Figure 
2. 
As 
of 
March 
2018 
(just 
before 
our 
experiment), 
there 
were 
approximately 
250 
thousand 
sellers 
on 
the 
Alibaba 
platform 
who 
oered 
the 
mobile 

25 
million 
unique 
page 
views 
on 
their 
coupon 
sub-pages 
and 
generate 
over 
RMB 
127 
million 
(equivalent 
to 
USD 
20 
million) 
in 
GMV. 
Consequently, 
even 
small 
improvements 
to 
this 
aspect 
of 
Alibaba's 
recommendation 
systems 
can 
lead 
to 
huge 
gains 
in 
prot. 


To 
help 
formalize 
our 
Alibaba 
Product 
Display 
Problem, 
we 
let 
N 
= 
f1;:. 
:;n} 
be 
the 
universe 
of 
products 
that 
a 
particular 
seller 
potentially 
could 
oer 
on 
the 
coupon 
sub-page. 
Sellers 
on 
the 
Alibaba 
platform 
typically 
have 
between 
100 
to 
2000 
unique 
SKUs, 
all 
of 
which 
are 
in 
the 
same 
product 
category 
and 
hence 
can 
be 
loosely 
considered 
substitutes. 
We 
let 
rj 
be 
the 
revenue 
of 
product 
j 
2N 
, 
which 
represents 
the 
revenue 
garnered 
from 
the 
sale 
of 
a 
single 
unit 
of 
product 
j. 
We 
let 
Pjt 
be 
the 
probability 
that 
customer 
t 
purchases 
product 
j. 
As 
indicated 
by 
its 
dependence 
on 
t, 
this 
purchase 
probability 
term 
will 
be 
uniquely 
determined 
for 
each 
arriving 
customer. 
The 
Alibaba 
Product 
Display 
Problem 
for 
customer 
t 
is 
given 
below: 


X 


max 
rj 
¬∑ 
Pjt. 
(Alibaba 
Product 
Display 
Problem) 


SN 
:jSj=6 


j2S 


In 
order 
to 
fully 
formulate 
the 
above 
problem, 
we 
must 
rst 
choose 
a 
functional 
form 
for 
the 
purchase 
probabilities 
Pjt. 
We 
consider 
two 
alternatives, 
both 
of 
which 
parameterize 
the 
purchase 
probability 
term 
using 
a 
number 
of 
product 
and 
customer 
features. 
In 
both 
cases, 
the 
dependence 








of 
Pjt 
on 
these 
features 
is 
estimated 
from 
historical 
sales 
data: 
the 
estimation 
problem. 
These 
estimates 
then 
seed 
the 
Alibaba 
Product 
Display 
Problem, 
for 
which 
an 
ecient 
algorithm 
must 
be 
developed: 
the 
assortment 
problem. 
Along 
these 
lines, 
we 
assume 
throughout 
the 
paper 
that 
an 
\approach‚Äù 
for 
the 
Alibaba 
Product 
Display 
Problem 
includes 
both 
a 
procedure 
for 
deriving 
estimates 
of 
the 
purchase 
probabilities 
from 
historical 
sales 
data 
and 
an 
algorithm 
to 
nd 
the 
optimal 
six 
product 
displays 
once 
the 

\solving‚Äù 
the 
Alibaba 
Product 
Display 
Problem, 
we 
are 
strictly 
referring 
to 
developing 
an 
algorithm 
to 
solve 
the 
assortment 
problem. 


3. 
The 
Estimation 
Problem 
In 
this 
section, 
we 
describe 
the 
two 
approaches 
used 
to 
estimate 
the 
purchase 
probabilities 
Pjt 
that 
seed 
the 
Alibaba 
Product 
Display 
Problem. 
The 
rst 
approach 
embeds 
hundreds 
of 
product 
and 
customer 
features 
within 
sophisticated 
machine 
learning 
algorithms. 
This 
approach 
is 
Alibaba's 
current 
practice 
for 
solving 
the 
estimation 
problem. 
The 
second 
approach 
ts 
featurized 
MNL 
models 
to 
the 
historical 
sales 
data 
using 
maximum 
likelihood 
estimation 
(MLE). 
While 
the 
latter 
approach 
can 
be 
described 
in 
full 
detail, 
we 
are 
are 
not 
able 
to 
provide 
the 
exact 
details 
of 
the 
machine-learning-based 
approach 
due 
to 
condentiality 
concerns; 
however, 
we 
intend 
to 
provide 
enough 
details 
so 
that 
the 
advantages 
and 
drawbacks 
of 
this 
approach 
can 
be 
well 
understood. 
Further, 
at 
the 
conclusion 
of 
this 
section, 
we 
provide 
a 
case 
study 
comparing 
the 
tting 
accuracy 
of 
an 
o-the-shelf 
machine 
learning 
algorithms 
with 
that 
of 
the 
MNL 
model 
using 
historical 
sales 
data 
from 
the 
top 
ten 
sellers 
(based 
on 
trac) 
from 
Tmall.com. 
The 
intent 
of 
this 
case 
study 
is 
to 
show 
that 
it 
is 
not 
dicult 
to 
develop 
machine-learninh-based 
estimation 
schemes 
that 
outperform 
the 
MNL 
model 
in 
terms 
of 
tting 
accuracy. 
However 
the 
essential 
question 
that 
we 
ultimately 
investigate 
in 
Section 
6, 
where 
the 
results 
of 
our 
large-scale 
eld 
experiment 
are 
revealed, 
is 
whether 
these 
gains 
in 
tting 
accuracy 
lead 
to 
more 
protable 
six-product 
oerings 
on 
the 
coupon 
sub-pages. 


Available 
sales 
data 
and 
product/customer 
features. 
Before 
diving 
into 
the 
details 
of 
either 
approach, 
we 
rst 
discuss 
the 
makeup 
of 
the 
available 
historical 
sales 
data 
used 
to 
t 
the 
machine 
learning 
algorithms 
and 
the 
MNL 
model. 
This 
training 
data 
is 
composed 
of 
historical 
sales 
information 
from 
œÑ 
past 
customers, 
each 
of 
whom 
is 
shown 
six 
products. 
For 
each 
arriving 
customer 
t, 
we 
let 
St 
N 
be 
the 
six 
displayed 
products, 
which 
the 
system 
stores 
as 
vectors 
of 
representative 
feature 
values. 
The 
product 
features 
that 
are 
used 
include 
high-dimensional 
static 
features, 
such 
as 
a 
one-hot 
encoding 
representation 
of 
product 
ID 
and 
seller 
ID, 
in 
addition 
to 
low-dimensional 
static 
features, 
such 
as 
product 
category. 
Dynamic 
product 
features, 
which 
are 
updated 
constantly 
based 
on 
customer 
interactions, 
are 
also 
included 
in 
the 
feature 
set. 
Examples 
of 
dynamic 
product 
features 
include 
the 
number 
of 
reviews 
and 
price, 
which 
are 
refreshed 
every 
second. 
Finally, 
we 








note 
that 
product 
features 
are 
also 
engineered 
from 
product 
descriptions 
and 
pictures. 
For 
example, 
there 
is 
a 
feature 
associated 
with 
the 
image 
quality 
of 
each 
product's 
icon 
that 
is 
displayed 
to 
customers 
within 
the 
app. 
The 
system 
also 
records 
an 
associated 
feature 
vector 
that 
describes 
the 
characteristics 
of 
the 
customer 
at 
hand. 
The 
customer-specic 
features 
include 
demographic 
information, 
such 
as 
age, 
gender, 
and 
registration 
time. 
Other 
customer 
features 
are 
descriptive 
of 
past 
behaviors 
within 
the 
app, 
e.g., 
the 
number 
of 
products 
viewed, 
collected, 
purchased, 
and 
returned 
in 

of 
each 
customer 
and 
product 
pair. 
These 
joint 
features 
can 
be 
thought 
of 
as 
scores 
that 
represent 
estimates 
of 
the 
extent 
to 
which 
the 
particular 
product 
will 
appeal 
to 
the 
particular 
customer. 
These 
scores 
are 
computed 
by 
a 
large 
collaborative 
ltering 
system 
(Linden 
et 
al. 
2003), 
which 
uses 
past 
purchase 
and 
click 
data 
from 
the 
given 
customer 
and 
other 
customers 
who 
are 
deemed 
to 
have 
similar 
preferences. 
Since 
these 
collaborative 
ltering 
scores 
depend 
on 
customer 
behavior 
within 
the 
app, 
they 
are 
dynamically 
updated 
so 
that 
they 
re
ect 
current 
trends. 
In 
total, 
hundreds 
of 
features 
‚Äì 
numerical 
and 
categorical, 
static 
and 
dynamic 
‚Äì 
are 
available 
to 
be 
used 
within 
the 
estimation 
schemes. 


3.1. 
Fitting 
Machine 
Learning 
Models 
In 
what 
follows, 
we 
formalize 
the 
machine-learning-based 
approach 
for 
estimating 
the 
purchase 
probabilities 
Pjt. 
Each 
observation 
within 
the 
training 
data 
set 
can 
be 
described 
as 
a 
triple 
(Xjt;Cjt;Zjt) 
corresponding 
to 
a 
specic 
arriving 
customer 
t 
and 
displayed 
product 
j 
‚àà 
St. 
The 
vector 
Xjt 
gives 
the 
features 
associated 
with 
the 
particular 
observation, 
while 
the 
output 
or 
target 
variables 
Cjt;Zjt 
2f0, 
1} 
denote 
whether 
customer 
t 
clicked 
or 
purchased 
displayed 
product 
j 
respectively. 
We 
set 
Cjt 
= 
1 
if 
customer 
t 
clicked 
on 
product 
j 
and 
Cjt 
= 
0 
otherwise. 
Similarly, 
we 
set 
Zjt 
= 
1 
if 
customer 
t 
purchased 
product 
j 
and 
Zjt 
= 
0 
otherwise. 
Note 
that 
we 
must 
have 
Zjt 
‚â§ 
Cjt 
since 
a 
product 
cannot 
be 
purchased 
unless 
it 
is 
clicked. 
In 
total, 
the 
training 
data 
consists 
of 
T 
=6œÑ 
(since 
each 
customer 
is 
shown 
six 
products) 
observations, 
which 
we 
represent 
as 
PurchaseHistoryML 
= 
f(Xjt;Cjt;Zjt): 
t 
=1;:::;T;j 
‚àà 
Stg. 
We 
note 
that 
for 
this 
approach, 
each 
observation 
(Xjt;Cjt;Zjt) 
‚àà 
PurchaseHistoryML 
does 
not 
encode 
the 
set 
of 
products 
that 
were 
oered 
alongside 
product 
j 
to 
customer 
t. 
On 
the 
one 
hand, 
this 
is 
the 
classic 
setup 
of 
supervised 
learning 
problems, 
which 
makes 
the 
task 
of 
estimating 
customer 
click 
and 
purchase 
probabilities 
amenable 
to 
the 
full 
suite 
of 
powerful 
machine 
learning 
tools. 
However, 
the 
drawback 
of 
this 
approach 
is 
that 
the 
estimates 
of 
the 
purchase 
probabilities 
are 
independent 
of 
the 
assortment 
of 
products 
displayed 
and 
hence 
do 
not 
account 
for 
customer 
substitution 
behaviors. 
Consequently, 


3 
"Collecting‚Äù 
a 
product 
on 
Alibaba 
is 
analogous 
to 
adding 
a 
product 
to 
a 
wish 
list 
on 
Amazon. 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 

Display 
Problem 
in 
identifying 
protable 
six-product 
displays 
could 
suer 
due 
to 
the 
fact 
that 
it 
does 
not 
account 
for 
key 
operational 
trade-os. 


The 
training 
data 
is 
used 
to 
solve 
two 
independent 
estimation 
problems, 
which 
are 
then 
combined 
to 
form 
estimates 
of 
the 
purchase 
probabilities 
Pjt. 
First, 
the 
training 
data 
is 
used 
to 
derive 
estimates 
of 
the 
click 
probabilities 
P(Cjt 
= 
1), 
which 
represent 
the 
probability 
that 
customer 
t 
will 
click 
on 
product 
j. 
To 
do 
so, 
various 
machine 
learning 
algorithms 
are 
employed, 
which 
are 
nely 
tuned 
to 
match 
the 
past 
click 
history 
described 
in 
PurchaseHistoryML. 
We 

f(Xjt), 
which 
maps 
customer 
and 
product 
features 
to 
estimates 
of 
click 
probabilities. 
Along 
the 
same 
lines, 
Alibaba 
tries 
a 
similar 
collection 
of 
machine 
learning 
approaches 
to 
uncover 
a 
function 
g(Xjt), 
which 
produces 
accurate 
estimates 
of 
the 
conditional 
purchase 
probabilities 
P(Zjt 
=1jCjt 
= 
1). 
Ultimately, 
Alibaba 
uses 
Pjt(Xjt)= 
f(Xjt) 
¬∑ 
g(Xjt) 
as 
their 
estimates 
of 
the 
purchase 
probabilities, 
where 
we 
now 
explicitly 
express 
this 
probability 
as 
a 
function 
of 
the 
feature 
vector 
Xjt. 
It 
is 
important 
to 
note 
that 
in 
this 
setting, 
the 
estimates 
of 
the 
purchase 
probabilities 
are 
independent 
of 
the 
displayed 
assortment. 


The 
current 
system 
implements 
various 
models 
and 
ensembles 
their 
predictions 
together 
for 
both 
estimation 
problems. 
These 
models 
include 
regularized 
logistic 
regression 
(Ravikumar 
et 
al. 
2010), 
gradient-boosted 
decision 
trees 
(Friedman 
2002), 
and 
deep 
learning 
(LeCun 
et 
al. 
2015). 
As 
of 
the 
time 
our 
system 
is 
deployed 
(i.e., 
March 
2018), 
regularized 
logistic 
regression 
and 
gradient-boosted 
decision 
trees 
contribute 
the 
most 
to 
the 
nal 
prediction 
outcome 
due 
to 
their 
superior 
prediction 
performance 
compared 
to 
that 
of 
deep 
neural 
networks. 
The 
implementation 
of 
these 
machine 
learning 
algorithms 
is 
conducted 
oine 
using 
historical 
purchases 
from 
a 
seven-day 
rolling 
window. 
For 
example, 
the 
model 
on 
March 
8, 
2018, 
will 
be 
trained 
on 
observations 
from 
March 
1, 
2018, 
to 
March 
7, 
2018, 
and 
the 
model 
on 
March 
9, 
2018, 
will 
be 
trained 
on 
data 
from 
March 
2, 
2018, 
to 
March 
8, 
2018. 
On 
average, 
we 
have 
between 
20 
million 
and 
30 
million 
observations 
within 
these 
seven-day 
windows. 
It 
takes 
approximately 
30 
minutes 
to 
train 
the 
machine 
learning 
model 
and 
upload 
the 
result 
to 
the 
parameter 
cache 
server 
to 
speed 
up 
inference. 


3.2. 
Fitting 
The 
MNL 
Model 
In 
this 
section, 
we 
formally 
introduce 
the 
MNL 
choice 
model 
and 
describe 
how 
its 
underlying 
parameters 
are 
t 
to 
historical 
sales 
data. 
The 
tted 
parameters 
of 
the 
underlying 
MNL 
model 
are 
then 
used 
to 
derive 
estimates 
of 
the 
purchase 
probabilities 
that 
seed 
the 
Alibaba 
Product 
Display 
Problem. 
In 
contrast 
to 
the 
machine-learning-based 
approach, 
the 
MNL 
model 
is 
simple, 
but 
it 
was 
created 
with 
the 
intention 
to 
capture 
customer 
purchasing 
behavior 
and, 
more 
specically, 
substitution 
patterns. 
Consequently, 
while 
the 
estimates 
produced 
from 
the 
tted 
MNL 
models 
might 
not 
be 
as 
accurate 
as 
those 
produced 
by 
the 
machine 
learning 
based 
approach, 
the 
resulting 
Alibaba 
Product 
Display 
Problem 
is 
more 
sophisticated 
due 
to 
the 
fact 
that 
the 
purchase 
probabilities 
will 
be 
a 
function 
of 
the 
displayed 
assortment 
of 
products. 








The 
MNL 
choice 
model. 
We 
begin 

MNL 
choice 
model 
falls 
under 
the 
general 
RUM 
framework, 
in 
which 
arriving 
customers 
associate 
random 
utilities 
with 
the 
oered 
products 
and 
are 
then 
assumed 
to 
purchase 
the 
product 
with 
the 
highest 
positive 
utility. 
Under 
the 
MNL 
choice 
model, 
the 
random 
utility 
Ujt 
that 
customer 
t 
associates 
with 
product 
j 
is 
written 
as 
the 
sum 
of 
a 
deterministic 
component 
Vjt 
and 
an 
i.i.d. 
Gumbel 
random 
variable 
denoted 
as 
jt. 
More 
formally, 
we 
have 
that 


Ujt 
= 
Vjt 
+ 
jt. 


In 
order 
to 
incorporate 
product 
and 
customer 
features 
within 
the 
above 
utility 
function, 
one 
can 
write 

where 
the 
vector 
Xjt 
denotes 
the 
values 
of 
the 
relevant 
features 
for 
customer 
t 
and 
product 
j. 
In 
this 
setting, 
we 
featurize 
the 
utility 
functions 
using 
only 
the 
top 
25 
product 
and 
customer 
features 
based 
on 
feature 
importance 
scores 
that 
the 
machine 
learning 
estimation 
algorithms 
return. 


With 
this 
notation 
in 
hand, 
we 
can 
present 
the 
explicit 
expression 
for 
the 
purchase 
probabilities 
under 
the 
MNL 
choice 
model. 
Again, 
we 
index 
the 
universe 
of 
n 
products 
by 
the 
set 
N 
= 
f1;:::;ng. 
In 
addition 
to 
these 
n 
products, 
we 
assume 
there 
is 
an 
ever-present 
dummy 
product 
with 
index 
0, 
which 
is 
included 
in 
each 
assortment 
that 
the 
retailer 
potentially 
could 
oer. 
This 
product 
is 
often 
labeled 
the 
no-purchase 
option 
and 
it 
represents 
the 
option 
for 
the 
customer 
to 
leave 
the 
store 
without 
making 
a 
purchase. 
Throughout 
the 
paper 
we 
assume 
that 
V0t 
= 
0, 
which 
is 
an 
assumption 
that 
can 
be 
made 
without 
loss 
of 
generality. 
Under 
the 
MNL 
model, 
if 
the 
retailer 
oers 
assortment 
St 
‚äÜ 
N 
to 
customer 
t, 
then 
the 
probability 
that 
product 
j 
‚àà 
St 
is 
purchased 
is 
given 
by 


0 


e 


Pjt(St;Xt)= 
P 
Xjt 
;
1+ 
i2S 
e0Xit 


where 
Xt 
= 
fXjt 
: 
j 
‚àà 
St} 
gives 
the 
features 
associated 
with 
each 
of 
the 
oered 
products. 
In 
this 
setting, 
the 
purchase 
probabilities 
depend 
explicitly 
on 
both 
the 
product/customer 
features 
and 
the 
set 
of 
displayed 
products. 
When 
we 
move 
to 
the 
assortment 
problem, 
the 
coecients 
Œ≤ 
will 
be 
xed 
and 
we 
will 
dene 
vjt 
= 
e0Xjt 
to 
denote 
the 
preference 
weight 
that 
customer 
t 
associates 
with 
product 
j. 


Fitting 
the 
MNL 
choice 
model. 
We 
use 
maximum 
likelihood 
estimation 
(MLE) 
to 
derive 
estimates 
for 
the 
Œ≤ 
coecients. 
We 
formulate 
the 
likelihood 
using 
historical 
sales 
data 
from 
œÑ 
customers. 
More 
specically, 
we 
represent 
the 
past 
purchasing 
history 
of 
the 
œÑ 
customers 
as 
the 
set 
PurchaseHistoryMNL 
= 
f(St;Xt;zt): 
t 
=1;:. 
:;œÑ 
g, 
where 
we 
note 
again 
that 
St 
denotes 
the 
set 
of 
six 
displayed 
products 
and 
Xt 
= 
fXjt 
: 
j 
‚àà 
St} 
gives 
their 
associated 
features. 
The 
term 
zt 
gives 
the 
product 
that 
was 
purchased, 
where 
we 
set 
zt 
= 
0 
if 
the 
customer 
did 
not 
purchase 
any 
of 
the 
oered 








products. 
For 
customers 
who 
purchased 
multiple 
products, 
we 
treat 
each 
purchase 
independently 
and 
hence 
create 

purchased. 
To 
illustrate 
how 
we 
handle 
events 
where 
an 
arriving 
customer 
makes 
multiple 
purchases, 
we 
consider 
a 
simplied, 
featureless 
setting 
where 
customer 
t 
is 
oered 
products 
St 
= 
f1, 
2, 
3} 
and 
purchases 
products 
1 
and 


2. 
In 
this 
case, 
our 
purchase 
history 
will 
contain 
the 
data 
points 
(f1, 
2, 
3g, 
1) 
and 
(f1, 
2, 
3g, 
2). 
With 
this 
notation 
in 
place, 
we 
formulate 
the 
MLE 
problem 
of 
interest 
below 
max 
LL(Œ≤ 
| 
PurchaseHistoryMNL) 
(1) 


Œ≤ 


where 




XX 


LL(Œ≤ 
| 
PurchaseHistoryMNL)= 
0Xzt;t 
‚àí 
log(1 
+ 
e 

objective 
is 
the 
log-likelihood 
written 
as 
a 
function 
of 
the 
purchasing 
history 
of 
the 
œÑ 
customers. 
In 
the 
above 
MLE 
problem, 
we 
seek 
the 
Œ≤ 
coecients, 
which 
maximize 
this 
log-likelihood 
function. 
It 
is 
well 
known 
(see 
McFadden 
(1974)) 
that 
the 
objective 
function 
in 
(1) 
is 
concave 
in 
the 
Œ≤ 
coecient. 
Hence, 
when 
œÑ 
is 
relatively 
small, 
o-the-shelf 
nonlinear 
optimization 
solvers, 
such 
as 
MATLAB's 
fmnincon, 
are 
sucient 
for 
solving 
the 
MLE 
problem. 
For 
example, 
Vulcano 
et 
al. 
(2012) 
and 
Topaloglu 
and 
Simsek 
(2017) 
employ 
this 
approach 
to 
estimate 
the 
parameters 
of 
an 
MNL 
choice 
model 
in 
test 
cases 
where 
œÑ 
never 
exceeds 
50,000. 


In 
our 
setting, 
we 
continuously 
resolve 
(1) 
on 
a 
rolling 
week-long 
basis 
similar 
to 
the 
machinelearning-
based 
approach, 
and 
hence 
we 
have 
œÑ 
‚âà 
20 
‚àí 
30 
million. 
Further, 
there 
is 
an 
inherent 
data 
censorship 
issue 
that 
results 
due 
to 
no-purchase 
events, 
further 
complicating 
the 
estimation 
process. 
Recall 
that 
when 
a 
no-purchase 
event 
is 
observed, 
we 
have 
zt 
= 
0. 
Unfortunately, 
it 
is 
impossible 
to 
know 
if 
the 
arriving 
customer 
did 
not 
make 
a 
purchase 
because 
she 
was 
not 
satised 
with 
the 
set 
of 
oered 
products 
or 
because 
she 
never 
intended 
to 
make 
a 
purchase 
in 
the 
rst 
place. 
We 
refer 
to 
customers 
of 
this 
latter 
type 
as 
\browsers.‚Äù 
The 
former 
scenario 
provides 
a 
signal 
of 
how 
the 
customer 
valued 
the 
set 
of 
oered 
products, 
while 
data 
from 
the 
latter 
case 
should 
be 
discarded. 
Consequently, 
appropriately 
dierentiating 
between 
these 
two 
cases 
is 
critical 
for 
deriving 
accurate 
estimates 
of 
the 
Œ≤ 
values. 
In 
our 
setting, 
approximately 
95% 
of 
the 
observations 
correspond 
to 
no-purchase 
events, 
and 
hence 
the 
manner 
in 
which 
this 
censorship 
issue 
is 
dealt 
with 
has 
nontrivial 
eects 
on 
the 
accuracy 
of 
the 
estimates 
produced. 


This 
censorship 
issue 
is 
not 
new 
when 
it 
comes 
to 
solving 
the 
estimation 
problem 
for 
various 
choice 
models. 
For 
example, 
van 
Ryzin 
et 
al. 
(2010) 
and 
van 
Ryzin 
and 
Vulcano 
(2017) 
develop 
EM 
algorithms 
to 
deal 
with 
the 
brick-and-mortar 
version 
of 
this 
censorship, 
in 
which 
time 
periods 
that 
have 
no 
observed 
sales 
are 
either 
the 
result 
of 
a 
no-purchase 
event 
or 
simply 
the 
fact 
that 
no 
customer 
arrived 
at 
the 
store. 
In 
this 
case, 
an 
accurate 
distinction 
between 
these 
two 
cases 








is 
essential 
for 
getting 
an 
accurate 
estimate 
of 
the 
probability 
that 
a 
customer 
arrives 
in 
each 
time 
period. 
In 
theory, 
these 
EM-based 
approaches 
could 

these 
algorithms 
is 
nearly 
impossible 
due 
to 
the 
scale 
of 
our 
problem. 
In 
particular, 
these 
EM-based 
approaches 
rely 
critically 
on 
an 
ecient 
way 
to 
solve 
the 
MLE 
problem 
when 
the 
censored 
data 
is 
revealed. 
Further, 
since 
EM 
algorithms 
are 
iterative 
approaches, 
the 
resulting 
\uncensored‚Äù 
or 
full-information 
MLE 
must 
be 
solved 
repeatedly, 
which 
is 
not 
tractable 
for 
the 
scale 
of 
problem 
we 
consider. 


The 
above 
discussion 
summarizes 
the 
two 
intertwined 
big-data 
and 
censorship 
diculties 
that 
must 
be 
overcome 
in 
order 
to 
solve 
problem 
(1) 
in 
our 
setting. 
In 
what 
follows, 
we 
provide 
a 
heuristic 
approach 
for 
handling 
these 
issues, 
which 
we 
show 
performs 
quite 
well 
in 
practice. 
The 
steps 
of 
this 
approach 
unfold 
as 
follows: 


Step 
1: 
Randomly 
sample 
10% 
of 
the 
no-purchase 
events. 


Step 
2: 
Solve 
problem 
(1) 
using 
the 
randomly 
sampled 
no-purchase 
events 
in 
addition 
to 
all 
data 


points 
(St;Xt;zt), 
such 
that 
zt 
6
= 
0. 


Step 
3: 
Scale 
down 
each 
of 
the 
estimated 
Œ≤ 
values 
by 
a 
constant 
. 


In 
what 
follows, 
we 
motivate 
and 
further 
explain 
the 
implementation 
details 
regarding 
the 
three 
steps 
outlined 
above. 
In 
the 
rst 
step, 
we 
downsample 
the 
no-purchase 
events 
so 
problem 
(1) 
is 
reasonably 
tractable. 
By 
discarding 
90% 
of 
the 
no-purchase 
events, 
we 
implicitly 
assume 
that 
90% 
of 
customers 
who 
do 
not 
make 
a 
purchase 
are 
browsers, 
which 
likely 
is 
an 
overestimate 
of 
this 
percentage 
that 
we 
adjust 
for 
in 
step 
3. 
In 
step 
2, 
we 
formulate 
and 
solve 
our 
MLE 
problem. 
Even 
after 
we 
downsample 
the 
no-purchase 
events, 
the 
optimization 
problem 
at 
hand 
is 
still 
not 
amenable 
to 
commercial 
nonlinear 
solvers. 
Consequently, 
we 
solve 
problem 
(1) 
using 
TensorFlow, 
which 
uses 
a 
highly 
parallelized 
implementation 
of 
stochastic 
gradient 
ascent. 
Even 
with 
this 
sophisticated 
machinery, 
at 
least 
an 
hour 
is 
still 
required 
to 
solve 
problem 
(1). 
Finally, 
in 
step 
3 
we 
adjust 
the 
preference 
weights 
of 
each 
product 
to 
account 
for 
the 
fact 
that 
our 
MNL 
model 
is 
likely 
t 
using 
a 
likelihood 
function 
that 
has 
too 
few 
no-purchase 
events 
and 
hence 
we 
have 
overestimated 
the 
preference 
weights 
of 
each 
product. 
Through 
extensive 
out-of-sample 
testing 
in 
which 
we 
implement 
this 
choice-modeling-based 
approach 
for 
dierent 
Œ¥ 
values, 
we 
nd 
that 
setting 
Œ¥ 
= 
2000 
is 
the 
best 
scaling 
coecient.4 


4 
In 
our 
out-of-sample 
tests, 
we 
try 
Œ¥ 
2f0, 
500, 
1000, 
1500, 
2000g. 
The 
MNL-choice-model-based 
approach 
is 
substantiall
y 
better 
than 
the 
ML 
approach 
for 
all 
. 
The 
largest 
performance 
dierence 
in 
terms 
of 
average 
revenue 
per 
visit 
between 
MNL 
approaches 
with 
dierent 
values 
of 
Œ¥ 
is 
less 
than 
5% 








3.3. 
Estimation 
Case 
Study: 
Machine 
Learning 
vs. 
MNL 
In 
this 
section, 
we 
present 
a 
case 
study 
in 
which 
we 
t 
both 
MNL 
and 
machine 
learning 
models 
to 
historical 
sales 
data 
generated 
in 
April 

sellers 
on 
Tmall.com. 
Due 
to 
the 
fact 
that 
we 
only 
use 
sales 
data 
from 
ten 
sellers 
to 
t 
our 
models, 
the 
scale 
of 
the 
estimation 
problems 
we 
consider 
is 
much 
smaller 
than 
the 
one 
encountered 
within 
the 
recommender 
systems 
we 
actually 
implement 
on 
Alibaba. 
Further, 
since 
the 
exact 
nature 
of 
the 
machine 
learning 
methods 
used 
by 
Alibaba 
must 
remain 
condential, 
we 
are 
not 
able 
to 
replicate 
their 
methods 
or 
results 
exactly 
in 
this 
case 
study. 
Instead, 
we 
t 
machine 
learning 
models 
inspired 
by 
the 
current 
practice 
at 
Alibaba 
in 
the 
sense 
that 
both 
estimation 
schemes 
rely 
on 
gradient 
boosted 
decision 
trees 
to 
estimate 
the 
click 
and 
purchase 
probabilities. 
It 
is 
important 
to 
note 
that 
the 
intent 
of 
this 
case 
study 
is 
not 
to 
perfectly 
replicate 
the 
estimation 
problem 
faced 
by 
Alibaba, 
but 
instead 
to 
show 
that 
it 
is 
relatively 
straightforward 
to 
t 
machine 
learning 
models 
that 
outperform 
the 
MNL 
ts 
in 
terms 
of 
predictions 
accuracy. 
In 
this 
way, 
we 
shed 
light 
on 
the 
following 
fundamental 
issue 
that 
sits 
at 
the 
core 
of 
our 
research: 
Machine 
learning 
methods 
are 
powerful 
tools 
for 
prediction 
and 
are 
often 
more 
accurate 
than 
MNL 
models; 
however, 
when 
these 
predictions 
seed 
subsequent 
optimization 
problems 
whose 
solutions 
guide 
key 
operational 
decisions, 
it 
is 
not 
guaranteed 
that 
higher 
prediction 
accuracy 
will 
lead 
to 
more 
protable 
decisions. 


Top 
ten 
seller 
statistics. 
Alibaba 
has 
provided 
us 
with 
two 
weeks 
of 
historical 
sales 
data 
from 
the 
ten 
sellers 
on 
Tmall.com 
that 
experienced 
the 
largest 
volume 
of 
trac 
in 
April 
2018. 
We 
note 
that 
this 
two 
week 
time 
period 
does 
not 
overlap 
with 
the 
time 
horizon 
of 
our 
eld 
experiments. 
Table 
1 
provides 
an 
extensive 
summary 
of 
the 
available 
sales 
data 
for 
each 
seller. 
Further, 
for 
each 
arriving 
customer 
t 
and 
oered 
product 
t 
‚àà 
St, 
the 
feature 
vector 
Xjt 
gives 
the 
values 
of 
the 
25 
features 
with 
the 
highest 
importance 
scores 
according 
to 
the 
machine 
learning 
approaches 
that 
have 
been 
utilized 
in 
the 
past. 
Among 
these 
top 
25 
features 
are 
product-specic 
features 
such 
as 
price, 
the 
number 
of 
good 
reviews, 
the 
number 
of 
times 
the 
product 
has 
been 
clicked, 
and 
the 
image 
quality 
of 
the 
associated 
picture 
displayed 
to 
each 
customer. 
In 
addition, 
we 
use 
customer-
specic 
features 
such 
as 
the 
given 
customer's 
spending 
and 
total 
number 
of 
products 
added 
to 
the 
shopping 
cart 
both 
in 
the 
last 
week 
and 
in 
the 
last 
month. 
Beyond 
these 
rather 
straightforward 
product/customer 
features, 
we 
also 
have 
access 
to 
joint 
features 
that 
are 
specic 
to 
each 
customer 
and 
product 
pair. 
For 
example, 
one 
such 
joint 
feature 
is 
the 
collaborative 
ltering 
score 
signifying 
the 
extent 
to 
which 
the 
particular 
product 
will 
appeal 
towards 
the 
particular 
customer. 
Once 
again, 
due 
to 
condentiality 
agreements, 
we 
cannot 
disclose 
the 
complete 
list 
of 
all 
25 
features. 


Accuracy 
metrics 
and 
models 
tested. 
For 
each 
seller, 
we 
randomly 
select 
75% 
of 
its 
sales 
data 
to 
be 
used 
for 
tting 
the 
models 
and 
hold-out 
the 
remaining 
25% 
of 
the 
data 
to 
test 
the 
accuracy 
of 
these 
models. 
After 
splitting 
the 
data 
in 
this 
way, 
we 
aggregate 
all 
of 
the 
training 
data 
from 
each 








Table 
1 
Key 
seller 
statistics 


Seller 
Product 
Category 
# 
products 
# 
clicks 
# 
purchases 
# 
customers 
conversion 
% 
1 
Electronics 
169 
8,338 
2,045 
41,765 
4.88 
2 
Women's 
Apparel 
118 
17,792 
2,163 
139,853 
1.49 
3 
Men's 
Apparel 
1,047 
11,508 
1,956 
213,678 

132 
10,296 
2,979 
90,467 
3.01 
6 
Furniture 
49 
4,949 
1,937 
33,579 
5.75 
7 
Cooking 
Appliances 
38 
3,376 
2,180 
37,925 
5.75 
8 
Cooking 
Appliances 
82 
4,220 
1,448 
40,108 
3.59 
9 
Women's 
Apparel 
501 
7,267 
2,127 
63,466 
3.23 
10 
Bed 
Linens 
115 
6,975 
1,767 
39,494 
4.43 


Notes. 
This 
table 
reports 
the 
key 
statistics, 
including 
categories, 
number 
of 
products 
and 
conversion 
rates, 
for 
the 
top 
ten 
sellers 
that 
we 
use 
for 
this 
case 
study. 


seller 
into 
a 
single 
training 
set. 
This 
set-up 
most 
closely 
resembles 
the 
current 
practice 
at 
Alibaba, 
where 
the 
machine 
learning 
models 
are 
t 
to 
sales 
data 
aggregated 
across 
all 
sellers. 
Once 
the 
MNL 
and 
machine 
learning 
models 
have 
been 
trained, 
we 
measure 
the 
accuracy 
of 
each 
tted 
model 
using 
two 
metrics 
that 
are 
computed 
using 
the 
sales 
data 
exclusively 
from 
each 
seller's 
testing 
set 
restricted 
to 
customers 
who 
purchase 
exactly 
one 
item. 
In 
computing 
these 
accuracy 
metrics, 
we 
ignore 
customers 
who 
make 
multiple 
purchases, 
which 
has 
a 
negligible 
eect 
on 
our 
results 
since 
multiple 
products 
were 
purchased 
in 
approximately 
0.01% 
of 
customer 
visits. 
That 
said, 
we 
defer 
explanations 
for 
why 
no-purchase 
events 
are 
ignored 
until 
the 
two 
accuracy 
metrics 
are 
formally 
dened, 
since 
this 
understanding 
will 
help 
elucidate 
our 
choice. 
The 
series 
of 
steps 
described 
above 
‚Äì 
75/25 
train/test 
split, 
tting 
the 
models, 
computing 
the 
accuracy 
metrics 
on 
the 
test 
data 
set 
‚Äì 
make 
up 
what 
we 
refer 
to 
as 
a 
single 
trial. 
We 
eventually 
present 
the 
average 
accuracy 
metrics 
for 
each 
seller 
over 
10 
trials. 


It 
is 
important 
to 
note 
that 
one 
potential 
metric 
that 
could 
be 
used 
to 
assess 
tting 
accuracy 
is 
the 
log-likelihood 
on 
a 
hold-out 
sample 
of 
sales 
data. 
This 
metric 
is 
often 
referred 
to 
as 
the 
out-of-sample 
log-likelihood, 
and 
it 
has 
been 
a 
popular 
metric 
for 
assessing 
the 
accuracy 
of 
tted 
customer 
choice 
models 
in 
the 
revenue 
management 
literature 
(see 
Topaloglu 
and 
Simsek 
(2017), 
for 
example). 
Unfortunately, 
comparing 
the 
out-of-sample 
log-likelihoods 
for 
the 
MNL 
and 
machine-learning-based 
approaches 
would 
not 
be 
an 
apples-to-apples 
comparison 
because 
the 
machine 
learning 
estimation 
procedures 
make 
predictions 
at 
the 
customer-product 
level, 
while 
the 
MNL 
choice 
model 
makes 
predictions 
at 
the 
oer-set 
level. 
Consequently, 
we 
instead 
use 
the 
following 
two 
metrics, 
which 
assess 
how 
the 
well 
the 
tted 
models 
are 
able 
to 
predict 
the 
product 
that 
the 
arriving 
customer 
ultimately 
purchased. 


The 
rst 
metric 
is 
the 
classication 
accuracy, 
which 
is 
a 
measure 
of 
how 
frequently 
we 
predict 
correctly 
the 
item 
that 
is 
purchased. 
For 
each 
model, 
this 
metric 
is 
the 
fraction 
of 
customers 
in 
the 
hold-out 
data 
set 
for 
which 
the 
tted 
model's 
predicted 
purchase 
probability 
for 
the 
product 
that 








was 
purchased 
is 
the 
largest 
among 
all 
displayed 
options, 
excluding 
the 
no-purchase 
option. 
The 
reason 
we 
ignore 
the 
latter 
option 
in 
computing 
this 
metric 
is 
similar 
to 
why 
we 
ignore 
sales 
data 
points 
in 
the 
test 
set 
that 
correspond 
to 
customers 
who 
select 
the 
no-purchase 
option. 
Essentially, 
since 
only 
1%-6% 
of 
customers 
made 
a 
purchase 
(see 
conversion 
rates 
in 
Table 
1), 
all 
tted 
choice 
models 
overwhelmingly 
predict 
that 
each 
customer 
will 
select 
the 
no-purchase 
option. 
As 
a 
result, 
unless 
the 
no-purchase 
option 
is 
ignored, 
there 
will 
be 
little 
dierentiation 
between 
the 
classication 
accuracy 
of 
the 
tted 
models. 


The 
second 
accuracy 
metric 
we 
compute 
is 
referred 
to 
as 
the 
average 
rank. 
For 
this 
purpose, 
we 
rst 
obtain 
the 
purchase 
probabilities 
of 
each 
displayed 
option 
(again, 
excluding 
the 
no-purchase 
option) 
under 
each 
of 
the 
tted 
models. 
Then, 
for 
each 
customer 
t, 
we 
sort 
the 
displayed 
options 
in 
order 
of 
decreasing 
purchase 
probabilities 
and 
subsequently 
nd 
the 
rank 
of 
the 
purchased 
product 
in 
this 
sorted 
list. 
Our 
convention 
is 
that 
the 
product 
with 
the 
largest 
predicted 
purchase 
probability 
is 
assigned 
a 
rank 
of 
1, 
the 
product 
with 
the 
second 
largest 
is 
ranked 
2, 
so 
on 
and 
so 
forth. 
With 
these 
denitions, 
the 
average 
rank 
metric 
is 
the 
average 
rank 
of 
the 
purchased 
product 
over 
all 
customers 
in 
the 
test 
set 
who 
purchase 
exactly 
one 
product. 


Given 
that 
we 
have 
access 
to 
hundreds 
of 
thousands 
of 
historical 
data 
points, 
even 
in 
this 
simplied 
case 
study, 
it 
is 
no 
simple 
task 
to 
produce 
accurate 
estimates 
of 
the 
purchase 
probabilities 
in 
an 
ecient 
manner, 
whether 
it 
be 
by 
tting 
MNL 
or 
machine 
learning 
models. 
As 
a 
result, 
the 
descriptions 
of 
the 
two 
tted 
models 
below 
give 
references 
to 
Appendices 
that 
provide 
our 
exact 
implementation. 


1. 
The 
MNL 
choice 
model 
(MNL): 
We 
t 
this 
model 
by 
solving 
problem 
(1) 
via 
Tensor
ow 
implementation, 
which 
is 
presented 
in 
Appendix 
C.1. 
We 
nd 
that 
in 
this 
simplied 
setting 
with 
ten 
sellers, 
downsampling 
the 
no-purchase 
events 
has 
a 
negligible 
eect 
on 
the 
accuracy 
of 
the 
tted 
models. 
2. 
The 
machine 
learning 
models 
(Trees): 
We 
use 
gradient 
boosted 
classication 
trees 
to 
estimate 
the 
click 
probabilities 
P(Cjt 
= 
1) 
and 
the 
conditional 
purchase 
probabilities 
P(Zjt 
= 
1jCjt 
= 
1). 
More 
specically, 
we 
use 
Catboost 
(Prokhorenkova 
et 
al. 
2018), 
a 
novel 
gradient 
boosting 
toolkit. 
The 
full 
details 
of 
our 
implementation 
are 
given 
in 
Appendix 
C.2. 
Results 
The 
results 
for 
each 
seller 
with 
regards 
to 
the 
two 
accuracy 
metrics 
are 
presented 
in 
Table 
2. 
The 
rst 
two 
columns 
identify 
the 
seller 
number 
and 
the 
tted 
model. 
Columns 
three 
and 
ve 
specify 
the 
mean 
classication 
accuracy 
and 
average 
rank 
respectively 
over 
20 
trials. 
Columns 
four 
and 
six 
correspond 
to 
the 
percentage 
improvement 
in 
performance 
of 
the 
machine 
learning 
models 
over 
the 
standard 
MNL 
ts. 
For 
all 
ten 
sellers 
and 
both 
accuracy 
metrics, 
the 
machine 
learning 
ts 
yield 
statistically 
signicant 
(p 
=0:05) 
improvements 
over 
the 
MNL 
ts. 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 

models. 


Fitted 
Classication 
Improvement 
Avg. 
Improvement 
Seller 
# 
Model 
Accuracy 
over 
MNL 
Rank 
over 
MNL 
1 
MNL 
0.42 
-1.96 
-
1 
Trees 
0.85 
102.38% 
1.20 
63.33% 
2 
MNL 
0.53 
-2.01 
-
2 
Trees 
0.60 
13.21% 
1.64 
22.56 
3 
MNL 
0.61 
-1.83 
-
3 
Trees 
0.73 
19.67% 
1.52 
20.39% 
4 
MNL 
0.76 
-1.49 
-
4 
Trees 
0.83 
9.21% 
1.30 
14.62% 
5 
MNL 
0.60 
-1.92 
-
5 
Trees 
0.67 
11.66% 
1.65 
16.36% 
6 
MNL 
0.81 
-1.30 
-
6 
Trees 
0.86 
6.17% 
1.20 
8.33% 
7 
MNL 
0.94 
-1.08 
-
7 
Trees 
0.96 
2.13% 
1.06 
1.89% 
8 
MNL 
0.83 
-1.27 
-
8 
Trees 
0.91 
9.64% 
1.16 
9.48% 
9 
MNL 
0.59 
-1.80 
-
9 
Trees 
0.80 
35.59% 
1.32 
36.36% 
10 
MNL 
0.84 
-1.29 
-10 
Trees 
0.86 
2.38% 
1.22 
5.73% 


Notes. 
This 
table 
shows 
the 
out-of-sample 
average 
classication 
accuracy 
and 
average 
rank 
of 
Machine 
Learning 
and 
MNL 
models 
over 
each 
of 
the 
top 
ten 
sellers. 


The 
result 
in 
Table 
2 
clearly 
show 
that 
a 
simple 
out-of-the-box 
machine 
learning 
method 
with 
minimal 
parameter 
tuning 
is 
able 
to 
outperform 
the 
MNL 
model 
with 
regards 
to 
both 
accuracy 
metrics. 
Of 
course, 
these 
results 
are 
not 
an 
exact 
replica 
of 
the 
accuracy 
we 
observe 
in 
the 
experimental 
setting, 
however 
they 
serve 
as 
strong 
empirical 
support 
of 
the 
notion 
that 
machine 
learning 
models 
have 
the 
potential 
to 
outperform 
simpler 
models 
in 
terms 
of 
prediction 
accuracy. 
However, 
as 
we 
go 
on 
to 
show 
in 
our 
eld 
experiments, 
this 
improvement 
in 
tting 
accuracy 
does 
not 
guarantee 
that 
more 
protable 
assortments 
will 
be 
displayed 
to 
each 
arriving 
customer. 
In 
the 
next 
section, 
we 
detail 
the 
subsequent 
assortment 
problems 
that 
result 
after 
the 
models 
have 
been 
estimated 
and 
show 
why 
this 
might 
be 
the 
case. 


4. 
The 
Assortment 
Problem 
In 
this 
section, 
we 
consider 
the 
assortment 
problem 
that 
results 
when 
the 
estimates 
of 
the 
purchase 
probabilities 
from 
the 
tted 
MNL 
and 
machine 
learning 
models 
are 
used 
to 
seed 
the 
Alibaba 
Product 
Display 
Problem. 
In 
the 
case 
of 
the 
machine 
learning 
approach, 
a 
simple 
greedy 
algorithm 
is 
all 
that 
is 
needed 
to 
choose 
the 
optimal 
six 
product 
displays. 
In 
contrast, 
when 
the 
purchase 
probabilities 
are 
dictated 
by 
a 
tted 
MNL 
model, 
the 
resulting 
assortment 
problem 
is 
a 
cardinality-
constrained 
assortment 
problem 
under 
the 
MNL 
choice 
model. 
As 
previously 
mentioned, 
a 
handful 
of 
past 
approaches 
for 
solving 
cardinality-constrained 
assortment 
problems 
exist 
under 
the 
MNL 
choice 
model. 
Since 
the 
problem 
must 
be 
solved 
in 
an 
online 
fashion 
within 
a 
threshold 
time 
of 
50 








milliseconds, 
we 
elect 
to 
employ 
a 

(2010), 
whose 
running 
time 
we 
are 
able 
to 
improve 
by 
a 
factor 
of 
O(log 
n). 
The 
details 
of 
this 
improved 
implementation 
are 
presented 
in 
Appendix 
A. 


4.1. 
The 
Machine 
Learning 
Fits 
After 
tting 
the 
machine 
learning 
models, 
we 
are 
able 
to 
derive 
estimates 
Pjt(Xjt) 
of 
the 
purchase 
probabilities 
for 
any 
customer 
t 
and 
product 
j. 
Upon 
the 
arrival 
of 
customer 
t, 
the 
system 
will 
rst 
nd 
all 
products 
with 
non-zero 
inventory 
and 
form 
the 
set 
N 
from 
this 
collection 
of 
available 
products. 
In 
this 
setting, 
it 
turns 
out 
that 
the 
Alibaba 
Product 
Display 
Problem 
can 
be 
solved 
with 
a 
straightforward 
greedy 
algorithm 
that 
rst 
sorts 
the 
products 
in 
descending 
order 
of 
rj 
¬∑ 
Pjt(Xjt) 
and 
then 
selects 
the 
top 
six 
products 
in 
this 
ordering. 
This 
algorithm 
is 
trivially 
optimal 
because 
the 
purchase 
probabilities 
do 
not 
depend 
on 
the 
set 
of 
oered 
products. 
Consequently, 
the 
problem 
of 
choosing 
the 
optimal 
six-product 
display 
simply 
becomes 
a 
cardinality-constrained 
knapsack 
problem, 
for 
which 
it 
is 
straightforward 
to 
see 
that 
the 
aforementioned 
simple 
greedy 
algorithm 
is 
optimal. 
Since 
the 
six-product 
displays 
must 
be 
generated 
in 
an 
online 
fashion 
for 
each 
arriving 
customer, 
the 
simplicity 
of 
this 
optimal 
greedy 
approach 
is 
to 
be 
valued. 
However, 
as 
we 
go 
on 
to 
demonstrate 
in 
Section 
6, 
what 
is 
gained 
in 
eciency 
is 
lost 
when 
sub-optimal 
product 
displays 
are 
chosen 
due 
to 
the 
fact 
that 
the 
greedy 
algorithm 
chooses 
to 
display 
a 
particular 
product 
without 
considering 
how 
this 
choice 
will 
aect 
the 
appeal 
of 
the 
other 
displayed 
products. 


As 
discussed 
at 
the 
beginning 
of 
this 
section, 
another 
drawback 
of 
the 
machine 
learning 
approach 
is 
that 
the 
estimates 
of 
the 
purchase 
probabilities 
should 
essentially 
be 
treated 
as 
black-boxes, 
since 
the 
tted 
models 
do 
not 
provide 
a 
closed 
form 
relationships 
between 
the 
features 
and 
the 
predicted 
purchase 
probabilities. 
This 
is 
in 
contrast 
to 
the 
tted 
MNL 
models, 
for 
which 
we 
assume 
the 
deterministic 
component 
of 
the 
random 
utility 
Vjt 
is 
a 
linear 
function 
of 
each 
feature. 
Consequently, 
under 
the 
tted 
machine 
learning 
models, 
it 
is 
not 
possible 
to 
formulate 
an 
an 
optimal 
pricing 
problem, 
which 
could 
perhaps 
be 
one 
key 
operational 
lever 
for 
the 
Alibaba 
or 
other 
retailers 
to 
increase 
revenue. 


4.2. 
The 
MNL 
Fits 
Next, 
we 
consider 
the 
cardinality-constrained 
assortment 
optimization 
problem 
that 
results 
when 
the 
purchase 
probabilities 
Pjt 
in 
the 
Alibaba 
Product 
Display 
Problem 
are 
dictated 
by 
our 
tted 
MNL 
choice 
model. 
Again, 
we 
consider 
a 
setting 
with 
n 
products 
indexed 
by 
the 
set 
N 
= 
f1;:::;ng, 
where 
the 
revenue 
of 
product 
j 
‚àà 
N 
is 
given 
by 
rj 
. 
For 
each 
customer 
who 
arrives, 
we 
compute 
the 
customer-specic 
preference 
weights 
vjt 
= 
Xjt, 
where 
‚àó 
is 
the 
optimal 
solution 
to 
problem 
(1), 
after 
being 
scaled 
down 
by 
. 
We 
encode 
our 
assortment 
decision 
through 
the 
binary 
vector 








y 
2f0, 
1gn 
, 
where 
we 
set 
yj 
= 
1 
if 
product 
j 
is 
oered 
g 
assortment 
y 
is 
denoted 
as 


P 


j2N 
rj 
vjtyj

R(y)= 
P 
. 


1+ 
i2N 
vit 
Pn

Finally, 
we 
denote 
the 
set 
of 
feasible 
assortments 
as 
F 
= 
fy 
2f0, 
1gn 
: 
yj 
=6g. 
Note 
that

j=1 


the 
cardinality 
constraint 
must 
be 
satised 
with 
equality 
in 
our 
setting, 
since 
for 
each 
arriving 
customer 
we 
must 
always 
display 
six 
products. 
The 
cardinality-constrained 
assortment 
problem 
of 
interest 
can 
be 
stated 
as 
follows: 


ZOP 
T 
= 
max 
R(y). 
(MNL-Card) 


y2F 


The 
rst 
optimal 
polynomial-time 
algorithm 
for 
problem 
MNL-Card 
is 
due 
to 
Rusmevichientong 
et 
al. 
(2010). 
They 
provide 
a 
purely 
combinatorial 
approach 
whose 
run 
time 
is 
O(n2 
log 
n). 
In 
Appendix 
A, 
we 
give 
a 
novel 
implementation 
of 
this 
algorithm, 
which 
improves 
upon 
this 
previous 
run 
time 
by 
a 
factor 
of 
O(log 
n). 


In 
Appendix 
B, 
we 
consider 
additional 
operational 
levers 
that 
could 
be 
used 
by 
Alibaba 
to 
increase 
revenue 
in 
this 
discount 
coupon 
setting. 
In 
particular, 
we 
consider 
variations 
of 
Alibaba 
Product 
Display 
Problem 
where, 
on 
top 
of 
product 
assortments, 
Alibaba 
can 
also 
control 
the 
price 
or 
the 
icon 
size 
of 
each 
displayed 
product 
on 
the 
coupon 
sub-page. 
We 
detail 
how 
these 
two 
constrained 
versions 
of 
the 
assortment 
problem 
under 
the 
MNL 
model 
can 
either 
be 
solved 
optimally 
or 
near-optimally. 
For 
the 
problem 
that 
considers 
the 
icon 
size 
of 
each 
displayed 
product, 
we 
develop 
a 
novel 
approximation 
scheme. 


5. 
Experiment 
Design 
and 
Data 
In 
this 
section, 
we 
discuss 
the 
design 
of 
our 
eld 
experiment. 
We 
then 
provide 
summary 
statistics 
of 
the 
raw 
data 
as 
well 
as 
the 
randomization 
check 
to 
demonstrate 
that 
our 
experiment 
is 
rigorously 
conducted. 


5.1. 
Experiment 
Design 
We 
nished 
implementing 
and 
testing 
our 
MNL-choice-model-based 
approach 
by 
the 
end 
of 
February 
2018. 
Recall 
that 
the 
machine-learning-based 
approach 
is 
Alibaba's 
current 
practice 
and 
hence 
there 
was 
no 
work 
to 
be 
done 
in 
terms 
of 
implementing 
this 
benchmark 
approach. 
Our 
experiment 
ocially 
started 
on 
March 
12, 
2018. 
The 
eld 
experiment 
lasted 
for 
two 
weeks, 
but 
due 
to 
security 
reasons 
we 
can 
only 
report 
the 
results 
from 
the 
rst 
week 
(i.e., 
March 
12, 
2018, 
to 
March 
18, 
2018).5 


Throughout 
the 
experiment, 
we 
test 
the 
following 
three 
approaches. 


5 
The 
number 
of 
customers 
aggregated 
across 
two 
weeks 
has 
surpassed 
the 
allowed 
number 
of 
customers 
to 
use 
in 
a 
research 
paper 
by 
the 
company. 
This 
is 
why 
we 
focus 
on 
the 
rst 
week 
of 
the 
data. 
However, 
our 
results 
do 
not 
change 
qualitatively 
if 
we 
use 
the 
second 
week 
of 
data. 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 
 
assigned 
to 
this 
approach 
see 
six 
product 
displays 
from 
the 
MNL-choice-model-based 
approach. 
Similar 
to 
the 
case 
study 

features 
based 
on 
importance 
scores 
in 
its 
featurization 
of 
the 
MNL 
utility 
functions. 
2. 
The 
same-feature-ML-based 
approach 
(SF-ML 
approach): 
Customers 
assigned 
to 
this 
approach 
see 
six 
product 
displays 
from 
the 
machine-learning-based 
approach, 
in 
which 
the 
features 
used 
within 
the 
machine 
learning 
estimation 
algorithms 
are 
the 
same 
set 
of 
25 
top 
features. 
3. 
The 
all-feature-ML-based 
approach 
(AF-ML 
approach): 
Customers 
assigned 
to 
this 
approach 
see 
six 
product 
displays 
from 
the 
machine-learning-based 
approach 
described 
in 
Section 
4.1, 
in 
which 
hundreds 
of 
features 
are 
used 
within 
the 
machine 
learning 
estimation 
algorithms. 
Before 
our 
work, 
this 
was 
the 
current 
product 
recommendation 
system 
for 
choosing 
the 
six-product 
displays 
on 
the 
coupon 
sub-pages. 
During 
the 
experimental 
week 
beginning 
on 
March 
12, 
2018, 
each 
customer 
who 
arrives 
at 
the 
coupon 
sub-pages 
for 
any 
participating 
seller 
is 
randomly 
assigned 
one 
of 
the 
three 
approaches 
based 
on 
a 
unique 
hash 
number 
derived 
from 
the 
given 
customer's 
ID 
and 
an 
experiment 
ID.6 
Each 
customer 
is 
only 
assigned 
to 
one 
of 
the 
three 
product 
recommendation 
approaches 
described 
above 
regardless 
of 
how 
many 
times 
she 
visits 
the 
coupon 
sub-page. 


Given 
this 
experimental 
set-up, 
we 
primarily 
focus 
on 
the 
comparison 
between 
the 
MNL-based 
approach 
and 
the 
machine-learning-based 
approach 
that 
use 
the 
top 
25 
features. 
However, 
as 
noted 
in 
Section 
1, 
we 
also 
implemented 
a 
full-feature 
version 
of 
our 
MNL-based 
approach 
in 
September 
2018 
and 
compare 
it 
with 
the 
full-feature 
machine-learning-based 
approach 
in 
a 
ve-day-long 
eld 
experiment. 
The 
results 
of 
this 
eld 
experiment 
are 
presented 
in 
Appendix 
D. 
We 
remind 
the 
reader 
that 
since 
Alibaba 
did 
not 
provide 
us 
with 
visit-level 
data 
for 
the 
new 
experiment, 
we 
were 
not 
able 
to 
explore 
the 
accuracy 
of 
the 
full-feature 
MNL 
model 
nor 
can 
we 
conduct 
heterogeneous 
treatment 
analysis. 


5.2. 
Data 
and 
Randomization 
Check 
Over 
the 
week 
of 
our 
experiment, 
we 
observe 
27 
million 
customer 
arrivals 
from 
14 
million 
unique 
customers. 
From 
these 
14 
million 
unique 
customers, 
we 
randomly 
select 
5 
million 
to 
be 
randomly 
assigned 
to 
one 
of 
our 
three 
approaches. 
(The 
remaining 
9 
million 
customers 
were 
participants 
in 


6 
To 
prevent 
our 
experiments 
from 
colliding 
with 
existing 
experiments 
on 
the 
Alibaba 
platform, 
we 
use 
a 
randomization 
procedure 
with 
hashing. 
In 
particular, 
during 
the 
experimental 
week, 
each 
arrival 
customer 
ID 
is 
concatenated 
with 
a 
unique 
number 
that 
is 
representative 
of 
our 
current 
experiment. 
The 
resulting 
concatenated 
number 
is 
then 
hashed 
into 
a 
byte 
stream 
using 
the 
MD5 
message-digest 
algorithm 
(Rivest 
1992). 
The 
rst 
six 
bytes 
of 
this 
byte 
stream 
are 
extracted 
and 
then 
divided 
by 
the 
largest 
six-digit 
hex 
number 
to 
get 
a 

oating 
point. 
We 
then 
assign 
customers 
randomly 
based 
on 
this 
unique 

oating 
point 
value. 








Table 
3 
Summary 
Statistics 



Randomization 
Check 


Seller 
Monthly 
GMV 
1.7 
million 
1.7 
million 
1.7 
million 
> 
0:3 
Seller 
Number 

2013 
2013 
2013 
> 
0:4 
Customer 
Registration 
Year 
2012 
2012 
2012 
> 
0:3 
Customer 
Gender 
(Male 
=1) 
0.26 
0.26 
0.26 
> 
0:5 
Customer 
Age 
30.2 
30.2 
30.3 
> 
0:3 


Panel 
B: 
Summary 
Statistics 


Number 
of 
Page 
Views 
3, 
469, 
129 
3, 
484, 
555 
3, 
467, 
965 
Number 
of 
Products 
Clicked 
421, 
896 
368, 
987 
423, 
046 
Number 
of 
Products 
Purchased 
86, 
585 
70, 
699 
90, 
033 
GMV 
(RMB) 
18 
million 
14 
million 
17:8 
million 


Notes. 
Panel 
A 
reports 
the 
average 
monthly 
GMV, 
average 
number 
of 
products 
available 
to 
the 
seller, 
average 
seller 
registration 
year, 
and 
average 
customer 
registration 
year, 
customer 
gender 
breakdown 
and 
average 
age 
for 
all 
sellers 
and 
customers 
assigned 
to 
each 
approach 
(i.e., 
MNL, 
SF-ML 
and 
AF-ML 
approach). 
T-tests 
between 
the 
dierences 
in 
averages 
of 
the 
three 
approaches 
have 
p 
‚àí 
value 
greater 
than 
0:05 
for 
all 
pair-wise 
comparisons. 
Panel 
B 
reports 
the 
total 
number 
of 
page 
views, 
number 
of 
products 
clicked/purchased 
and 
total 
GMV 
in 
each 
approach. 


other 
parallel 
experiments.) 
In 
particular, 
1, 
879, 
903 
customers 
are 
assigned 
to 
the 
MNL 
approach, 
1, 
879, 
598 
customers 
are 
assigned 
to 
the 
SF-ML 
approach, 
and 
1, 
876, 
940 
customers 
are 
assigned 
to 
the 
AF-ML 
approach. 
These 
5 
million 
unique 
customers 
generate 
10 
million 
arrivals 
to 
coupon 
sub-pages 
during 
the 
week 
of 
our 
experiment. 
(Given 
that 
our 
experiment 
relied 
on 
the 
unique 
experiment 
ID 
in 
hashing, 
there 
were 
no 
other 
major 
experiments 
during 
this 
time 
that 
collided 
with 
our 
experiment.) 


Next, 
we 
present 
customer 
and 
seller 
information 
from 
the 
three 
experiment 
groups 
to 
conrm 
that 
the 
customers 
and 
sellers 
assigned 
to 
each 
of 
the 
three 
approaches 
are 
comparable 
in 
terms 
of 
demographics, 
spending 
habits, 
and 
revenue. 
Panel 
A 
of 
Table 
3 
shows 
the 
averages 
of 
the 
total 
GMV 
in 
the 
month 
prior 
to 
the 
starting 
date 
of 
the 
experiment; 
the 
number 
of 
active 
products 
on 
March 
12, 
2018; 
registration 
year; 
customer 
age; 
customer 
gender 
breakdown; 
and 
customer 
registration 
year 
for 
each 
of 
the 
three 
approaches. 
It 
is 
clear 
that 
customers 
and 
sellers 
assigned 
to 
each 
of 
the 
three 
approaches 
have 
statistically 
indistinguishable 
metrics: 
the 
minimum 
p-value 
over 
all 
t-tests 
is 
greater 
than 
0:2. 
The 
results 
of 
our 
randomization 
checks 
suggest 
that 
any 
dierence 
between 
customers 
under 
these 
three 
approaches 
after 
the 
experiment 
was 
implemented 
should 
be 
attributed 
to 
dierences 
in 
the 
estimation 
and 
assortment 
algorithms 
implemented 
within 
each 
approach. 


Panel 
B 
of 
Table 
3 
shows 
the 
aggregate 
impressions 
made 
by 
the 
arriving 
customers. 
More 
specically, 
this 
table 
shows 
that 
the 
customers 
in 
our 
experiment 
generated 
3, 
469, 
129, 
3, 
484, 
555, 
and 
3, 
467, 
965 
page 
views 
under 
the 
MNL, 
SF-ML, 
and 
AF-ML 
approaches 
respectively. 
This 








means 
that 
on 
average, 
each 
customer 
viewed 
approximately 
1:85 
coupon 
sub-pages 
during 
the 
week 
of 

421, 
896 
displayed 
products, 
while 
customers 
assigned 
to 
the 
SF-ML 
and 
AF-ML 
approaches 
clicked 
on 
368, 
987 

the 
MNL, 
SF-ML, 
and 
AFML 
approaches 
respectively 
purchased 
86, 
585, 
70, 
699, 
and 
90, 
033 
products, 
leading 
to 
RMB 
18, 
14, 
and 
17:8 
million 
(equivalent 
to 
USD 
2:63, 
2:05 
and 
2:60 
million) 
respectively. 
These 
preliminary 
results 
suggest 
that 
on 
average, 
customers 
assigned 
to 
the 
MNL 
approach 
generated 
more 
revenue 
compared 
to 
those 
assigned 
to 
the 
SF-ML 
and 
AF-ML 
approaches. 


6. 
Main 
Results 
In 
this 
section, 
we 
present 
the 
results 
of 
our 
eld 
experiment. 
We 
begin 
by 
detailing 
the 
nancial 
performance 
of 
the 
three 
approaches. 
The 
metric 
Alibaba 
uses 
internally 
to 
assess 
the 
protability 
of 
product 
recommendation 
systems 
is 
GMV 
(used 
synonymously 
with 
revenue 
throughout) 
per 
customer 
visit, 
and 
hence 
we 
also 
adopt 
this 
metric 
as 
our 
means 
of 
judging 
the 
ecacy 
of 
the 
three 
approaches. 
After 
presenting 
these 
results, 
we 
dig 
deeper 
into 
the 
data 
in 
an 
attempt 
to 
better 
understand 
why 
some 
approaches 
perform 
better 
than 
others. 
First, 
we 
present 
the 
accuracy 
of 
the 
purchase 
probability 
estimates 
under 
each 
approach. 
One 
would 
expect 
the 
approaches 
with 
more 
accurate 
estimation 
schemes 
to 
perform 
better, 
but 
this 
might 
not 
always 
be 
the 
case. 
We 
then 
present 
the 
average 
price 
of 
the 
products 
purchased 
under 
each 
approach; 
we 
nd 
that 
the 
MNL 
approach 
recommends 
six-product 
displays 
that 
lead 
to 
more 
sales 
of 
protable 
products. 
Lastly, 
we 
document 
how 
the 
performance 
dierences 
among 
approaches 
may 
change 
with 
respect 
to 
dierences 
in 
seller 
characteristics. 
In 
presenting 
these 
results, 
the 
unit 
of 
analysis 
is 
the 
customer 
t 
who 
visited 
the 
coupon 
sub-page 
of 
seller 
k. 


6.1. 
Financial 
Performance 
We 
begin 
by 
presenting 
the 
GMV 
per 
customer 
visit 
generated 
by 
each 
of 
the 
three 
approaches. 
We 
dene 
RevenuePerVisitkt 
to 
be 
the 
revenue 
generated 
from 
customer 
t's 
visit 
to 
the 
coupon 
sub-page 
of 
seller 
k. 
Panel 
A 
of 
Table 
4 
shows 
the 
revenue 
per 
visit 
of 
the 
MNL, 
SF-ML, 
and 
AF-ML 
approaches 
during 
our 
experimental 
period. 
The 
rst 
row 
of 
Panel 
A 
shows 
that 
the 
MNL, 
SF-ML, 
and 
AF-ML 
approaches 
generate 
RMB 
5:17, 
4:04, 
and 
5:16 
per 
customer 
visit 
respectively 
(equivalent 
to 
USD 
0:768, 
0:600, 
and 
0:767). 
The 
revenue 
per 
visit 
under 
the 
MNL 
approach 
is 
RMB 
1:13, 
or 
28% 
larger 
than 
the 
revenue 
per 
visit 
under 
the 
SF-ML 
approach. 
Both 
the 
t-test 
and 
the 
nonparametric 
Wilcoxon 
test 
show 
that 
this 
dierence 
is 
highly 
signicant 
(all 
p-values 
< 
0:0001). 


While 
the 
MNL 
approach 
signicantly 
outperforms 
the 
SF-ML 
approach, 
its 
nancial 
performance 
surprisingly 
is 
also 
on 
par 
with 
that 
of 
the 
AF-ML 
approach, 
which 
uses 
hundreds 
of 
features 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 

Panel 
A: 
Summary 
Statistics 
of 
Financial 
Performance 


MNL 
SF-ML 
MNL 
AF-ML 
RevenuePerVisit 
(RMB) 
5.17 
4.04 



Relative 
Improvement 
28.0% 
0.2% 


Observations 
3,469,129 
3,484,555 
3,469,129 
3,467,965 


Panel 
B: 
 
variable: 


Revenue 
Revenue 
(1) 
(2) 


SF-ML 
-1.126 
‚àó 
-0.987 
‚àó 
(0.094) 
(0.073) 
AF-ML 
-0.015 
0.032 
(0.110) 
(0.077) 


Customer 
Controls 
No 
Yes 


Seller 
Fixed 
Eect 
No 
Yes 


Date 
Fixed 
Eect 
No 
Yes 


Observations 
10,421,649 
10,421,649 


Notes. 
p< 
0:10; 
‚àó 
p< 
0:05; 
p< 
0:01; 
‚àó 
p< 
0:001. 
Standard 
errors 
are 
robust 
and 
clustered 
at 
the 
customer 
level. 
Panel 
A 
reports 
the 
average 
nancial 
performance, 
in 
terms 
of 
revenue 
per 
customer 
visit, 
across 
dierent 
algorithms 
during 
our 
experimental 
period 
(March 
12, 
2018 
-March 
18, 
2018). 
Panel 
B 
reports 
the 
results 
from 
OLS 
regression 
that 
estimate 
the 
dierence 
between 
dierent 
models‚Äô 
revenue 
per 
customer 
visit. 
Column 


(1) 
of 
Panel 
B 
does 
not 
control 
for 
any 
additional 
control 
variables, 
while 
Column 
(2) 
of 
Panel 
B 
controls 
for 
customer 
characteristics, 
seller 
xed 
eects 
and 
date 
xed 
eects. 
within 
its 
estimation 
scheme 
compared 
to 
the 
25 
features 
used 
within 
the 
MNL 
approach. 
Both 
the 
t-test 
and 
the 
nonparametric 
Wilcoxon 
test 
show 
that 
the 
nancial 
performance 
dierence 
with 
respect 
to 
revenue 
generated 
per 
visit 
between 
these 
two 
approaches 
is 
not 
statistically 
signicant 
(all 
p-values 
> 
0:8346). 
This 
result 
shows 
the 
potential 
of 
the 
MNL 
approach: 
using 
only 
a 
small 
fraction 
of 
the 
features 
used 
by 
the 
AF-ML 
approach, 
our 
MNL 
approach 
can 
generate 
similar 
revenue 
per 
customer 
visit. 
This 
result 
leads 
us 
to 
believe 
that 
we 
would 
observe 
a 
sizable 
improvement 
if 
we 
extended 
the 
MNL 
approach 
to 
include 
all 
features. 
Alibaba 
has 
indeed 
indicated 
to 
us 
that 
they 
would 
like 
to 
prioritize 
implementing 
the 
MNL-based 
approach 
using 
all 
of 
their 
available 
features. 


Next, 
we 
test 
the 
dierences 
in 
nancial 
performance 
with 
respect 
to 
revenue 
generated 
per 
visit 
between 
the 
three 
approaches, 
controlling 
for 
specic 
customer 
and 
seller 
characteristics 
that 
may 
aect 
customer 
spending 
behavior. 
Since 
this 
is 
a 
eld 
experiment 
with 
proper 
randomization, 
control 
variables 
are 
added 
only 
to 
make 
the 
estimators 
more 
ecient. 
Specically, 
we 
use 
the 
following 
OLS 
regression 
specication: 


RevenuePerVisitkt 
= 


10

+ 

11

Approacht 
+ 
Xt 
+ 
Xk 
+ 
Dt 
+ 
kt. 
(2) 








In 
the 
expression 
above, 
Approacht 
is 

has 
been 
assigned. 
The 
terms 
Xt 
and 
Xk 
represent 
customer-and 
seller-specic 
features, 
including 
customer 
age, 
customer 
gender, 

the 
seller's 
registration 
year, 
the 
category 
of 
products 
sold 
by 
the 
seller, 
and 
the 
number 
of 
products 
ic 
xed 
eect. 
We 
report 
the 
robust 
standard 
errors 
clustered 
at 
the 
customer 
level 
in 
this 
analysis 
as 
well 
as 
all 
subsequent 
analyses 
presented 
in 
this 
paper. 
All 
of 
our 
ndings 
continue 
to 
hold 
if 
we 
cluster 
standard 
errors 
at 
both 
the 
customer 
and 
seller 
levels. 


Panel 
B 
of 
Table 
4 
gives 
the 
results 
from 
specication 
(2). 
In 
this 
specication, 
we 
use 
data 
from 
the 
MNL 
approach 
as 
the 
baseline, 
so 
the 
coecients 
of 
SF-ML 
and 
AF-ML 
approach 
indicators 
represent 
the 
nancial 
performance 
dierence 
between 
the 
MNL 
approach 
and 
each 
of 
the 
other 
two 
approaches. 
Column 
(1) 
of 
Panel 
B 
does 
not 
control 
for 
any 
additional 
variables, 
and 
we 
successfully 
recover 
the 
mean 
dierence 
from 
Panel 
A: 
a 
customer 
visit 
under 
the 
MNL 
approach 
generates 
1:126 
and 
0:015 
more 
RMB 
per 
visit 
compared 
to 
the 
SF-ML 
and 
AF-ML 
approaches. 
The 
dierence 
between 
the 
nancial 
performance 
of 
the 
MNL 
approach 
and 
the 
SF-ML 
approach 
is 
statistically 
signicant, 
while 
the 
nancial 
performance 
of 
the 
MNL 
approach 
is 
statistically 
indierent 
from 
that 
of 
the 
AF-ML 
approach. 
Column 
(2) 
of 
Panel 
B 
controls 
for 
the 
customer 
characteristics, 
seller 
xed 
eects, 
and 
date 
xed 
eects, 
and 
qualitatively 
we 
observe 
the 
same 
set 
of 
results. 


The 
results 
described 
above 
indicate 
that 
the 
MNL 
approach 
performs 
quite 
well 
in 
relation 
to 
both 
of 
the 
machine-learning-based 
approaches. 
In 
what 
follows, 
we 
show 
that 
this 
superior 
performance 
cannot 
be 
explained 
by 
superior 
prediction 
accuracy, 
since 
similar 
to 
the 
results 
of 
our 
case 
study 
presented 
in 
Section 
3.3, 
we 
nd 
again 
that 
Alibaba's 
machine 
learning 
models 
are 
far 
more 
accurate 
than 
the 
tted 
MNL 
models. 
As 
such, 
we 
subsequently 
provide 
an 
alternative 
explanation 
for 
why 
the 
MNL-based-approach 
performs 
so 
well, 
and 
also 
explore 
where 
there 
is 
potential 
for 
improvement 
with 
regard 
to 
the 
MNL 
approach. 


6.2. 
Purchase 
Probability 
Accuracy 
In 
this 
section, 
we 
comparing 
the 
tting 
accuracies 
of 
each 
tted 
model 
using 
the 
experimental 
sales 
data 
that 
was 
generated 
from 
March 
12, 
2018, 
to 
March 
18, 
2018, 
as 
the 
hold-out 
data 
set 
from 
which 
we 
compute 
the 
two 
accuracy 
metrics 
described 
in 
Section 
3.3. 
Due 
to 
the 
complexities 
of 
implementing 
various 
approaches, 
each 
customer 
can 
only 
be 
assigned 
to 
a 
single 
approach; 
therefore, 
for 
each 
customer 
visit 
we 
only 
have 
access 
to 
one 
set 
of 
purchase 
probability 
estimates. 
Moreover, 
given 
the 
historical 
data 
of 
each 
customer 
visit, 
we 
cannot 
retrospectively 
compute 
the 
purchase 
probabilities 
from 
other 
approaches 
that 
were 
not 
used 
to 
serve 
this 
visit 
because 
the 








Table 
5 
Model 
Prediction 
Performance 


Panel 
A: 
Summary 
Statistics 
of 
Prediction 
Performance 
on 

36.31% 
77.50% 
Dierence 
(All 
p-values) 
38.24% 
(< 
0:0001) 
41.19% 
(< 
0:0001) 
AverageeRank 
2.51 
1.51 
2.51 
1.43 
Dierence 

68,395 
82,957 
86,238 


Panel 
B: 
OLS 
Regression 
Results 
on 
Model 
Prediction 
Performance 


Dependent 

0.408 
‚àó 
ÙÄÄÄ1.075 
‚àó 
(0.003) 
(0.007) 
AF-ML 
0.437 
‚àó 
ÙÄÄÄ1.152 
‚àó 
(0.003) 
(0.006) 


Buyer 
Controls 
Yes 
Yes 
Seller 
Fixed 
Eect 
Yes 
Yes 
Date 
Fixed 
Eect 
Yes 
Yes 


Observations 
237,417 
237,417 


Note: 
p< 
0:10; 
p< 
0:05; 
p< 
0:01; 
p< 
0:001. 
Standard 
errors 
are 
robust 
and 
clustered 
at 
the 
customer 
level. 
Panel 
B 
reports 
the 
average 
prediction 
power 
of 
customers‚Äô 
purchasing 
behaviors 
during 
our 
experiment. 
In 
Panel 
C, 
Columns 
(1) 
and 
(2) 
report 
the 
reports 
from 
OLS 
regression 
on 
models‚Äô 
prediction 
power 
of 
customers‚Äô 
purchasing 
behaviors. 


dynamic 
features 
of 
this 
visit 
cannot 
be 
recorded 
by 
the 
system. 
As 
a 
result, 
for 
each 
approach 
we 
only 
compute 
the 
two 
accuracy 
metrics 
using 
data 
from 
customers 
who 
were 
assigned 
to 
that 
particular 
approach, 
which 
might 
result 
in 
accuracy 
scores 
that 
are 
biased 
by 
the 
underlying 
approach 
used 
to 
choose 
the 
six-product 
displays. 
For 
example, 
instead 
of 
solving 
problem 
MNL-Card, 
imagine 
that 
the 
MNL 
approach 
always 
recommended 
the 
product 
with 
the 
largest 
preference 
weight 
along 
with 
the 
ve 
products 
with 
lowest 
preference 
weights. 
This 
approach 
is 
not 
likely 
to 
be 
protable, 
but 
it 
will 
lead 
to 
a 
high 
classication 
accuracy 
for 
the 
MNL 
approach. 


The 
discussion 
above 
explains 
why 
there 
is 
a 
dierent 
number 
of 
observations 
for 
each 
approach 
in 
Table 
5. 
Further, 
this 
discussion 
is 
included 
for 
full 
transparency. 
While 
our 
results 
clearly 
are 
not 
biased 
to 
the 
extent 
of 
the 
example 
above, 
they 
should 
nonetheless 
be 
taken 
with 
a 
grain 
of 
salt; 
the 
results 
have 
qualitative 
signicance, 
but 
the 
exact 
accuracy 
scores 
might 
not 
perfectly 
re
ect 
the 
accuracy 
of 
the 
underlying 
estimation 
scheme. 
Again, 
the 
complexities 
and 
intricacies 
that 
come 
with 
implementing 
this 
sort 
of 
full-scale 
recommendation 
system 
signicantly 
complicate 
any 
sort 
of 
ex-post 
analysis. 


Panel 
A 
of 
Table 
5 
gives 
the 
accuracy 
of 
the 
tted 
model 
for 
each 
of 
the 
three 
approaches 
based 
on 
the 
two 
metrics 
we 
consider. 
The 
top 
two 
rows 
of 
Panel 
A 
show 
that 
the 
classication 
accuracies 
are 
36:31%, 
74:55%, 
and 
77:50% 
for 
the 
MNL, 
SF-ML, 
and 
AF-ML 
approaches 
respectively. 
These 








dierences 
in 
classication 
accuracies 
are 
highly 
signicantly 
(all 
p-values 
< 
0:0001). 
The 
bottom 
two 
rows 
of 
Panel 
A 
show 
that 
the 
average 
rank 
of 

MNL, 
SF-ML, 
and 
AF-ML 
approaches 
respectively 
and 
that 
the 
pair-wise 
dierences 
between 
these 
average 
ranks 
are 
statistically 

regression 
analyses 
to 
determine 
whether 
the 
dierences 
in 
prediction 
accuracy 
can 
be 
recovered 
when 
we 
control 
for 

terms 
to 
conduct 
this 
analysis: 
TopPurchasedkt 
and 
AveragePurchaseRankkt. 
TopPurchasedkt 
is 
a 
binary 
indicator 
that 
is 
1 
if 
customer 
t 
who 
visited 
seller 
k 
purchased 
the 
product 
with 
the 
highest 
predicted 
purchase 
probability 
and 
0 
otherwise. 
AveragePurchaseRankkt 
is 
the 
average 
rank 
of 
purchased 
products 
for 
customer 
t's 
visit 
to 
seller 
k. 
We 
use 
the 
following 
OLS 
regression 
specication: 


2
0 


21

TopPurchasedkt 
= 
Œ± 


+ 
Œ± 
Approacht 
+ 
Xt 
+ 
Xk 
+ 
Dt 
+ 
kt 
(3) 


3
0 


31

AveragePurchaseRankkt 
= 
Œ± 


+ 
Œ± 
Approacht 
+ 
Xt 
+ 
Xk 
+ 
Dt 
+ 
kt 
(4) 


where 
the 
set 
of 
controls 
is 
the 
same 
as 
in 
specication 
(2). 
Our 
results 
all 
hold 
true 
if 
we 
cluster 
standard 
errors 
at 
both 
the 
customer 
and 
seller 
levels 
or 
employ 
a 
logistic 
regression 
on 
TopPurchasedit 
(a 
binary 
dependent 
variable). 


Columns 
(1) 
and 
(2) 
in 
Panel 
B 
of 
Table 
5 
present 
results 
from 
specications 
(4) 
and 
(5). 
In 
these 
specications, 
we 
use 
the 
accuracy 
performance 
under 
the 
MNL 
approach 
as 
a 
baseline, 
so 
the 
coecients 
of 
the 
SF-ML 
and 
AF-ML 
indicators 
represent 
the 
dierence 
between 
the 
MNL 
approach 
and 
each 
of 
the 
machine 
learning 
approaches. 
The 
coecients 
of 
column 
(1) 
are 
all 
positively 
signicant, 
showing 
that 
both 
machine 
learning 
approaches 
have 
higher 
prediction 
accuracy 
compared 
to 
the 
MNL 
approach. 
Notice 
that 
the 
magnitude 
of 
the 
dierence 
(for 
example, 
29:8% 
between 
the 
MNL 
approach 
and 
the 
SF-ML 
approach) 
is 
similar 
to 
that 
in 
Panel 
A 
(i.e., 
28:43%). 
This 
shows 
that 
controlling 
for 
additional 
xed 
eects 
does 
not 
change 
our 
results 
much, 
which 
provides 
further 
evidence 
that 
our 
experiments 
are 
properly 
randomized. 
Column 
(2) 
echoes 
this 
result 
by 
showing 
that 
the 
average 
rank 
of 
purchased 
products 
under 
both 
machine 
learning 
approaches 
is 
lower 
than 
the 
average 
rank 
of 
the 
purchased 
products 
under 
the 
MNL 
approach. 


As 
foreshadowed 
by 
the 
case 
study 
presented 
in 
Section 
3.3, 
we 
demonstrate 
that 
while 
the 
MNL 
approach 
performs 
much 
better 
than 
the 
SF-ML 
approach 
and 
on 
par 
with 
the 
AF-ML 
approach 
in 
terms 
of 
revenue 
per 
visit, 
it 
actually 
has 
signicantly 
worse 
prediction 
accuracy 
than 
both 
machine 
learning 
approaches 
with 
respect 
to 
both 
accuracy 
metrics. 
Consequently, 
we 
seek 
an 
explanation 
for 
the 
superior 
nancial 
performance 
of 
the 
MNL 
approach. 
In 
what 
follows, 
we 
provide 
one 
such 
explanation: 
the 
MNL 
approach 
produces 
six-product 
displays 
that 
ultimately 
lead 
to 
higher 
revenue 
products 
being 
purchased. 





Feldman, 
Zhang 
and 
Liu: 
Customer 
Choice 
Models 
versus 
Machine 

Financial 
Performance 


Panel 
A: 
Summary 
Statistics 
of 
Mechanisms 


MNL 
SF-ML 
MNL 
AF-ML 
RevenuePerVisit 
(RMB) 
5.17 
4.04 
5.17 
5.16 
Dierence 
(All 
p-values) 
1.13 

2.49% 
Dierence 
(All 
p-values) 
0.43 
(< 
0:0001) 
-0.1 
(< 
0:0001) 
Observations 
3,469,129 
3,484,555 
3,469,129 
3,467,965 


PricePerPurchase 
216.2 
206.1 
216.2 
207.3 
Dierence 
(All 
p-values) 
10.1 
(< 
0:0001) 
9.9 
(< 
0:0001) 
Observations 
82,957 
68,395 
82,957 
86,238 


Panel 
B: 
OLS 
Regression 
Results 
on 
Model 
Financial 
Performance 


Dependent 
variable: 


Revenue 
PurchaseIncidence 
PricePerPurchase 
(1) 
(2) 
(3) 


SF-ML 
ÙÄÄÄ0.987 
‚àó 
ÙÄÄÄ0.004 
‚àó 
ÙÄÄÄ6.349 
‚àó 
(0.073) 
(0.0001) 
(1.820) 
AF-ML 
0.032 
0.001 
‚àó 
ÙÄÄÄ5.895 
‚àó 
(0.077) 
(0.0001) 
(2.069) 


Buyer 
Controls 
Yes 
Yes 
Yes 
Yes 
Seller 
Fixed 
Eect 
Yes 
Yes 
Yes 
Yes 
Date 
Fixed 
Eect 
Yes 
Yes 
Yes 
Yes 


Observations 
10,410,587 
10,410,587 
237,417 


Note: 
p< 
0:10; 
p< 
0:05; 
p< 
0:01; 
p< 
0:001. 
Standard 
errors 
are 
robust 
and 
clustered 
at 
the 
customer 
level. 
Panel 
A 
reports 
the 
average 
revenue 
per 
visit, 
average 
purchasing 
probability 
and 
average 
price 
conditional 
on 
purchasing 
across 
dierent 
algorithms 
duing 
our 
experiment, 
March 
12, 
2018 
to 
March 
18, 
2018. 
Panel 
B 
report 
the 
corresponding 
results 
from 
OLS 
regressions 
controlling 
for 
customer 
characteristics, 
seller 
xed 
eects 
and 
date 
xed 
eects. 


6.3. 
Average 
Purchase 
Price 
In 
this 
section, 
we 
provide 
one 
potential 
explanation 
for 
the 
superior 
nancial 
performance 
of 
the 
MNL 
approach. 
Specically, 
we 
show 
that 
on 
average 
the 
MNL-based 
approach 
chooses 
six-product 
displays 
that 
lead 
to 
purchases 
of 
higher 
revenue 
products. 
To 
formalize 
this 
analysis, 
we 
rst 
dene 
PurchaseIncidencekt 
as 
a 
binary 
indicator 
equal 
to 
1 
if 
customer 
t's 
visit 
to 
seller 
k 
results 
in 
a 
purchase 
and 
0 
otherwise. 
We 
also 
dene 
PricePerPurchasekt 
as 
the 
average 
price 
of 
the 
purchased 
products 
during 
customer 
t's 
visit 
to 
seller 
k. 


Panel 
A 
of 
Table 
6 
shows 
the 
RevenuePerVisitkt, 
PurchaseIncidencekt, 
and 
PricePerPurchasekt 
for 
all 
three 
approaches 
during 
our 
experimental 
period. 
The 
left 
side 
of 
Panel 
A 
shows 
that 
the 
MNL 
approach 
generates 
a 
higher 
revenue 
per 
visit 
and 
has 
a 
higher 
purchasing 
incidence 
than 
the 
SF-ML 
approach. 
In 
particular, 
under 
the 
MNL 
approach 
the 
average 
purchasing 
price 
is 
4.9% 
higher 
than 
the 
average 
purchasing 
price 
under 
the 
SF-ML 
approach. 
Further, 
under 
the 
MNL 
approach 
customers 
on 
average 
make 
a 
purchase 
22% 
more 
frequently 
than 
under 
the 
SF-ML 
approach. 
Hence, 
while 
the 
SF-ML 
approach 
produces 
more 
accurate 
estimates 
of 
the 
purchase 








probabilities, 
it 
is 
not 
able 
to 

those 
oered 
by 
the 
MNL 
approach. 


The 
comparison 
between 
the 
MNL 
approach 
and 
AF-ML 
approach 
is 
shown 
on 
the 
right 
side 
of 
Panel 
A 

to 
a 
signicantly 
higher 
average 
purchasing 
price 
(i.e., 
RMB 
216:2 
versus 
RMB 
207:3, 
p-value 
< 
0:00001) 
and 
signicantly 
lower 
purchasing 
incidence 
(i.e., 
2:39% 
versus 
2:49%, 
p-value 
< 
0:00001), 
which 
ultimately 
leads 
to 
similar 
revenue 
performance 
as 
the 
two 
metrics 
balance 
each 
other. 
It 
is 
interesting 
that 
there 
is 
only 
a 
small, 
albeit 
statistically 
signicant 
(p-value 
< 
0:01) 
improvement 
in 
the 
accuracy 
of 
the 
estimated 
purchase 
probabilities 
as 
we 
move 
from 
AF-ML 
to 
SF-ML, 
but 
there 
is 
a 
large 
improvement 
in 
the 
revenue 
per 
visit 
(also 
statistically 
signicant). 
This 
either 
demonstrates 
that 
the 
ecacy 
of 
the 
machinelearning-
based 
approaches 
is 
highly 
sensitive 
to 
the 
accuracy 
of 
the 
estimated 
purchase 
probabilities 
or 
it 
shows 
that 
additional 
accuracy 
metrics 
are 
needed 
to 
better 
tease 
out 
the 
dierences 
in 
the 
estimated 
purchase 
probabilities 
under 
the 
two 
approaches. 
Panel 
B 
of 
Table 
6 
reports 
the 
regression 
results, 
controlling 
for 
customer 
characteristics, 
seller 
xed 
eects, 
and 
date 
xed 
eects 
and 
using 
specications 
similar 
to 
specication 
2. 
The 
regression 
results 
generate 
the 
same 
insights 
as 
those 
in 
Panel 
A 
of 
Table 
6. 


6.4. 
Heterogeneous 
Treatment 
Eect 
and 
Weakness 
of 
the 
MNL-Based 
Approach 
In 
this 
section, 
we 
present 
several 
exploratory 
analyses 
about 
the 
heterogeneous 
treatment 
eects 
of 
using 
the 
MNL 
approach 
versus 
the 
machine-learning-based 
approaches. 
There 
are 
two 
salient 
limitations 
of 
using 
the 
MNL 
choice 
model 
to 
capture 
customer 
purchasing 
patterns 
in 
this 
setting. 
First, 
the 
MNL 
choice 
model 
assumes 
that 
each 
customer 
only 
buys 
a 
single 
product, 
while 
in 
practice 
customers 
often 
make 
multiple 
purchases. 
Second, 
the 
MNL 
choice 
model 
in 
its 
standard 
form 
cannot 
incorporate 
customer 
click 
behavior 
within 
how 
it 
models 
customer 
preferences. 
Based 
on 
these 
theoretical 
limitations, 
we 
identify 
two 
seller 
characteristics 
that 
may 
in
uence 
the 
performance 
of 
the 
MNL 
approach. 
We 
rst 
dene 
MultiPurchaseCountk 
to 
be 
the 
number 
of 
visits 
to 
seller 
k 
in 
which 
the 
customer 
makes 
multiple 
purchases. 
Second, 
we 
dene 
Click-to-Purchasek 
as 
the 
ratio 
of 
the 
number 
of 
clicked 
products 
to 
the 
number 
of 
purchased 
products 
across 
all 
visits 
to 
seller 
k. 


We 
rely 
on 
the 
following 
OLS 
regression 
specications 
to 
test 
the 
interaction 
between 
the 
algorithm 
indicator 
and 
the 
aforementioned 
list 
of 
moderating 
factors: 


RevenuePerVisitkt 
=

40

+ 

41

Approacht 
+ 


42

Moderating 
Factork+ 
(5) 




43

Approacht 
√ó 
ModeratingFactork 
+ 
Xt 
+ 
Xk 
+ 
Dt 
+ 
kt 








Table 
7 
Heterogeneous 
Treatment 
Eect 


Dependent 
variable: 


Revenue 
(1) 
(2) 

chaseCount 
0.030 
‚àó 
(0.012) 
SF-ML 
Click-to-Purchase 
0.067 
‚àó 
(0.035) 


Customer 
Controls 
Yes 
Yes 
Seller 
Fixed 
Eects 
No 
No 
Date 
Fixed 
Eects 
Yes 
Yes 
Observations 
5,326,664 

p< 
0:001. 
Standard 
errors 
are 
robust 
and 
clustered 
at 
the 
customer 
level. 
This 
table 
reports 
the 
results 
based 
on 
Equation 
5. 


where 
ModeratingFactori 
2fClick-to-Purchasek, 
MultiPurchaseCountkg. 
We 
only 
focus 
on 
the 
observations 
corresponding 
to 
customers 
who 
were 
assigned 
to 
the 
MNL 
and 
SF-ML 
approaches 
and 
from 
sellers 
who 
had 
at 
least 
100 
visits, 
who 
make 
up 
more 
than 
76% 
of 
the 
sellers.7 


Table 
7 
reports 
the 
results 
of 
our 
heterogeneous 
treatment 
analyses. 
Column 
(1) 
of 
Table 
7 
shows 
that 
the 
coecient 
of 
the 
interaction 
of 
the 
SF-ML 
indicator 
and 
MultiPurchaseCount 
is 
positive, 
demonstrating 
that 
the 
dierence 
in 
nancial 
performance 
of 
our 
MNL 
approach 
and 
the 
SF-ML 
approach 
shrinks 
when 
there 
are 
more 
multiple-purchase 
incidences. 
In 
other 
words, 
our 
MNL 
approach 
performs 
worse 
for 
sellers 
whose 
customers 
are 
more 
likely 
to 
purchase 
multiple 
items 
from 
an 
oer 
set. 
Column 
(2) 
also 
demonstrates 
a 
positive 
interaction 
term 
between 
the 
SFML 
indicator 
and 
Click-to-Purchase. 
Similarly, 
this 
demonstrates 
that 
the 
MNL-based 
approach 
performs 
worse 
when 
the 
ratio 
of 
clicks 
to 
purchases 
is 
high. 
Columns 
(1) 
and 
(2) 
collectively 
show 
that 
the 
theoretical 
limitations 
we 
identied 
with 
the 
MNL 
approach 
indeed 
aect 
its 
performance: 
a 
one 
unit 
increase 
in 
MultiPurchaseCountk 
(Click-to-purchasek) 
would 
translate 
to 
0:03 
(0:067) 
decrease 
in 
the 
nancial 
performance 
dierences 
between 
the 
MNL 
and 
SF-ML 
approaches. 
This 
is 
a 
sign 
that 
we 
need 
future 
work 
to 
build 
choice 
models 
that 
are 
able 
to 
model 
click 
behavior 
as 
well 
as 
customers 
buying 
multiple 
items 
from 
a 
set 
of 
oered 
products. 


7. 
Discussion 
and 
Conclusion 
In 
this 
paper, 
to 
the 
best 
of 
our 
knowledge, 
we 
document 
the 
rst 
full-scale 
implementation 
of 
a 
customer-choice-model-based 
product 
recommendation 
system. 
We 
nd 
that 
our 
MNL-based 
approach 
generates 
28% 
higher 
revenue 
per 
customer 
visit 
compared 
to 
the 
machine-learning-based 


7 
Our 
results 
are 
qualitatively 
robust 
if 
we 
use 
other 
numbers 
of 
visit 
cutos, 
such 
as 
500 
or 
1000. 








approach 
that 
uses 
the 
same 
set 
of 
features. 
Moreover, 
we 
nd 
that 
our 
MNL-based 
approach 
performs 
slightly 
better 
than 
the 
current 
full-feature 
machine-learning-based 
approach, 
which 
Alibaba 
has 
been 
improving 
for 
more 
than 
two 
years. 
We 
then 
show 
that 
our 
MNL 
approach 
performs 
well 
because 
it 
recommends 
more 
protable 
items 
by 
incorporating 
substitution 
behavior 
within 
the 
operational 
problem 
that 
guides 
product 
display 
decisions. 
However, 
while 
the 
machine 
learning 
approach 
can 
leverage 
big 
data 
to 
produce 
accurate 
estimates 
of 
the 
purchase 
probabilities, 
it 
sometimes 
fails 
to 
identify 
protable 
sets 
of 
products 
to 
display 
because 
it 
does 
not 
factor 
in 
the 
substitution 
behavior 
between 
products 
oered 
together. 
We 
are 
hopeful 
that 
our 
work 
inspires 
other 
companies 
to 
consider 
a 
choice-model-based 
approach 
within 
their 
product 
recommendation 
system 
and 
that 
it 
also 
encourages 
other 
researchers 
in 
operations 
management 
to 
seek 
out 
avenues 
to 
implement 
their 
algorithms 
in 
practice. 


In 
order 
to 
further 
improve 
choice-model-based 
recommendation 
systems, 
we 
again 
highlight 
several 
main 
diculties 
in 
implementing 
our 
MNL-based 
approach, 
and 
in 
doing 
so 
we 
also 
shed 
light 
on 
several 
potential 
research 
directions. 
The 
rst 
diculty 
we 
faced 
was 
properly 
dealing 
with 
the 
inherent 
censorship 
issue 
present 
in 
the 
sales 
data 
in 
the 
process 
of 
estimating 
our 
MNL 
model. 
Previous 
techniques 
used 
to 
uncensor 
the 
data 
are 
rendered 
ineective 
for 
the 
scale 
of 
problems 
common 
in 
industry, 
which 
calls 
for 
future 
research 
to 
deal 
with 
censorship 
in 
a 
big 
data 
setting. 
Second, 
we 
show 
that 
our 
MNL-based 
approach 
does 
not 
perform 
well 
when 
customers 
purchase 
multiple 
items 
from 
the 
oer 
set. 
Unfortunately, 
to 
the 
best 
of 
our 
knowledge 
there 
is 
no 
choice 
model 
that 
is 
able 
to 
capture 
multiple 
purchase 
events 
from 
a 
single 
customer 
visit. 
Developing 
such 
a 
model 
represents 
a 
natural 
next 
step 
to 
expand 
the 
breadth 
of 
retailing 
scenarios 
that 
can 
be 
captured 
using 
choice 
models. 
Lastly, 
in 
Section 
6.4, 
we 
demonstrate 
that 
the 
click 
data 
potentially 
provides 
useful 
signals 
of 
customer 
preference. 
Since 
our 
MNL-based 
approach 
ignores 
click 
behavior 
in 
both 
the 
estimation 
and 
assortment 
phases, 
one 
interesting 
direction 
may 
consider 
incorporating 
click 
behavior 
within 
the 
MNL 
choice 
model 
framework. 